	.text
	.file	"LLVMDialectModule"
	.section	.rodata,"a",@progbits
	.p2align	6, 0x0                          # -- Begin function matmul_kernel
.LCPI0_0:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	11                              # 0xb
	.long	12                              # 0xc
	.long	13                              # 0xd
	.long	14                              # 0xe
	.long	15                              # 0xf
.LCPI0_1:
	.long	16                              # 0x10
	.long	17                              # 0x11
	.long	18                              # 0x12
	.long	19                              # 0x13
	.long	20                              # 0x14
	.long	21                              # 0x15
	.long	22                              # 0x16
	.long	23                              # 0x17
	.long	24                              # 0x18
	.long	25                              # 0x19
	.long	26                              # 0x1a
	.long	27                              # 0x1b
	.long	28                              # 0x1c
	.long	29                              # 0x1d
	.long	30                              # 0x1e
	.long	31                              # 0x1f
.LCPI0_10:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	25                              # 0x19
	.zero	4
	.zero	4
	.zero	4
	.zero	4
	.zero	4
	.zero	4
	.zero	4
.LCPI0_18:
	.quad	0                               # 0x0
	.quad	1                               # 0x1
	.quad	2                               # 0x2
	.quad	3                               # 0x3
	.quad	13                              # 0xd
	.zero	8
	.zero	8
	.zero	8
.LCPI0_19:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	26                              # 0x1a
	.zero	4
	.zero	4
	.zero	4
	.zero	4
	.zero	4
	.zero	4
.LCPI0_27:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	27                              # 0x1b
	.zero	4
	.zero	4
	.zero	4
	.zero	4
	.zero	4
	.zero	4
	.zero	4
.LCPI0_28:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	27                              # 0x1b
	.zero	4
	.zero	4
	.zero	4
	.zero	4
	.zero	4
	.zero	4
.LCPI0_29:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	27                              # 0x1b
	.zero	4
	.zero	4
	.zero	4
	.zero	4
	.zero	4
.LCPI0_37:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	28                              # 0x1c
	.zero	4
	.zero	4
	.zero	4
	.zero	4
	.zero	4
	.zero	4
.LCPI0_38:
	.quad	0                               # 0x0
	.quad	1                               # 0x1
	.quad	2                               # 0x2
	.quad	3                               # 0x3
	.quad	4                               # 0x4
	.quad	14                              # 0xe
	.zero	8
	.zero	8
.LCPI0_39:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	28                              # 0x1c
	.zero	4
	.zero	4
	.zero	4
	.zero	4
.LCPI0_47:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	29                              # 0x1d
	.zero	4
	.zero	4
	.zero	4
	.zero	4
	.zero	4
	.zero	4
	.zero	4
.LCPI0_48:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	29                              # 0x1d
	.zero	4
	.zero	4
	.zero	4
	.zero	4
	.zero	4
	.zero	4
.LCPI0_49:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	29                              # 0x1d
	.zero	4
	.zero	4
	.zero	4
	.zero	4
	.zero	4
.LCPI0_50:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	29                              # 0x1d
	.zero	4
	.zero	4
	.zero	4
	.zero	4
.LCPI0_51:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	11                              # 0xb
	.long	29                              # 0x1d
	.zero	4
	.zero	4
	.zero	4
.LCPI0_59:
	.quad	0                               # 0x0
	.quad	1                               # 0x1
	.quad	2                               # 0x2
	.quad	3                               # 0x3
	.quad	15                              # 0xf
	.zero	8
	.zero	8
	.zero	8
.LCPI0_60:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	30                              # 0x1e
	.zero	4
	.zero	4
	.zero	4
	.zero	4
	.zero	4
	.zero	4
.LCPI0_61:
	.quad	0                               # 0x0
	.quad	1                               # 0x1
	.quad	2                               # 0x2
	.quad	3                               # 0x3
	.quad	4                               # 0x4
	.quad	15                              # 0xf
	.zero	8
	.zero	8
.LCPI0_62:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	30                              # 0x1e
	.zero	4
	.zero	4
	.zero	4
	.zero	4
.LCPI0_63:
	.quad	0                               # 0x0
	.quad	1                               # 0x1
	.quad	2                               # 0x2
	.quad	3                               # 0x3
	.quad	4                               # 0x4
	.quad	5                               # 0x5
	.quad	15                              # 0xf
	.zero	8
.LCPI0_64:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	11                              # 0xb
	.long	12                              # 0xc
	.long	30                              # 0x1e
	.zero	4
	.zero	4
.LCPI0_72:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	31                              # 0x1f
	.zero	4
	.zero	4
	.zero	4
	.zero	4
	.zero	4
	.zero	4
	.zero	4
.LCPI0_73:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	31                              # 0x1f
	.zero	4
	.zero	4
	.zero	4
	.zero	4
	.zero	4
	.zero	4
.LCPI0_74:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	31                              # 0x1f
	.zero	4
	.zero	4
	.zero	4
	.zero	4
	.zero	4
.LCPI0_75:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	31                              # 0x1f
	.zero	4
	.zero	4
	.zero	4
	.zero	4
.LCPI0_76:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	11                              # 0xb
	.long	31                              # 0x1f
	.zero	4
	.zero	4
	.zero	4
.LCPI0_77:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	11                              # 0xb
	.long	12                              # 0xc
	.long	31                              # 0x1f
	.zero	4
	.zero	4
.LCPI0_78:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	11                              # 0xb
	.long	12                              # 0xc
	.long	13                              # 0xd
	.long	31                              # 0x1f
	.zero	4
.LCPI0_90:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	17                              # 0x11
	.zero	4
	.zero	4
	.zero	4
	.zero	4
	.zero	4
	.zero	4
	.zero	4
.LCPI0_91:
	.quad	0                               # 0x0
	.quad	1                               # 0x1
	.quad	2                               # 0x2
	.quad	3                               # 0x3
	.quad	9                               # 0x9
	.zero	8
	.zero	8
	.zero	8
.LCPI0_92:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	19                              # 0x13
	.zero	4
	.zero	4
	.zero	4
	.zero	4
	.zero	4
	.zero	4
	.zero	4
.LCPI0_93:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	21                              # 0x15
	.zero	4
	.zero	4
	.zero	4
	.zero	4
	.zero	4
	.zero	4
	.zero	4
.LCPI0_94:
	.quad	0                               # 0x0
	.quad	1                               # 0x1
	.quad	2                               # 0x2
	.quad	3                               # 0x3
	.quad	11                              # 0xb
	.zero	8
	.zero	8
	.zero	8
.LCPI0_95:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	23                              # 0x17
	.zero	4
	.zero	4
	.zero	4
	.zero	4
	.zero	4
	.zero	4
	.zero	4
.LCPI0_96:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	16                              # 0x10
	.zero	4
	.zero	4
	.zero	4
	.zero	4
	.zero	4
	.zero	4
.LCPI0_97:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	17                              # 0x11
	.zero	4
	.zero	4
	.zero	4
	.zero	4
	.zero	4
	.zero	4
.LCPI0_98:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	18                              # 0x12
	.zero	4
	.zero	4
	.zero	4
	.zero	4
	.zero	4
	.zero	4
.LCPI0_99:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	19                              # 0x13
	.zero	4
	.zero	4
	.zero	4
	.zero	4
	.zero	4
	.zero	4
.LCPI0_100:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	20                              # 0x14
	.zero	4
	.zero	4
	.zero	4
	.zero	4
	.zero	4
	.zero	4
.LCPI0_101:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	21                              # 0x15
	.zero	4
	.zero	4
	.zero	4
	.zero	4
	.zero	4
	.zero	4
.LCPI0_102:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	22                              # 0x16
	.zero	4
	.zero	4
	.zero	4
	.zero	4
	.zero	4
	.zero	4
.LCPI0_103:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	23                              # 0x17
	.zero	4
	.zero	4
	.zero	4
	.zero	4
	.zero	4
	.zero	4
.LCPI0_104:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	24                              # 0x18
	.zero	4
	.zero	4
	.zero	4
	.zero	4
	.zero	4
	.zero	4
.LCPI0_105:
	.quad	0                               # 0x0
	.quad	1                               # 0x1
	.quad	2                               # 0x2
	.quad	3                               # 0x3
	.quad	4                               # 0x4
	.quad	8                               # 0x8
	.zero	8
	.zero	8
.LCPI0_106:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	17                              # 0x11
	.zero	4
	.zero	4
	.zero	4
	.zero	4
	.zero	4
.LCPI0_107:
	.quad	0                               # 0x0
	.quad	1                               # 0x1
	.quad	2                               # 0x2
	.quad	3                               # 0x3
	.quad	4                               # 0x4
	.quad	9                               # 0x9
	.zero	8
	.zero	8
.LCPI0_108:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	19                              # 0x13
	.zero	4
	.zero	4
	.zero	4
	.zero	4
	.zero	4
.LCPI0_109:
	.quad	0                               # 0x0
	.quad	1                               # 0x1
	.quad	2                               # 0x2
	.quad	3                               # 0x3
	.quad	4                               # 0x4
	.quad	10                              # 0xa
	.zero	8
	.zero	8
.LCPI0_110:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	21                              # 0x15
	.zero	4
	.zero	4
	.zero	4
	.zero	4
	.zero	4
.LCPI0_111:
	.quad	0                               # 0x0
	.quad	1                               # 0x1
	.quad	2                               # 0x2
	.quad	3                               # 0x3
	.quad	4                               # 0x4
	.quad	11                              # 0xb
	.zero	8
	.zero	8
.LCPI0_112:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	23                              # 0x17
	.zero	4
	.zero	4
	.zero	4
	.zero	4
	.zero	4
.LCPI0_113:
	.quad	0                               # 0x0
	.quad	1                               # 0x1
	.quad	2                               # 0x2
	.quad	3                               # 0x3
	.quad	4                               # 0x4
	.quad	12                              # 0xc
	.zero	8
	.zero	8
.LCPI0_114:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	25                              # 0x19
	.zero	4
	.zero	4
	.zero	4
	.zero	4
	.zero	4
.LCPI0_115:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	16                              # 0x10
	.zero	4
	.zero	4
	.zero	4
	.zero	4
.LCPI0_116:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	17                              # 0x11
	.zero	4
	.zero	4
	.zero	4
	.zero	4
.LCPI0_117:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	18                              # 0x12
	.zero	4
	.zero	4
	.zero	4
	.zero	4
.LCPI0_118:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	19                              # 0x13
	.zero	4
	.zero	4
	.zero	4
	.zero	4
.LCPI0_119:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	20                              # 0x14
	.zero	4
	.zero	4
	.zero	4
	.zero	4
.LCPI0_120:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	21                              # 0x15
	.zero	4
	.zero	4
	.zero	4
	.zero	4
.LCPI0_121:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	22                              # 0x16
	.zero	4
	.zero	4
	.zero	4
	.zero	4
.LCPI0_122:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	23                              # 0x17
	.zero	4
	.zero	4
	.zero	4
	.zero	4
.LCPI0_123:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	24                              # 0x18
	.zero	4
	.zero	4
	.zero	4
	.zero	4
.LCPI0_124:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	25                              # 0x19
	.zero	4
	.zero	4
	.zero	4
	.zero	4
.LCPI0_125:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	26                              # 0x1a
	.zero	4
	.zero	4
	.zero	4
	.zero	4
.LCPI0_126:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	11                              # 0xb
	.long	17                              # 0x11
	.zero	4
	.zero	4
	.zero	4
.LCPI0_127:
	.quad	0                               # 0x0
	.quad	1                               # 0x1
	.quad	2                               # 0x2
	.quad	3                               # 0x3
	.quad	4                               # 0x4
	.quad	5                               # 0x5
	.quad	9                               # 0x9
	.zero	8
.LCPI0_128:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	11                              # 0xb
	.long	19                              # 0x13
	.zero	4
	.zero	4
	.zero	4
.LCPI0_129:
	.quad	0                               # 0x0
	.quad	1                               # 0x1
	.quad	2                               # 0x2
	.quad	3                               # 0x3
	.quad	4                               # 0x4
	.quad	5                               # 0x5
	.quad	10                              # 0xa
	.zero	8
.LCPI0_130:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	11                              # 0xb
	.long	21                              # 0x15
	.zero	4
	.zero	4
	.zero	4
.LCPI0_131:
	.quad	0                               # 0x0
	.quad	1                               # 0x1
	.quad	2                               # 0x2
	.quad	3                               # 0x3
	.quad	4                               # 0x4
	.quad	5                               # 0x5
	.quad	11                              # 0xb
	.zero	8
.LCPI0_132:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	11                              # 0xb
	.long	23                              # 0x17
	.zero	4
	.zero	4
	.zero	4
.LCPI0_133:
	.quad	0                               # 0x0
	.quad	1                               # 0x1
	.quad	2                               # 0x2
	.quad	3                               # 0x3
	.quad	4                               # 0x4
	.quad	5                               # 0x5
	.quad	12                              # 0xc
	.zero	8
.LCPI0_134:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	11                              # 0xb
	.long	25                              # 0x19
	.zero	4
	.zero	4
	.zero	4
.LCPI0_135:
	.quad	0                               # 0x0
	.quad	1                               # 0x1
	.quad	2                               # 0x2
	.quad	3                               # 0x3
	.quad	4                               # 0x4
	.quad	5                               # 0x5
	.quad	13                              # 0xd
	.zero	8
.LCPI0_136:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	11                              # 0xb
	.long	27                              # 0x1b
	.zero	4
	.zero	4
	.zero	4
.LCPI0_137:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	11                              # 0xb
	.long	12                              # 0xc
	.long	16                              # 0x10
	.zero	4
	.zero	4
.LCPI0_138:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	11                              # 0xb
	.long	12                              # 0xc
	.long	17                              # 0x11
	.zero	4
	.zero	4
.LCPI0_139:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	11                              # 0xb
	.long	12                              # 0xc
	.long	18                              # 0x12
	.zero	4
	.zero	4
.LCPI0_140:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	11                              # 0xb
	.long	12                              # 0xc
	.long	19                              # 0x13
	.zero	4
	.zero	4
.LCPI0_141:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	11                              # 0xb
	.long	12                              # 0xc
	.long	20                              # 0x14
	.zero	4
	.zero	4
.LCPI0_142:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	11                              # 0xb
	.long	12                              # 0xc
	.long	21                              # 0x15
	.zero	4
	.zero	4
.LCPI0_143:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	11                              # 0xb
	.long	12                              # 0xc
	.long	22                              # 0x16
	.zero	4
	.zero	4
.LCPI0_144:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	11                              # 0xb
	.long	12                              # 0xc
	.long	23                              # 0x17
	.zero	4
	.zero	4
.LCPI0_145:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	11                              # 0xb
	.long	12                              # 0xc
	.long	24                              # 0x18
	.zero	4
	.zero	4
.LCPI0_146:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	11                              # 0xb
	.long	12                              # 0xc
	.long	25                              # 0x19
	.zero	4
	.zero	4
.LCPI0_147:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	11                              # 0xb
	.long	12                              # 0xc
	.long	26                              # 0x1a
	.zero	4
	.zero	4
.LCPI0_148:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	11                              # 0xb
	.long	12                              # 0xc
	.long	27                              # 0x1b
	.zero	4
	.zero	4
.LCPI0_149:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	11                              # 0xb
	.long	12                              # 0xc
	.long	28                              # 0x1c
	.zero	4
	.zero	4
.LCPI0_150:
	.quad	0                               # 0x0
	.quad	1                               # 0x1
	.quad	2                               # 0x2
	.quad	3                               # 0x3
	.quad	4                               # 0x4
	.quad	5                               # 0x5
	.quad	6                               # 0x6
	.quad	8                               # 0x8
.LCPI0_151:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	11                              # 0xb
	.long	12                              # 0xc
	.long	13                              # 0xd
	.long	17                              # 0x11
	.zero	4
.LCPI0_152:
	.quad	0                               # 0x0
	.quad	1                               # 0x1
	.quad	2                               # 0x2
	.quad	3                               # 0x3
	.quad	4                               # 0x4
	.quad	5                               # 0x5
	.quad	6                               # 0x6
	.quad	9                               # 0x9
.LCPI0_153:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	11                              # 0xb
	.long	12                              # 0xc
	.long	13                              # 0xd
	.long	19                              # 0x13
	.zero	4
.LCPI0_154:
	.quad	0                               # 0x0
	.quad	1                               # 0x1
	.quad	2                               # 0x2
	.quad	3                               # 0x3
	.quad	4                               # 0x4
	.quad	5                               # 0x5
	.quad	6                               # 0x6
	.quad	10                              # 0xa
.LCPI0_155:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	11                              # 0xb
	.long	12                              # 0xc
	.long	13                              # 0xd
	.long	21                              # 0x15
	.zero	4
.LCPI0_156:
	.quad	0                               # 0x0
	.quad	1                               # 0x1
	.quad	2                               # 0x2
	.quad	3                               # 0x3
	.quad	4                               # 0x4
	.quad	5                               # 0x5
	.quad	6                               # 0x6
	.quad	11                              # 0xb
.LCPI0_157:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	11                              # 0xb
	.long	12                              # 0xc
	.long	13                              # 0xd
	.long	23                              # 0x17
	.zero	4
.LCPI0_158:
	.quad	0                               # 0x0
	.quad	1                               # 0x1
	.quad	2                               # 0x2
	.quad	3                               # 0x3
	.quad	4                               # 0x4
	.quad	5                               # 0x5
	.quad	6                               # 0x6
	.quad	12                              # 0xc
.LCPI0_159:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	11                              # 0xb
	.long	12                              # 0xc
	.long	13                              # 0xd
	.long	25                              # 0x19
	.zero	4
.LCPI0_160:
	.quad	0                               # 0x0
	.quad	1                               # 0x1
	.quad	2                               # 0x2
	.quad	3                               # 0x3
	.quad	4                               # 0x4
	.quad	5                               # 0x5
	.quad	6                               # 0x6
	.quad	13                              # 0xd
.LCPI0_161:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	11                              # 0xb
	.long	12                              # 0xc
	.long	13                              # 0xd
	.long	27                              # 0x1b
	.zero	4
.LCPI0_162:
	.quad	0                               # 0x0
	.quad	1                               # 0x1
	.quad	2                               # 0x2
	.quad	3                               # 0x3
	.quad	4                               # 0x4
	.quad	5                               # 0x5
	.quad	6                               # 0x6
	.quad	14                              # 0xe
.LCPI0_163:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	11                              # 0xb
	.long	12                              # 0xc
	.long	13                              # 0xd
	.long	29                              # 0x1d
	.zero	4
.LCPI0_164:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	11                              # 0xb
	.long	12                              # 0xc
	.long	13                              # 0xd
	.long	14                              # 0xe
	.long	16                              # 0x10
.LCPI0_165:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	11                              # 0xb
	.long	12                              # 0xc
	.long	13                              # 0xd
	.long	14                              # 0xe
	.long	17                              # 0x11
.LCPI0_166:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	11                              # 0xb
	.long	12                              # 0xc
	.long	13                              # 0xd
	.long	14                              # 0xe
	.long	18                              # 0x12
.LCPI0_167:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	11                              # 0xb
	.long	12                              # 0xc
	.long	13                              # 0xd
	.long	14                              # 0xe
	.long	19                              # 0x13
.LCPI0_168:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	11                              # 0xb
	.long	12                              # 0xc
	.long	13                              # 0xd
	.long	14                              # 0xe
	.long	20                              # 0x14
.LCPI0_169:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	11                              # 0xb
	.long	12                              # 0xc
	.long	13                              # 0xd
	.long	14                              # 0xe
	.long	21                              # 0x15
.LCPI0_170:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	11                              # 0xb
	.long	12                              # 0xc
	.long	13                              # 0xd
	.long	14                              # 0xe
	.long	22                              # 0x16
.LCPI0_171:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	11                              # 0xb
	.long	12                              # 0xc
	.long	13                              # 0xd
	.long	14                              # 0xe
	.long	23                              # 0x17
.LCPI0_172:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	11                              # 0xb
	.long	12                              # 0xc
	.long	13                              # 0xd
	.long	14                              # 0xe
	.long	24                              # 0x18
.LCPI0_173:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	11                              # 0xb
	.long	12                              # 0xc
	.long	13                              # 0xd
	.long	14                              # 0xe
	.long	25                              # 0x19
.LCPI0_174:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	11                              # 0xb
	.long	12                              # 0xc
	.long	13                              # 0xd
	.long	14                              # 0xe
	.long	26                              # 0x1a
.LCPI0_175:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	11                              # 0xb
	.long	12                              # 0xc
	.long	13                              # 0xd
	.long	14                              # 0xe
	.long	27                              # 0x1b
.LCPI0_176:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	11                              # 0xb
	.long	12                              # 0xc
	.long	13                              # 0xd
	.long	14                              # 0xe
	.long	28                              # 0x1c
.LCPI0_177:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	11                              # 0xb
	.long	12                              # 0xc
	.long	13                              # 0xd
	.long	14                              # 0xe
	.long	29                              # 0x1d
.LCPI0_178:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	11                              # 0xb
	.long	12                              # 0xc
	.long	13                              # 0xd
	.long	14                              # 0xe
	.long	30                              # 0x1e
	.section	.rodata.cst32,"aM",@progbits,32
	.p2align	5, 0x0
.LCPI0_2:
	.long	0                               # 0x0
	.long	4                               # 0x4
	.long	8                               # 0x8
	.long	12                              # 0xc
	.long	16                              # 0x10
	.long	20                              # 0x14
	.long	24                              # 0x18
	.long	28                              # 0x1c
.LCPI0_6:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	25                              # 0x19
	.zero	4
	.zero	4
	.zero	4
.LCPI0_7:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	25                              # 0x19
	.zero	4
	.zero	4
.LCPI0_8:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	25                              # 0x19
	.zero	4
.LCPI0_9:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	25                              # 0x19
.LCPI0_14:
	.quad	0                               # 0x0
	.quad	1                               # 0x1
	.quad	13                              # 0xd
	.zero	8
.LCPI0_15:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	26                              # 0x1a
	.zero	4
	.zero	4
.LCPI0_16:
	.quad	0                               # 0x0
	.quad	1                               # 0x1
	.quad	2                               # 0x2
	.quad	13                              # 0xd
.LCPI0_17:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	26                              # 0x1a
.LCPI0_23:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	27                              # 0x1b
	.zero	4
	.zero	4
	.zero	4
.LCPI0_24:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	27                              # 0x1b
	.zero	4
	.zero	4
.LCPI0_25:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	27                              # 0x1b
	.zero	4
.LCPI0_26:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	27                              # 0x1b
.LCPI0_33:
	.quad	0                               # 0x0
	.quad	1                               # 0x1
	.quad	14                              # 0xe
	.zero	8
.LCPI0_34:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	28                              # 0x1c
	.zero	4
	.zero	4
.LCPI0_35:
	.quad	0                               # 0x0
	.quad	1                               # 0x1
	.quad	2                               # 0x2
	.quad	14                              # 0xe
.LCPI0_36:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	28                              # 0x1c
.LCPI0_43:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	29                              # 0x1d
	.zero	4
	.zero	4
	.zero	4
.LCPI0_44:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	29                              # 0x1d
	.zero	4
	.zero	4
.LCPI0_45:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	29                              # 0x1d
	.zero	4
.LCPI0_46:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	29                              # 0x1d
.LCPI0_55:
	.quad	0                               # 0x0
	.quad	1                               # 0x1
	.quad	15                              # 0xf
	.zero	8
.LCPI0_56:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	30                              # 0x1e
	.zero	4
	.zero	4
.LCPI0_57:
	.quad	0                               # 0x0
	.quad	1                               # 0x1
	.quad	2                               # 0x2
	.quad	15                              # 0xf
.LCPI0_58:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	30                              # 0x1e
.LCPI0_68:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	31                              # 0x1f
	.zero	4
	.zero	4
	.zero	4
.LCPI0_69:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	31                              # 0x1f
	.zero	4
	.zero	4
.LCPI0_70:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	31                              # 0x1f
	.zero	4
.LCPI0_71:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	31                              # 0x1f
.LCPI0_82:
	.quad	0                               # 0x0
	.quad	1                               # 0x1
	.quad	12                              # 0xc
	.zero	8
.LCPI0_83:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	24                              # 0x18
	.zero	4
	.zero	4
.LCPI0_84:
	.quad	0                               # 0x0
	.quad	1                               # 0x1
	.quad	2                               # 0x2
	.quad	12                              # 0xc
.LCPI0_85:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.zero	4
	.zero	4
	.long	4                               # 0x4
	.long	8                               # 0x8
	.zero	4
	.zero	4
.LCPI0_86:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	8                               # 0x8
.LCPI0_89:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	24                              # 0x18
	.section	.rodata.cst16,"aM",@progbits,16
	.p2align	4, 0x0
.LCPI0_3:
	.long	0                               # 0x0
	.long	25                              # 0x19
	.zero	4
	.zero	4
.LCPI0_4:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	25                              # 0x19
	.zero	4
.LCPI0_5:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	25                              # 0x19
.LCPI0_11:
	.long	0                               # 0x0
	.long	26                              # 0x1a
	.zero	4
	.zero	4
.LCPI0_12:
	.byte	0                               # 0x0
	.byte	0                               # 0x0
	.byte	0                               # 0x0
	.byte	0                               # 0x0
	.byte	0                               # 0x0
	.byte	0                               # 0x0
	.byte	0                               # 0x0
	.byte	0                               # 0x0
	.byte	13                              # 0xd
	.byte	0                               # 0x0
	.byte	0                               # 0x0
	.byte	0                               # 0x0
	.byte	0                               # 0x0
	.byte	0                               # 0x0
	.byte	0                               # 0x0
	.byte	0                               # 0x0
.LCPI0_13:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	26                              # 0x1a
.LCPI0_20:
	.long	0                               # 0x0
	.long	27                              # 0x1b
	.zero	4
	.zero	4
.LCPI0_21:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	27                              # 0x1b
	.zero	4
.LCPI0_22:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	27                              # 0x1b
.LCPI0_30:
	.long	0                               # 0x0
	.long	28                              # 0x1c
	.zero	4
	.zero	4
.LCPI0_31:
	.byte	0                               # 0x0
	.byte	0                               # 0x0
	.byte	0                               # 0x0
	.byte	0                               # 0x0
	.byte	0                               # 0x0
	.byte	0                               # 0x0
	.byte	0                               # 0x0
	.byte	0                               # 0x0
	.byte	14                              # 0xe
	.byte	0                               # 0x0
	.byte	0                               # 0x0
	.byte	0                               # 0x0
	.byte	0                               # 0x0
	.byte	0                               # 0x0
	.byte	0                               # 0x0
	.byte	0                               # 0x0
.LCPI0_32:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	28                              # 0x1c
.LCPI0_40:
	.long	0                               # 0x0
	.long	29                              # 0x1d
	.zero	4
	.zero	4
.LCPI0_41:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	29                              # 0x1d
	.zero	4
.LCPI0_42:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	29                              # 0x1d
.LCPI0_52:
	.long	0                               # 0x0
	.long	30                              # 0x1e
	.zero	4
	.zero	4
.LCPI0_53:
	.byte	0                               # 0x0
	.byte	0                               # 0x0
	.byte	0                               # 0x0
	.byte	0                               # 0x0
	.byte	0                               # 0x0
	.byte	0                               # 0x0
	.byte	0                               # 0x0
	.byte	0                               # 0x0
	.byte	15                              # 0xf
	.byte	0                               # 0x0
	.byte	0                               # 0x0
	.byte	0                               # 0x0
	.byte	0                               # 0x0
	.byte	0                               # 0x0
	.byte	0                               # 0x0
	.byte	0                               # 0x0
.LCPI0_54:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	30                              # 0x1e
.LCPI0_65:
	.long	0                               # 0x0
	.long	31                              # 0x1f
	.zero	4
	.zero	4
.LCPI0_66:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	31                              # 0x1f
	.zero	4
.LCPI0_67:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	31                              # 0x1f
.LCPI0_79:
	.long	0                               # 0x0
	.long	24                              # 0x18
	.zero	4
	.zero	4
.LCPI0_80:
	.byte	0                               # 0x0
	.byte	0                               # 0x0
	.byte	0                               # 0x0
	.byte	0                               # 0x0
	.byte	0                               # 0x0
	.byte	0                               # 0x0
	.byte	0                               # 0x0
	.byte	0                               # 0x0
	.byte	12                              # 0xc
	.byte	0                               # 0x0
	.byte	0                               # 0x0
	.byte	0                               # 0x0
	.byte	0                               # 0x0
	.byte	0                               # 0x0
	.byte	0                               # 0x0
	.byte	0                               # 0x0
.LCPI0_81:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	24                              # 0x18
.LCPI0_87:
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	5                               # 0x5
	.long	7                               # 0x7
.LCPI0_88:
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	7                               # 0x7
	.long	7                               # 0x7
.LCPI0_179:
	.byte	16                              # 0x10
	.byte	17                              # 0x11
	.byte	18                              # 0x12
	.byte	19                              # 0x13
	.byte	20                              # 0x14
	.byte	21                              # 0x15
	.byte	22                              # 0x16
	.byte	23                              # 0x17
	.byte	24                              # 0x18
	.byte	25                              # 0x19
	.byte	26                              # 0x1a
	.byte	27                              # 0x1b
	.byte	28                              # 0x1c
	.byte	29                              # 0x1d
	.byte	30                              # 0x1e
	.byte	31                              # 0x1f
.LCPI0_180:
	.byte	0                               # 0x0
	.byte	1                               # 0x1
	.byte	2                               # 0x2
	.byte	3                               # 0x3
	.byte	4                               # 0x4
	.byte	5                               # 0x5
	.byte	6                               # 0x6
	.byte	7                               # 0x7
	.byte	8                               # 0x8
	.byte	9                               # 0x9
	.byte	10                              # 0xa
	.byte	11                              # 0xb
	.byte	12                              # 0xc
	.byte	13                              # 0xd
	.byte	14                              # 0xe
	.byte	15                              # 0xf
.LCPI0_182:
	.long	0                               # 0x0
	.long	25                              # 0x19
	.long	0                               # 0x0
	.long	0                               # 0x0
.LCPI0_183:
	.long	0                               # 0x0
	.long	26                              # 0x1a
	.long	0                               # 0x0
	.long	0                               # 0x0
.LCPI0_184:
	.long	0                               # 0x0
	.long	27                              # 0x1b
	.long	0                               # 0x0
	.long	0                               # 0x0
.LCPI0_185:
	.long	0                               # 0x0
	.long	28                              # 0x1c
	.long	0                               # 0x0
	.long	0                               # 0x0
.LCPI0_186:
	.long	0                               # 0x0
	.long	29                              # 0x1d
	.long	0                               # 0x0
	.long	0                               # 0x0
.LCPI0_187:
	.long	0                               # 0x0
	.long	30                              # 0x1e
	.long	0                               # 0x0
	.long	0                               # 0x0
.LCPI0_188:
	.long	0                               # 0x0
	.long	31                              # 0x1f
	.long	0                               # 0x0
	.long	0                               # 0x0
.LCPI0_189:
	.long	0                               # 0x0
	.long	24                              # 0x18
	.long	0                               # 0x0
	.long	0                               # 0x0
	.section	.rodata.cst4,"aM",@progbits,4
.LCPI0_181:
	.byte	0                               # 0x0
	.byte	4                               # 0x4
	.byte	8                               # 0x8
	.byte	12                              # 0xc
	.text
	.globl	matmul_kernel
	.p2align	4
	.type	matmul_kernel,@function
matmul_kernel:                          # @matmul_kernel
.Lfunc_begin0:
	.file	1 "/root/triton/triton-cpu/./python/tutorials" "03-matrix-multiplication-cpu.py"
	.loc	1 165 0                         # 03-matrix-multiplication-cpu.py:165:0
	.cfi_sections .debug_frame
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset %rbp, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	andq	$-64, %rsp
	subq	$56704, %rsp                    # imm = 0xDD80
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
                                        # kill: def $r8d killed $r8d def $r8
                                        # kill: def $ecx killed $ecx def $rcx
	movq	%rdx, 5656(%rsp)                # 8-byte Spill
	movl	40(%rbp), %r10d
.Ltmp0:
	.file	2 "/root/triton/triton-cpu/python/triton/language" "standard.py"
	.loc	2 40 22 prologue_end            # standard.py:40:22
	leal	31(%rcx), %eax
	.loc	2 40 28 is_stmt 0               # standard.py:40:28
	leal	62(%rcx), %ebx
	testl	%eax, %eax
	cmovnsl	%eax, %ebx
	sarl	$5, %ebx
.Ltmp1:
	.loc	2 40 22                         # standard.py:40:22
	leal	31(%r8), %eax
	.loc	2 40 28                         # standard.py:40:28
	leal	62(%r8), %r14d
	testl	%eax, %eax
	cmovnsl	%eax, %r14d
	sarl	$5, %r14d
.Ltmp2:
	.loc	1 190 38 is_stmt 1              # 03-matrix-multiplication-cpu.py:190:38
	shll	$3, %r14d
	.loc	1 191 22                        # 03-matrix-multiplication-cpu.py:191:22
	movl	%r10d, %eax
	cltd
	idivl	%r14d
	movl	%eax, %r11d
	.loc	1 192 29                        # 03-matrix-multiplication-cpu.py:192:29
	leal	(,%r11,8), %eax
	.loc	1 193 35                        # 03-matrix-multiplication-cpu.py:193:35
	subl	%eax, %ebx
	.loc	1 193 48 is_stmt 0              # 03-matrix-multiplication-cpu.py:193:48
	cmpl	$8, %ebx
	movl	$8, %r15d
	cmovll	%ebx, %r15d
	.loc	1 194 33 is_stmt 1              # 03-matrix-multiplication-cpu.py:194:33
	movl	%r10d, %eax
	cltd
	idivl	%r15d
	movl	%edx, %ebx
	.loc	1 195 19                        # 03-matrix-multiplication-cpu.py:195:19
	imull	%r11d, %r14d
	subl	%r14d, %r10d
	.loc	1 195 40 is_stmt 0              # 03-matrix-multiplication-cpu.py:195:40
	movl	%r10d, %eax
	cltd
	idivl	%r15d
                                        # kill: def $r9d killed $r9d def $r9
	.loc	1 194 27 is_stmt 1              # 03-matrix-multiplication-cpu.py:194:27
	leal	(%rbx,%r11,8), %r15d
	.loc	1 204 23                        # 03-matrix-multiplication-cpu.py:204:23
	shll	$5, %r15d
	.loc	1 205 23                        # 03-matrix-multiplication-cpu.py:205:23
	shll	$5, %eax
.Ltmp3:
	.loc	2 40 22                         # standard.py:40:22
	leal	31(%r9), %edx
	.loc	2 40 28 is_stmt 0               # standard.py:40:28
	addl	$62, %r9d
	testl	%edx, %edx
	cmovnsl	%edx, %r9d
.Ltmp4:
	.loc	1 216 22 is_stmt 1              # 03-matrix-multiplication-cpu.py:216:22
	cmpl	$32, %edx
	movl	%r15d, 56(%rsp)                 # 4-byte Spill
	movl	%eax, 440(%rsp)                 # 4-byte Spill
	jl	.LBB0_1
# %bb.2:                                # %.lr.ph
	.loc	1 205 38                        # 03-matrix-multiplication-cpu.py:205:38
	vpmovsxbd	.LCPI0_179(%rip), %zmm3 # zmm3 = [16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31]
	vpbroadcastd	%eax, %zmm1
	vpord	%zmm3, %zmm1, %zmm2
	.loc	1 205 68 is_stmt 0              # 03-matrix-multiplication-cpu.py:205:68
	vextracti32x4	$3, %zmm2, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%r8d
	movl	%edx, 448(%rsp)                 # 4-byte Spill
	vmovd	%xmm0, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r14d
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r11d
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r13d
	vextracti32x4	$2, %zmm2, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%r8d
	movq	%rdi, 1088(%rsp)                # 8-byte Spill
	movl	%edx, %edi
	vmovd	%xmm0, %eax
	cltd
	idivl	%r8d
	movq	%r9, 424(%rsp)                  # 8-byte Spill
	movq	%rsi, 256(%rsp)                 # 8-byte Spill
	movl	%edx, %r10d
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%r8d
	movl	%edx, 512(%rsp)                 # 4-byte Spill
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%r8d
	movl	%edx, 704(%rsp)                 # 4-byte Spill
	vextracti128	$1, %ymm2, %xmm0
	vpextrd	$1, %xmm0, %eax
	cltd
	idivl	%r8d
	movl	%edx, %ebx
	vmovd	%xmm0, %eax
	cltd
	idivl	%r8d
	movl	%edx, %esi
	vpextrd	$2, %xmm0, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r12d
	vpextrd	$3, %xmm0, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r15d
	vmovd	%r14d, %xmm0
	vpinsrd	$1, 448(%rsp), %xmm0, %xmm0     # 4-byte Folded Reload
	vpextrd	$1, %xmm2, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r9d
	vpinsrd	$2, %r11d, %xmm0, %xmm0
	vpinsrd	$3, %r13d, %xmm0, %xmm0
	vmovd	%xmm2, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r11d
	vmovd	%r10d, %xmm4
	vpinsrd	$1, %edi, %xmm4, %xmm5
	vpextrd	$2, %xmm2, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r10d
	.loc	1 205 38                        # 03-matrix-multiplication-cpu.py:205:38
	vpmovsxbd	.LCPI0_180(%rip), %zmm4 # zmm4 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]
	.loc	1 205 68                        # 03-matrix-multiplication-cpu.py:205:68
	vpextrd	$3, %xmm2, %eax
	cltd
	idivl	%r8d
	movl	%edx, %edi
	.loc	1 205 38                        # 03-matrix-multiplication-cpu.py:205:38
	vpord	%zmm4, %zmm1, %zmm1
	.loc	1 205 68                        # 03-matrix-multiplication-cpu.py:205:68
	vextracti32x4	$3, %zmm1, %xmm6
	vpextrd	$1, %xmm6, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r14d
	vpinsrd	$2, 512(%rsp), %xmm5, %xmm2     # 4-byte Folded Reload
	vpinsrd	$3, 704(%rsp), %xmm2, %xmm2     # 4-byte Folded Reload
	vmovd	%xmm6, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r13d
	vmovd	%esi, %xmm5
	vpinsrd	$1, %ebx, %xmm5, %xmm5
	vpextrd	$2, %xmm6, %eax
	cltd
	idivl	%r8d
	movl	%edx, %esi
	vpinsrd	$2, %r12d, %xmm5, %xmm5
	vpextrd	$3, %xmm6, %eax
	cltd
	idivl	%r8d
	movl	%edx, %ebx
	vpinsrd	$3, %r15d, %xmm5, %xmm5
	vextracti32x4	$2, %zmm1, %xmm6
	vpextrd	$1, %xmm6, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r15d
	vmovd	%r11d, %xmm7
	vpinsrd	$1, %r9d, %xmm7, %xmm7
	vmovd	%xmm6, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r11d
	vpinsrd	$2, %r10d, %xmm7, %xmm7
	vpextrd	$2, %xmm6, %eax
	cltd
	movl	24(%rbp), %r12d
	idivl	%r8d
	movl	%edx, %r9d
	vpinsrd	$3, %edi, %xmm7, %xmm7
	vmovd	%r13d, %xmm8
	vpextrd	$3, %xmm6, %eax
	cltd
	idivl	%r8d
	movl	%edx, %edi
	vpinsrd	$1, %r14d, %xmm8, %xmm6
	vextracti128	$1, %ymm1, %xmm8
	vpextrd	$1, %xmm8, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r10d
	vpinsrd	$2, %esi, %xmm6, %xmm6
	vpinsrd	$3, %ebx, %xmm6, %xmm6
	vmovd	%xmm8, %eax
	cltd
	idivl	%r8d
	movl	%edx, %esi
	vmovd	%r11d, %xmm9
	vpinsrd	$1, %r15d, %xmm9, %xmm9
	vpextrd	$2, %xmm8, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r11d
	vinserti128	$1, %xmm0, %ymm2, %ymm0
	vpinsrd	$2, %r9d, %xmm9, %xmm2
	vpextrd	$3, %xmm8, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r9d
	vinserti128	$1, %xmm5, %ymm7, %ymm5
	vpinsrd	$3, %edi, %xmm2, %xmm2
	vpextrd	$1, %xmm1, %eax
	cltd
	idivl	%r8d
	movl	%edx, %edi
	vinserti64x4	$1, %ymm0, %zmm5, %zmm0
	vmovd	%esi, %xmm5
	vpinsrd	$1, %r10d, %xmm5, %xmm5
	vmovd	%xmm1, %eax
	cltd
	idivl	%r8d
	movl	%edx, %esi
	vinserti128	$1, %xmm6, %ymm2, %ymm2
	vpinsrd	$2, %r11d, %xmm5, %xmm5
	vpextrd	$2, %xmm1, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r10d
	vpinsrd	$3, %r9d, %xmm5, %xmm5
	vmovd	%esi, %xmm6
	vpextrd	$3, %xmm1, %eax
	cltd
	idivl	%r8d
	vpinsrd	$1, %edi, %xmm6, %xmm1
	vpinsrd	$2, %r10d, %xmm1, %xmm1
	vpinsrd	$3, %edx, %xmm1, %xmm1
	vinserti128	$1, %xmm5, %ymm1, %ymm1
	vinserti64x4	$1, %ymm2, %zmm1, %zmm2
	.loc	1 208 52 is_stmt 1              # 03-matrix-multiplication-cpu.py:208:52
	movl	%r12d, %eax
	shll	$5, %eax
	movl	%eax, %edx
	subl	%r12d, %edx
	movq	%r12, %rdi
	vpbroadcastd	%edx, %zmm1
	vpaddd	%zmm1, %zmm2, %zmm5
	vpaddd	%zmm1, %zmm0, %zmm1
	.loc	1 208 22 is_stmt 0              # 03-matrix-multiplication-cpu.py:208:22
	vpslld	$2, %zmm1, %zmm1
	vpslld	$2, %zmm5, %zmm5
	vpmovsxdq	%ymm5, %zmm6
	vextracti64x4	$1, %zmm5, %ymm5
	vpmovsxdq	%ymm5, %zmm5
	vpmovsxdq	%ymm1, %zmm7
	vextracti64x4	$1, %zmm1, %ymm1
	vpmovsxdq	%ymm1, %zmm8
	movq	256(%rsp), %rdx                 # 8-byte Reload
	vpbroadcastq	%rdx, %zmm1
	vpaddq	%zmm8, %zmm1, %zmm16
	vpaddq	%zmm7, %zmm1, %zmm7
	vmovdqa64	%zmm7, 44224(%rsp)      # 64-byte Spill
	vpaddq	%zmm5, %zmm1, %zmm5
	vmovdqa64	%zmm5, 44160(%rsp)      # 64-byte Spill
	vpaddq	%zmm6, %zmm1, %zmm5
	vmovdqa64	%zmm5, 44096(%rsp)      # 64-byte Spill
	.loc	1 208 52                        # 03-matrix-multiplication-cpu.py:208:52
	leal	(%r12,%r12), %r15d
	subl	%r15d, %eax
	vpbroadcastd	%eax, %zmm5
	vpaddd	%zmm5, %zmm2, %zmm6
	vpaddd	%zmm5, %zmm0, %zmm5
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	vpslld	$2, %zmm5, %zmm5
	vpslld	$2, %zmm6, %zmm6
	vpmovsxdq	%ymm6, %zmm7
	vextracti64x4	$1, %zmm6, %ymm6
	vpmovsxdq	%ymm6, %zmm6
	vpmovsxdq	%ymm5, %zmm8
	vextracti64x4	$1, %zmm5, %ymm5
	vpmovsxdq	%ymm5, %zmm5
	vpaddq	%zmm5, %zmm1, %zmm5
	vmovdqa64	%zmm5, 44032(%rsp)      # 64-byte Spill
	vpaddq	%zmm8, %zmm1, %zmm5
	vmovdqa64	%zmm5, 43968(%rsp)      # 64-byte Spill
	vpaddq	%zmm6, %zmm1, %zmm5
	vmovdqa64	%zmm5, 43904(%rsp)      # 64-byte Spill
	vpaddq	%zmm7, %zmm1, %zmm5
	vmovdqa64	%zmm5, 43840(%rsp)      # 64-byte Spill
	.loc	1 208 52                        # 03-matrix-multiplication-cpu.py:208:52
	leal	(%r12,%r12,8), %r8d
	leal	(%r8,%r8,2), %eax
	leal	(%rax,%r12), %edx
	leal	(%r12,%rdx), %esi
	vpbroadcastd	%esi, %zmm5
	vpaddd	%zmm5, %zmm2, %zmm6
	vpaddd	%zmm5, %zmm0, %zmm5
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	vpslld	$2, %zmm5, %zmm5
	vpslld	$2, %zmm6, %zmm6
	vpmovsxdq	%ymm6, %zmm7
	vextracti64x4	$1, %zmm6, %ymm6
	vpmovsxdq	%ymm6, %zmm6
	vpmovsxdq	%ymm5, %zmm8
	vextracti64x4	$1, %zmm5, %ymm5
	vpmovsxdq	%ymm5, %zmm5
	vpaddq	%zmm5, %zmm1, %zmm5
	vmovdqa64	%zmm5, 43776(%rsp)      # 64-byte Spill
	vpaddq	%zmm8, %zmm1, %zmm5
	vmovdqa64	%zmm5, 43712(%rsp)      # 64-byte Spill
	vpaddq	%zmm6, %zmm1, %zmm5
	vmovdqa64	%zmm5, 43648(%rsp)      # 64-byte Spill
	vpaddq	%zmm7, %zmm1, %zmm5
	vmovdqa64	%zmm5, 43584(%rsp)      # 64-byte Spill
	.loc	1 208 52                        # 03-matrix-multiplication-cpu.py:208:52
	vpbroadcastd	%edx, %zmm5
	vpaddd	%zmm5, %zmm2, %zmm6
	vpaddd	%zmm5, %zmm0, %zmm5
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	vpslld	$2, %zmm5, %zmm5
	vpslld	$2, %zmm6, %zmm6
	vpmovsxdq	%ymm6, %zmm7
	vextracti64x4	$1, %zmm6, %ymm6
	vpmovsxdq	%ymm6, %zmm6
	vpmovsxdq	%ymm5, %zmm8
	vextracti64x4	$1, %zmm5, %ymm5
	vpmovsxdq	%ymm5, %zmm5
	vpaddq	%zmm5, %zmm1, %zmm5
	vmovdqa64	%zmm5, 43520(%rsp)      # 64-byte Spill
	vpaddq	%zmm8, %zmm1, %zmm5
	vmovdqa64	%zmm5, 43456(%rsp)      # 64-byte Spill
	vpaddq	%zmm6, %zmm1, %zmm5
	vmovdqa64	%zmm5, 43392(%rsp)      # 64-byte Spill
	vpaddq	%zmm7, %zmm1, %zmm5
	vmovdqa64	%zmm5, 43328(%rsp)      # 64-byte Spill
	.loc	1 208 52                        # 03-matrix-multiplication-cpu.py:208:52
	vpbroadcastd	%eax, %zmm5
	vpaddd	%zmm5, %zmm2, %zmm6
	vpaddd	%zmm5, %zmm0, %zmm5
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	vpslld	$2, %zmm5, %zmm5
	vpslld	$2, %zmm6, %zmm6
	vpmovsxdq	%ymm6, %zmm7
	vextracti64x4	$1, %zmm6, %ymm6
	vpmovsxdq	%ymm6, %zmm6
	vpmovsxdq	%ymm5, %zmm8
	vextracti64x4	$1, %zmm5, %ymm5
	vpmovsxdq	%ymm5, %zmm5
	vpaddq	%zmm5, %zmm1, %zmm5
	vmovdqa64	%zmm5, 43264(%rsp)      # 64-byte Spill
	vpaddq	%zmm8, %zmm1, %zmm5
	vmovdqa64	%zmm5, 43200(%rsp)      # 64-byte Spill
	vpaddq	%zmm6, %zmm1, %zmm5
	vmovdqa64	%zmm5, 43136(%rsp)      # 64-byte Spill
	vpaddq	%zmm7, %zmm1, %zmm5
	vmovdqa64	%zmm5, 43072(%rsp)      # 64-byte Spill
	.loc	1 208 52                        # 03-matrix-multiplication-cpu.py:208:52
	leal	(%r12,%r12,4), %r13d
	leal	(%r13,%r13,4), %eax
	leal	(%rax,%r12), %edx
	vpbroadcastd	%edx, %zmm5
	vpaddd	%zmm5, %zmm2, %zmm6
	vpaddd	%zmm5, %zmm0, %zmm5
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	vpslld	$2, %zmm5, %zmm5
	vpslld	$2, %zmm6, %zmm6
	vpmovsxdq	%ymm6, %zmm7
	vextracti64x4	$1, %zmm6, %ymm6
	vpmovsxdq	%ymm6, %zmm6
	vpmovsxdq	%ymm5, %zmm8
	vextracti64x4	$1, %zmm5, %ymm5
	vpmovsxdq	%ymm5, %zmm5
	vpaddq	%zmm5, %zmm1, %zmm5
	vmovdqa64	%zmm5, 43008(%rsp)      # 64-byte Spill
	vpaddq	%zmm8, %zmm1, %zmm5
	vmovdqa64	%zmm5, 42944(%rsp)      # 64-byte Spill
	vpaddq	%zmm6, %zmm1, %zmm5
	vmovdqa64	%zmm5, 38016(%rsp)      # 64-byte Spill
	vpaddq	%zmm7, %zmm1, %zmm5
	vmovdqa64	%zmm5, 37952(%rsp)      # 64-byte Spill
	.loc	1 208 52                        # 03-matrix-multiplication-cpu.py:208:52
	vpbroadcastd	%eax, %zmm6
	leal	(,%r12,8), %r12d
	leal	(%r12,%r12,2), %eax
	vpbroadcastd	%eax, %zmm5
	movl	56(%rsp), %eax                  # 4-byte Reload
	.loc	1 204 38 is_stmt 1              # 03-matrix-multiplication-cpu.py:204:38
	vpbroadcastd	%eax, %zmm7
	vpord	%zmm3, %zmm7, %zmm3
	vpord	%zmm4, %zmm7, %zmm7
	.loc	1 204 68 is_stmt 0              # 03-matrix-multiplication-cpu.py:204:68
	vextracti128	$1, %ymm7, %xmm4
	vpextrd	$1, %xmm4, 832(%rsp)            # 4-byte Folded Spill
	vmovd	%xmm4, 1472(%rsp)               # 4-byte Folded Spill
	vpextrd	$2, %xmm4, 1728(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm4, 192(%rsp)            # 4-byte Folded Spill
	vextracti32x4	$2, %zmm7, %xmm4
	vpextrd	$1, %xmm4, 1216(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm4, 1280(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm4, 1984(%rsp)           # 4-byte Folded Spill
	vmovd	%xmm4, 64(%rsp)                 # 4-byte Folded Spill
	.loc	1 208 52 is_stmt 1              # 03-matrix-multiplication-cpu.py:208:52
	vpaddd	%zmm6, %zmm2, %zmm8
	vpaddd	%zmm6, %zmm0, %zmm4
	.loc	1 208 22 is_stmt 0              # 03-matrix-multiplication-cpu.py:208:22
	vpslld	$2, %zmm4, %zmm4
	vpslld	$2, %zmm8, %zmm6
	.loc	1 204 68 is_stmt 1              # 03-matrix-multiplication-cpu.py:204:68
	vmovd	%xmm7, %eax
	vpextrd	$1, %xmm7, %ebx
	vpextrd	$2, %xmm7, %r11d
	vpextrd	$3, %xmm7, %r14d
	vextracti32x4	$3, %zmm7, %xmm7
	vmovd	%xmm7, 2048(%rsp)               # 4-byte Folded Spill
	vpextrd	$1, %xmm7, 2112(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm7, 128(%rsp)            # 4-byte Folded Spill
	vpextrd	$3, %xmm7, 896(%rsp)            # 4-byte Folded Spill
	vextracti128	$1, %ymm3, %xmm7
	vmovd	%xmm7, 2432(%rsp)               # 4-byte Folded Spill
	vpextrd	$1, %xmm7, 2496(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm7, 2560(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm7, 768(%rsp)            # 4-byte Folded Spill
	vextracti32x4	$2, %zmm3, %xmm7
	vmovd	%xmm7, 1344(%rsp)               # 4-byte Folded Spill
	vpextrd	$1, %xmm7, 2176(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm7, 2240(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm7, 1792(%rsp)           # 4-byte Folded Spill
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	vpmovsxdq	%ymm6, %zmm7
	vextracti64x4	$1, %zmm6, %ymm6
	vpmovsxdq	%ymm6, %zmm6
	.loc	1 204 68                        # 03-matrix-multiplication-cpu.py:204:68
	vmovd	%xmm3, 3072(%rsp)               # 4-byte Folded Spill
	vpextrd	$1, %xmm3, 3136(%rsp)           # 4-byte Folded Spill
	vpextrd	$2, %xmm3, 2368(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm3, 640(%rsp)            # 4-byte Folded Spill
	vextracti32x4	$3, %zmm3, %xmm3
	vmovd	%xmm3, 1408(%rsp)               # 4-byte Folded Spill
	vpextrd	$1, %xmm3, 960(%rsp)            # 4-byte Folded Spill
	vpextrd	$2, %xmm3, 1024(%rsp)           # 4-byte Folded Spill
	vpextrd	$3, %xmm3, 1856(%rsp)           # 4-byte Folded Spill
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	vpmovsxdq	%ymm4, %zmm3
	vextracti64x4	$1, %zmm4, %ymm4
	vpmovsxdq	%ymm4, %zmm4
	vpaddq	%zmm4, %zmm1, %zmm4
	vmovdqa64	%zmm4, 42880(%rsp)      # 64-byte Spill
	vpaddq	%zmm3, %zmm1, %zmm3
	vmovdqa64	%zmm3, 37888(%rsp)      # 64-byte Spill
	vpaddq	%zmm6, %zmm1, %zmm3
	vmovdqa64	%zmm3, 37824(%rsp)      # 64-byte Spill
	vpaddq	%zmm7, %zmm1, %zmm9
	.loc	1 208 52 is_stmt 0              # 03-matrix-multiplication-cpu.py:208:52
	vpaddd	%zmm5, %zmm2, %zmm3
	vpaddd	%zmm5, %zmm0, %zmm4
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	vpslld	$2, %zmm4, %zmm4
	vpslld	$2, %zmm3, %zmm3
	vpmovsxdq	%ymm3, %zmm5
	vextracti64x4	$1, %zmm3, %ymm3
	vpmovsxdq	%ymm3, %zmm3
	vpmovsxdq	%ymm4, %zmm6
	vextracti64x4	$1, %zmm4, %ymm4
	vpmovsxdq	%ymm4, %zmm4
	vpaddq	%zmm4, %zmm1, %zmm12
	vpaddq	%zmm6, %zmm1, %zmm10
	vpaddq	%zmm3, %zmm1, %zmm27
	vpaddq	%zmm5, %zmm1, %zmm3
	vmovdqa64	%zmm3, 42816(%rsp)      # 64-byte Spill
	movq	%rdi, %rsi
	.loc	1 208 52                        # 03-matrix-multiplication-cpu.py:208:52
	leal	(%rdi,%rdi,2), %r10d
	leal	(,%r10,8), %edx
	subl	%esi, %edx
	vpbroadcastd	%edx, %zmm3
	vpaddd	%zmm3, %zmm2, %zmm4
	vpaddd	%zmm3, %zmm0, %zmm3
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	vpslld	$2, %zmm3, %zmm3
	vpslld	$2, %zmm4, %zmm4
	vpmovsxdq	%ymm4, %zmm5
	vextracti64x4	$1, %zmm4, %ymm4
	vpmovsxdq	%ymm4, %zmm4
	vpmovsxdq	%ymm3, %zmm6
	vextracti64x4	$1, %zmm3, %ymm3
	vpmovsxdq	%ymm3, %zmm3
	vpaddq	%zmm3, %zmm1, %zmm3
	vmovdqa64	%zmm3, 42752(%rsp)      # 64-byte Spill
	vpaddq	%zmm6, %zmm1, %zmm3
	vmovdqa64	%zmm3, 42688(%rsp)      # 64-byte Spill
	vpaddq	%zmm4, %zmm1, %zmm3
	vmovdqa64	%zmm3, 42624(%rsp)      # 64-byte Spill
	vpaddq	%zmm5, %zmm1, %zmm3
	vmovdqa64	%zmm3, 42560(%rsp)      # 64-byte Spill
	.loc	1 208 52                        # 03-matrix-multiplication-cpu.py:208:52
	leal	(%rdi,%r13,4), %edx
	leal	(%rdi,%rdx), %edi
	vpbroadcastd	%edi, %zmm3
	vpaddd	%zmm3, %zmm2, %zmm4
	vpaddd	%zmm3, %zmm0, %zmm3
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	vpslld	$2, %zmm3, %zmm3
	vpslld	$2, %zmm4, %zmm4
	vpmovsxdq	%ymm4, %zmm5
	vextracti64x4	$1, %zmm4, %ymm4
	vpmovsxdq	%ymm4, %zmm4
	vpmovsxdq	%ymm3, %zmm6
	vextracti64x4	$1, %zmm3, %ymm3
	vpmovsxdq	%ymm3, %zmm3
	vpaddq	%zmm3, %zmm1, %zmm3
	vmovdqa64	%zmm3, 42496(%rsp)      # 64-byte Spill
	vpaddq	%zmm6, %zmm1, %zmm3
	vmovdqa64	%zmm3, 42432(%rsp)      # 64-byte Spill
	vpaddq	%zmm4, %zmm1, %zmm3
	vmovdqa64	%zmm3, 42368(%rsp)      # 64-byte Spill
	vpaddq	%zmm5, %zmm1, %zmm3
	vmovdqa64	%zmm3, 42304(%rsp)      # 64-byte Spill
	.loc	1 208 52                        # 03-matrix-multiplication-cpu.py:208:52
	vpbroadcastd	%edx, %zmm3
	vpaddd	%zmm3, %zmm2, %zmm4
	vpaddd	%zmm3, %zmm0, %zmm3
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	vpslld	$2, %zmm3, %zmm3
	vpslld	$2, %zmm4, %zmm4
	vpmovsxdq	%ymm4, %zmm5
	vextracti64x4	$1, %zmm4, %ymm4
	vpmovsxdq	%ymm4, %zmm4
	vpmovsxdq	%ymm3, %zmm6
	vextracti64x4	$1, %zmm3, %ymm3
	vpmovsxdq	%ymm3, %zmm3
	vpaddq	%zmm3, %zmm1, %zmm3
	vmovdqa64	%zmm3, 42240(%rsp)      # 64-byte Spill
	vpaddq	%zmm6, %zmm1, %zmm3
	vmovdqa64	%zmm3, 42176(%rsp)      # 64-byte Spill
	vpaddq	%zmm4, %zmm1, %zmm3
	vmovdqa64	%zmm3, 42112(%rsp)      # 64-byte Spill
	vpaddq	%zmm5, %zmm1, %zmm3
	vmovdqa64	%zmm3, 42048(%rsp)      # 64-byte Spill
	.loc	1 208 52                        # 03-matrix-multiplication-cpu.py:208:52
	leal	(,%rsi,4), %edi
	leal	(%rdi,%rdi,4), %edx
	vpbroadcastd	%edx, %zmm3
	vpaddd	%zmm3, %zmm2, %zmm4
	vpaddd	%zmm3, %zmm0, %zmm3
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	vpslld	$2, %zmm3, %zmm3
	vpslld	$2, %zmm4, %zmm4
	vpmovsxdq	%ymm4, %zmm5
	vextracti64x4	$1, %zmm4, %ymm4
	vpmovsxdq	%ymm4, %zmm4
	vpmovsxdq	%ymm3, %zmm6
	vextracti64x4	$1, %zmm3, %ymm3
	vpmovsxdq	%ymm3, %zmm3
	vpaddq	%zmm3, %zmm1, %zmm3
	vmovdqa64	%zmm3, 41984(%rsp)      # 64-byte Spill
	vpaddq	%zmm6, %zmm1, %zmm3
	vmovdqa64	%zmm3, 41920(%rsp)      # 64-byte Spill
	vpaddq	%zmm4, %zmm1, %zmm3
	vmovdqa64	%zmm3, 41856(%rsp)      # 64-byte Spill
	vpaddq	%zmm5, %zmm1, %zmm3
	vmovdqa64	%zmm3, 41792(%rsp)      # 64-byte Spill
	.loc	1 208 52                        # 03-matrix-multiplication-cpu.py:208:52
	leal	(%rsi,%r8,2), %edx
	vpbroadcastd	%edx, %zmm3
	vpaddd	%zmm3, %zmm2, %zmm4
	vpaddd	%zmm3, %zmm0, %zmm3
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	vpslld	$2, %zmm3, %zmm3
	vpslld	$2, %zmm4, %zmm4
	vpmovsxdq	%ymm4, %zmm5
	vextracti64x4	$1, %zmm4, %ymm4
	vpmovsxdq	%ymm4, %zmm4
	vpmovsxdq	%ymm3, %zmm6
	vextracti64x4	$1, %zmm3, %ymm3
	vpmovsxdq	%ymm3, %zmm3
	vpaddq	%zmm3, %zmm1, %zmm3
	vmovdqa64	%zmm3, 41728(%rsp)      # 64-byte Spill
	vpaddq	%zmm6, %zmm1, %zmm3
	vmovdqa64	%zmm3, 41664(%rsp)      # 64-byte Spill
	vpaddq	%zmm4, %zmm1, %zmm3
	vmovdqa64	%zmm3, 41600(%rsp)      # 64-byte Spill
	vpaddq	%zmm5, %zmm1, %zmm3
	vmovdqa64	%zmm3, 41536(%rsp)      # 64-byte Spill
	.loc	1 208 52                        # 03-matrix-multiplication-cpu.py:208:52
	leal	(%r15,%r15,8), %edx
	vpbroadcastd	%edx, %zmm3
	vpaddd	%zmm3, %zmm2, %zmm4
	vpaddd	%zmm3, %zmm0, %zmm3
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	vpslld	$2, %zmm3, %zmm3
	vpslld	$2, %zmm4, %zmm4
	vpmovsxdq	%ymm4, %zmm5
	vextracti64x4	$1, %zmm4, %ymm4
	vpmovsxdq	%ymm4, %zmm4
	vpmovsxdq	%ymm3, %zmm6
	vextracti64x4	$1, %zmm3, %ymm3
	vpmovsxdq	%ymm3, %zmm3
	vpaddq	%zmm3, %zmm1, %zmm3
	vmovdqa64	%zmm3, 41472(%rsp)      # 64-byte Spill
	vpaddq	%zmm6, %zmm1, %zmm3
	vmovdqa64	%zmm3, 41408(%rsp)      # 64-byte Spill
	vpaddq	%zmm4, %zmm1, %zmm3
	vmovdqa64	%zmm3, 41344(%rsp)      # 64-byte Spill
	vpaddq	%zmm5, %zmm1, %zmm3
	vmovdqa64	%zmm3, 41280(%rsp)      # 64-byte Spill
	.loc	1 208 52                        # 03-matrix-multiplication-cpu.py:208:52
	movl	%esi, %edx
	shll	$4, %edx
	leal	(%rsi,%rdx), %r9d
	vpbroadcastd	%r9d, %zmm3
	vpaddd	%zmm3, %zmm2, %zmm4
	vpaddd	%zmm3, %zmm0, %zmm3
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	vpslld	$2, %zmm3, %zmm3
	vpslld	$2, %zmm4, %zmm4
	vpmovsxdq	%ymm4, %zmm5
	vextracti64x4	$1, %zmm4, %ymm4
	vpmovsxdq	%ymm4, %zmm4
	vpmovsxdq	%ymm3, %zmm6
	vextracti64x4	$1, %zmm3, %ymm3
	vpmovsxdq	%ymm3, %zmm3
	vpaddq	%zmm3, %zmm1, %zmm3
	vmovdqa64	%zmm3, 41216(%rsp)      # 64-byte Spill
	vpaddq	%zmm6, %zmm1, %zmm3
	vmovdqa64	%zmm3, 41152(%rsp)      # 64-byte Spill
	vpaddq	%zmm4, %zmm1, %zmm3
	vmovdqa64	%zmm3, 41088(%rsp)      # 64-byte Spill
	vpaddq	%zmm5, %zmm1, %zmm3
	vmovdqa64	%zmm3, 41024(%rsp)      # 64-byte Spill
	.loc	1 208 52                        # 03-matrix-multiplication-cpu.py:208:52
	vpbroadcastd	%esi, %zmm3
	vpslld	$4, %zmm3, %zmm4
	vpaddd	%zmm4, %zmm2, %zmm5
	vpaddd	%zmm4, %zmm0, %zmm4
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	vpslld	$2, %zmm4, %zmm4
	vpslld	$2, %zmm5, %zmm5
	vpmovsxdq	%ymm5, %zmm6
	vextracti64x4	$1, %zmm5, %ymm5
	vpmovsxdq	%ymm5, %zmm5
	vpmovsxdq	%ymm4, %zmm7
	vextracti64x4	$1, %zmm4, %ymm4
	vpmovsxdq	%ymm4, %zmm4
	vpaddq	%zmm4, %zmm1, %zmm4
	vmovdqa64	%zmm4, 40960(%rsp)      # 64-byte Spill
	vpaddq	%zmm7, %zmm1, %zmm4
	vmovdqa64	%zmm4, 40896(%rsp)      # 64-byte Spill
	vpaddq	%zmm5, %zmm1, %zmm4
	vmovdqa64	%zmm4, 40832(%rsp)      # 64-byte Spill
	vpaddq	%zmm6, %zmm1, %zmm4
	vmovdqa64	%zmm4, 40768(%rsp)      # 64-byte Spill
	.loc	1 208 52                        # 03-matrix-multiplication-cpu.py:208:52
	leal	(%r13,%r13,2), %r9d
	vpbroadcastd	%r9d, %zmm4
	vpaddd	%zmm4, %zmm2, %zmm5
	vpaddd	%zmm4, %zmm0, %zmm4
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	vpslld	$2, %zmm4, %zmm4
	vpslld	$2, %zmm5, %zmm5
	vpmovsxdq	%ymm5, %zmm6
	vextracti64x4	$1, %zmm5, %ymm5
	vpmovsxdq	%ymm5, %zmm5
	vpmovsxdq	%ymm4, %zmm7
	vextracti64x4	$1, %zmm4, %ymm4
	vpmovsxdq	%ymm4, %zmm4
	vpaddq	%zmm4, %zmm1, %zmm4
	vmovdqa64	%zmm4, 40704(%rsp)      # 64-byte Spill
	vpaddq	%zmm7, %zmm1, %zmm4
	vmovdqa64	%zmm4, 40640(%rsp)      # 64-byte Spill
	vpaddq	%zmm5, %zmm1, %zmm4
	vmovdqa64	%zmm4, 40576(%rsp)      # 64-byte Spill
	vpaddq	%zmm6, %zmm1, %zmm4
	vmovdqa64	%zmm4, 40512(%rsp)      # 64-byte Spill
	.loc	1 208 52                        # 03-matrix-multiplication-cpu.py:208:52
	subl	%r15d, %edx
	vpbroadcastd	%edx, %zmm4
	vpaddd	%zmm4, %zmm2, %zmm5
	vpaddd	%zmm4, %zmm0, %zmm4
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	vpslld	$2, %zmm4, %zmm4
	vpslld	$2, %zmm5, %zmm5
	vpmovsxdq	%ymm5, %zmm6
	vextracti64x4	$1, %zmm5, %ymm5
	vpmovsxdq	%ymm5, %zmm5
	vpmovsxdq	%ymm4, %zmm7
	vextracti64x4	$1, %zmm4, %ymm4
	vpmovsxdq	%ymm4, %zmm4
	vpaddq	%zmm4, %zmm1, %zmm4
	vmovdqa64	%zmm4, 40448(%rsp)      # 64-byte Spill
	vpaddq	%zmm7, %zmm1, %zmm4
	vmovdqa64	%zmm4, 40384(%rsp)      # 64-byte Spill
	vpaddq	%zmm5, %zmm1, %zmm4
	vmovdqa64	%zmm4, 40320(%rsp)      # 64-byte Spill
	vpaddq	%zmm6, %zmm1, %zmm4
	vmovdqa64	%zmm4, 40256(%rsp)      # 64-byte Spill
	.loc	1 208 52                        # 03-matrix-multiplication-cpu.py:208:52
	leal	(%rsi,%r10,4), %edx
	vpbroadcastd	%edx, %zmm4
	vpaddd	%zmm4, %zmm2, %zmm5
	vpaddd	%zmm4, %zmm0, %zmm4
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	vpslld	$2, %zmm4, %zmm4
	vpslld	$2, %zmm5, %zmm5
	vpmovsxdq	%ymm5, %zmm6
	vextracti64x4	$1, %zmm5, %ymm5
	vpmovsxdq	%ymm5, %zmm5
	vpmovsxdq	%ymm4, %zmm7
	vextracti64x4	$1, %zmm4, %ymm4
	vpmovsxdq	%ymm4, %zmm4
	.loc	1 204 68 is_stmt 1              # 03-matrix-multiplication-cpu.py:204:68
	cltd
	idivl	%ecx
	movl	%edx, 256(%rsp)                 # 4-byte Spill
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	vpaddq	%zmm4, %zmm1, %zmm4
	vmovdqa64	%zmm4, 40192(%rsp)      # 64-byte Spill
	vpaddq	%zmm7, %zmm1, %zmm4
	vmovdqa64	%zmm4, 40128(%rsp)      # 64-byte Spill
	vpaddq	%zmm5, %zmm1, %zmm4
	vmovdqa64	%zmm4, 40064(%rsp)      # 64-byte Spill
	vpaddq	%zmm6, %zmm1, %zmm4
	vmovdqa64	%zmm4, 40000(%rsp)      # 64-byte Spill
	.loc	1 208 52 is_stmt 0              # 03-matrix-multiplication-cpu.py:208:52
	leal	(%rdi,%rdi,2), %edi
	.loc	1 204 68 is_stmt 1              # 03-matrix-multiplication-cpu.py:204:68
	movl	%ebx, %eax
	cltd
	idivl	%ecx
	movl	%edx, 512(%rsp)                 # 4-byte Spill
	.loc	1 208 52                        # 03-matrix-multiplication-cpu.py:208:52
	vpbroadcastd	%edi, %zmm4
	vpaddd	%zmm4, %zmm2, %zmm5
	vpaddd	%zmm4, %zmm0, %zmm4
	.loc	1 208 22 is_stmt 0              # 03-matrix-multiplication-cpu.py:208:22
	vpslld	$2, %zmm4, %zmm4
	vpslld	$2, %zmm5, %zmm5
	.loc	1 204 68 is_stmt 1              # 03-matrix-multiplication-cpu.py:204:68
	movl	%r11d, %eax
	cltd
	idivl	%ecx
	movl	%edx, 448(%rsp)                 # 4-byte Spill
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	vpmovsxdq	%ymm5, %zmm6
	vextracti64x4	$1, %zmm5, %ymm5
	vpmovsxdq	%ymm5, %zmm5
	vpmovsxdq	%ymm4, %zmm7
	vextracti64x4	$1, %zmm4, %ymm4
	.loc	1 204 68                        # 03-matrix-multiplication-cpu.py:204:68
	movl	%r14d, %eax
	cltd
	idivl	%ecx
	movl	%edx, 704(%rsp)                 # 4-byte Spill
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	vpmovsxdq	%ymm4, %zmm4
	vpaddq	%zmm4, %zmm1, %zmm4
	vmovdqa64	%zmm4, 39936(%rsp)      # 64-byte Spill
	vpaddq	%zmm7, %zmm1, %zmm4
	vmovdqa64	%zmm4, 39872(%rsp)      # 64-byte Spill
	vpaddq	%zmm5, %zmm1, %zmm4
	vmovdqa64	%zmm4, 39808(%rsp)      # 64-byte Spill
	vpaddq	%zmm6, %zmm1, %zmm4
	vmovdqa64	%zmm4, 39744(%rsp)      # 64-byte Spill
	.loc	1 208 52 is_stmt 0              # 03-matrix-multiplication-cpu.py:208:52
	leal	(%rsi,%r13,2), %eax
	vpbroadcastd	%eax, %zmm4
	vpaddd	%zmm4, %zmm2, %zmm5
	vpaddd	%zmm4, %zmm0, %zmm4
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	vpslld	$2, %zmm4, %zmm4
	vpslld	$2, %zmm5, %zmm5
	movl	1472(%rsp), %eax                # 4-byte Reload
	.loc	1 204 68 is_stmt 1              # 03-matrix-multiplication-cpu.py:204:68
	cltd
	idivl	%ecx
	movl	%edx, 1472(%rsp)                # 4-byte Spill
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	vpmovsxdq	%ymm5, %zmm6
	vextracti64x4	$1, %zmm5, %ymm5
	vpmovsxdq	%ymm5, %zmm5
	vpmovsxdq	%ymm4, %zmm7
	vextracti64x4	$1, %zmm4, %ymm4
	movl	832(%rsp), %eax                 # 4-byte Reload
	.loc	1 204 68                        # 03-matrix-multiplication-cpu.py:204:68
	cltd
	idivl	%ecx
	movl	%edx, 832(%rsp)                 # 4-byte Spill
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	vpmovsxdq	%ymm4, %zmm4
	vpaddq	%zmm4, %zmm1, %zmm4
	vmovdqa64	%zmm4, 39680(%rsp)      # 64-byte Spill
	vpaddq	%zmm7, %zmm1, %zmm4
	vmovdqa64	%zmm4, 39616(%rsp)      # 64-byte Spill
	vpaddq	%zmm5, %zmm1, %zmm4
	vmovdqa64	%zmm4, 39552(%rsp)      # 64-byte Spill
	vpaddq	%zmm6, %zmm1, %zmm4
	vmovdqa64	%zmm4, 39488(%rsp)      # 64-byte Spill
	.loc	1 208 52 is_stmt 0              # 03-matrix-multiplication-cpu.py:208:52
	leal	(%r15,%r15,4), %eax
	vpbroadcastd	%eax, %zmm4
	vpaddd	%zmm4, %zmm2, %zmm5
	vpaddd	%zmm4, %zmm0, %zmm4
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	vpslld	$2, %zmm4, %zmm4
	vpslld	$2, %zmm5, %zmm5
	movl	1728(%rsp), %eax                # 4-byte Reload
	.loc	1 204 68 is_stmt 1              # 03-matrix-multiplication-cpu.py:204:68
	cltd
	idivl	%ecx
	movl	%edx, 1728(%rsp)                # 4-byte Spill
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	vpmovsxdq	%ymm5, %zmm6
	vextracti64x4	$1, %zmm5, %ymm5
	vpmovsxdq	%ymm5, %zmm5
	vpmovsxdq	%ymm4, %zmm7
	vextracti64x4	$1, %zmm4, %ymm4
	movl	192(%rsp), %eax                 # 4-byte Reload
	.loc	1 204 68                        # 03-matrix-multiplication-cpu.py:204:68
	cltd
	idivl	%ecx
	movl	%edx, 192(%rsp)                 # 4-byte Spill
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	vpmovsxdq	%ymm4, %zmm4
	vpaddq	%zmm4, %zmm1, %zmm4
	vmovdqa64	%zmm4, 39424(%rsp)      # 64-byte Spill
	vpaddq	%zmm7, %zmm1, %zmm4
	vmovdqa64	%zmm4, 39360(%rsp)      # 64-byte Spill
	vpaddq	%zmm5, %zmm1, %zmm4
	vmovdqa64	%zmm4, 39296(%rsp)      # 64-byte Spill
	vpaddq	%zmm6, %zmm1, %zmm4
	vmovdqa64	%zmm4, 39232(%rsp)      # 64-byte Spill
	movl	64(%rsp), %eax                  # 4-byte Reload
	.loc	1 204 68                        # 03-matrix-multiplication-cpu.py:204:68
	cltd
	idivl	%ecx
	movl	%edx, 64(%rsp)                  # 4-byte Spill
	.loc	1 208 52                        # 03-matrix-multiplication-cpu.py:208:52
	vpbroadcastd	%r8d, %zmm4
	vpaddd	%zmm4, %zmm2, %zmm5
	vpaddd	%zmm4, %zmm0, %zmm4
	.loc	1 208 22 is_stmt 0              # 03-matrix-multiplication-cpu.py:208:22
	vpslld	$2, %zmm4, %zmm4
	vpslld	$2, %zmm5, %zmm5
	movl	1216(%rsp), %eax                # 4-byte Reload
	.loc	1 204 68 is_stmt 1              # 03-matrix-multiplication-cpu.py:204:68
	cltd
	idivl	%ecx
	movl	%edx, 1216(%rsp)                # 4-byte Spill
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	vpmovsxdq	%ymm5, %zmm6
	vextracti64x4	$1, %zmm5, %ymm5
	vpmovsxdq	%ymm5, %zmm5
	vpmovsxdq	%ymm4, %zmm7
	vextracti64x4	$1, %zmm4, %ymm4
	movl	1280(%rsp), %eax                # 4-byte Reload
	.loc	1 204 68                        # 03-matrix-multiplication-cpu.py:204:68
	cltd
	idivl	%ecx
	movl	%edx, 1280(%rsp)                # 4-byte Spill
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	vpmovsxdq	%ymm4, %zmm4
	vpaddq	%zmm4, %zmm1, %zmm4
	vmovdqa64	%zmm4, 39168(%rsp)      # 64-byte Spill
	vpaddq	%zmm7, %zmm1, %zmm4
	vmovdqa64	%zmm4, 39104(%rsp)      # 64-byte Spill
	vpaddq	%zmm5, %zmm1, %zmm4
	vmovdqa64	%zmm4, 39040(%rsp)      # 64-byte Spill
	vpaddq	%zmm6, %zmm1, %zmm4
	vmovdqa64	%zmm4, 38976(%rsp)      # 64-byte Spill
	movl	1984(%rsp), %eax                # 4-byte Reload
	.loc	1 204 68                        # 03-matrix-multiplication-cpu.py:204:68
	cltd
	idivl	%ecx
	movl	%edx, 1984(%rsp)                # 4-byte Spill
	.loc	1 208 52                        # 03-matrix-multiplication-cpu.py:208:52
	vpslld	$3, %zmm3, %zmm4
	vpaddd	%zmm4, %zmm2, %zmm5
	vpaddd	%zmm4, %zmm0, %zmm4
	.loc	1 208 22 is_stmt 0              # 03-matrix-multiplication-cpu.py:208:22
	vpslld	$2, %zmm4, %zmm4
	vpslld	$2, %zmm5, %zmm5
	vpmovsxdq	%ymm5, %zmm6
	vextracti64x4	$1, %zmm5, %ymm5
	vpmovsxdq	%ymm5, %zmm5
	vpmovsxdq	%ymm4, %zmm7
	vextracti64x4	$1, %zmm4, %ymm4
	vpmovsxdq	%ymm4, %zmm4
	movl	2048(%rsp), %eax                # 4-byte Reload
	.loc	1 204 68 is_stmt 1              # 03-matrix-multiplication-cpu.py:204:68
	cltd
	idivl	%ecx
	movl	%edx, 2048(%rsp)                # 4-byte Spill
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	vpaddq	%zmm4, %zmm1, %zmm4
	vmovdqa64	%zmm4, 38912(%rsp)      # 64-byte Spill
	vpaddq	%zmm7, %zmm1, %zmm4
	vmovdqa64	%zmm4, 38848(%rsp)      # 64-byte Spill
	vpaddq	%zmm5, %zmm1, %zmm4
	vmovdqa64	%zmm4, 38784(%rsp)      # 64-byte Spill
	vpaddq	%zmm6, %zmm1, %zmm4
	vmovdqa64	%zmm4, 38720(%rsp)      # 64-byte Spill
	.loc	1 208 52 is_stmt 0              # 03-matrix-multiplication-cpu.py:208:52
	subl	%esi, %r12d
	movl	2112(%rsp), %eax                # 4-byte Reload
	.loc	1 204 68 is_stmt 1              # 03-matrix-multiplication-cpu.py:204:68
	cltd
	idivl	%ecx
	movl	%edx, 2112(%rsp)                # 4-byte Spill
	.loc	1 208 52                        # 03-matrix-multiplication-cpu.py:208:52
	vpbroadcastd	%r12d, %zmm4
	vpaddd	%zmm4, %zmm2, %zmm5
	vpaddd	%zmm4, %zmm0, %zmm4
	.loc	1 208 22 is_stmt 0              # 03-matrix-multiplication-cpu.py:208:22
	vpslld	$2, %zmm4, %zmm4
	vpslld	$2, %zmm5, %zmm5
	vpmovsxdq	%ymm5, %zmm6
	vextracti64x4	$1, %zmm5, %ymm5
	vpmovsxdq	%ymm5, %zmm5
	vpmovsxdq	%ymm4, %zmm7
	vextracti64x4	$1, %zmm4, %ymm4
	vpmovsxdq	%ymm4, %zmm4
	movl	128(%rsp), %eax                 # 4-byte Reload
	.loc	1 204 68 is_stmt 1              # 03-matrix-multiplication-cpu.py:204:68
	cltd
	idivl	%ecx
	movl	%edx, 128(%rsp)                 # 4-byte Spill
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	vpaddq	%zmm4, %zmm1, %zmm4
	vmovdqa64	%zmm4, 38656(%rsp)      # 64-byte Spill
	vpaddq	%zmm7, %zmm1, %zmm4
	vmovdqa64	%zmm4, 38592(%rsp)      # 64-byte Spill
	vpaddq	%zmm5, %zmm1, %zmm4
	vmovdqa64	%zmm4, 38528(%rsp)      # 64-byte Spill
	vpaddq	%zmm6, %zmm1, %zmm4
	vmovdqa64	%zmm4, 38464(%rsp)      # 64-byte Spill
	.loc	1 208 52 is_stmt 0              # 03-matrix-multiplication-cpu.py:208:52
	leal	(%r15,%r15,2), %esi
	movl	896(%rsp), %eax                 # 4-byte Reload
	.loc	1 204 68 is_stmt 1              # 03-matrix-multiplication-cpu.py:204:68
	cltd
	idivl	%ecx
	movl	%edx, 896(%rsp)                 # 4-byte Spill
	.loc	1 208 52                        # 03-matrix-multiplication-cpu.py:208:52
	vpbroadcastd	%esi, %zmm4
	vpaddd	%zmm4, %zmm2, %zmm5
	vpaddd	%zmm4, %zmm0, %zmm4
	.loc	1 208 22 is_stmt 0              # 03-matrix-multiplication-cpu.py:208:22
	vpslld	$2, %zmm4, %zmm4
	vpslld	$2, %zmm5, %zmm5
	movl	3072(%rsp), %eax                # 4-byte Reload
	.loc	1 204 68 is_stmt 1              # 03-matrix-multiplication-cpu.py:204:68
	cltd
	idivl	%ecx
	movl	%edx, 3072(%rsp)                # 4-byte Spill
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	vpmovsxdq	%ymm5, %zmm6
	vextracti64x4	$1, %zmm5, %ymm5
	vpmovsxdq	%ymm5, %zmm5
	vpmovsxdq	%ymm4, %zmm7
	vextracti64x4	$1, %zmm4, %ymm4
	movl	3136(%rsp), %eax                # 4-byte Reload
	.loc	1 204 68                        # 03-matrix-multiplication-cpu.py:204:68
	cltd
	idivl	%ecx
	movl	%edx, 3136(%rsp)                # 4-byte Spill
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	vpmovsxdq	%ymm4, %zmm4
	vpaddq	%zmm4, %zmm1, %zmm4
	vmovdqa64	%zmm4, 38400(%rsp)      # 64-byte Spill
	vpaddq	%zmm7, %zmm1, %zmm4
	vmovdqa64	%zmm4, 38336(%rsp)      # 64-byte Spill
	vpaddq	%zmm5, %zmm1, %zmm4
	vmovdqa64	%zmm4, 38272(%rsp)      # 64-byte Spill
	vpaddq	%zmm6, %zmm1, %zmm4
	vmovdqa64	%zmm4, 38208(%rsp)      # 64-byte Spill
	movl	2368(%rsp), %eax                # 4-byte Reload
	.loc	1 204 68                        # 03-matrix-multiplication-cpu.py:204:68
	cltd
	idivl	%ecx
	movl	%edx, 2368(%rsp)                # 4-byte Spill
	.loc	1 208 52                        # 03-matrix-multiplication-cpu.py:208:52
	vpbroadcastd	%r13d, %zmm4
	vpaddd	%zmm4, %zmm2, %zmm5
	vpaddd	%zmm4, %zmm0, %zmm4
	.loc	1 208 22 is_stmt 0              # 03-matrix-multiplication-cpu.py:208:22
	vpslld	$2, %zmm4, %zmm4
	vpslld	$2, %zmm5, %zmm5
	movl	640(%rsp), %eax                 # 4-byte Reload
	.loc	1 204 68 is_stmt 1              # 03-matrix-multiplication-cpu.py:204:68
	cltd
	idivl	%ecx
	movl	%edx, 640(%rsp)                 # 4-byte Spill
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	vpmovsxdq	%ymm5, %zmm6
	vextracti64x4	$1, %zmm5, %ymm5
	vpmovsxdq	%ymm5, %zmm5
	vpmovsxdq	%ymm4, %zmm7
	vextracti64x4	$1, %zmm4, %ymm4
	movl	2432(%rsp), %eax                # 4-byte Reload
	.loc	1 204 68                        # 03-matrix-multiplication-cpu.py:204:68
	cltd
	idivl	%ecx
	movl	%edx, %r15d
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	vpmovsxdq	%ymm4, %zmm4
	vpaddq	%zmm4, %zmm1, %zmm4
	vmovdqa64	%zmm4, 38144(%rsp)      # 64-byte Spill
	vpaddq	%zmm7, %zmm1, %zmm4
	vmovdqa64	%zmm4, 38080(%rsp)      # 64-byte Spill
	vpaddq	%zmm5, %zmm1, %zmm11
	vpaddq	%zmm6, %zmm1, %zmm17
	movl	2496(%rsp), %eax                # 4-byte Reload
	.loc	1 204 68                        # 03-matrix-multiplication-cpu.py:204:68
	cltd
	idivl	%ecx
	movl	%edx, %r9d
	.loc	1 208 52                        # 03-matrix-multiplication-cpu.py:208:52
	vpslld	$2, %zmm3, %zmm4
	vpaddd	%zmm4, %zmm2, %zmm5
	vpaddd	%zmm4, %zmm0, %zmm4
	.loc	1 208 22 is_stmt 0              # 03-matrix-multiplication-cpu.py:208:22
	vpslld	$2, %zmm4, %zmm4
	vpslld	$2, %zmm5, %zmm5
	movl	2560(%rsp), %eax                # 4-byte Reload
	.loc	1 204 68 is_stmt 1              # 03-matrix-multiplication-cpu.py:204:68
	cltd
	idivl	%ecx
	movl	%edx, %esi
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	vpmovsxdq	%ymm5, %zmm6
	vextracti64x4	$1, %zmm5, %ymm5
	vpmovsxdq	%ymm5, %zmm5
	vpmovsxdq	%ymm4, %zmm7
	vextracti64x4	$1, %zmm4, %ymm4
	movl	768(%rsp), %eax                 # 4-byte Reload
	.loc	1 204 68                        # 03-matrix-multiplication-cpu.py:204:68
	cltd
	idivl	%ecx
	movl	%edx, %r11d
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	vpmovsxdq	%ymm4, %zmm4
	vpaddq	%zmm4, %zmm1, %zmm18
	vpaddq	%zmm7, %zmm1, %zmm20
	vpaddq	%zmm5, %zmm1, %zmm15
	vpaddq	%zmm6, %zmm1, %zmm21
	movl	1344(%rsp), %eax                # 4-byte Reload
	.loc	1 204 68                        # 03-matrix-multiplication-cpu.py:204:68
	cltd
	idivl	%ecx
	movl	%edx, %ebx
	.loc	1 208 52                        # 03-matrix-multiplication-cpu.py:208:52
	vpbroadcastd	%r10d, %zmm4
	vpaddd	%zmm4, %zmm2, %zmm5
	vpaddd	%zmm4, %zmm0, %zmm4
	.loc	1 208 22 is_stmt 0              # 03-matrix-multiplication-cpu.py:208:22
	vpslld	$2, %zmm4, %zmm4
	vpslld	$2, %zmm5, %zmm5
	vpmovsxdq	%ymm5, %zmm6
	vextracti64x4	$1, %zmm5, %ymm5
	vpmovsxdq	%ymm5, %zmm5
	vpmovsxdq	%ymm4, %zmm7
	vextracti64x4	$1, %zmm4, %ymm4
	vpmovsxdq	%ymm4, %zmm4
	movl	2176(%rsp), %eax                # 4-byte Reload
	.loc	1 204 68 is_stmt 1              # 03-matrix-multiplication-cpu.py:204:68
	cltd
	idivl	%ecx
	movl	%edx, %r14d
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	vpaddq	%zmm4, %zmm1, %zmm22
	vpaddq	%zmm7, %zmm1, %zmm24
	vpaddq	%zmm5, %zmm1, %zmm19
	vpaddq	%zmm6, %zmm1, %zmm25
	.loc	1 208 52 is_stmt 0              # 03-matrix-multiplication-cpu.py:208:52
	vpaddd	%zmm3, %zmm3, %zmm4
	vpaddd	%zmm4, %zmm2, %zmm5
	vpaddd	%zmm4, %zmm0, %zmm4
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	vpslld	$2, %zmm4, %zmm4
	vpslld	$2, %zmm5, %zmm5
	vpmovsxdq	%ymm5, %zmm6
	vextracti64x4	$1, %zmm5, %ymm5
	movl	2240(%rsp), %eax                # 4-byte Reload
	.loc	1 204 68 is_stmt 1              # 03-matrix-multiplication-cpu.py:204:68
	cltd
	idivl	%ecx
	movl	%edx, %r10d
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	vpmovsxdq	%ymm5, %zmm5
	vpmovsxdq	%ymm4, %zmm7
	vextracti64x4	$1, %zmm4, %ymm4
	vpmovsxdq	%ymm4, %zmm4
	vpaddq	%zmm4, %zmm1, %zmm26
	movl	1792(%rsp), %eax                # 4-byte Reload
	.loc	1 204 68                        # 03-matrix-multiplication-cpu.py:204:68
	cltd
	idivl	%ecx
	movl	%edx, %r12d
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	vpaddq	%zmm7, %zmm1, %zmm29
	vmovdqa64	%zmm27, %zmm7
	vpaddq	%zmm5, %zmm1, %zmm23
	vpaddq	%zmm6, %zmm1, %zmm30
	.loc	1 208 52 is_stmt 0              # 03-matrix-multiplication-cpu.py:208:52
	vpaddd	%zmm3, %zmm2, %zmm4
	vpaddd	%zmm3, %zmm0, %zmm3
	movl	1408(%rsp), %eax                # 4-byte Reload
	.loc	1 204 68 is_stmt 1              # 03-matrix-multiplication-cpu.py:204:68
	cltd
	idivl	%ecx
	movl	%edx, %edi
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	vpslld	$2, %zmm3, %zmm3
	vpslld	$2, %zmm4, %zmm4
	vpmovsxdq	%ymm4, %zmm5
	vextracti64x4	$1, %zmm4, %ymm4
	vpmovsxdq	%ymm4, %zmm4
	vpmovsxdq	%ymm3, %zmm6
	vextracti64x4	$1, %zmm3, %ymm3
	vpmovsxdq	%ymm3, %zmm3
	vpaddq	%zmm3, %zmm1, %zmm31
	vpaddq	%zmm6, %zmm1, %zmm8
	vmovdqa64	%zmm10, %zmm6
	vpaddq	%zmm4, %zmm1, %zmm28
	movl	960(%rsp), %eax                 # 4-byte Reload
	.loc	1 204 68                        # 03-matrix-multiplication-cpu.py:204:68
	cltd
	idivl	%ecx
	movl	%edx, %r8d
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	vpaddq	%zmm5, %zmm1, %zmm10
	vmovdqa64	%zmm12, %zmm5
	vpslld	$2, %zmm0, %zmm0
	vpslld	$2, %zmm2, %zmm2
	vpmovsxdq	%ymm2, %zmm3
	vextracti64x4	$1, %zmm2, %ymm2
	movl	1024(%rsp), %eax                # 4-byte Reload
	.loc	1 204 68                        # 03-matrix-multiplication-cpu.py:204:68
	cltd
	idivl	%ecx
	movl	%edx, %r13d
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	vpmovsxdq	%ymm2, %zmm2
	vpmovsxdq	%ymm0, %zmm4
	vextracti64x4	$1, %zmm0, %ymm0
	vpmovsxdq	%ymm0, %zmm0
	vpaddq	%zmm0, %zmm1, %zmm12
	movl	1856(%rsp), %eax                # 4-byte Reload
	.loc	1 204 68                        # 03-matrix-multiplication-cpu.py:204:68
	cltd
	idivl	%ecx
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	vpaddq	%zmm4, %zmm1, %zmm13
	vmovdqa64	%zmm9, %zmm4
	vpaddq	%zmm2, %zmm1, %zmm9
	vpaddq	%zmm3, %zmm1, %zmm14
	vmovdqa64	37824(%rsp), %zmm3      # 64-byte Reload
	movl	16(%rbp), %eax
	.loc	1 207 53                        # 03-matrix-multiplication-cpu.py:207:53
	imull	%eax, %edx
	.loc	1 207 22 is_stmt 0              # 03-matrix-multiplication-cpu.py:207:22
	vmovd	%edx, %xmm0
	vpslld	$2, %xmm0, %xmm1
	.loc	1 208 52 is_stmt 1              # 03-matrix-multiplication-cpu.py:208:52
	vpmovsxbd	.LCPI0_181(%rip), %xmm0 # xmm0 = [0,4,8,12]
	vpaddd	%xmm0, %xmm1, %xmm1
	vpmovsxdq	%xmm1, %xmm2
	.loc	1 207 22                        # 03-matrix-multiplication-cpu.py:207:22
	vmovq	1088(%rsp), %xmm1               # 8-byte Folded Reload
                                        # xmm1 = mem[0],zero
	.loc	1 208 52                        # 03-matrix-multiplication-cpu.py:208:52
	vpaddq	%xmm2, %xmm1, %xmm2
	vmovq	%xmm2, 432(%rsp)                # 8-byte Folded Spill
	.loc	1 207 53                        # 03-matrix-multiplication-cpu.py:207:53
	imull	%eax, %r13d
	.loc	1 207 22 is_stmt 0              # 03-matrix-multiplication-cpu.py:207:22
	vmovd	%r13d, %xmm2
	vpslld	$2, %xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vpmovsxdq	%xmm2, %xmm2
	vpaddq	%xmm2, %xmm1, %xmm2
	vmovq	%xmm2, 3056(%rsp)               # 8-byte Folded Spill
	.loc	1 207 53                        # 03-matrix-multiplication-cpu.py:207:53
	imull	%eax, %r8d
	.loc	1 207 22                        # 03-matrix-multiplication-cpu.py:207:22
	vmovd	%r8d, %xmm2
	vpslld	$2, %xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vpmovsxdq	%xmm2, %xmm2
	vpaddq	%xmm2, %xmm1, %xmm2
	vmovq	%xmm2, 3048(%rsp)               # 8-byte Folded Spill
	.loc	1 207 53                        # 03-matrix-multiplication-cpu.py:207:53
	imull	%eax, %edi
	.loc	1 207 22                        # 03-matrix-multiplication-cpu.py:207:22
	vmovd	%edi, %xmm2
	vpslld	$2, %xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vpmovsxdq	%xmm2, %xmm2
	vpaddq	%xmm2, %xmm1, %xmm2
	vmovq	%xmm2, 3040(%rsp)               # 8-byte Folded Spill
	.loc	1 207 53                        # 03-matrix-multiplication-cpu.py:207:53
	imull	%eax, %r12d
	.loc	1 207 22                        # 03-matrix-multiplication-cpu.py:207:22
	vmovd	%r12d, %xmm2
	vpslld	$2, %xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vpmovsxdq	%xmm2, %xmm2
	vpaddq	%xmm2, %xmm1, %xmm2
	vmovq	%xmm2, 3032(%rsp)               # 8-byte Folded Spill
	.loc	1 207 53                        # 03-matrix-multiplication-cpu.py:207:53
	imull	%eax, %r10d
	.loc	1 207 22                        # 03-matrix-multiplication-cpu.py:207:22
	vmovd	%r10d, %xmm2
	vpslld	$2, %xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vpmovsxdq	%xmm2, %xmm2
	vpaddq	%xmm2, %xmm1, %xmm2
	vmovq	%xmm2, 3024(%rsp)               # 8-byte Folded Spill
	.loc	1 207 53                        # 03-matrix-multiplication-cpu.py:207:53
	imull	%eax, %r14d
	.loc	1 207 22                        # 03-matrix-multiplication-cpu.py:207:22
	vmovd	%r14d, %xmm2
	vpslld	$2, %xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vpmovsxdq	%xmm2, %xmm2
	vpaddq	%xmm2, %xmm1, %xmm2
	vmovq	%xmm2, %rcx
	.loc	1 207 53                        # 03-matrix-multiplication-cpu.py:207:53
	imull	%eax, %ebx
	.loc	1 207 22                        # 03-matrix-multiplication-cpu.py:207:22
	vmovd	%ebx, %xmm2
	vpslld	$2, %xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vpmovsxdq	%xmm2, %xmm2
	vpaddq	%xmm2, %xmm1, %xmm2
	vmovq	%xmm2, %rbx
	.loc	1 207 53                        # 03-matrix-multiplication-cpu.py:207:53
	imull	%eax, %r11d
	.loc	1 207 22                        # 03-matrix-multiplication-cpu.py:207:22
	vmovd	%r11d, %xmm2
	vpslld	$2, %xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vpmovsxdq	%xmm2, %xmm2
	vpaddq	%xmm2, %xmm1, %xmm2
	vmovq	%xmm2, %r11
	.loc	1 207 53                        # 03-matrix-multiplication-cpu.py:207:53
	imull	%eax, %esi
	.loc	1 207 22                        # 03-matrix-multiplication-cpu.py:207:22
	vmovd	%esi, %xmm2
	vpslld	$2, %xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vpmovsxdq	%xmm2, %xmm2
	vpaddq	%xmm2, %xmm1, %xmm2
	vmovq	%xmm2, %rsi
	.loc	1 207 53                        # 03-matrix-multiplication-cpu.py:207:53
	imull	%eax, %r9d
	.loc	1 207 22                        # 03-matrix-multiplication-cpu.py:207:22
	vmovd	%r9d, %xmm2
	vpslld	$2, %xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vpmovsxdq	%xmm2, %xmm2
	vpaddq	%xmm2, %xmm1, %xmm2
	vmovq	%xmm2, %r9
	.loc	1 207 53                        # 03-matrix-multiplication-cpu.py:207:53
	imull	%eax, %r15d
	.loc	1 207 22                        # 03-matrix-multiplication-cpu.py:207:22
	vmovd	%r15d, %xmm2
	vpslld	$2, %xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vpmovsxdq	%xmm2, %xmm2
	vpaddq	%xmm2, %xmm1, %xmm2
	vmovq	%xmm2, 3016(%rsp)               # 8-byte Folded Spill
	movl	640(%rsp), %edx                 # 4-byte Reload
	.loc	1 207 53                        # 03-matrix-multiplication-cpu.py:207:53
	imull	%eax, %edx
	.loc	1 207 22                        # 03-matrix-multiplication-cpu.py:207:22
	vmovd	%edx, %xmm2
	vpslld	$2, %xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vpmovsxdq	%xmm2, %xmm2
	vpaddq	%xmm2, %xmm1, %xmm2
	vmovq	%xmm2, 3008(%rsp)               # 8-byte Folded Spill
	movl	2368(%rsp), %edx                # 4-byte Reload
	.loc	1 207 53                        # 03-matrix-multiplication-cpu.py:207:53
	imull	%eax, %edx
	.loc	1 207 22                        # 03-matrix-multiplication-cpu.py:207:22
	vmovd	%edx, %xmm2
	vpslld	$2, %xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vpmovsxdq	%xmm2, %xmm2
	vpaddq	%xmm2, %xmm1, %xmm2
	vmovq	%xmm2, 3000(%rsp)               # 8-byte Folded Spill
	movl	3136(%rsp), %edx                # 4-byte Reload
	.loc	1 207 53                        # 03-matrix-multiplication-cpu.py:207:53
	imull	%eax, %edx
	.loc	1 207 22                        # 03-matrix-multiplication-cpu.py:207:22
	vmovd	%edx, %xmm2
	vpslld	$2, %xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vpmovsxdq	%xmm2, %xmm2
	vpaddq	%xmm2, %xmm1, %xmm2
	vmovq	%xmm2, 2992(%rsp)               # 8-byte Folded Spill
	movl	3072(%rsp), %edx                # 4-byte Reload
	.loc	1 207 53                        # 03-matrix-multiplication-cpu.py:207:53
	imull	%eax, %edx
	.loc	1 207 22                        # 03-matrix-multiplication-cpu.py:207:22
	vmovd	%edx, %xmm2
	vpslld	$2, %xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vpmovsxdq	%xmm2, %xmm2
	vpaddq	%xmm2, %xmm1, %xmm2
	vmovq	%xmm2, 2984(%rsp)               # 8-byte Folded Spill
	movl	896(%rsp), %edx                 # 4-byte Reload
	.loc	1 207 53                        # 03-matrix-multiplication-cpu.py:207:53
	imull	%eax, %edx
	.loc	1 207 22                        # 03-matrix-multiplication-cpu.py:207:22
	vmovd	%edx, %xmm2
	vpslld	$2, %xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vpmovsxdq	%xmm2, %xmm2
	vpaddq	%xmm2, %xmm1, %xmm2
	vmovq	%xmm2, 2976(%rsp)               # 8-byte Folded Spill
	movl	128(%rsp), %edx                 # 4-byte Reload
	.loc	1 207 53                        # 03-matrix-multiplication-cpu.py:207:53
	imull	%eax, %edx
	.loc	1 207 22                        # 03-matrix-multiplication-cpu.py:207:22
	vmovd	%edx, %xmm2
	vpslld	$2, %xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vpmovsxdq	%xmm2, %xmm2
	vpaddq	%xmm2, %xmm1, %xmm2
	vmovq	%xmm2, 2968(%rsp)               # 8-byte Folded Spill
	movl	2112(%rsp), %edx                # 4-byte Reload
	.loc	1 207 53                        # 03-matrix-multiplication-cpu.py:207:53
	imull	%eax, %edx
	.loc	1 207 22                        # 03-matrix-multiplication-cpu.py:207:22
	vmovd	%edx, %xmm2
	vpslld	$2, %xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vpmovsxdq	%xmm2, %xmm2
	vpaddq	%xmm2, %xmm1, %xmm2
	vmovq	%xmm2, 2960(%rsp)               # 8-byte Folded Spill
	movl	2048(%rsp), %edx                # 4-byte Reload
	.loc	1 207 53                        # 03-matrix-multiplication-cpu.py:207:53
	imull	%eax, %edx
	.loc	1 207 22                        # 03-matrix-multiplication-cpu.py:207:22
	vmovd	%edx, %xmm2
	vpslld	$2, %xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vpmovsxdq	%xmm2, %xmm2
	vpaddq	%xmm2, %xmm1, %xmm2
	vmovq	%xmm2, 2952(%rsp)               # 8-byte Folded Spill
	movl	1984(%rsp), %edx                # 4-byte Reload
	.loc	1 207 53                        # 03-matrix-multiplication-cpu.py:207:53
	imull	%eax, %edx
	.loc	1 207 22                        # 03-matrix-multiplication-cpu.py:207:22
	vmovd	%edx, %xmm2
	vpslld	$2, %xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vpmovsxdq	%xmm2, %xmm2
	vpaddq	%xmm2, %xmm1, %xmm2
	vmovq	%xmm2, 2944(%rsp)               # 8-byte Folded Spill
	movl	1280(%rsp), %edx                # 4-byte Reload
	.loc	1 207 53                        # 03-matrix-multiplication-cpu.py:207:53
	imull	%eax, %edx
	.loc	1 207 22                        # 03-matrix-multiplication-cpu.py:207:22
	vmovd	%edx, %xmm2
	vpslld	$2, %xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vpmovsxdq	%xmm2, %xmm2
	vpaddq	%xmm2, %xmm1, %xmm2
	vmovq	%xmm2, 2936(%rsp)               # 8-byte Folded Spill
	movl	1216(%rsp), %edx                # 4-byte Reload
	.loc	1 207 53                        # 03-matrix-multiplication-cpu.py:207:53
	imull	%eax, %edx
	.loc	1 207 22                        # 03-matrix-multiplication-cpu.py:207:22
	vmovd	%edx, %xmm2
	vpslld	$2, %xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vpmovsxdq	%xmm2, %xmm2
	vpaddq	%xmm2, %xmm1, %xmm2
	vmovq	%xmm2, 2928(%rsp)               # 8-byte Folded Spill
	movl	64(%rsp), %edx                  # 4-byte Reload
	.loc	1 207 53                        # 03-matrix-multiplication-cpu.py:207:53
	imull	%eax, %edx
	.loc	1 207 22                        # 03-matrix-multiplication-cpu.py:207:22
	vmovd	%edx, %xmm2
	vpslld	$2, %xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vpmovsxdq	%xmm2, %xmm2
	vpaddq	%xmm2, %xmm1, %xmm2
	vmovq	%xmm2, 2920(%rsp)               # 8-byte Folded Spill
	movl	192(%rsp), %edx                 # 4-byte Reload
	.loc	1 207 53                        # 03-matrix-multiplication-cpu.py:207:53
	imull	%eax, %edx
	.loc	1 207 22                        # 03-matrix-multiplication-cpu.py:207:22
	vmovd	%edx, %xmm2
	vpslld	$2, %xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vpmovsxdq	%xmm2, %xmm2
	vpaddq	%xmm2, %xmm1, %xmm2
	vmovq	%xmm2, %r15
	movl	1728(%rsp), %edx                # 4-byte Reload
	.loc	1 207 53                        # 03-matrix-multiplication-cpu.py:207:53
	imull	%eax, %edx
	.loc	1 207 22                        # 03-matrix-multiplication-cpu.py:207:22
	vmovd	%edx, %xmm2
	vpslld	$2, %xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vpmovsxdq	%xmm2, %xmm2
	vpaddq	%xmm2, %xmm1, %xmm2
	vmovq	%xmm2, %rdi
	movl	832(%rsp), %edx                 # 4-byte Reload
	.loc	1 207 53                        # 03-matrix-multiplication-cpu.py:207:53
	imull	%eax, %edx
	.loc	1 207 22                        # 03-matrix-multiplication-cpu.py:207:22
	vmovd	%edx, %xmm2
	vpslld	$2, %xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vpmovsxdq	%xmm2, %xmm2
	vpaddq	%xmm2, %xmm1, %xmm2
	vmovq	%xmm2, %r10
	movl	1472(%rsp), %edx                # 4-byte Reload
	.loc	1 207 53                        # 03-matrix-multiplication-cpu.py:207:53
	imull	%eax, %edx
	.loc	1 207 22                        # 03-matrix-multiplication-cpu.py:207:22
	vmovd	%edx, %xmm2
	vpslld	$2, %xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vpmovsxdq	%xmm2, %xmm2
	vpaddq	%xmm2, %xmm1, %xmm2
	vmovq	%xmm2, %r13
	movl	704(%rsp), %edx                 # 4-byte Reload
	.loc	1 207 53                        # 03-matrix-multiplication-cpu.py:207:53
	imull	%eax, %edx
	.loc	1 207 22                        # 03-matrix-multiplication-cpu.py:207:22
	vmovd	%edx, %xmm2
	vpslld	$2, %xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vpmovsxdq	%xmm2, %xmm2
	vpaddq	%xmm2, %xmm1, %xmm2
	vmovq	%xmm2, %r8
	movl	448(%rsp), %edx                 # 4-byte Reload
	.loc	1 207 53                        # 03-matrix-multiplication-cpu.py:207:53
	imull	%eax, %edx
	.loc	1 207 22                        # 03-matrix-multiplication-cpu.py:207:22
	vmovd	%edx, %xmm2
	vpslld	$2, %xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vpmovsxdq	%xmm2, %xmm2
	vpaddq	%xmm2, %xmm1, %xmm2
	vmovq	%xmm2, %r12
	movl	512(%rsp), %edx                 # 4-byte Reload
	.loc	1 207 53                        # 03-matrix-multiplication-cpu.py:207:53
	imull	%eax, %edx
	.loc	1 207 22                        # 03-matrix-multiplication-cpu.py:207:22
	vmovd	%edx, %xmm2
	vpslld	$2, %xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm2
	vpmovsxdq	%xmm2, %xmm2
	vpaddq	%xmm2, %xmm1, %xmm2
	vmovq	%xmm2, %rdx
	movl	256(%rsp), %r14d                # 4-byte Reload
	.loc	1 207 53                        # 03-matrix-multiplication-cpu.py:207:53
	imull	%eax, %r14d
	.loc	1 207 22                        # 03-matrix-multiplication-cpu.py:207:22
	vmovd	%r14d, %xmm2
	vpslld	$2, %xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm0
	vmovdqa64	37888(%rsp), %zmm2      # 64-byte Reload
	vpmovsxdq	%xmm0, %xmm0
	vpaddq	%xmm0, %xmm1, %xmm0
	vmovdqa64	37952(%rsp), %zmm1      # 64-byte Reload
	vmovq	%xmm0, %r14
	movw	$512, %ax                       # imm = 0x200
	kmovd	%eax, %k1
	movb	$32, %al
	kmovd	%eax, %k2
	movw	$2048, %ax                      # imm = 0x800
	kmovd	%eax, %k3
	movb	$64, %al
	kmovd	%eax, %k4
	movw	$8192, %ax                      # imm = 0x2000
	kmovd	%eax, %k5
	movb	$-128, %al
	kmovd	%eax, %k6
	movw	$-32768, %ax                    # imm = 0x8000
	kmovd	%eax, %k7
	movq	424(%rsp), %rax                 # 8-byte Reload
	.loc	1 0 0                           # 03-matrix-multiplication-cpu.py:0:0
	sarl	$5, %eax
	movq	%rax, 424(%rsp)                 # 8-byte Spill
	.loc	1 229 33 is_stmt 1              # 03-matrix-multiplication-cpu.py:229:33
	movl	24(%rbp), %eax
                                        # kill: def $eax killed $eax killed $rax
	shll	$7, %eax
	vpbroadcastd	%eax, %ymm0
	movq	432(%rsp), %rax                 # 8-byte Reload
	vpmovsxdq	%ymm0, %zmm0
	vmovdqa64	%zmm0, 44288(%rsp)      # 64-byte Spill
	vmovdqa64	38016(%rsp), %zmm0      # 64-byte Reload
	movl	$0, 60(%rsp)                    # 4-byte Folded Spill
	vpxord	%xmm27, %xmm27, %xmm27
	vmovdqa64	%zmm27, 24000(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm16, %zmm27
	vbroadcasti32x4	.LCPI0_88(%rip), %ymm16 # ymm16 = [8,9,7,7,8,9,7,7]
                                        # ymm16 = mem[0,1,2,3,0,1,2,3]
	vmovdqa64	%ymm16, 19936(%rsp)     # 32-byte Spill
	vbroadcasti32x4	.LCPI0_87(%rip), %ymm16 # ymm16 = [8,9,5,7,8,9,5,7]
                                        # ymm16 = mem[0,1,2,3,0,1,2,3]
	vmovdqa64	%ymm16, 19904(%rsp)     # 32-byte Spill
	vpxord	%xmm16, %xmm16, %xmm16
	vmovdqa64	%zmm16, 19968(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm16, 20096(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm16, 20032(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm16, 20224(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm16, 20160(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm16, 20352(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm16, 20288(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm16, 20480(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm16, 20416(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm16, 20608(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm16, 20544(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm16, 20736(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm16, 20672(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm16, 20864(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm16, 20800(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm16, 20992(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm16, 20928(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm16, 21120(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm16, 21056(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm16, 21248(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm16, 21184(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm16, 21376(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm16, 21312(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm16, 21504(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm16, 21440(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm16, 21632(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm16, 21568(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm16, 21760(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm16, 21696(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm16, 21888(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm16, 21824(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm16, 22016(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm16, 21952(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm16, 22144(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm16, 22080(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm16, 22272(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm16, 22208(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm16, 22400(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm16, 22336(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm16, 22528(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm16, 22464(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm16, 22656(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm16, 22592(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm16, 22784(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm16, 22720(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm16, 22912(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm16, 22848(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm16, 23040(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm16, 22976(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm16, 23168(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm16, 23104(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm16, 23296(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm16, 23232(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm16, 23424(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm16, 23360(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm16, 23552(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm16, 23488(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm16, 23680(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm16, 23616(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm16, 23808(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm16, 23744(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm16, 23936(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm16, 23872(%rsp)     # 64-byte Spill
	.loc	1 0 33 is_stmt 0                # :0:33
.Ltmp5:
	.p2align	4
.LBB0_3:                                # =>This Loop Header: Depth=1
                                        #     Child Loop BB0_4 Depth 2
	.loc	1 223 20 is_stmt 1              # 03-matrix-multiplication-cpu.py:223:20
	vmovups	(%r14), %zmm16
	vmovaps	%zmm16, 5952(%rsp)              # 64-byte Spill
	movq	%r14, 5664(%rsp)                # 8-byte Spill
	vmovups	64(%r14), %zmm16
	vmovaps	%zmm16, 5888(%rsp)              # 64-byte Spill
	vmovups	(%rdx), %zmm16
	vmovaps	%zmm16, 3776(%rsp)              # 64-byte Spill
	movq	%rdx, 5672(%rsp)                # 8-byte Spill
	vmovups	64(%rdx), %zmm16
	vmovaps	%zmm16, 3712(%rsp)              # 64-byte Spill
	vmovups	(%r12), %zmm16
	vmovaps	%zmm16, 3904(%rsp)              # 64-byte Spill
	movq	%r12, 5680(%rsp)                # 8-byte Spill
	vmovups	64(%r12), %zmm16
	vmovaps	%zmm16, 3840(%rsp)              # 64-byte Spill
	vmovups	(%r8), %zmm16
	vmovaps	%zmm16, 4032(%rsp)              # 64-byte Spill
	movq	%r8, 5688(%rsp)                 # 8-byte Spill
	vmovups	64(%r8), %zmm16
	vmovaps	%zmm16, 3968(%rsp)              # 64-byte Spill
	vmovups	(%r13), %zmm16
	vmovaps	%zmm16, 4160(%rsp)              # 64-byte Spill
	movq	%r13, 5696(%rsp)                # 8-byte Spill
	vmovups	64(%r13), %zmm16
	vmovaps	%zmm16, 4096(%rsp)              # 64-byte Spill
	vmovups	(%r10), %zmm16
	vmovaps	%zmm16, 4288(%rsp)              # 64-byte Spill
	movq	%r10, 5704(%rsp)                # 8-byte Spill
	vmovups	64(%r10), %zmm16
	vmovaps	%zmm16, 4224(%rsp)              # 64-byte Spill
	vmovups	(%rdi), %zmm16
	vmovaps	%zmm16, 6080(%rsp)              # 64-byte Spill
	movq	%rdi, 5712(%rsp)                # 8-byte Spill
	vmovups	64(%rdi), %zmm16
	vmovaps	%zmm16, 6016(%rsp)              # 64-byte Spill
	vmovups	(%r15), %zmm16
	vmovaps	%zmm16, 4416(%rsp)              # 64-byte Spill
	movq	%r15, 5720(%rsp)                # 8-byte Spill
	vmovups	64(%r15), %zmm16
	vmovaps	%zmm16, 4352(%rsp)              # 64-byte Spill
	movq	2920(%rsp), %rdx                # 8-byte Reload
	vmovups	(%rdx), %zmm16
	vmovaps	%zmm16, 4544(%rsp)              # 64-byte Spill
	vmovups	64(%rdx), %zmm16
	vmovaps	%zmm16, 4480(%rsp)              # 64-byte Spill
	movq	2928(%rsp), %rdx                # 8-byte Reload
	vmovups	(%rdx), %zmm16
	vmovaps	%zmm16, 4672(%rsp)              # 64-byte Spill
	vmovups	64(%rdx), %zmm16
	vmovaps	%zmm16, 4608(%rsp)              # 64-byte Spill
	movq	2936(%rsp), %rdx                # 8-byte Reload
	vmovups	(%rdx), %zmm16
	vmovaps	%zmm16, 3264(%rsp)              # 64-byte Spill
	vmovups	64(%rdx), %zmm16
	vmovaps	%zmm16, 3200(%rsp)              # 64-byte Spill
	movq	2944(%rsp), %rdx                # 8-byte Reload
	vmovups	(%rdx), %zmm16
	vmovaps	%zmm16, 3392(%rsp)              # 64-byte Spill
	vmovups	64(%rdx), %zmm16
	vmovaps	%zmm16, 3328(%rsp)              # 64-byte Spill
	movq	2952(%rsp), %rdx                # 8-byte Reload
	vmovups	(%rdx), %zmm16
	vmovaps	%zmm16, 3520(%rsp)              # 64-byte Spill
	vmovups	64(%rdx), %zmm16
	vmovaps	%zmm16, 3456(%rsp)              # 64-byte Spill
	movq	2960(%rsp), %rdx                # 8-byte Reload
	vmovups	(%rdx), %zmm16
	vmovaps	%zmm16, 3648(%rsp)              # 64-byte Spill
	vmovups	64(%rdx), %zmm16
	vmovaps	%zmm16, 3584(%rsp)              # 64-byte Spill
	movq	2968(%rsp), %rdx                # 8-byte Reload
	vmovups	(%rdx), %zmm16
	vmovaps	%zmm16, 4800(%rsp)              # 64-byte Spill
	vmovups	64(%rdx), %zmm16
	vmovaps	%zmm16, 4736(%rsp)              # 64-byte Spill
	movq	2976(%rsp), %rdx                # 8-byte Reload
	vmovups	(%rdx), %zmm16
	vmovaps	%zmm16, 6208(%rsp)              # 64-byte Spill
	vmovups	64(%rdx), %zmm16
	vmovaps	%zmm16, 6144(%rsp)              # 64-byte Spill
	movq	2984(%rsp), %rdx                # 8-byte Reload
	vmovups	(%rdx), %zmm16
	vmovaps	%zmm16, 6272(%rsp)              # 64-byte Spill
	vmovups	64(%rdx), %zmm16
	vmovaps	%zmm16, 7744(%rsp)              # 64-byte Spill
	movq	2992(%rsp), %rdx                # 8-byte Reload
	vmovups	(%rdx), %zmm16
	vmovaps	%zmm16, 6400(%rsp)              # 64-byte Spill
	vmovups	64(%rdx), %zmm16
	vmovaps	%zmm16, 6336(%rsp)              # 64-byte Spill
	movq	3000(%rsp), %rdx                # 8-byte Reload
	vmovups	(%rdx), %zmm16
	vmovaps	%zmm16, 6784(%rsp)              # 64-byte Spill
	vmovups	64(%rdx), %zmm16
	vmovaps	%zmm16, 4864(%rsp)              # 64-byte Spill
	movq	3008(%rsp), %rdx                # 8-byte Reload
	vmovups	(%rdx), %zmm16
	vmovaps	%zmm16, 6464(%rsp)              # 64-byte Spill
	vmovups	64(%rdx), %zmm16
	vmovaps	%zmm16, 6720(%rsp)              # 64-byte Spill
	movq	3016(%rsp), %rdx                # 8-byte Reload
	vmovups	(%rdx), %zmm16
	vmovaps	%zmm16, 7808(%rsp)              # 64-byte Spill
	vmovups	64(%rdx), %zmm16
	vmovaps	%zmm16, 6848(%rsp)              # 64-byte Spill
	vmovups	(%r9), %zmm16
	vmovaps	%zmm16, 7872(%rsp)              # 64-byte Spill
	movq	%r9, 3064(%rsp)                 # 8-byte Spill
	vmovups	64(%r9), %zmm16
	vmovaps	%zmm16, 6912(%rsp)              # 64-byte Spill
	vmovups	(%rsi), %zmm16
	vmovaps	%zmm16, 8576(%rsp)              # 64-byte Spill
	movq	%rsi, 5728(%rsp)                # 8-byte Spill
	vmovups	64(%rsi), %zmm16
	vmovaps	%zmm16, 8512(%rsp)              # 64-byte Spill
	vmovups	(%r11), %zmm16
	vmovaps	%zmm16, 6976(%rsp)              # 64-byte Spill
	movq	%r11, 5736(%rsp)                # 8-byte Spill
	vmovups	64(%r11), %zmm16
	vmovaps	%zmm16, 7936(%rsp)              # 64-byte Spill
	vmovups	(%rbx), %zmm16
	vmovaps	%zmm16, 8000(%rsp)              # 64-byte Spill
	movq	%rbx, 5744(%rsp)                # 8-byte Spill
	vmovups	64(%rbx), %zmm16
	vmovaps	%zmm16, 8640(%rsp)              # 64-byte Spill
	vmovups	(%rcx), %zmm16
	vmovaps	%zmm16, 8128(%rsp)              # 64-byte Spill
	movq	%rcx, 5752(%rsp)                # 8-byte Spill
	vmovups	64(%rcx), %zmm16
	vmovaps	%zmm16, 8064(%rsp)              # 64-byte Spill
	movq	3024(%rsp), %rcx                # 8-byte Reload
	vmovups	(%rcx), %zmm16
	vmovaps	%zmm16, 4928(%rsp)              # 64-byte Spill
	vmovups	64(%rcx), %zmm16
	vmovaps	%zmm16, 6528(%rsp)              # 64-byte Spill
	movq	3032(%rsp), %rcx                # 8-byte Reload
	vmovups	(%rcx), %zmm16
	vmovaps	%zmm16, 4992(%rsp)              # 64-byte Spill
	vmovups	64(%rcx), %zmm16
	vmovaps	%zmm16, 6592(%rsp)              # 64-byte Spill
	movq	3040(%rsp), %rcx                # 8-byte Reload
	vmovups	(%rcx), %zmm16
	vmovaps	%zmm16, 5120(%rsp)              # 64-byte Spill
	vmovups	64(%rcx), %zmm16
	vmovaps	%zmm16, 5056(%rsp)              # 64-byte Spill
	movq	3048(%rsp), %rcx                # 8-byte Reload
	vmovups	(%rcx), %zmm16
	vmovaps	%zmm16, 5248(%rsp)              # 64-byte Spill
	vmovups	64(%rcx), %zmm16
	vmovaps	%zmm16, 5184(%rsp)              # 64-byte Spill
	movq	3056(%rsp), %rcx                # 8-byte Reload
	vmovups	(%rcx), %zmm16
	vmovaps	%zmm16, 5376(%rsp)              # 64-byte Spill
	vmovups	64(%rcx), %zmm16
	vmovaps	%zmm16, 5312(%rsp)              # 64-byte Spill
	vmovups	(%rax), %zmm16
	vmovaps	%zmm16, 7104(%rsp)              # 64-byte Spill
	movq	%rax, 432(%rsp)                 # 8-byte Spill
	vmovdqu64	64(%rax), %zmm16
	vmovdqa64	%zmm16, 7040(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm9, 46784(%rsp)      # 64-byte Spill
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	vmovdqa64	%zmm9, 48512(%rsp)
	vmovdqa64	%zmm14, 46720(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm14, 48448(%rsp)
	vmovdqa64	%zmm13, 46848(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm13, 48576(%rsp)
	vmovdqa64	%zmm12, 46912(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm12, 48640(%rsp)
	vmovdqa64	%zmm28, 47040(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm28, 48768(%rsp)
	vmovdqa64	%zmm10, 46976(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm10, 48704(%rsp)
	vmovdqa64	%zmm8, 47104(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm8, 48832(%rsp)
	vmovdqa64	%zmm31, 47168(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm31, 48896(%rsp)
	vmovdqa64	%zmm23, 47296(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm23, 49024(%rsp)
	vmovdqa64	%zmm30, 47232(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm30, 48960(%rsp)
	vmovdqa64	%zmm29, 47360(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm29, 49088(%rsp)
	vmovdqa64	%zmm26, 47424(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm26, 49152(%rsp)
	vmovdqa64	%zmm19, 47552(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm19, 49280(%rsp)
	vmovdqa64	%zmm25, 47488(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm25, 49216(%rsp)
	vmovdqa64	%zmm24, 47616(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm24, 49344(%rsp)
	vmovdqa64	%zmm22, 47680(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm22, 49408(%rsp)
	vmovdqa64	%zmm15, 47808(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm15, 49536(%rsp)
	vmovdqa64	%zmm21, 47744(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm21, 49472(%rsp)
	vmovdqa64	%zmm20, 47872(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm20, 49600(%rsp)
	vmovdqa64	%zmm18, 47936(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm18, 49664(%rsp)
	vmovdqa64	%zmm11, 48064(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm11, 49792(%rsp)
	vmovdqa64	%zmm17, 48000(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm17, 49728(%rsp)
	vmovaps	38080(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 49856(%rsp)
	vmovaps	38144(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 49920(%rsp)
	vmovaps	38272(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 50048(%rsp)
	vmovaps	38208(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 49984(%rsp)
	vmovaps	38336(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 50112(%rsp)
	vmovaps	38400(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 50176(%rsp)
	vmovaps	38528(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 50304(%rsp)
	vmovaps	38464(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 50240(%rsp)
	vmovaps	38592(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 50368(%rsp)
	vmovaps	38656(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 50432(%rsp)
	vmovaps	38784(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 50560(%rsp)
	vmovaps	38720(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 50496(%rsp)
	vmovaps	38848(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 50624(%rsp)
	vmovaps	38912(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 50688(%rsp)
	vmovaps	39040(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 50816(%rsp)
	vmovaps	38976(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 50752(%rsp)
	vmovaps	39104(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 50880(%rsp)
	vmovaps	39168(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 50944(%rsp)
	vmovaps	39296(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 51072(%rsp)
	vmovaps	39232(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 51008(%rsp)
	vmovaps	39360(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 51136(%rsp)
	vmovaps	39424(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 51200(%rsp)
	vmovaps	39552(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 51328(%rsp)
	vmovaps	39488(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 51264(%rsp)
	vmovaps	39616(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 51392(%rsp)
	vmovaps	39680(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 51456(%rsp)
	vmovaps	39808(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 51584(%rsp)
	vmovaps	39744(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 51520(%rsp)
	vmovaps	39872(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 51648(%rsp)
	vmovaps	39936(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 51712(%rsp)
	vmovaps	40064(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 51840(%rsp)
	vmovaps	40000(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 51776(%rsp)
	vmovaps	40128(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 51904(%rsp)
	vmovaps	40192(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 51968(%rsp)
	vmovaps	40320(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 52096(%rsp)
	vmovaps	40256(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 52032(%rsp)
	vmovaps	40384(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 52160(%rsp)
	vmovaps	40448(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 52224(%rsp)
	vmovaps	40576(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 52352(%rsp)
	vmovaps	40512(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 52288(%rsp)
	vmovaps	40640(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 52416(%rsp)
	vmovaps	40704(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 52480(%rsp)
	vmovaps	40832(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 52608(%rsp)
	vmovaps	40768(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 52544(%rsp)
	vmovaps	40896(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 52672(%rsp)
	vmovaps	40960(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 52736(%rsp)
	vmovaps	41088(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 52864(%rsp)
	vmovaps	41024(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 52800(%rsp)
	vmovaps	41152(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 52928(%rsp)
	vmovaps	41216(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 52992(%rsp)
	vmovaps	41344(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 53120(%rsp)
	vmovaps	41280(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 53056(%rsp)
	vmovaps	41408(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 53184(%rsp)
	vmovaps	41472(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 53248(%rsp)
	vmovaps	41600(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 53376(%rsp)
	vmovaps	41536(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 53312(%rsp)
	vmovaps	41664(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 53440(%rsp)
	vmovaps	41728(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 53504(%rsp)
	vmovaps	41856(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 53632(%rsp)
	vmovaps	41792(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 53568(%rsp)
	vmovaps	41920(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 53696(%rsp)
	vmovaps	41984(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 53760(%rsp)
	vmovaps	42112(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 53888(%rsp)
	vmovaps	42048(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 53824(%rsp)
	vmovaps	42176(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 53952(%rsp)
	vmovaps	42240(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 54016(%rsp)
	vmovaps	42368(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 54144(%rsp)
	vmovaps	42304(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 54080(%rsp)
	vmovaps	42432(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 54208(%rsp)
	vmovaps	42496(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 54272(%rsp)
	vmovaps	42624(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 54400(%rsp)
	vmovaps	42560(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 54336(%rsp)
	vmovaps	42688(%rsp), %zmm8              # 64-byte Reload
	vmovaps	%zmm8, 54464(%rsp)
	vmovdqa64	42752(%rsp), %zmm8      # 64-byte Reload
	vmovdqa64	%zmm8, 54528(%rsp)
	vmovdqa64	%zmm7, 48128(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm7, 54656(%rsp)
	vmovdqa64	42816(%rsp), %zmm7      # 64-byte Reload
	vmovdqa64	%zmm7, 54592(%rsp)
	vmovdqa64	%zmm6, 48192(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm6, 54720(%rsp)
	vmovdqa64	%zmm5, 48256(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm5, 54784(%rsp)
	vmovdqa64	%zmm3, 37824(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm3, 54912(%rsp)
	vmovdqa64	%zmm4, 48320(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm4, 54848(%rsp)
	vmovdqa64	%zmm2, 37888(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm2, 54976(%rsp)
	vmovdqa64	42880(%rsp), %zmm2      # 64-byte Reload
	vmovdqa64	%zmm2, 55040(%rsp)
	vmovdqa64	%zmm0, 38016(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 55168(%rsp)
	vmovdqa64	%zmm1, 37952(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm1, 55104(%rsp)
	vmovaps	42944(%rsp), %zmm0              # 64-byte Reload
	vmovaps	%zmm0, 55232(%rsp)
	vmovaps	43008(%rsp), %zmm0              # 64-byte Reload
	vmovaps	%zmm0, 55296(%rsp)
	vmovaps	43136(%rsp), %zmm0              # 64-byte Reload
	vmovaps	%zmm0, 55424(%rsp)
	vmovaps	43072(%rsp), %zmm0              # 64-byte Reload
	vmovaps	%zmm0, 55360(%rsp)
	vmovaps	43200(%rsp), %zmm0              # 64-byte Reload
	vmovaps	%zmm0, 55488(%rsp)
	vmovaps	43264(%rsp), %zmm0              # 64-byte Reload
	vmovaps	%zmm0, 55552(%rsp)
	vmovaps	43392(%rsp), %zmm0              # 64-byte Reload
	vmovaps	%zmm0, 55680(%rsp)
	vmovaps	43328(%rsp), %zmm0              # 64-byte Reload
	vmovaps	%zmm0, 55616(%rsp)
	vmovaps	43456(%rsp), %zmm0              # 64-byte Reload
	vmovaps	%zmm0, 55744(%rsp)
	vmovaps	43520(%rsp), %zmm0              # 64-byte Reload
	vmovaps	%zmm0, 55808(%rsp)
	vmovaps	43648(%rsp), %zmm0              # 64-byte Reload
	vmovaps	%zmm0, 55936(%rsp)
	vmovaps	43584(%rsp), %zmm0              # 64-byte Reload
	vmovaps	%zmm0, 55872(%rsp)
	vmovaps	43712(%rsp), %zmm0              # 64-byte Reload
	vmovaps	%zmm0, 56000(%rsp)
	vmovaps	43776(%rsp), %zmm0              # 64-byte Reload
	vmovaps	%zmm0, 56064(%rsp)
	vmovaps	43904(%rsp), %zmm0              # 64-byte Reload
	vmovaps	%zmm0, 56192(%rsp)
	vmovaps	43840(%rsp), %zmm0              # 64-byte Reload
	vmovaps	%zmm0, 56128(%rsp)
	vmovaps	43968(%rsp), %zmm0              # 64-byte Reload
	vmovaps	%zmm0, 56256(%rsp)
	vmovaps	44032(%rsp), %zmm0              # 64-byte Reload
	vmovaps	%zmm0, 56320(%rsp)
	vmovaps	44160(%rsp), %zmm0              # 64-byte Reload
	vmovaps	%zmm0, 56448(%rsp)
	vmovaps	44096(%rsp), %zmm0              # 64-byte Reload
	vmovaps	%zmm0, 56384(%rsp)
	vmovaps	44224(%rsp), %zmm0              # 64-byte Reload
	vmovaps	%zmm0, 56512(%rsp)
	vmovdqa64	%zmm27, 48384(%rsp)     # 64-byte Spill
	vmovdqa64	%zmm27, 56576(%rsp)
	movl	$124, %eax
	.loc	1 0 22 is_stmt 0                # :0:22
.Ltmp6:
	.p2align	4
.LBB0_4:                                # %.preheader
                                        #   Parent Loop BB0_3 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	movq	48200(%rsp,%rax,2), %r10
	movq	48208(%rsp,%rax,2), %r14
	.loc	1 224 20 is_stmt 1              # 03-matrix-multiplication-cpu.py:224:20
	vmovss	(%r10), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 23940(%rsp,%rax)
	vmovss	(%r14), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 23944(%rsp,%rax)
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	movq	48216(%rsp,%rax,2), %r10
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovss	(%r10), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 23948(%rsp,%rax)
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	movq	48224(%rsp,%rax,2), %r10
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovss	(%r10), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 23952(%rsp,%rax)
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	movq	48232(%rsp,%rax,2), %r10
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovss	(%r10), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 23956(%rsp,%rax)
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	movq	48240(%rsp,%rax,2), %r10
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovss	(%r10), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 23960(%rsp,%rax)
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	movq	48248(%rsp,%rax,2), %r10
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovss	(%r10), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 23964(%rsp,%rax)
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	movq	48256(%rsp,%rax,2), %r10
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovss	(%r10), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 23968(%rsp,%rax)
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	movq	48264(%rsp,%rax,2), %r10
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovss	(%r10), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 23972(%rsp,%rax)
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	movq	48272(%rsp,%rax,2), %r10
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovss	(%r10), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 23976(%rsp,%rax)
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	movq	48280(%rsp,%rax,2), %r10
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovss	(%r10), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 23980(%rsp,%rax)
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	movq	48288(%rsp,%rax,2), %r10
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovss	(%r10), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 23984(%rsp,%rax)
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	movq	48296(%rsp,%rax,2), %r10
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovss	(%r10), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 23988(%rsp,%rax)
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	movq	48304(%rsp,%rax,2), %r10
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovss	(%r10), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 23992(%rsp,%rax)
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	movq	48312(%rsp,%rax,2), %r10
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovss	(%r10), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 23996(%rsp,%rax)
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	movq	48320(%rsp,%rax,2), %r10
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovss	(%r10), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 24000(%rsp,%rax)
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	movq	48328(%rsp,%rax,2), %r10
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovss	(%r10), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 24004(%rsp,%rax)
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	movq	48336(%rsp,%rax,2), %r10
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovss	(%r10), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 24008(%rsp,%rax)
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	movq	48344(%rsp,%rax,2), %r10
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovss	(%r10), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 24012(%rsp,%rax)
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	movq	48352(%rsp,%rax,2), %r10
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovss	(%r10), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 24016(%rsp,%rax)
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	movq	48360(%rsp,%rax,2), %r10
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovss	(%r10), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 24020(%rsp,%rax)
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	movq	48368(%rsp,%rax,2), %r10
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovss	(%r10), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 24024(%rsp,%rax)
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	movq	48376(%rsp,%rax,2), %r10
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovss	(%r10), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 24028(%rsp,%rax)
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	movq	48384(%rsp,%rax,2), %r10
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovss	(%r10), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 24032(%rsp,%rax)
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	movq	48392(%rsp,%rax,2), %r10
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovss	(%r10), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 24036(%rsp,%rax)
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	movq	48400(%rsp,%rax,2), %r10
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovss	(%r10), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 24040(%rsp,%rax)
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	movq	48408(%rsp,%rax,2), %r10
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovss	(%r10), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 24044(%rsp,%rax)
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	movq	48416(%rsp,%rax,2), %r10
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovss	(%r10), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 24048(%rsp,%rax)
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	movq	48424(%rsp,%rax,2), %r10
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovss	(%r10), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 24052(%rsp,%rax)
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	movq	48432(%rsp,%rax,2), %r10
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovss	(%r10), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 24056(%rsp,%rax)
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	movq	48440(%rsp,%rax,2), %r10
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovss	(%r10), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 24060(%rsp,%rax)
	.loc	1 208 22                        # 03-matrix-multiplication-cpu.py:208:22
	movq	48448(%rsp,%rax,2), %r10
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovd	(%r10), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	vmovd	%xmm0, 24064(%rsp,%rax)
	subq	$-128, %rax
	cmpq	$4220, %rax                     # imm = 0x107C
	jne	.LBB0_4
# %bb.5:                                #   in Loop: Header=BB0_3 Depth=1
	vmovaps	24192(%rsp), %zmm0
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vmovaps	24064(%rsp), %xmm1
	vunpcklps	%xmm0, %xmm1, %xmm1     # xmm1 = xmm1[0],xmm0[0],xmm1[1],xmm0[1]
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	24320(%rsp), %zmm2
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vmovlhps	%xmm2, %xmm1, %xmm1             # xmm1 = xmm1[0],xmm2[0]
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	24448(%rsp), %zmm6
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vinsertps	$48, %xmm6, %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],xmm6[0]
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	24576(%rsp), %zmm5
	vmovaps	24704(%rsp), %zmm4
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vinsertf128	$1, %xmm5, %ymm1, %ymm7
	vmovaps	.LCPI0_85(%rip), %ymm3          # ymm3 = [0,1,u,u,4,8,u,u]
	vpermt2ps	%ymm4, %ymm3, %ymm7
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	24832(%rsp), %zmm3
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vinsertf128	$1, %xmm3, %ymm1, %ymm1
	vbroadcastss	24068(%rsp), %xmm8
	vshufpd	$2, %ymm1, %ymm7, %ymm10        # ymm10 = ymm7[0],ymm1[1],ymm7[2],ymm1[2]
	vblendps	$2, %xmm0, %xmm8, %xmm1         # xmm1 = xmm8[0],xmm0[1],xmm8[2,3]
	vshufps	$212, %xmm2, %xmm1, %xmm1       # xmm1 = xmm1[0,1],xmm2[1,3]
	vinsertps	$112, %xmm6, %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],xmm6[1]
	vbroadcastss	24580(%rsp), %ymm7
	vblendps	$240, %ymm7, %ymm1, %ymm7       # ymm7 = ymm1[0,1,2,3],ymm7[4,5,6,7]
	vinsertf128	$1, %xmm4, %ymm1, %ymm1
	vblendps	$34, %ymm1, %ymm7, %ymm1        # ymm1 = ymm7[0],ymm1[1],ymm7[2,3,4],ymm1[5],ymm7[6,7]
	vbroadcastss	24836(%rsp), %ymm7
	vblendps	$192, %ymm7, %ymm1, %ymm1       # ymm1 = ymm1[0,1,2,3,4,5],ymm7[6,7]
	vbroadcastss	24964(%rsp), %ymm7
	vblendps	$128, %ymm7, %ymm1, %ymm1       # ymm1 = ymm1[0,1,2,3,4,5,6],ymm7[7]
	vmovaps	%zmm1, 1152(%rsp)               # 64-byte Spill
	vbroadcastss	24072(%rsp), %xmm1
	vinsertps	$156, %xmm0, %xmm1, %xmm1 # xmm1 = xmm1[0],xmm0[2],zero,zero
	vbroadcastsd	24584(%rsp), %ymm7
	vblendps	$240, %ymm7, %ymm1, %ymm1       # ymm1 = ymm1[0,1,2,3],ymm7[4,5,6,7]
	vbroadcastss	24712(%rsp), %ymm7
	vblendps	$32, %ymm7, %ymm1, %ymm1        # ymm1 = ymm1[0,1,2,3,4],ymm7[5],ymm1[6,7]
	vinsertps	$179, %xmm6, %xmm2, %xmm7 # xmm7 = zero,zero,xmm2[2],xmm6[2]
	vinsertf128	$1, %xmm3, %ymm7, %ymm7
	vblendps	$204, %ymm7, %ymm1, %ymm1       # ymm1 = ymm1[0,1],ymm7[2,3],ymm1[4,5],ymm7[6,7]
	vbroadcastss	24968(%rsp), %ymm7
	vblendps	$128, %ymm7, %ymm1, %ymm1       # ymm1 = ymm1[0,1,2,3,4,5,6],ymm7[7]
	vmovaps	%zmm1, 64(%rsp)                 # 64-byte Spill
	vbroadcastss	24100(%rsp), %xmm9
	vmovsd	.LCPI0_182(%rip), %xmm14        # xmm14 = [0,25,0,0]
	vpermt2ps	%zmm0, %zmm14, %zmm9
	vmovaps	%zmm14, %zmm27
	vbroadcastss	24104(%rsp), %xmm12
	vmovsd	.LCPI0_183(%rip), %xmm22        # xmm22 = [0,26,0,0]
	vpermt2ps	%zmm0, %zmm22, %zmm12
	vbroadcastss	24108(%rsp), %xmm13
	vmovsd	.LCPI0_184(%rip), %xmm30        # xmm30 = [0,27,0,0]
	vpermt2ps	%zmm0, %zmm30, %zmm13
	vbroadcastss	24112(%rsp), %xmm15
	vmovsd	.LCPI0_185(%rip), %xmm1         # xmm1 = [0,28,0,0]
	vpermt2ps	%zmm0, %zmm1, %zmm15
	vbroadcastss	24116(%rsp), %xmm8
	vmovsd	.LCPI0_186(%rip), %xmm1         # xmm1 = [0,29,0,0]
	vpermt2ps	%zmm0, %zmm1, %zmm8
	vbroadcastss	24120(%rsp), %xmm14
	vmovsd	.LCPI0_187(%rip), %xmm1         # xmm1 = [0,30,0,0]
	vpermt2ps	%zmm0, %zmm1, %zmm14
	vbroadcastss	24124(%rsp), %xmm18
	vmovsd	.LCPI0_188(%rip), %xmm1         # xmm1 = [0,31,0,0]
	vpermt2ps	%zmm0, %zmm1, %zmm18
	vbroadcastss	24096(%rsp), %xmm7
	vmovsd	.LCPI0_189(%rip), %xmm1         # xmm1 = [0,24,0,0]
	vpermt2ps	%zmm0, %zmm1, %zmm7
	vbroadcastss	24076(%rsp), %xmm1
	vinsertps	$220, %xmm0, %xmm1, %xmm0 # xmm0 = xmm1[0],xmm0[3],zero,zero
	vmovaps	.LCPI0_4(%rip), %xmm16          # xmm16 = [0,1,25,u]
	vpermt2ps	%zmm2, %zmm16, %zmm9
	vmovaps	.LCPI0_5(%rip), %xmm17          # xmm17 = [0,1,2,25]
	vpermt2ps	%zmm6, %zmm17, %zmm9
	vmovaps	%zmm9, %zmm11
	vmovapd	.LCPI0_12(%rip), %xmm23         # xmm23 = [0,0,0,0,0,0,0,0,13,0,0,0,0,0,0,0]
	vpermt2pd	%zmm2, %zmm23, %zmm12
	vmovaps	.LCPI0_13(%rip), %xmm24         # xmm24 = [0,1,2,26]
	vpermt2ps	%zmm6, %zmm24, %zmm12
	vmovaps	.LCPI0_21(%rip), %xmm31         # xmm31 = [0,1,27,u]
	vpermt2ps	%zmm2, %zmm31, %zmm13
	vmovaps	.LCPI0_22(%rip), %xmm1          # xmm1 = [0,1,2,27]
	vpermt2ps	%zmm6, %zmm1, %zmm13
	vmovapd	.LCPI0_31(%rip), %xmm1          # xmm1 = [0,0,0,0,0,0,0,0,14,0,0,0,0,0,0,0]
	vpermt2pd	%zmm2, %zmm1, %zmm15
	vmovaps	.LCPI0_32(%rip), %xmm16         # xmm16 = [0,1,2,28]
	vpermt2ps	%zmm6, %zmm16, %zmm15
	vmovaps	.LCPI0_41(%rip), %xmm1          # xmm1 = [0,1,29,u]
	vpermt2ps	%zmm2, %zmm1, %zmm8
	vmovaps	.LCPI0_42(%rip), %xmm1          # xmm1 = [0,1,2,29]
	vpermt2ps	%zmm6, %zmm1, %zmm8
	vmovaps	%zmm8, %zmm9
	vmovapd	.LCPI0_53(%rip), %xmm1          # xmm1 = [0,0,0,0,0,0,0,0,15,0,0,0,0,0,0,0]
	vpermt2pd	%zmm2, %zmm1, %zmm14
	vmovaps	.LCPI0_54(%rip), %xmm1          # xmm1 = [0,1,2,30]
	vpermt2ps	%zmm6, %zmm1, %zmm14
	vmovaps	.LCPI0_66(%rip), %xmm1          # xmm1 = [0,1,31,u]
	vpermt2ps	%zmm2, %zmm1, %zmm18
	vmovaps	.LCPI0_67(%rip), %xmm1          # xmm1 = [0,1,2,31]
	vpermt2ps	%zmm6, %zmm1, %zmm18
	vmovaps	%zmm18, %zmm17
	vmovapd	.LCPI0_80(%rip), %xmm1          # xmm1 = [0,0,0,0,0,0,0,0,12,0,0,0,0,0,0,0]
	vpermt2pd	%zmm2, %zmm1, %zmm7
	vmovaps	.LCPI0_81(%rip), %xmm24         # xmm24 = [0,1,2,24]
	vpermt2ps	%zmm6, %zmm24, %zmm7
	vmovaps	%zmm7, %zmm8
	vshufps	$244, %xmm2, %xmm0, %xmm0       # xmm0 = xmm0[0,1],xmm2[3,3]
	vblendps	$8, %xmm6, %xmm0, %xmm1         # xmm1 = xmm0[0,1,2],xmm6[3]
	vbroadcastss	24588(%rsp), %ymm6
	vblendps	$240, %ymm6, %ymm0, %ymm0       # ymm0 = ymm0[0,1,2,3],ymm6[4,5,6,7]
	vbroadcastss	24716(%rsp), %ymm6
	vblendps	$32, %ymm6, %ymm0, %ymm0        # ymm0 = ymm0[0,1,2,3,4],ymm6[5],ymm0[6,7]
	vbroadcastss	24844(%rsp), %ymm6
	vblendps	$192, %ymm6, %ymm0, %ymm6       # ymm6 = ymm0[0,1,2,3,4,5],ymm6[6,7]
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	24960(%rsp), %zmm0
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vinsertf128	$1, %xmm0, %ymm1, %ymm1
	vblendps	$136, %ymm1, %ymm6, %ymm1       # ymm1 = ymm6[0,1,2],ymm1[3],ymm6[4,5,6],ymm1[7]
	vmovaps	%zmm1, 8832(%rsp)               # 64-byte Spill
	vbroadcastss	24080(%rsp), %xmm1
	vunpcklps	24208(%rsp), %xmm1, %xmm6 # xmm6 = xmm1[0],mem[0],xmm1[1],mem[1]
	vmovaps	24336(%rsp), %xmm1
	vbroadcastss	24464(%rsp), %xmm7
	vmovlhps	%xmm1, %xmm6, %xmm6             # xmm6 = xmm6[0],xmm1[0]
	vblendps	$8, %xmm7, %xmm6, %xmm6         # xmm6 = xmm6[0,1,2],xmm7[3]
	vmovaps	.LCPI0_6(%rip), %ymm18          # ymm18 = [0,1,2,3,25,u,u,u]
	vpermt2ps	%zmm5, %zmm18, %zmm11
	vmovapd	.LCPI0_14(%rip), %ymm25         # ymm25 = [0,1,13,u]
	vpermt2pd	%zmm5, %zmm25, %zmm12
	vmovaps	.LCPI0_23(%rip), %ymm7          # ymm7 = [0,1,2,3,27,u,u,u]
	vpermt2ps	%zmm5, %zmm7, %zmm13
	vmovapd	.LCPI0_33(%rip), %ymm7          # ymm7 = [0,1,14,u]
	vpermt2pd	%zmm5, %zmm7, %zmm15
	vmovaps	.LCPI0_43(%rip), %ymm7          # ymm7 = [0,1,2,3,29,u,u,u]
	vpermt2ps	%zmm5, %zmm7, %zmm9
	vmovapd	.LCPI0_55(%rip), %ymm7          # ymm7 = [0,1,15,u]
	vpermt2pd	%zmm5, %zmm7, %zmm14
	vmovaps	.LCPI0_68(%rip), %ymm7          # ymm7 = [0,1,2,3,31,u,u,u]
	vpermt2ps	%zmm5, %zmm7, %zmm17
	vmovapd	.LCPI0_82(%rip), %ymm7          # ymm7 = [0,1,12,u]
	vpermt2pd	%zmm5, %zmm7, %zmm8
	vbroadcastss	24720(%rsp), %ymm7
	vblendps	$240, %ymm5, %ymm6, %ymm5       # ymm5 = ymm6[0,1,2,3],ymm5[4,5,6,7]
	vblendps	$32, %ymm7, %ymm5, %ymm5        # ymm5 = ymm5[0,1,2,3,4],ymm7[5],ymm5[6,7]
	vbroadcastsd	24848(%rsp), %ymm6
	vbroadcastss	24976(%rsp), %ymm7
	vblendps	$192, %ymm6, %ymm5, %ymm5       # ymm5 = ymm5[0,1,2,3,4,5],ymm6[6,7]
	vblendps	$128, %ymm7, %ymm5, %ymm5       # ymm5 = ymm5[0,1,2,3,4,5,6],ymm7[7]
	vmovaps	%zmm5, 1344(%rsp)               # 64-byte Spill
	vbroadcastss	24212(%rsp), %xmm5
	vbroadcastss	24084(%rsp), %xmm6
	vblendps	$2, %xmm5, %xmm6, %xmm5         # xmm5 = xmm6[0],xmm5[1],xmm6[2,3]
	vmovaps	%ymm2, %ymm6
	vmovaps	19904(%rsp), %ymm22             # 32-byte Reload
	vpermt2ps	%ymm5, %ymm22, %ymm6
	vbroadcastss	24468(%rsp), %xmm5
	vbroadcastss	24596(%rsp), %ymm7
	vblendps	$8, %xmm5, %xmm6, %xmm5         # xmm5 = xmm6[0,1,2],xmm5[3]
	vblendps	$240, %ymm7, %ymm5, %ymm5       # ymm5 = ymm5[0,1,2,3],ymm7[4,5,6,7]
	vmovaps	.LCPI0_7(%rip), %ymm19          # ymm19 = [0,1,2,3,4,25,u,u]
	vpermt2ps	%zmm4, %zmm19, %zmm11
	vmovaps	.LCPI0_15(%rip), %ymm26         # ymm26 = [0,1,2,3,4,26,u,u]
	vpermt2ps	%zmm4, %zmm26, %zmm12
	vmovaps	.LCPI0_24(%rip), %ymm6          # ymm6 = [0,1,2,3,4,27,u,u]
	vpermt2ps	%zmm4, %zmm6, %zmm13
	vmovaps	.LCPI0_34(%rip), %ymm6          # ymm6 = [0,1,2,3,4,28,u,u]
	vpermt2ps	%zmm4, %zmm6, %zmm15
	vmovaps	.LCPI0_44(%rip), %ymm6          # ymm6 = [0,1,2,3,4,29,u,u]
	vpermt2ps	%zmm4, %zmm6, %zmm9
	vmovaps	.LCPI0_56(%rip), %ymm6          # ymm6 = [0,1,2,3,4,30,u,u]
	vpermt2ps	%zmm4, %zmm6, %zmm14
	vmovaps	.LCPI0_69(%rip), %ymm6          # ymm6 = [0,1,2,3,4,31,u,u]
	vpermt2ps	%zmm4, %zmm6, %zmm17
	vmovaps	.LCPI0_83(%rip), %ymm6          # ymm6 = [0,1,2,3,4,24,u,u]
	vpermt2ps	%zmm4, %zmm6, %zmm8
	vbroadcastss	24852(%rsp), %ymm6
	vblendps	$32, %ymm4, %ymm5, %ymm4        # ymm4 = ymm5[0,1,2,3,4],ymm4[5],ymm5[6,7]
	vblendps	$192, %ymm6, %ymm4, %ymm4       # ymm4 = ymm4[0,1,2,3,4,5],ymm6[6,7]
	vbroadcastss	24980(%rsp), %ymm5
	vbroadcastss	24216(%rsp), %xmm6
	vblendps	$128, %ymm5, %ymm4, %ymm4       # ymm4 = ymm4[0,1,2,3,4,5,6],ymm5[7]
	vmovaps	%zmm4, 1600(%rsp)               # 64-byte Spill
	vbroadcastss	24088(%rsp), %xmm4
	vblendps	$2, %xmm6, %xmm4, %xmm4         # xmm4 = xmm4[0],xmm6[1],xmm4[2,3]
	vbroadcastss	24472(%rsp), %xmm5
	vblendps	$3, %xmm4, %xmm1, %xmm1         # xmm1 = xmm4[0,1],xmm1[2,3]
	vblendps	$8, %xmm5, %xmm1, %xmm1         # xmm1 = xmm1[0,1,2],xmm5[3]
	vbroadcastsd	24600(%rsp), %ymm4
	vbroadcastss	24728(%rsp), %ymm5
	vblendps	$240, %ymm4, %ymm1, %ymm1       # ymm1 = ymm1[0,1,2,3],ymm4[4,5,6,7]
	vblendps	$32, %ymm5, %ymm1, %ymm1        # ymm1 = ymm1[0,1,2,3,4],ymm5[5],ymm1[6,7]
	vmovaps	.LCPI0_8(%rip), %ymm20          # ymm20 = [0,1,2,3,4,5,25,u]
	vpermt2ps	%zmm3, %zmm20, %zmm11
	vmovapd	.LCPI0_16(%rip), %ymm28         # ymm28 = [0,1,2,13]
	vpermt2pd	%zmm3, %zmm28, %zmm12
	vmovaps	.LCPI0_25(%rip), %ymm18         # ymm18 = [0,1,2,3,4,5,27,u]
	vpermt2ps	%zmm3, %zmm18, %zmm13
	vmovapd	.LCPI0_35(%rip), %ymm25         # ymm25 = [0,1,2,14]
	vpermt2pd	%zmm3, %zmm25, %zmm15
	vmovaps	.LCPI0_45(%rip), %ymm4          # ymm4 = [0,1,2,3,4,5,29,u]
	vpermt2ps	%zmm3, %zmm4, %zmm9
	vmovapd	.LCPI0_57(%rip), %ymm31         # ymm31 = [0,1,2,15]
	vpermt2pd	%zmm3, %zmm31, %zmm14
	vmovaps	.LCPI0_70(%rip), %ymm30         # ymm30 = [0,1,2,3,4,5,31,u]
	vpermt2ps	%zmm3, %zmm30, %zmm17
	vmovaps	%zmm17, %zmm5
	vmovapd	.LCPI0_84(%rip), %ymm23         # ymm23 = [0,1,2,12]
	vpermt2pd	%zmm3, %zmm23, %zmm8
	vbroadcastss	24984(%rsp), %ymm4
	vblendps	$192, %ymm3, %ymm1, %ymm1       # ymm1 = ymm1[0,1,2,3,4,5],ymm3[6,7]
	vblendps	$128, %ymm4, %ymm1, %ymm1       # ymm1 = ymm1[0,1,2,3,4,5,6],ymm4[7]
	vmovaps	%zmm1, 1216(%rsp)               # 64-byte Spill
	vbroadcastss	24220(%rsp), %xmm1
	vbroadcastss	24092(%rsp), %xmm3
	vblendps	$2, %xmm1, %xmm3, %xmm1         # xmm1 = xmm3[0],xmm1[1],xmm3[2,3]
	vmovaps	19936(%rsp), %ymm17             # 32-byte Reload
	vpermt2ps	%ymm1, %ymm17, %ymm2
	vbroadcastss	24476(%rsp), %xmm1
	vbroadcastss	24604(%rsp), %ymm3
	vblendps	$8, %xmm1, %xmm2, %xmm1         # xmm1 = xmm2[0,1,2],xmm1[3]
	vblendps	$240, %ymm3, %ymm1, %ymm1       # ymm1 = ymm1[0,1,2,3],ymm3[4,5,6,7]
	vbroadcastss	24732(%rsp), %ymm2
	vbroadcastss	24860(%rsp), %ymm3
	vblendps	$32, %ymm2, %ymm1, %ymm1        # ymm1 = ymm1[0,1,2,3,4],ymm2[5],ymm1[6,7]
	vblendps	$192, %ymm3, %ymm1, %ymm1       # ymm1 = ymm1[0,1,2,3,4,5],ymm3[6,7]
	vmovaps	.LCPI0_9(%rip), %ymm21          # ymm21 = [0,1,2,3,4,5,6,25]
	vpermt2ps	%zmm0, %zmm21, %zmm11
	vmovaps	%zmm11, 512(%rsp)               # 64-byte Spill
	vmovaps	.LCPI0_17(%rip), %ymm29         # ymm29 = [0,1,2,3,4,5,6,26]
	vpermt2ps	%zmm0, %zmm29, %zmm12
	vmovaps	%zmm12, 704(%rsp)               # 64-byte Spill
	vmovaps	.LCPI0_26(%rip), %ymm18         # ymm18 = [0,1,2,3,4,5,6,27]
	vpermt2ps	%zmm0, %zmm18, %zmm13
	vmovaps	%zmm13, 1408(%rsp)              # 64-byte Spill
	vmovaps	.LCPI0_36(%rip), %ymm19         # ymm19 = [0,1,2,3,4,5,6,28]
	vpermt2ps	%zmm0, %zmm19, %zmm15
	vmovaps	%zmm15, 768(%rsp)               # 64-byte Spill
	vmovaps	.LCPI0_46(%rip), %ymm20         # ymm20 = [0,1,2,3,4,5,6,29]
	vpermt2ps	%zmm0, %zmm20, %zmm9
	vmovaps	%zmm9, 2560(%rsp)               # 64-byte Spill
	vmovaps	.LCPI0_58(%rip), %ymm25         # ymm25 = [0,1,2,3,4,5,6,30]
	vpermt2ps	%zmm0, %zmm25, %zmm14
	vmovaps	%zmm14, 1024(%rsp)              # 64-byte Spill
	vmovaps	.LCPI0_71(%rip), %ymm26         # ymm26 = [0,1,2,3,4,5,6,31]
	vpermt2ps	%zmm0, %zmm26, %zmm5
	vmovaps	%zmm5, 960(%rsp)                # 64-byte Spill
	vmovaps	.LCPI0_86(%rip), %ymm18         # ymm18 = [0,1,2,3,4,5,6,8]
	vpermt2ps	%ymm0, %ymm18, %ymm10
	vmovaps	%zmm10, 18624(%rsp)             # 64-byte Spill
	vblendps	$128, %ymm0, %ymm1, %ymm1       # ymm1 = ymm1[0,1,2,3,4,5,6],ymm0[7]
	vmovaps	%zmm1, 1280(%rsp)               # 64-byte Spill
	vmovaps	.LCPI0_89(%rip), %ymm28         # ymm28 = [0,1,2,3,4,5,6,24]
	vpermt2ps	%zmm0, %zmm28, %zmm8
	vmovaps	%zmm8, 37760(%rsp)              # 64-byte Spill
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	24256(%rsp), %zmm1
	vmovaps	24384(%rsp), %zmm13
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vbroadcastss	24132(%rsp), %xmm2
	vblendps	$2, %xmm1, %xmm2, %xmm2         # xmm2 = xmm2[0],xmm1[1],xmm2[2,3]
	vshufps	$212, %xmm13, %xmm2, %xmm2      # xmm2 = xmm2[0,1],xmm13[1,3]
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	24512(%rsp), %zmm4
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vbroadcastss	24644(%rsp), %ymm3
	vinsertps	$112, %xmm4, %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],xmm4[1]
	vblendps	$240, %ymm3, %ymm2, %ymm5       # ymm5 = ymm2[0,1,2,3],ymm3[4,5,6,7]
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	24768(%rsp), %zmm3
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vinsertf128	$1, %xmm3, %ymm2, %ymm2
	vblendps	$34, %ymm2, %ymm5, %ymm2        # ymm2 = ymm5[0],ymm2[1],ymm5[2,3,4],ymm2[5],ymm5[6,7]
	vbroadcastss	24900(%rsp), %ymm5
	vblendps	$192, %ymm5, %ymm2, %ymm2       # ymm2 = ymm2[0,1,2,3,4,5],ymm5[6,7]
	vbroadcastss	25028(%rsp), %ymm5
	vblendps	$128, %ymm5, %ymm2, %ymm0       # ymm0 = ymm2[0,1,2,3,4,5,6],ymm5[7]
	vmovaps	%zmm0, 192(%rsp)                # 64-byte Spill
	vbroadcastss	24136(%rsp), %xmm2
	vinsertps	$156, %xmm1, %xmm2, %xmm2 # xmm2 = xmm2[0],xmm1[2],zero,zero
	vbroadcastsd	24648(%rsp), %ymm5
	vblendps	$240, %ymm5, %ymm2, %ymm2       # ymm2 = ymm2[0,1,2,3],ymm5[4,5,6,7]
	vbroadcastss	24776(%rsp), %ymm5
	vblendps	$32, %ymm5, %ymm2, %ymm5        # ymm5 = ymm2[0,1,2,3,4],ymm5[5],ymm2[6,7]
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	24896(%rsp), %zmm2
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vinsertps	$179, %xmm4, %xmm13, %xmm6 # xmm6 = zero,zero,xmm13[2],xmm4[2]
	vinsertf128	$1, %xmm2, %ymm6, %ymm6
	vblendps	$204, %ymm6, %ymm5, %ymm5       # ymm5 = ymm5[0,1],ymm6[2,3],ymm5[4,5],ymm6[6,7]
	vbroadcastss	24164(%rsp), %xmm15
	vbroadcastss	25032(%rsp), %ymm6
	vblendps	$128, %ymm6, %ymm5, %ymm0       # ymm0 = ymm5[0,1,2,3,4,5,6],ymm6[7]
	vmovaps	%zmm0, 640(%rsp)                # 64-byte Spill
	vpermt2ps	%zmm1, %zmm27, %zmm15
	vbroadcastss	24168(%rsp), %xmm14
	vbroadcastss	24172(%rsp), %xmm9
	vmovsd	.LCPI0_183(%rip), %xmm0         # xmm0 = [0,26,0,0]
	vpermt2ps	%zmm1, %zmm0, %zmm14
	vmovsd	.LCPI0_184(%rip), %xmm0         # xmm0 = [0,27,0,0]
	vpermt2ps	%zmm1, %zmm0, %zmm9
	vbroadcastss	24176(%rsp), %xmm12
	vbroadcastss	24180(%rsp), %xmm8
	vmovsd	.LCPI0_185(%rip), %xmm0         # xmm0 = [0,28,0,0]
	vpermt2ps	%zmm1, %zmm0, %zmm12
	vmovsd	.LCPI0_186(%rip), %xmm0         # xmm0 = [0,29,0,0]
	vpermt2ps	%zmm1, %zmm0, %zmm8
	vbroadcastss	24184(%rsp), %xmm0
	vbroadcastss	24188(%rsp), %xmm27
	vmovsd	.LCPI0_187(%rip), %xmm5         # xmm5 = [0,30,0,0]
	vpermt2ps	%zmm1, %zmm5, %zmm0
	vmovsd	.LCPI0_188(%rip), %xmm5         # xmm5 = [0,31,0,0]
	vpermt2ps	%zmm1, %zmm5, %zmm27
	vbroadcastss	24160(%rsp), %xmm7
	vmovaps	24128(%rsp), %xmm5
	vmovsd	.LCPI0_189(%rip), %xmm6         # xmm6 = [0,24,0,0]
	vpermt2ps	%zmm1, %zmm6, %zmm7
	vunpcklps	%xmm1, %xmm5, %xmm5     # xmm5 = xmm5[0],xmm1[0],xmm5[1],xmm1[1]
	vbroadcastss	24140(%rsp), %xmm6
	vinsertps	$220, %xmm1, %xmm6, %xmm1 # xmm1 = xmm6[0],xmm1[3],zero,zero
	vmovaps	.LCPI0_4(%rip), %xmm6           # xmm6 = [0,1,25,u]
	vpermt2ps	%zmm13, %zmm6, %zmm15
	vmovaps	.LCPI0_5(%rip), %xmm6           # xmm6 = [0,1,2,25]
	vpermt2ps	%zmm4, %zmm6, %zmm15
	vmovapd	.LCPI0_12(%rip), %xmm6          # xmm6 = [0,0,0,0,0,0,0,0,13,0,0,0,0,0,0,0]
	vmovaps	%zmm13, %zmm10
	vpermt2pd	%zmm13, %zmm6, %zmm14
	vmovaps	.LCPI0_13(%rip), %xmm6          # xmm6 = [0,1,2,26]
	vpermt2ps	%zmm4, %zmm6, %zmm14
	vmovaps	%zmm9, %zmm13
	vmovaps	.LCPI0_21(%rip), %xmm6          # xmm6 = [0,1,27,u]
	vpermt2ps	%zmm10, %zmm6, %zmm13
	vmovaps	.LCPI0_22(%rip), %xmm6          # xmm6 = [0,1,2,27]
	vpermt2ps	%zmm4, %zmm6, %zmm13
	vmovapd	.LCPI0_31(%rip), %xmm6          # xmm6 = [0,0,0,0,0,0,0,0,14,0,0,0,0,0,0,0]
	vpermt2pd	%zmm10, %zmm6, %zmm12
	vpermt2ps	%zmm4, %zmm16, %zmm12
	vmovaps	%zmm12, %zmm16
	vmovaps	.LCPI0_41(%rip), %xmm6          # xmm6 = [0,1,29,u]
	vpermt2ps	%zmm10, %zmm6, %zmm8
	vmovaps	.LCPI0_42(%rip), %xmm6          # xmm6 = [0,1,2,29]
	vpermt2ps	%zmm4, %zmm6, %zmm8
	vmovaps	%zmm8, %zmm12
	vmovapd	.LCPI0_53(%rip), %xmm6          # xmm6 = [0,0,0,0,0,0,0,0,15,0,0,0,0,0,0,0]
	vpermt2pd	%zmm10, %zmm6, %zmm0
	vmovaps	.LCPI0_54(%rip), %xmm6          # xmm6 = [0,1,2,30]
	vpermt2ps	%zmm4, %zmm6, %zmm0
	vmovaps	.LCPI0_66(%rip), %xmm6          # xmm6 = [0,1,31,u]
	vpermt2ps	%zmm10, %zmm6, %zmm27
	vmovaps	.LCPI0_67(%rip), %xmm6          # xmm6 = [0,1,2,31]
	vpermt2ps	%zmm4, %zmm6, %zmm27
	vmovapd	.LCPI0_80(%rip), %xmm6          # xmm6 = [0,0,0,0,0,0,0,0,12,0,0,0,0,0,0,0]
	vpermt2pd	%zmm10, %zmm6, %zmm7
	vpermt2ps	%zmm4, %zmm24, %zmm7
	vmovaps	%zmm7, %zmm11
	vmovlhps	%xmm10, %xmm5, %xmm5            # xmm5 = xmm5[0],xmm10[0]
	vinsertps	$48, %xmm4, %xmm5, %xmm24 # xmm24 = xmm5[0,1,2],xmm4[0]
	vshufps	$244, %xmm10, %xmm1, %xmm1      # xmm1 = xmm1[0,1],xmm10[3,3]
	vmovaps	%zmm10, %zmm5
	vblendps	$8, %xmm4, %xmm1, %xmm6         # xmm6 = xmm1[0,1,2],xmm4[3]
	vbroadcastss	24652(%rsp), %ymm4
	vblendps	$240, %ymm4, %ymm1, %ymm1       # ymm1 = ymm1[0,1,2,3],ymm4[4,5,6,7]
	vbroadcastss	24780(%rsp), %ymm4
	vblendps	$32, %ymm4, %ymm1, %ymm1        # ymm1 = ymm1[0,1,2,3,4],ymm4[5],ymm1[6,7]
	vbroadcastss	24908(%rsp), %ymm4
	vblendps	$192, %ymm4, %ymm1, %ymm1       # ymm1 = ymm1[0,1,2,3,4,5],ymm4[6,7]
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	25024(%rsp), %zmm4
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vinsertf128	$1, %xmm4, %ymm6, %ymm6
	vblendps	$136, %ymm6, %ymm1, %ymm1       # ymm1 = ymm1[0,1,2],ymm6[3],ymm1[4,5,6],ymm6[7]
	vmovaps	%zmm1, 9024(%rsp)               # 64-byte Spill
	vbroadcastss	24144(%rsp), %xmm1
	vunpcklps	24272(%rsp), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[1],mem[1]
	vmovaps	24400(%rsp), %xmm6
	vmovlhps	%xmm6, %xmm1, %xmm1             # xmm1 = xmm1[0],xmm6[0]
	vbroadcastss	24528(%rsp), %xmm7
	vblendps	$8, %xmm7, %xmm1, %xmm7         # xmm7 = xmm1[0,1,2],xmm7[3]
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	24640(%rsp), %zmm8
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vmovaps	.LCPI0_6(%rip), %ymm1           # ymm1 = [0,1,2,3,25,u,u,u]
	vpermt2ps	%zmm8, %zmm1, %zmm15
	vmovapd	.LCPI0_14(%rip), %ymm1          # ymm1 = [0,1,13,u]
	vpermt2pd	%zmm8, %zmm1, %zmm14
	vmovaps	.LCPI0_23(%rip), %ymm1          # ymm1 = [0,1,2,3,27,u,u,u]
	vpermt2ps	%zmm8, %zmm1, %zmm13
	vmovapd	.LCPI0_33(%rip), %ymm1          # ymm1 = [0,1,14,u]
	vpermt2pd	%zmm8, %zmm1, %zmm16
	vmovaps	.LCPI0_43(%rip), %ymm1          # ymm1 = [0,1,2,3,29,u,u,u]
	vpermt2ps	%zmm8, %zmm1, %zmm12
	vmovapd	.LCPI0_55(%rip), %ymm1          # ymm1 = [0,1,15,u]
	vpermt2pd	%zmm8, %zmm1, %zmm0
	vmovaps	.LCPI0_68(%rip), %ymm1          # ymm1 = [0,1,2,3,31,u,u,u]
	vpermt2ps	%zmm8, %zmm1, %zmm27
	vmovapd	.LCPI0_82(%rip), %ymm1          # ymm1 = [0,1,12,u]
	vpermt2pd	%zmm8, %zmm1, %zmm11
	vinsertf32x4	$1, %xmm8, %ymm24, %ymm1
	vblendps	$240, %ymm8, %ymm7, %ymm7       # ymm7 = ymm7[0,1,2,3],ymm8[4,5,6,7]
	vbroadcastss	24784(%rsp), %ymm8
	vblendps	$32, %ymm8, %ymm7, %ymm7        # ymm7 = ymm7[0,1,2,3,4],ymm8[5],ymm7[6,7]
	vbroadcastsd	24912(%rsp), %ymm8
	vblendps	$192, %ymm8, %ymm7, %ymm7       # ymm7 = ymm7[0,1,2,3,4,5],ymm8[6,7]
	vbroadcastss	25040(%rsp), %ymm8
	vbroadcastss	24276(%rsp), %xmm9
	vbroadcastss	24148(%rsp), %xmm10
	vblendps	$128, %ymm8, %ymm7, %ymm7       # ymm7 = ymm7[0,1,2,3,4,5,6],ymm8[7]
	vmovaps	%zmm7, 896(%rsp)                # 64-byte Spill
	vblendps	$2, %xmm9, %xmm10, %xmm7        # xmm7 = xmm10[0],xmm9[1],xmm10[2,3]
	vmovaps	%zmm5, %zmm9
	vmovaps	%ymm9, %ymm8
	vpermt2ps	%ymm7, %ymm22, %ymm8
	vbroadcastss	24532(%rsp), %xmm7
	vblendps	$8, %xmm7, %xmm8, %xmm7         # xmm7 = xmm8[0,1,2],xmm7[3]
	vbroadcastss	24660(%rsp), %ymm8
	vblendps	$240, %ymm8, %ymm7, %ymm7       # ymm7 = ymm7[0,1,2,3],ymm8[4,5,6,7]
	vmovaps	.LCPI0_7(%rip), %ymm8           # ymm8 = [0,1,2,3,4,25,u,u]
	vpermt2ps	%zmm3, %zmm8, %zmm15
	vmovaps	.LCPI0_15(%rip), %ymm8          # ymm8 = [0,1,2,3,4,26,u,u]
	vpermt2ps	%zmm3, %zmm8, %zmm14
	vmovaps	.LCPI0_24(%rip), %ymm8          # ymm8 = [0,1,2,3,4,27,u,u]
	vpermt2ps	%zmm3, %zmm8, %zmm13
	vmovaps	.LCPI0_34(%rip), %ymm8          # ymm8 = [0,1,2,3,4,28,u,u]
	vpermt2ps	%zmm3, %zmm8, %zmm16
	vmovaps	.LCPI0_44(%rip), %ymm8          # ymm8 = [0,1,2,3,4,29,u,u]
	vpermt2ps	%zmm3, %zmm8, %zmm12
	vmovaps	.LCPI0_56(%rip), %ymm8          # ymm8 = [0,1,2,3,4,30,u,u]
	vpermt2ps	%zmm3, %zmm8, %zmm0
	vmovaps	.LCPI0_69(%rip), %ymm8          # ymm8 = [0,1,2,3,4,31,u,u]
	vpermt2ps	%zmm3, %zmm8, %zmm27
	vmovaps	.LCPI0_83(%rip), %ymm8          # ymm8 = [0,1,2,3,4,24,u,u]
	vpermt2ps	%zmm3, %zmm8, %zmm11
	vmovaps	.LCPI0_85(%rip), %ymm8          # ymm8 = [0,1,u,u,4,8,u,u]
	vpermt2ps	%ymm3, %ymm8, %ymm1
	vblendps	$32, %ymm3, %ymm7, %ymm3        # ymm3 = ymm7[0,1,2,3,4],ymm3[5],ymm7[6,7]
	vbroadcastss	24916(%rsp), %ymm7
	vblendps	$192, %ymm7, %ymm3, %ymm3       # ymm3 = ymm3[0,1,2,3,4,5],ymm7[6,7]
	vbroadcastss	25044(%rsp), %ymm7
	vblendps	$128, %ymm7, %ymm3, %ymm3       # ymm3 = ymm3[0,1,2,3,4,5,6],ymm7[7]
	vmovaps	%zmm3, 8960(%rsp)               # 64-byte Spill
	vbroadcastss	24280(%rsp), %xmm3
	vbroadcastss	24152(%rsp), %xmm7
	vblendps	$2, %xmm3, %xmm7, %xmm3         # xmm3 = xmm7[0],xmm3[1],xmm7[2,3]
	vblendps	$3, %xmm3, %xmm6, %xmm3         # xmm3 = xmm3[0,1],xmm6[2,3]
	vbroadcastss	24536(%rsp), %xmm6
	vblendps	$8, %xmm6, %xmm3, %xmm3         # xmm3 = xmm3[0,1,2],xmm6[3]
	vbroadcastsd	24664(%rsp), %ymm6
	vblendps	$240, %ymm6, %ymm3, %ymm3       # ymm3 = ymm3[0,1,2,3],ymm6[4,5,6,7]
	vbroadcastss	24792(%rsp), %ymm6
	vblendps	$32, %ymm6, %ymm3, %ymm3        # ymm3 = ymm3[0,1,2,3,4],ymm6[5],ymm3[6,7]
	vmovaps	.LCPI0_8(%rip), %ymm6           # ymm6 = [0,1,2,3,4,5,25,u]
	vpermt2ps	%zmm2, %zmm6, %zmm15
	vmovapd	.LCPI0_16(%rip), %ymm22         # ymm22 = [0,1,2,13]
	vpermt2pd	%zmm2, %zmm22, %zmm14
	vmovaps	.LCPI0_25(%rip), %ymm6          # ymm6 = [0,1,2,3,4,5,27,u]
	vpermt2ps	%zmm2, %zmm6, %zmm13
	vmovapd	.LCPI0_35(%rip), %ymm6          # ymm6 = [0,1,2,14]
	vpermt2pd	%zmm2, %zmm6, %zmm16
	vmovaps	.LCPI0_45(%rip), %ymm6          # ymm6 = [0,1,2,3,4,5,29,u]
	vpermt2ps	%zmm2, %zmm6, %zmm12
	vpermt2pd	%zmm2, %zmm31, %zmm0
	vmovapd	%zmm0, %zmm7
	vpermt2ps	%zmm2, %zmm30, %zmm27
	vpermt2pd	%zmm2, %zmm23, %zmm11
	vinsertf32x4	$1, %xmm2, %ymm24, %ymm5
	vblendps	$192, %ymm2, %ymm3, %ymm2       # ymm2 = ymm3[0,1,2,3,4,5],ymm2[6,7]
	vbroadcastss	25048(%rsp), %ymm3
	vbroadcastss	24284(%rsp), %xmm6
	vblendps	$128, %ymm3, %ymm2, %ymm0       # ymm0 = ymm2[0,1,2,3,4,5,6],ymm3[7]
	vmovaps	%zmm0, 8448(%rsp)               # 64-byte Spill
	vbroadcastss	24156(%rsp), %xmm2
	vblendps	$2, %xmm6, %xmm2, %xmm2         # xmm2 = xmm2[0],xmm6[1],xmm2[2,3]
	vshufpd	$2, %ymm5, %ymm1, %ymm3         # ymm3 = ymm1[0],ymm5[1],ymm1[2],ymm5[2]
	vmovaps	%zmm9, %zmm0
	vpermt2ps	%ymm2, %ymm17, %ymm0
	vbroadcastss	24540(%rsp), %xmm1
	vblendps	$8, %xmm1, %xmm0, %xmm0         # xmm0 = xmm0[0,1,2],xmm1[3]
	vbroadcastss	24668(%rsp), %ymm1
	vblendps	$240, %ymm1, %ymm0, %ymm0       # ymm0 = ymm0[0,1,2,3],ymm1[4,5,6,7]
	vbroadcastss	24796(%rsp), %ymm1
	vblendps	$32, %ymm1, %ymm0, %ymm0        # ymm0 = ymm0[0,1,2,3,4],ymm1[5],ymm0[6,7]
	vbroadcastss	24924(%rsp), %ymm1
	vblendps	$192, %ymm1, %ymm0, %ymm0       # ymm0 = ymm0[0,1,2,3,4,5],ymm1[6,7]
	vpermt2ps	%zmm4, %zmm21, %zmm15
	vmovaps	%zmm15, 1472(%rsp)              # 64-byte Spill
	vpermt2ps	%zmm4, %zmm29, %zmm14
	vmovaps	%zmm14, 2496(%rsp)              # 64-byte Spill
	vmovaps	.LCPI0_26(%rip), %ymm1          # ymm1 = [0,1,2,3,4,5,6,27]
	vpermt2ps	%zmm4, %zmm1, %zmm13
	vmovaps	%zmm13, 1856(%rsp)              # 64-byte Spill
	vpermt2ps	%zmm4, %zmm19, %zmm16
	vmovaps	%zmm16, 18496(%rsp)             # 64-byte Spill
	vpermt2ps	%zmm4, %zmm20, %zmm12
	vmovaps	%zmm12, 1088(%rsp)              # 64-byte Spill
	vpermt2ps	%zmm4, %zmm25, %zmm7
	vmovaps	%zmm7, 1728(%rsp)               # 64-byte Spill
	vpermt2ps	%zmm4, %zmm26, %zmm27
	vmovaps	%zmm27, 1792(%rsp)              # 64-byte Spill
	vpermt2ps	%ymm4, %ymm18, %ymm3
	vmovaps	%zmm3, 37568(%rsp)              # 64-byte Spill
	vblendps	$128, %ymm4, %ymm0, %ymm0       # ymm0 = ymm0[0,1,2,3,4,5,6],ymm4[7]
	vmovaps	%zmm0, 1536(%rsp)               # 64-byte Spill
	vpermt2ps	%zmm4, %zmm28, %zmm11
	vmovaps	%zmm11, 18688(%rsp)             # 64-byte Spill
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	26240(%rsp), %zmm3
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vbroadcastss	26116(%rsp), %xmm0
	vblendps	$2, %xmm3, %xmm0, %xmm0         # xmm0 = xmm0[0],xmm3[1],xmm0[2,3]
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	26368(%rsp), %zmm5
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vshufps	$212, %xmm5, %xmm0, %xmm0       # xmm0 = xmm0[0,1],xmm5[1,3]
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	26496(%rsp), %zmm4
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vinsertps	$112, %xmm4, %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],xmm4[1]
	vbroadcastss	26628(%rsp), %ymm1
	vblendps	$240, %ymm1, %ymm0, %ymm1       # ymm1 = ymm0[0,1,2,3],ymm1[4,5,6,7]
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	26752(%rsp), %zmm7
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vinsertf128	$1, %xmm7, %ymm0, %ymm0
	vblendps	$34, %ymm0, %ymm1, %ymm0        # ymm0 = ymm1[0],ymm0[1],ymm1[2,3,4],ymm0[5],ymm1[6,7]
	vbroadcastss	26884(%rsp), %ymm1
	vblendps	$192, %ymm1, %ymm0, %ymm0       # ymm0 = ymm0[0,1,2,3,4,5],ymm1[6,7]
	vbroadcastss	27012(%rsp), %ymm1
	vblendps	$128, %ymm1, %ymm0, %ymm0       # ymm0 = ymm0[0,1,2,3,4,5,6],ymm1[7]
	vmovaps	%zmm0, 576(%rsp)                # 64-byte Spill
	vbroadcastss	26120(%rsp), %xmm0
	vinsertps	$156, %xmm3, %xmm0, %xmm0 # xmm0 = xmm0[0],xmm3[2],zero,zero
	vbroadcastsd	26632(%rsp), %ymm1
	vblendps	$240, %ymm1, %ymm0, %ymm0       # ymm0 = ymm0[0,1,2,3],ymm1[4,5,6,7]
	vbroadcastss	26760(%rsp), %ymm1
	vblendps	$32, %ymm1, %ymm0, %ymm0        # ymm0 = ymm0[0,1,2,3,4],ymm1[5],ymm0[6,7]
	vinsertps	$179, %xmm4, %xmm5, %xmm1 # xmm1 = zero,zero,xmm5[2],xmm4[2]
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	26880(%rsp), %zmm6
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vinsertf128	$1, %xmm6, %ymm1, %ymm1
	vblendps	$204, %ymm1, %ymm0, %ymm0       # ymm0 = ymm0[0,1],ymm1[2,3],ymm0[4,5],ymm1[6,7]
	vbroadcastss	27016(%rsp), %ymm1
	vblendps	$128, %ymm1, %ymm0, %ymm0       # ymm0 = ymm0[0,1,2,3,4,5,6],ymm1[7]
	vmovaps	%zmm0, 19840(%rsp)              # 64-byte Spill
	vbroadcastss	26148(%rsp), %xmm0
	vmovsd	.LCPI0_182(%rip), %xmm1         # xmm1 = [0,25,0,0]
	vpermt2ps	%zmm3, %zmm1, %zmm0
	vmovaps	%zmm0, %zmm13
	vbroadcastss	26152(%rsp), %xmm0
	vmovsd	.LCPI0_183(%rip), %xmm1         # xmm1 = [0,26,0,0]
	vpermt2ps	%zmm3, %zmm1, %zmm0
	vmovaps	%zmm0, %zmm12
	vbroadcastss	26156(%rsp), %xmm0
	vmovsd	.LCPI0_184(%rip), %xmm30        # xmm30 = [0,27,0,0]
	vpermt2ps	%zmm3, %zmm30, %zmm0
	vmovaps	%zmm0, %zmm11
	vbroadcastss	26160(%rsp), %xmm0
	vmovsd	.LCPI0_185(%rip), %xmm1         # xmm1 = [0,28,0,0]
	vpermt2ps	%zmm3, %zmm1, %zmm0
	vmovaps	%zmm0, %zmm10
	vbroadcastss	26164(%rsp), %xmm0
	vmovsd	.LCPI0_186(%rip), %xmm1         # xmm1 = [0,29,0,0]
	vpermt2ps	%zmm3, %zmm1, %zmm0
	vmovaps	%zmm0, %zmm9
	vbroadcastss	26168(%rsp), %xmm16
	vmovsd	.LCPI0_187(%rip), %xmm0         # xmm0 = [0,30,0,0]
	vpermt2ps	%zmm3, %zmm0, %zmm16
	vbroadcastss	26172(%rsp), %xmm2
	vmovsd	.LCPI0_188(%rip), %xmm0         # xmm0 = [0,31,0,0]
	vpermt2ps	%zmm3, %zmm0, %zmm2
	vbroadcastss	26144(%rsp), %xmm0
	vmovsd	.LCPI0_189(%rip), %xmm1         # xmm1 = [0,24,0,0]
	vpermt2ps	%zmm3, %zmm1, %zmm0
	vmovaps	26112(%rsp), %xmm1
	vunpcklps	%xmm3, %xmm1, %xmm1     # xmm1 = xmm1[0],xmm3[0],xmm1[1],xmm3[1]
	vbroadcastss	26124(%rsp), %xmm8
	vinsertps	$220, %xmm3, %xmm8, %xmm8 # xmm8 = xmm8[0],xmm3[3],zero,zero
	vmovaps	.LCPI0_4(%rip), %xmm23          # xmm23 = [0,1,25,u]
	vpermt2ps	%zmm5, %zmm23, %zmm13
	vmovaps	.LCPI0_5(%rip), %xmm3           # xmm3 = [0,1,2,25]
	vpermt2ps	%zmm4, %zmm3, %zmm13
	vmovaps	%zmm13, %zmm27
	vmovapd	.LCPI0_12(%rip), %xmm3          # xmm3 = [0,0,0,0,0,0,0,0,13,0,0,0,0,0,0,0]
	vpermt2pd	%zmm5, %zmm3, %zmm12
	vmovaps	.LCPI0_13(%rip), %xmm3          # xmm3 = [0,1,2,26]
	vpermt2ps	%zmm4, %zmm3, %zmm12
	vmovaps	%zmm12, %zmm15
	vmovaps	.LCPI0_21(%rip), %xmm3          # xmm3 = [0,1,27,u]
	vpermt2ps	%zmm5, %zmm3, %zmm11
	vmovaps	.LCPI0_22(%rip), %xmm3          # xmm3 = [0,1,2,27]
	vpermt2ps	%zmm4, %zmm3, %zmm11
	vmovaps	%zmm11, %zmm14
	vmovapd	.LCPI0_31(%rip), %xmm3          # xmm3 = [0,0,0,0,0,0,0,0,14,0,0,0,0,0,0,0]
	vpermt2pd	%zmm5, %zmm3, %zmm10
	vmovaps	.LCPI0_32(%rip), %xmm3          # xmm3 = [0,1,2,28]
	vpermt2ps	%zmm4, %zmm3, %zmm10
	vmovaps	%zmm10, %zmm13
	vmovaps	.LCPI0_41(%rip), %xmm3          # xmm3 = [0,1,29,u]
	vpermt2ps	%zmm5, %zmm3, %zmm9
	vmovaps	.LCPI0_42(%rip), %xmm3          # xmm3 = [0,1,2,29]
	vpermt2ps	%zmm4, %zmm3, %zmm9
	vmovaps	%zmm9, %zmm12
	vmovapd	.LCPI0_53(%rip), %xmm3          # xmm3 = [0,0,0,0,0,0,0,0,15,0,0,0,0,0,0,0]
	vpermt2pd	%zmm5, %zmm3, %zmm16
	vmovaps	.LCPI0_54(%rip), %xmm3          # xmm3 = [0,1,2,30]
	vpermt2ps	%zmm4, %zmm3, %zmm16
	vmovaps	.LCPI0_66(%rip), %xmm24         # xmm24 = [0,1,31,u]
	vpermt2ps	%zmm5, %zmm24, %zmm2
	vmovaps	.LCPI0_67(%rip), %xmm3          # xmm3 = [0,1,2,31]
	vpermt2ps	%zmm4, %zmm3, %zmm2
	vmovaps	%zmm2, %zmm3
	vmovapd	.LCPI0_80(%rip), %xmm2          # xmm2 = [0,0,0,0,0,0,0,0,12,0,0,0,0,0,0,0]
	vpermt2pd	%zmm5, %zmm2, %zmm0
	vmovaps	.LCPI0_81(%rip), %xmm2          # xmm2 = [0,1,2,24]
	vpermt2ps	%zmm4, %zmm2, %zmm0
	vmovlhps	%xmm5, %xmm1, %xmm1             # xmm1 = xmm1[0],xmm5[0]
	vinsertps	$48, %xmm4, %xmm1, %xmm17 # xmm17 = xmm1[0,1,2],xmm4[0]
	vshufps	$244, %xmm5, %xmm8, %xmm1       # xmm1 = xmm8[0,1],xmm5[3,3]
	vbroadcastss	26636(%rsp), %ymm8
	vblendps	$8, %xmm4, %xmm1, %xmm4         # xmm4 = xmm1[0,1,2],xmm4[3]
	vblendps	$240, %ymm8, %ymm1, %ymm1       # ymm1 = ymm1[0,1,2,3],ymm8[4,5,6,7]
	vbroadcastss	26764(%rsp), %ymm8
	vbroadcastss	26892(%rsp), %ymm9
	vblendps	$32, %ymm8, %ymm1, %ymm1        # ymm1 = ymm1[0,1,2,3,4],ymm8[5],ymm1[6,7]
	vblendps	$192, %ymm9, %ymm1, %ymm1       # ymm1 = ymm1[0,1,2,3,4,5],ymm9[6,7]
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	27008(%rsp), %zmm8
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vinsertf128	$1, %xmm8, %ymm4, %ymm4
	vblendps	$136, %ymm4, %ymm1, %ymm1       # ymm1 = ymm1[0,1,2],ymm4[3],ymm1[4,5,6],ymm4[7]
	vmovaps	%zmm1, 8768(%rsp)               # 64-byte Spill
	vbroadcastss	26128(%rsp), %xmm1
	vunpcklps	26256(%rsp), %xmm1, %xmm9 # xmm9 = xmm1[0],mem[0],xmm1[1],mem[1]
	vmovaps	26384(%rsp), %xmm1
	vmovlhps	%xmm1, %xmm9, %xmm9             # xmm9 = xmm9[0],xmm1[0]
	vbroadcastss	26512(%rsp), %xmm10
	vblendps	$8, %xmm10, %xmm9, %xmm9        # xmm9 = xmm9[0,1,2],xmm10[3]
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	26624(%rsp), %zmm10
	vmovaps	.LCPI0_6(%rip), %ymm2           # ymm2 = [0,1,2,3,25,u,u,u]
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vpermt2ps	%zmm10, %zmm2, %zmm27
	vmovapd	.LCPI0_14(%rip), %ymm31         # ymm31 = [0,1,13,u]
	vpermt2pd	%zmm10, %zmm31, %zmm15
	vmovaps	.LCPI0_23(%rip), %ymm2          # ymm2 = [0,1,2,3,27,u,u,u]
	vpermt2ps	%zmm10, %zmm2, %zmm14
	vmovapd	.LCPI0_33(%rip), %ymm2          # ymm2 = [0,1,14,u]
	vpermt2pd	%zmm10, %zmm2, %zmm13
	vmovaps	.LCPI0_43(%rip), %ymm2          # ymm2 = [0,1,2,3,29,u,u,u]
	vpermt2ps	%zmm10, %zmm2, %zmm12
	vmovapd	.LCPI0_55(%rip), %ymm2          # ymm2 = [0,1,15,u]
	vpermt2pd	%zmm10, %zmm2, %zmm16
	vmovaps	.LCPI0_68(%rip), %ymm2          # ymm2 = [0,1,2,3,31,u,u,u]
	vpermt2ps	%zmm10, %zmm2, %zmm3
	vmovapd	.LCPI0_82(%rip), %ymm18         # ymm18 = [0,1,12,u]
	vpermt2pd	%zmm10, %zmm18, %zmm0
	vinsertf32x4	$1, %xmm10, %ymm17, %ymm11
	vblendps	$240, %ymm10, %ymm9, %ymm9      # ymm9 = ymm9[0,1,2,3],ymm10[4,5,6,7]
	vbroadcastss	26768(%rsp), %ymm10
	vblendps	$32, %ymm10, %ymm9, %ymm9       # ymm9 = ymm9[0,1,2,3,4],ymm10[5],ymm9[6,7]
	vbroadcastsd	26896(%rsp), %ymm10
	vblendps	$192, %ymm10, %ymm9, %ymm9      # ymm9 = ymm9[0,1,2,3,4,5],ymm10[6,7]
	vbroadcastss	27024(%rsp), %ymm10
	vblendps	$128, %ymm10, %ymm9, %ymm2      # ymm2 = ymm9[0,1,2,3,4,5,6],ymm10[7]
	vmovaps	%zmm2, 18432(%rsp)              # 64-byte Spill
	vbroadcastss	26260(%rsp), %xmm9
	vbroadcastss	26132(%rsp), %xmm10
	vblendps	$2, %xmm9, %xmm10, %xmm9        # xmm9 = xmm10[0],xmm9[1],xmm10[2,3]
	vmovaps	%ymm5, %ymm10
	vmovaps	19904(%rsp), %ymm4              # 32-byte Reload
	vpermt2ps	%ymm9, %ymm4, %ymm10
	vbroadcastss	26516(%rsp), %xmm9
	vblendps	$8, %xmm9, %xmm10, %xmm9        # xmm9 = xmm10[0,1,2],xmm9[3]
	vbroadcastss	26644(%rsp), %ymm10
	vblendps	$240, %ymm10, %ymm9, %ymm9      # ymm9 = ymm9[0,1,2,3],ymm10[4,5,6,7]
	vmovaps	.LCPI0_7(%rip), %ymm2           # ymm2 = [0,1,2,3,4,25,u,u]
	vpermt2ps	%zmm7, %zmm2, %zmm27
	vmovaps	.LCPI0_15(%rip), %ymm2          # ymm2 = [0,1,2,3,4,26,u,u]
	vpermt2ps	%zmm7, %zmm2, %zmm15
	vmovaps	.LCPI0_24(%rip), %ymm2          # ymm2 = [0,1,2,3,4,27,u,u]
	vpermt2ps	%zmm7, %zmm2, %zmm14
	vmovaps	.LCPI0_34(%rip), %ymm2          # ymm2 = [0,1,2,3,4,28,u,u]
	vpermt2ps	%zmm7, %zmm2, %zmm13
	vmovaps	.LCPI0_44(%rip), %ymm2          # ymm2 = [0,1,2,3,4,29,u,u]
	vpermt2ps	%zmm7, %zmm2, %zmm12
	vmovaps	.LCPI0_56(%rip), %ymm2          # ymm2 = [0,1,2,3,4,30,u,u]
	vpermt2ps	%zmm7, %zmm2, %zmm16
	vmovaps	.LCPI0_69(%rip), %ymm2          # ymm2 = [0,1,2,3,4,31,u,u]
	vpermt2ps	%zmm7, %zmm2, %zmm3
	vmovaps	.LCPI0_83(%rip), %ymm2          # ymm2 = [0,1,2,3,4,24,u,u]
	vpermt2ps	%zmm7, %zmm2, %zmm0
	vmovaps	.LCPI0_85(%rip), %ymm29         # ymm29 = [0,1,u,u,4,8,u,u]
	vpermt2ps	%ymm7, %ymm29, %ymm11
	vbroadcastss	26900(%rsp), %ymm10
	vblendps	$32, %ymm7, %ymm9, %ymm7        # ymm7 = ymm9[0,1,2,3,4],ymm7[5],ymm9[6,7]
	vblendps	$192, %ymm10, %ymm7, %ymm7      # ymm7 = ymm7[0,1,2,3,4,5],ymm10[6,7]
	vbroadcastss	27028(%rsp), %ymm9
	vbroadcastss	26264(%rsp), %xmm10
	vblendps	$128, %ymm9, %ymm7, %ymm2       # ymm2 = ymm7[0,1,2,3,4,5,6],ymm9[7]
	vmovaps	%zmm2, 8704(%rsp)               # 64-byte Spill
	vbroadcastss	26136(%rsp), %xmm7
	vblendps	$2, %xmm10, %xmm7, %xmm7        # xmm7 = xmm7[0],xmm10[1],xmm7[2,3]
	vbroadcastss	26520(%rsp), %xmm10
	vblendps	$3, %xmm7, %xmm1, %xmm1         # xmm1 = xmm7[0,1],xmm1[2,3]
	vblendps	$8, %xmm10, %xmm1, %xmm1        # xmm1 = xmm1[0,1,2],xmm10[3]
	vbroadcastsd	26648(%rsp), %ymm7
	vbroadcastss	26776(%rsp), %ymm10
	vblendps	$240, %ymm7, %ymm1, %ymm1       # ymm1 = ymm1[0,1,2,3],ymm7[4,5,6,7]
	vblendps	$32, %ymm10, %ymm1, %ymm1       # ymm1 = ymm1[0,1,2,3,4],ymm10[5],ymm1[6,7]
	vmovaps	.LCPI0_8(%rip), %ymm28          # ymm28 = [0,1,2,3,4,5,25,u]
	vpermt2ps	%zmm6, %zmm28, %zmm27
	vmovapd	%zmm22, %zmm19
	vpermt2pd	%zmm6, %zmm22, %zmm15
	vmovaps	.LCPI0_25(%rip), %ymm20         # ymm20 = [0,1,2,3,4,5,27,u]
	vpermt2ps	%zmm6, %zmm20, %zmm14
	vmovapd	.LCPI0_35(%rip), %ymm21         # ymm21 = [0,1,2,14]
	vpermt2pd	%zmm6, %zmm21, %zmm13
	vmovaps	%zmm12, %zmm10
	vmovaps	.LCPI0_45(%rip), %ymm22         # ymm22 = [0,1,2,3,4,5,29,u]
	vpermt2ps	%zmm6, %zmm22, %zmm10
	vmovapd	.LCPI0_57(%rip), %ymm25         # ymm25 = [0,1,2,15]
	vpermt2pd	%zmm6, %zmm25, %zmm16
	vmovaps	.LCPI0_70(%rip), %ymm2          # ymm2 = [0,1,2,3,4,5,31,u]
	vpermt2ps	%zmm6, %zmm2, %zmm3
	vmovaps	%zmm3, %zmm2
	vmovapd	.LCPI0_84(%rip), %ymm26         # ymm26 = [0,1,2,12]
	vpermt2pd	%zmm6, %zmm26, %zmm0
	vinsertf32x4	$1, %xmm6, %ymm17, %ymm3
	vblendps	$192, %ymm6, %ymm1, %ymm1       # ymm1 = ymm1[0,1,2,3,4,5],ymm6[6,7]
	vbroadcastss	27032(%rsp), %ymm6
	vbroadcastss	26268(%rsp), %xmm7
	vbroadcastss	26140(%rsp), %xmm12
	vblendps	$128, %ymm6, %ymm1, %ymm1       # ymm1 = ymm1[0,1,2,3,4,5,6],ymm6[7]
	vmovaps	%zmm1, 11008(%rsp)              # 64-byte Spill
	vblendps	$2, %xmm7, %xmm12, %xmm1        # xmm1 = xmm12[0],xmm7[1],xmm12[2,3]
	vmovaps	19936(%rsp), %ymm9              # 32-byte Reload
	vpermt2ps	%ymm1, %ymm9, %ymm5
	vbroadcastss	26524(%rsp), %xmm1
	vblendps	$8, %xmm1, %xmm5, %xmm1         # xmm1 = xmm5[0,1,2],xmm1[3]
	vbroadcastss	26652(%rsp), %ymm5
	vblendps	$240, %ymm5, %ymm1, %ymm1       # ymm1 = ymm1[0,1,2,3],ymm5[4,5,6,7]
	vbroadcastss	26780(%rsp), %ymm5
	vblendps	$32, %ymm5, %ymm1, %ymm1        # ymm1 = ymm1[0,1,2,3,4],ymm5[5],ymm1[6,7]
	vbroadcastss	26908(%rsp), %ymm5
	vblendps	$192, %ymm5, %ymm1, %ymm1       # ymm1 = ymm1[0,1,2,3,4,5],ymm5[6,7]
	vshufpd	$2, %ymm3, %ymm11, %ymm3        # ymm3 = ymm11[0],ymm3[1],ymm11[2],ymm3[2]
	vmovaps	.LCPI0_9(%rip), %ymm17          # ymm17 = [0,1,2,3,4,5,6,25]
	vpermt2ps	%zmm8, %zmm17, %zmm27
	vmovaps	%zmm27, 448(%rsp)               # 64-byte Spill
	vmovaps	.LCPI0_17(%rip), %ymm29         # ymm29 = [0,1,2,3,4,5,6,26]
	vpermt2ps	%zmm8, %zmm29, %zmm15
	vmovaps	%zmm15, 2048(%rsp)              # 64-byte Spill
	vmovaps	.LCPI0_26(%rip), %ymm5          # ymm5 = [0,1,2,3,4,5,6,27]
	vpermt2ps	%zmm8, %zmm5, %zmm14
	vmovaps	%zmm14, 256(%rsp)               # 64-byte Spill
	vmovaps	.LCPI0_36(%rip), %ymm5          # ymm5 = [0,1,2,3,4,5,6,28]
	vpermt2ps	%zmm8, %zmm5, %zmm13
	vmovaps	%zmm13, 37440(%rsp)             # 64-byte Spill
	vmovaps	.LCPI0_46(%rip), %ymm5          # ymm5 = [0,1,2,3,4,5,6,29]
	vpermt2ps	%zmm8, %zmm5, %zmm10
	vmovaps	%zmm10, 2368(%rsp)              # 64-byte Spill
	vmovaps	.LCPI0_58(%rip), %ymm5          # ymm5 = [0,1,2,3,4,5,6,30]
	vpermt2ps	%zmm8, %zmm5, %zmm16
	vmovaps	%zmm16, 1984(%rsp)              # 64-byte Spill
	vmovaps	.LCPI0_71(%rip), %ymm5          # ymm5 = [0,1,2,3,4,5,6,31]
	vpermt2ps	%zmm8, %zmm5, %zmm2
	vmovaps	%zmm2, 832(%rsp)                # 64-byte Spill
	vmovaps	.LCPI0_86(%rip), %ymm2          # ymm2 = [0,1,2,3,4,5,6,8]
	vpermt2ps	%ymm8, %ymm2, %ymm3
	vmovaps	%zmm3, 18560(%rsp)              # 64-byte Spill
	vblendps	$128, %ymm8, %ymm1, %ymm1       # ymm1 = ymm1[0,1,2,3,4,5,6],ymm8[7]
	vmovaps	%zmm1, 10944(%rsp)              # 64-byte Spill
	vmovaps	.LCPI0_89(%rip), %ymm3          # ymm3 = [0,1,2,3,4,5,6,24]
	vpermt2ps	%zmm8, %zmm3, %zmm0
	vmovaps	%zmm0, 37696(%rsp)              # 64-byte Spill
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	26304(%rsp), %zmm8
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vbroadcastss	26180(%rsp), %xmm1
	vblendps	$2, %xmm8, %xmm1, %xmm1         # xmm1 = xmm1[0],xmm8[1],xmm1[2,3]
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	26432(%rsp), %zmm5
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vshufps	$212, %xmm5, %xmm1, %xmm6       # xmm6 = xmm1[0,1],xmm5[1,3]
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	26560(%rsp), %zmm1
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vinsertps	$112, %xmm1, %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],xmm1[1]
	vbroadcastss	26692(%rsp), %ymm7
	vblendps	$240, %ymm7, %ymm6, %ymm11      # ymm11 = ymm6[0,1,2,3],ymm7[4,5,6,7]
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	26816(%rsp), %zmm7
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vinsertf128	$1, %xmm7, %ymm6, %ymm6
	vblendps	$34, %ymm6, %ymm11, %ymm6       # ymm6 = ymm11[0],ymm6[1],ymm11[2,3,4],ymm6[5],ymm11[6,7]
	vbroadcastss	26948(%rsp), %ymm11
	vbroadcastss	27076(%rsp), %ymm12
	vblendps	$192, %ymm11, %ymm6, %ymm6      # ymm6 = ymm6[0,1,2,3,4,5],ymm11[6,7]
	vblendps	$128, %ymm12, %ymm6, %ymm0      # ymm0 = ymm6[0,1,2,3,4,5,6],ymm12[7]
	vmovaps	%zmm0, 128(%rsp)                # 64-byte Spill
	vbroadcastss	26184(%rsp), %xmm6
	vbroadcastsd	26696(%rsp), %ymm11
	vinsertps	$156, %xmm8, %xmm6, %xmm6 # xmm6 = xmm6[0],xmm8[2],zero,zero
	vblendps	$240, %ymm11, %ymm6, %ymm6      # ymm6 = ymm6[0,1,2,3],ymm11[4,5,6,7]
	vbroadcastss	26824(%rsp), %ymm11
	vblendps	$32, %ymm11, %ymm6, %ymm11      # ymm11 = ymm6[0,1,2,3,4],ymm11[5],ymm6[6,7]
	vinsertps	$179, %xmm1, %xmm5, %xmm12 # xmm12 = zero,zero,xmm5[2],xmm1[2]
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	26944(%rsp), %zmm6
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vinsertf128	$1, %xmm6, %ymm12, %ymm12
	vblendps	$204, %ymm12, %ymm11, %ymm11    # ymm11 = ymm11[0,1],ymm12[2,3],ymm11[4,5],ymm12[6,7]
	vbroadcastss	27080(%rsp), %ymm12
	vblendps	$128, %ymm12, %ymm11, %ymm0     # ymm0 = ymm11[0,1,2,3,4,5,6],ymm12[7]
	vmovaps	%zmm0, 1664(%rsp)               # 64-byte Spill
	vbroadcastss	26212(%rsp), %xmm15
	vmovsd	.LCPI0_182(%rip), %xmm0         # xmm0 = [0,25,0,0]
	vpermt2ps	%zmm8, %zmm0, %zmm15
	vbroadcastss	26216(%rsp), %xmm14
	vmovsd	.LCPI0_183(%rip), %xmm0         # xmm0 = [0,26,0,0]
	vpermt2ps	%zmm8, %zmm0, %zmm14
	vbroadcastss	26220(%rsp), %xmm13
	vpermt2ps	%zmm8, %zmm30, %zmm13
	vbroadcastss	26224(%rsp), %xmm2
	vmovsd	.LCPI0_185(%rip), %xmm0         # xmm0 = [0,28,0,0]
	vpermt2ps	%zmm8, %zmm0, %zmm2
	vbroadcastss	26228(%rsp), %xmm10
	vmovsd	.LCPI0_186(%rip), %xmm0         # xmm0 = [0,29,0,0]
	vpermt2ps	%zmm8, %zmm0, %zmm10
	vbroadcastss	26232(%rsp), %xmm27
	vmovsd	.LCPI0_187(%rip), %xmm0         # xmm0 = [0,30,0,0]
	vpermt2ps	%zmm8, %zmm0, %zmm27
	vbroadcastss	26236(%rsp), %xmm16
	vmovsd	.LCPI0_188(%rip), %xmm0         # xmm0 = [0,31,0,0]
	vpermt2ps	%zmm8, %zmm0, %zmm16
	vbroadcastss	26208(%rsp), %xmm0
	vmovsd	.LCPI0_189(%rip), %xmm11        # xmm11 = [0,24,0,0]
	vpermt2ps	%zmm8, %zmm11, %zmm0
	vmovaps	26176(%rsp), %xmm11
	vunpcklps	%xmm8, %xmm11, %xmm11   # xmm11 = xmm11[0],xmm8[0],xmm11[1],xmm8[1]
	vbroadcastss	26188(%rsp), %xmm12
	vinsertps	$220, %xmm8, %xmm12, %xmm8 # xmm8 = xmm12[0],xmm8[3],zero,zero
	vpermt2ps	%zmm5, %zmm23, %zmm15
	vmovaps	.LCPI0_5(%rip), %xmm12          # xmm12 = [0,1,2,25]
	vpermt2ps	%zmm1, %zmm12, %zmm15
	vmovaps	%zmm15, %zmm30
	vmovapd	.LCPI0_12(%rip), %xmm12         # xmm12 = [0,0,0,0,0,0,0,0,13,0,0,0,0,0,0,0]
	vpermt2pd	%zmm5, %zmm12, %zmm14
	vmovaps	.LCPI0_13(%rip), %xmm12         # xmm12 = [0,1,2,26]
	vpermt2ps	%zmm1, %zmm12, %zmm14
	vmovaps	%zmm14, %zmm23
	vmovaps	.LCPI0_21(%rip), %xmm12         # xmm12 = [0,1,27,u]
	vpermt2ps	%zmm5, %zmm12, %zmm13
	vmovaps	.LCPI0_22(%rip), %xmm12         # xmm12 = [0,1,2,27]
	vpermt2ps	%zmm1, %zmm12, %zmm13
	vmovaps	%zmm13, %zmm15
	vmovapd	.LCPI0_31(%rip), %xmm12         # xmm12 = [0,0,0,0,0,0,0,0,14,0,0,0,0,0,0,0]
	vpermt2pd	%zmm5, %zmm12, %zmm2
	vmovaps	.LCPI0_32(%rip), %xmm12         # xmm12 = [0,1,2,28]
	vpermt2ps	%zmm1, %zmm12, %zmm2
	vmovaps	.LCPI0_41(%rip), %xmm12         # xmm12 = [0,1,29,u]
	vpermt2ps	%zmm5, %zmm12, %zmm10
	vmovaps	.LCPI0_42(%rip), %xmm12         # xmm12 = [0,1,2,29]
	vpermt2ps	%zmm1, %zmm12, %zmm10
	vmovaps	%zmm10, %zmm13
	vmovapd	.LCPI0_53(%rip), %xmm10         # xmm10 = [0,0,0,0,0,0,0,0,15,0,0,0,0,0,0,0]
	vpermt2pd	%zmm5, %zmm10, %zmm27
	vmovaps	.LCPI0_54(%rip), %xmm10         # xmm10 = [0,1,2,30]
	vpermt2ps	%zmm1, %zmm10, %zmm27
	vpermt2ps	%zmm5, %zmm24, %zmm16
	vmovaps	.LCPI0_67(%rip), %xmm10         # xmm10 = [0,1,2,31]
	vpermt2ps	%zmm1, %zmm10, %zmm16
	vmovaps	%zmm16, %zmm24
	vmovapd	.LCPI0_80(%rip), %xmm10         # xmm10 = [0,0,0,0,0,0,0,0,12,0,0,0,0,0,0,0]
	vpermt2pd	%zmm5, %zmm10, %zmm0
	vmovaps	.LCPI0_81(%rip), %xmm10         # xmm10 = [0,1,2,24]
	vpermt2ps	%zmm1, %zmm10, %zmm0
	vmovaps	%zmm0, %zmm10
	vmovlhps	%xmm5, %xmm11, %xmm11           # xmm11 = xmm11[0],xmm5[0]
	vinsertps	$48, %xmm1, %xmm11, %xmm16 # xmm16 = xmm11[0,1,2],xmm1[0]
	vshufps	$244, %xmm5, %xmm8, %xmm8       # xmm8 = xmm8[0,1],xmm5[3,3]
	vblendps	$8, %xmm1, %xmm8, %xmm1         # xmm1 = xmm8[0,1,2],xmm1[3]
	vbroadcastss	26700(%rsp), %ymm12
	vblendps	$240, %ymm12, %ymm8, %ymm8      # ymm8 = ymm8[0,1,2,3],ymm12[4,5,6,7]
	vbroadcastss	26828(%rsp), %ymm12
	vblendps	$32, %ymm12, %ymm8, %ymm8       # ymm8 = ymm8[0,1,2,3,4],ymm12[5],ymm8[6,7]
	vbroadcastss	26956(%rsp), %ymm12
	vblendps	$192, %ymm12, %ymm8, %ymm12     # ymm12 = ymm8[0,1,2,3,4,5],ymm12[6,7]
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	27072(%rsp), %zmm8
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vinsertf128	$1, %xmm8, %ymm1, %ymm1
	vblendps	$136, %ymm1, %ymm12, %ymm0      # ymm0 = ymm12[0,1,2],ymm1[3],ymm12[4,5,6],ymm1[7]
	vmovaps	%zmm0, 11136(%rsp)              # 64-byte Spill
	vbroadcastss	26192(%rsp), %xmm1
	vunpcklps	26320(%rsp), %xmm1, %xmm12 # xmm12 = xmm1[0],mem[0],xmm1[1],mem[1]
	vmovaps	26448(%rsp), %xmm1
	vbroadcastss	26576(%rsp), %xmm14
	vmovlhps	%xmm1, %xmm12, %xmm12           # xmm12 = xmm12[0],xmm1[0]
	vblendps	$8, %xmm14, %xmm12, %xmm14      # xmm14 = xmm12[0,1,2],xmm14[3]
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	26688(%rsp), %zmm0
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vmovaps	.LCPI0_6(%rip), %ymm11          # ymm11 = [0,1,2,3,25,u,u,u]
	vpermt2ps	%zmm0, %zmm11, %zmm30
	vpermt2pd	%zmm0, %zmm31, %zmm23
	vmovaps	.LCPI0_23(%rip), %ymm11         # ymm11 = [0,1,2,3,27,u,u,u]
	vpermt2ps	%zmm0, %zmm11, %zmm15
	vmovapd	.LCPI0_33(%rip), %ymm11         # ymm11 = [0,1,14,u]
	vpermt2pd	%zmm0, %zmm11, %zmm2
	vmovaps	.LCPI0_43(%rip), %ymm11         # ymm11 = [0,1,2,3,29,u,u,u]
	vpermt2ps	%zmm0, %zmm11, %zmm13
	vmovapd	.LCPI0_55(%rip), %ymm11         # ymm11 = [0,1,15,u]
	vpermt2pd	%zmm0, %zmm11, %zmm27
	vmovaps	.LCPI0_68(%rip), %ymm11         # ymm11 = [0,1,2,3,31,u,u,u]
	vpermt2ps	%zmm0, %zmm11, %zmm24
	vpermt2pd	%zmm0, %zmm18, %zmm10
	vinsertf32x4	$1, %xmm0, %ymm16, %ymm12
	vblendps	$240, %ymm0, %ymm14, %ymm0      # ymm0 = ymm14[0,1,2,3],ymm0[4,5,6,7]
	vbroadcastss	26832(%rsp), %ymm14
	vbroadcastsd	26960(%rsp), %ymm11
	vblendps	$32, %ymm14, %ymm0, %ymm0       # ymm0 = ymm0[0,1,2,3,4],ymm14[5],ymm0[6,7]
	vblendps	$192, %ymm11, %ymm0, %ymm0      # ymm0 = ymm0[0,1,2,3,4,5],ymm11[6,7]
	vbroadcastss	27088(%rsp), %ymm11
	vbroadcastss	26324(%rsp), %xmm14
	vblendps	$128, %ymm11, %ymm0, %ymm0      # ymm0 = ymm0[0,1,2,3,4,5,6],ymm11[7]
	vmovaps	%zmm0, 37376(%rsp)              # 64-byte Spill
	vbroadcastss	26196(%rsp), %xmm0
	vblendps	$2, %xmm14, %xmm0, %xmm0        # xmm0 = xmm0[0],xmm14[1],xmm0[2,3]
	vmovaps	%ymm5, %ymm11
	vbroadcastss	26580(%rsp), %xmm14
	vpermt2ps	%ymm0, %ymm4, %ymm11
	vblendps	$8, %xmm14, %xmm11, %xmm0       # xmm0 = xmm11[0,1,2],xmm14[3]
	vbroadcastss	26708(%rsp), %ymm11
	vblendps	$240, %ymm11, %ymm0, %ymm0      # ymm0 = ymm0[0,1,2,3],ymm11[4,5,6,7]
	vmovaps	.LCPI0_7(%rip), %ymm4           # ymm4 = [0,1,2,3,4,25,u,u]
	vpermt2ps	%zmm7, %zmm4, %zmm30
	vmovaps	.LCPI0_15(%rip), %ymm4          # ymm4 = [0,1,2,3,4,26,u,u]
	vpermt2ps	%zmm7, %zmm4, %zmm23
	vmovaps	.LCPI0_24(%rip), %ymm4          # ymm4 = [0,1,2,3,4,27,u,u]
	vpermt2ps	%zmm7, %zmm4, %zmm15
	vmovaps	.LCPI0_34(%rip), %ymm4          # ymm4 = [0,1,2,3,4,28,u,u]
	vpermt2ps	%zmm7, %zmm4, %zmm2
	vmovaps	.LCPI0_44(%rip), %ymm4          # ymm4 = [0,1,2,3,4,29,u,u]
	vpermt2ps	%zmm7, %zmm4, %zmm13
	vmovaps	.LCPI0_56(%rip), %ymm4          # ymm4 = [0,1,2,3,4,30,u,u]
	vpermt2ps	%zmm7, %zmm4, %zmm27
	vmovaps	.LCPI0_69(%rip), %ymm4          # ymm4 = [0,1,2,3,4,31,u,u]
	vpermt2ps	%zmm7, %zmm4, %zmm24
	vmovaps	.LCPI0_83(%rip), %ymm4          # ymm4 = [0,1,2,3,4,24,u,u]
	vpermt2ps	%zmm7, %zmm4, %zmm10
	vmovaps	.LCPI0_85(%rip), %ymm4          # ymm4 = [0,1,u,u,4,8,u,u]
	vpermt2ps	%ymm7, %ymm4, %ymm12
	vblendps	$32, %ymm7, %ymm0, %ymm0        # ymm0 = ymm0[0,1,2,3,4],ymm7[5],ymm0[6,7]
	vbroadcastss	26964(%rsp), %ymm7
	vblendps	$192, %ymm7, %ymm0, %ymm0       # ymm0 = ymm0[0,1,2,3,4,5],ymm7[6,7]
	vbroadcastss	27092(%rsp), %ymm7
	vbroadcastss	26328(%rsp), %xmm11
	vbroadcastss	26200(%rsp), %xmm14
	vblendps	$128, %ymm7, %ymm0, %ymm0       # ymm0 = ymm0[0,1,2,3,4,5,6],ymm7[7]
	vmovaps	%zmm0, 8896(%rsp)               # 64-byte Spill
	vblendps	$2, %xmm11, %xmm14, %xmm0       # xmm0 = xmm14[0],xmm11[1],xmm14[2,3]
	vblendps	$3, %xmm0, %xmm1, %xmm0         # xmm0 = xmm0[0,1],xmm1[2,3]
	vbroadcastss	26584(%rsp), %xmm1
	vblendps	$8, %xmm1, %xmm0, %xmm0         # xmm0 = xmm0[0,1,2],xmm1[3]
	vbroadcastsd	26712(%rsp), %ymm1
	vblendps	$240, %ymm1, %ymm0, %ymm0       # ymm0 = ymm0[0,1,2,3],ymm1[4,5,6,7]
	vbroadcastss	26840(%rsp), %ymm1
	vblendps	$32, %ymm1, %ymm0, %ymm0        # ymm0 = ymm0[0,1,2,3,4],ymm1[5],ymm0[6,7]
	vpermt2ps	%zmm6, %zmm28, %zmm30
	vpermt2pd	%zmm6, %zmm19, %zmm23
	vpermt2ps	%zmm6, %zmm20, %zmm15
	vpermt2pd	%zmm6, %zmm21, %zmm2
	vpermt2ps	%zmm6, %zmm22, %zmm13
	vpermt2pd	%zmm6, %zmm25, %zmm27
	vmovaps	.LCPI0_70(%rip), %ymm1          # ymm1 = [0,1,2,3,4,5,31,u]
	vpermt2ps	%zmm6, %zmm1, %zmm24
	vpermt2pd	%zmm6, %zmm26, %zmm10
	vinsertf32x4	$1, %xmm6, %ymm16, %ymm1
	vblendps	$192, %ymm6, %ymm0, %ymm0       # ymm0 = ymm0[0,1,2,3,4,5],ymm6[6,7]
	vbroadcastss	27096(%rsp), %ymm6
	vblendps	$128, %ymm6, %ymm0, %ymm0       # ymm0 = ymm0[0,1,2,3,4,5,6],ymm6[7]
	vmovaps	%zmm0, 8320(%rsp)               # 64-byte Spill
	vbroadcastss	26332(%rsp), %xmm0
	vbroadcastss	26204(%rsp), %xmm6
	vblendps	$2, %xmm0, %xmm6, %xmm0         # xmm0 = xmm6[0],xmm0[1],xmm6[2,3]
	vpermt2ps	%ymm0, %ymm9, %ymm5
	vbroadcastss	26588(%rsp), %xmm0
	vblendps	$8, %xmm0, %xmm5, %xmm0         # xmm0 = xmm5[0,1,2],xmm0[3]
	vbroadcastss	26716(%rsp), %ymm5
	vblendps	$240, %ymm5, %ymm0, %ymm0       # ymm0 = ymm0[0,1,2,3],ymm5[4,5,6,7]
	vbroadcastss	26844(%rsp), %ymm5
	vblendps	$32, %ymm5, %ymm0, %ymm0        # ymm0 = ymm0[0,1,2,3,4],ymm5[5],ymm0[6,7]
	vbroadcastss	26972(%rsp), %ymm5
	vblendps	$192, %ymm5, %ymm0, %ymm0       # ymm0 = ymm0[0,1,2,3,4,5],ymm5[6,7]
	vshufpd	$2, %ymm1, %ymm12, %ymm1        # ymm1 = ymm12[0],ymm1[1],ymm12[2],ymm1[2]
	vpermt2ps	%zmm8, %zmm17, %zmm30
	vmovaps	%zmm30, 3072(%rsp)              # 64-byte Spill
	vpermt2ps	%zmm8, %zmm29, %zmm23
	vmovaps	%zmm23, 2112(%rsp)              # 64-byte Spill
	vmovaps	.LCPI0_26(%rip), %ymm4          # ymm4 = [0,1,2,3,4,5,6,27]
	vpermt2ps	%zmm8, %zmm4, %zmm15
	vmovaps	%zmm15, 3136(%rsp)              # 64-byte Spill
	vmovaps	.LCPI0_36(%rip), %ymm4          # ymm4 = [0,1,2,3,4,5,6,28]
	vpermt2ps	%zmm8, %zmm4, %zmm2
	vmovaps	%zmm2, 18368(%rsp)              # 64-byte Spill
	vmovaps	.LCPI0_46(%rip), %ymm2          # ymm2 = [0,1,2,3,4,5,6,29]
	vpermt2ps	%zmm8, %zmm2, %zmm13
	vmovaps	%zmm13, 2240(%rsp)              # 64-byte Spill
	vmovaps	.LCPI0_58(%rip), %ymm2          # ymm2 = [0,1,2,3,4,5,6,30]
	vpermt2ps	%zmm8, %zmm2, %zmm27
	vmovaps	%zmm27, 2176(%rsp)              # 64-byte Spill
	vmovaps	.LCPI0_71(%rip), %ymm2          # ymm2 = [0,1,2,3,4,5,6,31]
	vpermt2ps	%zmm8, %zmm2, %zmm24
	vmovaps	%zmm24, 2432(%rsp)              # 64-byte Spill
	vmovaps	.LCPI0_86(%rip), %ymm2          # ymm2 = [0,1,2,3,4,5,6,8]
	vpermt2ps	%ymm8, %ymm2, %ymm1
	vmovaps	%zmm1, 37504(%rsp)              # 64-byte Spill
	vblendps	$128, %ymm8, %ymm0, %ymm0       # ymm0 = ymm0[0,1,2,3,4,5,6],ymm8[7]
	vmovaps	%zmm0, 8384(%rsp)               # 64-byte Spill
	vpermt2ps	%zmm8, %zmm3, %zmm10
	vmovaps	%zmm10, 37632(%rsp)             # 64-byte Spill
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	27136(%rsp), %zmm1
	vmovaps	%zmm1, 5824(%rsp)               # 64-byte Spill
	vmovaps	.LCPI0_90(%rip), %zmm0          # zmm0 = [0,1,2,3,4,5,6,7,17,u,u,u,u,u,u,u]
	vmovaps	576(%rsp), %zmm2                # 64-byte Reload
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vpermt2ps	%zmm1, %zmm0, %zmm2
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	27264(%rsp), %zmm0
	vmovaps	%zmm0, 7360(%rsp)               # 64-byte Spill
	vmovaps	.LCPI0_97(%rip), %zmm19         # zmm19 = [0,1,2,3,4,5,6,7,8,17,u,u,u,u,u,u]
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vpermt2ps	%zmm0, %zmm19, %zmm2
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	27392(%rsp), %zmm0
	vmovaps	%zmm0, 2304(%rsp)               # 64-byte Spill
	vmovaps	.LCPI0_106(%rip), %zmm8         # zmm8 = [0,1,2,3,4,5,6,7,8,9,17,u,u,u,u,u]
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vpermt2ps	%zmm0, %zmm8, %zmm2
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	27520(%rsp), %zmm0
	vmovaps	%zmm0, 7680(%rsp)               # 64-byte Spill
	vmovaps	.LCPI0_116(%rip), %zmm5         # zmm5 = [0,1,2,3,4,5,6,7,8,9,10,17,u,u,u,u]
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vpermt2ps	%zmm0, %zmm5, %zmm2
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	27648(%rsp), %zmm0
	vmovaps	%zmm0, 7616(%rsp)               # 64-byte Spill
	vmovaps	.LCPI0_126(%rip), %zmm5         # zmm5 = [0,1,2,3,4,5,6,7,8,9,10,11,17,u,u,u]
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vpermt2ps	%zmm0, %zmm5, %zmm2
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	27776(%rsp), %zmm0
	vmovaps	%zmm0, 7552(%rsp)               # 64-byte Spill
	vmovaps	.LCPI0_138(%rip), %zmm5         # zmm5 = [0,1,2,3,4,5,6,7,8,9,10,11,12,17,u,u]
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vpermt2ps	%zmm0, %zmm5, %zmm2
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	27904(%rsp), %zmm0
	vmovaps	%zmm0, 7488(%rsp)               # 64-byte Spill
	vmovaps	.LCPI0_151(%rip), %zmm5         # zmm5 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,17,u]
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vpermt2ps	%zmm0, %zmm5, %zmm2
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	28032(%rsp), %zmm0
	vmovaps	%zmm0, 7424(%rsp)               # 64-byte Spill
	vmovaps	.LCPI0_165(%rip), %zmm5         # zmm5 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,17]
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vpermt2ps	%zmm0, %zmm5, %zmm2
	vmulps	5888(%rsp), %zmm2, %zmm0        # 64-byte Folded Reload
	vmovaps	%zmm0, 19584(%rsp)              # 64-byte Spill
	vmulps	3712(%rsp), %zmm2, %zmm0        # 64-byte Folded Reload
	vmovaps	%zmm0, 11072(%rsp)              # 64-byte Spill
	vmulps	3840(%rsp), %zmm2, %zmm0        # 64-byte Folded Reload
	vmovaps	%zmm0, 19264(%rsp)              # 64-byte Spill
	vmulps	3968(%rsp), %zmm2, %zmm0        # 64-byte Folded Reload
	vmovaps	%zmm0, 19328(%rsp)              # 64-byte Spill
	vmulps	4096(%rsp), %zmm2, %zmm0        # 64-byte Folded Reload
	vmovaps	%zmm0, 19392(%rsp)              # 64-byte Spill
	vmulps	4224(%rsp), %zmm2, %zmm0        # 64-byte Folded Reload
	vmovaps	%zmm0, 19456(%rsp)              # 64-byte Spill
	vmulps	6016(%rsp), %zmm2, %zmm0        # 64-byte Folded Reload
	vmovaps	%zmm0, 19520(%rsp)              # 64-byte Spill
	vmulps	4352(%rsp), %zmm2, %zmm0        # 64-byte Folded Reload
	vmovaps	%zmm0, 19648(%rsp)              # 64-byte Spill
	vmulps	4480(%rsp), %zmm2, %zmm0        # 64-byte Folded Reload
	vmovaps	%zmm0, 19712(%rsp)              # 64-byte Spill
	vmulps	4608(%rsp), %zmm2, %zmm0        # 64-byte Folded Reload
	vmovaps	%zmm0, 19776(%rsp)              # 64-byte Spill
	vmulps	3200(%rsp), %zmm2, %zmm27       # 64-byte Folded Reload
	vmulps	3328(%rsp), %zmm2, %zmm30       # 64-byte Folded Reload
	vmulps	3456(%rsp), %zmm2, %zmm24       # 64-byte Folded Reload
	vmulps	3584(%rsp), %zmm2, %zmm25       # 64-byte Folded Reload
	vmulps	4736(%rsp), %zmm2, %zmm22       # 64-byte Folded Reload
	vmulps	6144(%rsp), %zmm2, %zmm19       # 64-byte Folded Reload
	vmovaps	7744(%rsp), %zmm26              # 64-byte Reload
	vmulps	%zmm2, %zmm26, %zmm18
	vmulps	6336(%rsp), %zmm2, %zmm17       # 64-byte Folded Reload
	vmulps	4864(%rsp), %zmm2, %zmm16       # 64-byte Folded Reload
	vmulps	6720(%rsp), %zmm2, %zmm14       # 64-byte Folded Reload
	vmulps	6848(%rsp), %zmm2, %zmm15       # 64-byte Folded Reload
	vmulps	6912(%rsp), %zmm2, %zmm13       # 64-byte Folded Reload
	vmulps	8512(%rsp), %zmm2, %zmm12       # 64-byte Folded Reload
	vmulps	7936(%rsp), %zmm2, %zmm11       # 64-byte Folded Reload
	vmovaps	8640(%rsp), %zmm28              # 64-byte Reload
	vmulps	%zmm2, %zmm28, %zmm10
	vmovaps	8064(%rsp), %zmm21              # 64-byte Reload
	vmulps	%zmm2, %zmm21, %zmm9
	vmovaps	6528(%rsp), %zmm20              # 64-byte Reload
	vmulps	%zmm2, %zmm20, %zmm8
	vmovaps	6592(%rsp), %zmm23              # 64-byte Reload
	vmulps	%zmm2, %zmm23, %zmm7
	vmulps	5056(%rsp), %zmm2, %zmm6        # 64-byte Folded Reload
	vmulps	5184(%rsp), %zmm2, %zmm5        # 64-byte Folded Reload
	vmulps	5312(%rsp), %zmm2, %zmm4        # 64-byte Folded Reload
	vmulps	7040(%rsp), %zmm2, %zmm3        # 64-byte Folded Reload
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	25088(%rsp), %zmm1
	vmovaps	%zmm1, 8192(%rsp)               # 64-byte Spill
	vmovaps	25216(%rsp), %zmm29
	vmovaps	%zmm29, 5760(%rsp)              # 64-byte Spill
	vmovaps	1152(%rsp), %zmm0               # 64-byte Reload
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vmovaps	.LCPI0_90(%rip), %zmm2          # zmm2 = [0,1,2,3,4,5,6,7,17,u,u,u,u,u,u,u]
	vpermt2ps	%zmm1, %zmm2, %zmm0
	vmovaps	.LCPI0_97(%rip), %zmm2          # zmm2 = [0,1,2,3,4,5,6,7,8,17,u,u,u,u,u,u]
	vpermt2ps	%zmm29, %zmm2, %zmm0
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	25344(%rsp), %zmm1
	vmovaps	%zmm1, 7168(%rsp)               # 64-byte Spill
	vmovaps	25472(%rsp), %zmm29
	vmovaps	%zmm29, 8256(%rsp)              # 64-byte Spill
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vmovaps	.LCPI0_106(%rip), %zmm2         # zmm2 = [0,1,2,3,4,5,6,7,8,9,17,u,u,u,u,u]
	vpermt2ps	%zmm1, %zmm2, %zmm0
	vmovaps	.LCPI0_116(%rip), %zmm2         # zmm2 = [0,1,2,3,4,5,6,7,8,9,10,17,u,u,u,u]
	vpermt2ps	%zmm29, %zmm2, %zmm0
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	25600(%rsp), %zmm1
	vmovaps	%zmm1, 7232(%rsp)               # 64-byte Spill
	vmovaps	25728(%rsp), %zmm29
	vmovaps	%zmm29, 7296(%rsp)              # 64-byte Spill
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vmovaps	.LCPI0_126(%rip), %zmm2         # zmm2 = [0,1,2,3,4,5,6,7,8,9,10,11,17,u,u,u]
	vpermt2ps	%zmm1, %zmm2, %zmm0
	vmovaps	.LCPI0_138(%rip), %zmm2         # zmm2 = [0,1,2,3,4,5,6,7,8,9,10,11,12,17,u,u]
	vpermt2ps	%zmm29, %zmm2, %zmm0
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	25856(%rsp), %zmm29
	vmovaps	%zmm29, 1920(%rsp)              # 64-byte Spill
	vmovaps	25984(%rsp), %zmm1
	vmovaps	%zmm1, 576(%rsp)                # 64-byte Spill
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vmovaps	.LCPI0_151(%rip), %zmm2         # zmm2 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,17,u]
	vpermt2ps	%zmm29, %zmm2, %zmm0
	vmovaps	.LCPI0_165(%rip), %zmm2         # zmm2 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,17]
	vpermt2ps	%zmm1, %zmm2, %zmm0
	vmovaps	19584(%rsp), %zmm2              # 64-byte Reload
	vfmadd231ps	5952(%rsp), %zmm0, %zmm2 # 64-byte Folded Reload
                                        # zmm2 = (zmm0 * mem) + zmm2
	vmovaps	%zmm2, 19584(%rsp)              # 64-byte Spill
	vmovaps	11072(%rsp), %zmm2              # 64-byte Reload
	vfmadd231ps	3776(%rsp), %zmm0, %zmm2 # 64-byte Folded Reload
                                        # zmm2 = (zmm0 * mem) + zmm2
	vmovaps	%zmm2, 11072(%rsp)              # 64-byte Spill
	vmovaps	19264(%rsp), %zmm31             # 64-byte Reload
	vfmadd231ps	3904(%rsp), %zmm0, %zmm31 # 64-byte Folded Reload
                                        # zmm31 = (zmm0 * mem) + zmm31
	vmovaps	%zmm31, 19264(%rsp)             # 64-byte Spill
	vmovaps	19328(%rsp), %zmm31             # 64-byte Reload
	vfmadd231ps	4032(%rsp), %zmm0, %zmm31 # 64-byte Folded Reload
                                        # zmm31 = (zmm0 * mem) + zmm31
	vmovaps	%zmm31, 19328(%rsp)             # 64-byte Spill
	vmovaps	19392(%rsp), %zmm31             # 64-byte Reload
	vfmadd231ps	4160(%rsp), %zmm0, %zmm31 # 64-byte Folded Reload
                                        # zmm31 = (zmm0 * mem) + zmm31
	vmovaps	%zmm31, 19392(%rsp)             # 64-byte Spill
	vmovaps	19456(%rsp), %zmm31             # 64-byte Reload
	vfmadd231ps	4288(%rsp), %zmm0, %zmm31 # 64-byte Folded Reload
                                        # zmm31 = (zmm0 * mem) + zmm31
	vmovaps	%zmm31, 19456(%rsp)             # 64-byte Spill
	vmovaps	19520(%rsp), %zmm1              # 64-byte Reload
	vfmadd231ps	6080(%rsp), %zmm0, %zmm1 # 64-byte Folded Reload
                                        # zmm1 = (zmm0 * mem) + zmm1
	vmovaps	%zmm1, 19520(%rsp)              # 64-byte Spill
	vmovaps	19648(%rsp), %zmm31             # 64-byte Reload
	vfmadd231ps	4416(%rsp), %zmm0, %zmm31 # 64-byte Folded Reload
                                        # zmm31 = (zmm0 * mem) + zmm31
	vmovaps	%zmm31, 19648(%rsp)             # 64-byte Spill
	vmovaps	19712(%rsp), %zmm1              # 64-byte Reload
	vfmadd231ps	4544(%rsp), %zmm0, %zmm1 # 64-byte Folded Reload
                                        # zmm1 = (zmm0 * mem) + zmm1
	vmovaps	%zmm1, 19712(%rsp)              # 64-byte Spill
	vmovaps	19776(%rsp), %zmm1              # 64-byte Reload
	vfmadd231ps	4672(%rsp), %zmm0, %zmm1 # 64-byte Folded Reload
                                        # zmm1 = (zmm0 * mem) + zmm1
	vmovaps	%zmm1, 19776(%rsp)              # 64-byte Spill
	vfmadd231ps	3264(%rsp), %zmm0, %zmm27 # 64-byte Folded Reload
                                        # zmm27 = (zmm0 * mem) + zmm27
	vmovaps	%zmm27, 35200(%rsp)             # 64-byte Spill
	vfmadd231ps	3392(%rsp), %zmm0, %zmm30 # 64-byte Folded Reload
                                        # zmm30 = (zmm0 * mem) + zmm30
	vmovaps	%zmm30, 35968(%rsp)             # 64-byte Spill
	vfmadd231ps	3520(%rsp), %zmm0, %zmm24 # 64-byte Folded Reload
                                        # zmm24 = (zmm0 * mem) + zmm24
	vmovaps	%zmm24, 36352(%rsp)             # 64-byte Spill
	vfmadd231ps	3648(%rsp), %zmm0, %zmm25 # 64-byte Folded Reload
                                        # zmm25 = (zmm0 * mem) + zmm25
	vmovaps	%zmm25, 17600(%rsp)             # 64-byte Spill
	vfmadd231ps	4800(%rsp), %zmm0, %zmm22 # 64-byte Folded Reload
                                        # zmm22 = (zmm0 * mem) + zmm22
	vmovaps	%zmm22, 17920(%rsp)             # 64-byte Spill
	vfmadd231ps	6208(%rsp), %zmm0, %zmm19 # 64-byte Folded Reload
                                        # zmm19 = (zmm0 * mem) + zmm19
	vmovaps	%zmm19, 18048(%rsp)             # 64-byte Spill
	vfmadd231ps	6272(%rsp), %zmm0, %zmm18 # 64-byte Folded Reload
                                        # zmm18 = (zmm0 * mem) + zmm18
	vmovaps	%zmm18, 16320(%rsp)             # 64-byte Spill
	vfmadd231ps	6400(%rsp), %zmm0, %zmm17 # 64-byte Folded Reload
                                        # zmm17 = (zmm0 * mem) + zmm17
	vmovaps	%zmm17, 33792(%rsp)             # 64-byte Spill
	vfmadd231ps	6784(%rsp), %zmm0, %zmm16 # 64-byte Folded Reload
                                        # zmm16 = (zmm0 * mem) + zmm16
	vmovaps	%zmm16, 35776(%rsp)             # 64-byte Spill
	vfmadd231ps	6464(%rsp), %zmm0, %zmm14 # 64-byte Folded Reload
                                        # zmm14 = (zmm0 * mem) + zmm14
	vmovaps	%zmm14, 35264(%rsp)             # 64-byte Spill
	vfmadd231ps	7808(%rsp), %zmm0, %zmm15 # 64-byte Folded Reload
                                        # zmm15 = (zmm0 * mem) + zmm15
	vmovaps	%zmm15, 35456(%rsp)             # 64-byte Spill
	vfmadd231ps	7872(%rsp), %zmm0, %zmm13 # 64-byte Folded Reload
                                        # zmm13 = (zmm0 * mem) + zmm13
	vmovaps	%zmm13, 17088(%rsp)             # 64-byte Spill
	vfmadd231ps	8576(%rsp), %zmm0, %zmm12 # 64-byte Folded Reload
                                        # zmm12 = (zmm0 * mem) + zmm12
	vmovaps	%zmm12, 18240(%rsp)             # 64-byte Spill
	vfmadd231ps	6976(%rsp), %zmm0, %zmm11 # 64-byte Folded Reload
                                        # zmm11 = (zmm0 * mem) + zmm11
	vmovaps	%zmm11, 37056(%rsp)             # 64-byte Spill
	vfmadd231ps	8000(%rsp), %zmm0, %zmm10 # 64-byte Folded Reload
                                        # zmm10 = (zmm0 * mem) + zmm10
	vmovaps	%zmm10, 36864(%rsp)             # 64-byte Spill
	vfmadd231ps	8128(%rsp), %zmm0, %zmm9 # 64-byte Folded Reload
                                        # zmm9 = (zmm0 * mem) + zmm9
	vmovaps	%zmm9, 36800(%rsp)              # 64-byte Spill
	vfmadd231ps	4928(%rsp), %zmm0, %zmm8 # 64-byte Folded Reload
                                        # zmm8 = (zmm0 * mem) + zmm8
	vmovaps	%zmm8, 44928(%rsp)              # 64-byte Spill
	vfmadd231ps	4992(%rsp), %zmm0, %zmm7 # 64-byte Folded Reload
                                        # zmm7 = (zmm0 * mem) + zmm7
	vmovaps	%zmm7, 45312(%rsp)              # 64-byte Spill
	vfmadd231ps	5120(%rsp), %zmm0, %zmm6 # 64-byte Folded Reload
                                        # zmm6 = (zmm0 * mem) + zmm6
	vmovaps	%zmm6, 45696(%rsp)              # 64-byte Spill
	vfmadd231ps	5248(%rsp), %zmm0, %zmm5 # 64-byte Folded Reload
                                        # zmm5 = (zmm0 * mem) + zmm5
	vmovaps	%zmm5, 46080(%rsp)              # 64-byte Spill
	vfmadd231ps	5376(%rsp), %zmm0, %zmm4 # 64-byte Folded Reload
                                        # zmm4 = (zmm0 * mem) + zmm4
	vmovaps	%zmm4, 46400(%rsp)              # 64-byte Spill
	vfmadd231ps	7104(%rsp), %zmm0, %zmm3 # 64-byte Folded Reload
                                        # zmm3 = (zmm0 * mem) + zmm3
	vmovaps	%zmm3, 46656(%rsp)              # 64-byte Spill
	vmovapd	.LCPI0_91(%rip), %zmm30         # zmm30 = [0,1,2,3,9,u,u,u]
	vmovapd	19840(%rsp), %zmm1              # 64-byte Reload
	vpermt2pd	5824(%rsp), %zmm30, %zmm1 # 64-byte Folded Reload
	vmovaps	.LCPI0_98(%rip), %zmm22         # zmm22 = [0,1,2,3,4,5,6,7,8,18,u,u,u,u,u,u]
	vpermt2ps	7360(%rsp), %zmm22, %zmm1 # 64-byte Folded Reload
	vmovapd	.LCPI0_107(%rip), %zmm18        # zmm18 = [0,1,2,3,4,9,u,u]
	vpermt2pd	2304(%rsp), %zmm18, %zmm1 # 64-byte Folded Reload
	vmovaps	.LCPI0_117(%rip), %zmm6         # zmm6 = [0,1,2,3,4,5,6,7,8,9,10,18,u,u,u,u]
	vpermt2ps	7680(%rsp), %zmm6, %zmm1 # 64-byte Folded Reload
	vmovapd	.LCPI0_127(%rip), %zmm8         # zmm8 = [0,1,2,3,4,5,9,u]
	vpermt2pd	7616(%rsp), %zmm8, %zmm1 # 64-byte Folded Reload
	vmovaps	.LCPI0_139(%rip), %zmm19        # zmm19 = [0,1,2,3,4,5,6,7,8,9,10,11,12,18,u,u]
	vpermt2ps	7552(%rsp), %zmm19, %zmm1 # 64-byte Folded Reload
	vmovapd	.LCPI0_152(%rip), %zmm0         # zmm0 = [0,1,2,3,4,5,6,9]
	vpermt2pd	7488(%rsp), %zmm0, %zmm1 # 64-byte Folded Reload
	vmovaps	.LCPI0_166(%rip), %zmm0         # zmm0 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,18]
	vpermt2ps	7424(%rsp), %zmm0, %zmm1 # 64-byte Folded Reload
	vmulps	5888(%rsp), %zmm1, %zmm16       # 64-byte Folded Reload
	vmulps	3712(%rsp), %zmm1, %zmm14       # 64-byte Folded Reload
	vmulps	3840(%rsp), %zmm1, %zmm13       # 64-byte Folded Reload
	vmulps	3968(%rsp), %zmm1, %zmm12       # 64-byte Folded Reload
	vmulps	4096(%rsp), %zmm1, %zmm11       # 64-byte Folded Reload
	vmulps	4224(%rsp), %zmm1, %zmm10       # 64-byte Folded Reload
	vmulps	6016(%rsp), %zmm1, %zmm9        # 64-byte Folded Reload
	vmulps	4352(%rsp), %zmm1, %zmm8        # 64-byte Folded Reload
	vmulps	4480(%rsp), %zmm1, %zmm7        # 64-byte Folded Reload
	vmulps	4608(%rsp), %zmm1, %zmm6        # 64-byte Folded Reload
	vmulps	3200(%rsp), %zmm1, %zmm5        # 64-byte Folded Reload
	vmulps	3328(%rsp), %zmm1, %zmm4        # 64-byte Folded Reload
	vmulps	3456(%rsp), %zmm1, %zmm2        # 64-byte Folded Reload
	vmulps	3584(%rsp), %zmm1, %zmm0        # 64-byte Folded Reload
	vmulps	4736(%rsp), %zmm1, %zmm3        # 64-byte Folded Reload
	vmulps	6144(%rsp), %zmm1, %zmm30       # 64-byte Folded Reload
	vmulps	%zmm1, %zmm26, %zmm19
	vmulps	6336(%rsp), %zmm1, %zmm18       # 64-byte Folded Reload
	vmulps	4864(%rsp), %zmm1, %zmm26       # 64-byte Folded Reload
	vmulps	6720(%rsp), %zmm1, %zmm17       # 64-byte Folded Reload
	vmulps	6848(%rsp), %zmm1, %zmm27       # 64-byte Folded Reload
	vmulps	6912(%rsp), %zmm1, %zmm22       # 64-byte Folded Reload
	vmulps	8512(%rsp), %zmm1, %zmm24       # 64-byte Folded Reload
	vmovaps	%zmm24, 1152(%rsp)              # 64-byte Spill
	vmulps	7936(%rsp), %zmm1, %zmm29       # 64-byte Folded Reload
	vmulps	%zmm1, %zmm28, %zmm25
	vmulps	%zmm1, %zmm21, %zmm24
	vmulps	%zmm1, %zmm20, %zmm21
	vmulps	%zmm1, %zmm23, %zmm20
	vmulps	5056(%rsp), %zmm1, %zmm23       # 64-byte Folded Reload
	vmulps	5184(%rsp), %zmm1, %zmm28       # 64-byte Folded Reload
	vmulps	5312(%rsp), %zmm1, %zmm15       # 64-byte Folded Reload
	vmovaps	%zmm15, 19840(%rsp)             # 64-byte Spill
	vmulps	7040(%rsp), %zmm1, %zmm15       # 64-byte Folded Reload
	vmovapd	64(%rsp), %zmm1                 # 64-byte Reload
	vmovapd	.LCPI0_91(%rip), %zmm31         # zmm31 = [0,1,2,3,9,u,u,u]
	vpermt2pd	8192(%rsp), %zmm31, %zmm1 # 64-byte Folded Reload
	vmovaps	.LCPI0_98(%rip), %zmm31         # zmm31 = [0,1,2,3,4,5,6,7,8,18,u,u,u,u,u,u]
	vpermt2ps	5760(%rsp), %zmm31, %zmm1 # 64-byte Folded Reload
	vmovapd	.LCPI0_107(%rip), %zmm31        # zmm31 = [0,1,2,3,4,9,u,u]
	vpermt2pd	7168(%rsp), %zmm31, %zmm1 # 64-byte Folded Reload
	vmovaps	.LCPI0_117(%rip), %zmm31        # zmm31 = [0,1,2,3,4,5,6,7,8,9,10,18,u,u,u,u]
	vpermt2ps	8256(%rsp), %zmm31, %zmm1 # 64-byte Folded Reload
	vmovapd	.LCPI0_127(%rip), %zmm31        # zmm31 = [0,1,2,3,4,5,9,u]
	vpermt2pd	7232(%rsp), %zmm31, %zmm1 # 64-byte Folded Reload
	vmovaps	.LCPI0_139(%rip), %zmm31        # zmm31 = [0,1,2,3,4,5,6,7,8,9,10,11,12,18,u,u]
	vpermt2ps	7296(%rsp), %zmm31, %zmm1 # 64-byte Folded Reload
	vmovapd	.LCPI0_152(%rip), %zmm31        # zmm31 = [0,1,2,3,4,5,6,9]
	vpermt2pd	1920(%rsp), %zmm31, %zmm1 # 64-byte Folded Reload
	vmovaps	.LCPI0_166(%rip), %zmm31        # zmm31 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,18]
	vpermt2ps	576(%rsp), %zmm31, %zmm1 # 64-byte Folded Reload
	vfmadd231ps	5952(%rsp), %zmm1, %zmm16 # 64-byte Folded Reload
                                        # zmm16 = (zmm1 * mem) + zmm16
	vmovaps	%zmm16, 14848(%rsp)             # 64-byte Spill
	vfmadd231ps	3776(%rsp), %zmm1, %zmm14 # 64-byte Folded Reload
                                        # zmm14 = (zmm1 * mem) + zmm14
	vmovaps	%zmm14, 15680(%rsp)             # 64-byte Spill
	vfmadd231ps	3904(%rsp), %zmm1, %zmm13 # 64-byte Folded Reload
                                        # zmm13 = (zmm1 * mem) + zmm13
	vmovaps	%zmm13, 9920(%rsp)              # 64-byte Spill
	vfmadd231ps	4032(%rsp), %zmm1, %zmm12 # 64-byte Folded Reload
                                        # zmm12 = (zmm1 * mem) + zmm12
	vmovaps	%zmm12, 31424(%rsp)             # 64-byte Spill
	vfmadd231ps	4160(%rsp), %zmm1, %zmm11 # 64-byte Folded Reload
                                        # zmm11 = (zmm1 * mem) + zmm11
	vmovaps	%zmm11, 31552(%rsp)             # 64-byte Spill
	vfmadd231ps	4288(%rsp), %zmm1, %zmm10 # 64-byte Folded Reload
                                        # zmm10 = (zmm1 * mem) + zmm10
	vmovaps	%zmm10, 31936(%rsp)             # 64-byte Spill
	vfmadd231ps	6080(%rsp), %zmm1, %zmm9 # 64-byte Folded Reload
                                        # zmm9 = (zmm1 * mem) + zmm9
	vmovaps	%zmm9, 32768(%rsp)              # 64-byte Spill
	vfmadd231ps	4416(%rsp), %zmm1, %zmm8 # 64-byte Folded Reload
                                        # zmm8 = (zmm1 * mem) + zmm8
	vmovaps	%zmm8, 33088(%rsp)              # 64-byte Spill
	vfmadd231ps	4544(%rsp), %zmm1, %zmm7 # 64-byte Folded Reload
                                        # zmm7 = (zmm1 * mem) + zmm7
	vmovaps	%zmm7, 33600(%rsp)              # 64-byte Spill
	vfmadd231ps	4672(%rsp), %zmm1, %zmm6 # 64-byte Folded Reload
                                        # zmm6 = (zmm1 * mem) + zmm6
	vmovaps	%zmm6, 34112(%rsp)              # 64-byte Spill
	vfmadd231ps	3264(%rsp), %zmm1, %zmm5 # 64-byte Folded Reload
                                        # zmm5 = (zmm1 * mem) + zmm5
	vmovaps	%zmm5, 34944(%rsp)              # 64-byte Spill
	vfmadd231ps	3392(%rsp), %zmm1, %zmm4 # 64-byte Folded Reload
                                        # zmm4 = (zmm1 * mem) + zmm4
	vmovaps	%zmm4, 35584(%rsp)              # 64-byte Spill
	vfmadd231ps	3520(%rsp), %zmm1, %zmm2 # 64-byte Folded Reload
                                        # zmm2 = (zmm1 * mem) + zmm2
	vmovaps	%zmm2, 36160(%rsp)              # 64-byte Spill
	vfmadd231ps	3648(%rsp), %zmm1, %zmm0 # 64-byte Folded Reload
                                        # zmm0 = (zmm1 * mem) + zmm0
	vmovaps	%zmm0, 17408(%rsp)              # 64-byte Spill
	vfmadd231ps	4800(%rsp), %zmm1, %zmm3 # 64-byte Folded Reload
                                        # zmm3 = (zmm1 * mem) + zmm3
	vmovaps	%zmm3, 17792(%rsp)              # 64-byte Spill
	vfmadd231ps	6208(%rsp), %zmm1, %zmm30 # 64-byte Folded Reload
                                        # zmm30 = (zmm1 * mem) + zmm30
	vmovaps	%zmm30, 17984(%rsp)             # 64-byte Spill
	vfmadd231ps	6272(%rsp), %zmm1, %zmm19 # 64-byte Folded Reload
                                        # zmm19 = (zmm1 * mem) + zmm19
	vmovaps	%zmm19, 15808(%rsp)             # 64-byte Spill
	vfmadd231ps	6400(%rsp), %zmm1, %zmm18 # 64-byte Folded Reload
                                        # zmm18 = (zmm1 * mem) + zmm18
	vmovaps	%zmm18, 33664(%rsp)             # 64-byte Spill
	vfmadd231ps	6784(%rsp), %zmm1, %zmm26 # 64-byte Folded Reload
                                        # zmm26 = (zmm1 * mem) + zmm26
	vmovaps	%zmm26, 16832(%rsp)             # 64-byte Spill
	vfmadd231ps	6464(%rsp), %zmm1, %zmm17 # 64-byte Folded Reload
                                        # zmm17 = (zmm1 * mem) + zmm17
	vmovaps	%zmm17, 34432(%rsp)             # 64-byte Spill
	vfmadd231ps	7808(%rsp), %zmm1, %zmm27 # 64-byte Folded Reload
                                        # zmm27 = (zmm1 * mem) + zmm27
	vmovaps	%zmm27, 35136(%rsp)             # 64-byte Spill
	vfmadd231ps	7872(%rsp), %zmm1, %zmm22 # 64-byte Folded Reload
                                        # zmm22 = (zmm1 * mem) + zmm22
	vmovaps	%zmm22, 35712(%rsp)             # 64-byte Spill
	vmovaps	1152(%rsp), %zmm0               # 64-byte Reload
	vfmadd231ps	8576(%rsp), %zmm1, %zmm0 # 64-byte Folded Reload
                                        # zmm0 = (zmm1 * mem) + zmm0
	vmovaps	%zmm0, 1152(%rsp)               # 64-byte Spill
	vfmadd231ps	6976(%rsp), %zmm1, %zmm29 # 64-byte Folded Reload
                                        # zmm29 = (zmm1 * mem) + zmm29
	vmovaps	%zmm29, 18304(%rsp)             # 64-byte Spill
	vfmadd231ps	8000(%rsp), %zmm1, %zmm25 # 64-byte Folded Reload
                                        # zmm25 = (zmm1 * mem) + zmm25
	vmovaps	%zmm25, 36672(%rsp)             # 64-byte Spill
	vfmadd231ps	8128(%rsp), %zmm1, %zmm24 # 64-byte Folded Reload
                                        # zmm24 = (zmm1 * mem) + zmm24
	vmovaps	%zmm24, 36608(%rsp)             # 64-byte Spill
	vfmadd231ps	4928(%rsp), %zmm1, %zmm21 # 64-byte Folded Reload
                                        # zmm21 = (zmm1 * mem) + zmm21
	vmovaps	%zmm21, 44800(%rsp)             # 64-byte Spill
	vfmadd231ps	4992(%rsp), %zmm1, %zmm20 # 64-byte Folded Reload
                                        # zmm20 = (zmm1 * mem) + zmm20
	vmovaps	%zmm20, 45184(%rsp)             # 64-byte Spill
	vfmadd231ps	5120(%rsp), %zmm1, %zmm23 # 64-byte Folded Reload
                                        # zmm23 = (zmm1 * mem) + zmm23
	vmovaps	%zmm23, 45568(%rsp)             # 64-byte Spill
	vfmadd231ps	5248(%rsp), %zmm1, %zmm28 # 64-byte Folded Reload
                                        # zmm28 = (zmm1 * mem) + zmm28
	vmovaps	%zmm28, 45952(%rsp)             # 64-byte Spill
	vmovaps	19840(%rsp), %zmm0              # 64-byte Reload
	vfmadd231ps	5376(%rsp), %zmm1, %zmm0 # 64-byte Folded Reload
                                        # zmm0 = (zmm1 * mem) + zmm0
	vmovaps	%zmm0, 19840(%rsp)              # 64-byte Spill
	vfmadd231ps	7104(%rsp), %zmm1, %zmm15 # 64-byte Folded Reload
                                        # zmm15 = (zmm1 * mem) + zmm15
	vmovaps	%zmm15, 46592(%rsp)             # 64-byte Spill
	vmovaps	.LCPI0_92(%rip), %zmm1          # zmm1 = [0,1,2,3,4,5,6,7,19,u,u,u,u,u,u,u]
	vmovaps	8768(%rsp), %zmm0               # 64-byte Reload
	vpermt2ps	5824(%rsp), %zmm1, %zmm0 # 64-byte Folded Reload
	vmovaps	.LCPI0_99(%rip), %zmm23         # zmm23 = [0,1,2,3,4,5,6,7,8,19,u,u,u,u,u,u]
	vpermt2ps	7360(%rsp), %zmm23, %zmm0 # 64-byte Folded Reload
	vmovaps	.LCPI0_108(%rip), %zmm15        # zmm15 = [0,1,2,3,4,5,6,7,8,9,19,u,u,u,u,u]
	vpermt2ps	2304(%rsp), %zmm15, %zmm0 # 64-byte Folded Reload
	vmovaps	.LCPI0_118(%rip), %zmm7         # zmm7 = [0,1,2,3,4,5,6,7,8,9,10,19,u,u,u,u]
	vpermt2ps	7680(%rsp), %zmm7, %zmm0 # 64-byte Folded Reload
	vmovaps	.LCPI0_128(%rip), %zmm2         # zmm2 = [0,1,2,3,4,5,6,7,8,9,10,11,19,u,u,u]
	vpermt2ps	7616(%rsp), %zmm2, %zmm0 # 64-byte Folded Reload
	vmovaps	.LCPI0_140(%rip), %zmm10        # zmm10 = [0,1,2,3,4,5,6,7,8,9,10,11,12,19,u,u]
	vpermt2ps	7552(%rsp), %zmm10, %zmm0 # 64-byte Folded Reload
	vmovaps	.LCPI0_153(%rip), %zmm19        # zmm19 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,19,u]
	vpermt2ps	7488(%rsp), %zmm19, %zmm0 # 64-byte Folded Reload
	vmovaps	.LCPI0_167(%rip), %zmm11        # zmm11 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,19]
	vpermt2ps	7424(%rsp), %zmm11, %zmm0 # 64-byte Folded Reload
	vmulps	5888(%rsp), %zmm0, %zmm8        # 64-byte Folded Reload
	vmulps	3712(%rsp), %zmm0, %zmm9        # 64-byte Folded Reload
	vmulps	3840(%rsp), %zmm0, %zmm6        # 64-byte Folded Reload
	vmulps	3968(%rsp), %zmm0, %zmm5        # 64-byte Folded Reload
	vmulps	4096(%rsp), %zmm0, %zmm4        # 64-byte Folded Reload
	vmulps	4224(%rsp), %zmm0, %zmm23       # 64-byte Folded Reload
	vmulps	6016(%rsp), %zmm0, %zmm3        # 64-byte Folded Reload
	vmulps	4352(%rsp), %zmm0, %zmm2        # 64-byte Folded Reload
	vmulps	4480(%rsp), %zmm0, %zmm30       # 64-byte Folded Reload
	vmulps	4608(%rsp), %zmm0, %zmm1        # 64-byte Folded Reload
	vmulps	3200(%rsp), %zmm0, %zmm18       # 64-byte Folded Reload
	vmulps	3328(%rsp), %zmm0, %zmm11       # 64-byte Folded Reload
	vmulps	3456(%rsp), %zmm0, %zmm7        # 64-byte Folded Reload
	vmulps	3584(%rsp), %zmm0, %zmm19       # 64-byte Folded Reload
	vmulps	4736(%rsp), %zmm0, %zmm13       # 64-byte Folded Reload
	vmulps	6144(%rsp), %zmm0, %zmm12       # 64-byte Folded Reload
	vmovaps	7744(%rsp), %zmm29              # 64-byte Reload
	vmulps	%zmm0, %zmm29, %zmm24
	vmulps	6336(%rsp), %zmm0, %zmm15       # 64-byte Folded Reload
	vmulps	4864(%rsp), %zmm0, %zmm25       # 64-byte Folded Reload
	vmulps	6720(%rsp), %zmm0, %zmm21       # 64-byte Folded Reload
	vmulps	6848(%rsp), %zmm0, %zmm27       # 64-byte Folded Reload
	vmovaps	6912(%rsp), %zmm28              # 64-byte Reload
	vmulps	%zmm0, %zmm28, %zmm26
	vmulps	8512(%rsp), %zmm0, %zmm10       # 64-byte Folded Reload
	vmovaps	%zmm10, 6656(%rsp)              # 64-byte Spill
	vmulps	7936(%rsp), %zmm0, %zmm10       # 64-byte Folded Reload
	vmovaps	%zmm10, 64(%rsp)                # 64-byte Spill
	vmulps	8640(%rsp), %zmm0, %zmm10       # 64-byte Folded Reload
	vmovaps	%zmm10, 18944(%rsp)             # 64-byte Spill
	vmulps	8064(%rsp), %zmm0, %zmm10       # 64-byte Folded Reload
	vmovaps	%zmm10, 18880(%rsp)             # 64-byte Spill
	vmulps	6528(%rsp), %zmm0, %zmm14       # 64-byte Folded Reload
	vmulps	6592(%rsp), %zmm0, %zmm22       # 64-byte Folded Reload
	vmulps	5056(%rsp), %zmm0, %zmm16       # 64-byte Folded Reload
	vmulps	5184(%rsp), %zmm0, %zmm17       # 64-byte Folded Reload
	vmulps	5312(%rsp), %zmm0, %zmm20       # 64-byte Folded Reload
	vmulps	7040(%rsp), %zmm0, %zmm10       # 64-byte Folded Reload
	vmovaps	8832(%rsp), %zmm0               # 64-byte Reload
	vmovaps	.LCPI0_92(%rip), %zmm31         # zmm31 = [0,1,2,3,4,5,6,7,19,u,u,u,u,u,u,u]
	vpermt2ps	8192(%rsp), %zmm31, %zmm0 # 64-byte Folded Reload
	vmovaps	.LCPI0_99(%rip), %zmm31         # zmm31 = [0,1,2,3,4,5,6,7,8,19,u,u,u,u,u,u]
	vpermt2ps	5760(%rsp), %zmm31, %zmm0 # 64-byte Folded Reload
	vmovaps	.LCPI0_108(%rip), %zmm31        # zmm31 = [0,1,2,3,4,5,6,7,8,9,19,u,u,u,u,u]
	vpermt2ps	7168(%rsp), %zmm31, %zmm0 # 64-byte Folded Reload
	vmovaps	.LCPI0_118(%rip), %zmm31        # zmm31 = [0,1,2,3,4,5,6,7,8,9,10,19,u,u,u,u]
	vpermt2ps	8256(%rsp), %zmm31, %zmm0 # 64-byte Folded Reload
	vmovaps	.LCPI0_128(%rip), %zmm31        # zmm31 = [0,1,2,3,4,5,6,7,8,9,10,11,19,u,u,u]
	vpermt2ps	7232(%rsp), %zmm31, %zmm0 # 64-byte Folded Reload
	vmovaps	.LCPI0_140(%rip), %zmm31        # zmm31 = [0,1,2,3,4,5,6,7,8,9,10,11,12,19,u,u]
	vpermt2ps	7296(%rsp), %zmm31, %zmm0 # 64-byte Folded Reload
	vmovaps	.LCPI0_153(%rip), %zmm31        # zmm31 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,19,u]
	vpermt2ps	1920(%rsp), %zmm31, %zmm0 # 64-byte Folded Reload
	vmovaps	.LCPI0_167(%rip), %zmm31        # zmm31 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,19]
	vpermt2ps	576(%rsp), %zmm31, %zmm0 # 64-byte Folded Reload
	vfmadd231ps	5952(%rsp), %zmm0, %zmm8 # 64-byte Folded Reload
                                        # zmm8 = (zmm0 * mem) + zmm8
	vmovaps	%zmm8, 12544(%rsp)              # 64-byte Spill
	vfmadd231ps	3776(%rsp), %zmm0, %zmm9 # 64-byte Folded Reload
                                        # zmm9 = (zmm0 * mem) + zmm9
	vmovaps	%zmm9, 32896(%rsp)              # 64-byte Spill
	vfmadd231ps	3904(%rsp), %zmm0, %zmm6 # 64-byte Folded Reload
                                        # zmm6 = (zmm0 * mem) + zmm6
	vmovaps	%zmm6, 9856(%rsp)               # 64-byte Spill
	vfmadd231ps	4032(%rsp), %zmm0, %zmm5 # 64-byte Folded Reload
                                        # zmm5 = (zmm0 * mem) + zmm5
	vmovaps	%zmm5, 9984(%rsp)               # 64-byte Spill
	vfmadd231ps	4160(%rsp), %zmm0, %zmm4 # 64-byte Folded Reload
                                        # zmm4 = (zmm0 * mem) + zmm4
	vmovaps	%zmm4, 31488(%rsp)              # 64-byte Spill
	vfmadd231ps	4288(%rsp), %zmm0, %zmm23 # 64-byte Folded Reload
                                        # zmm23 = (zmm0 * mem) + zmm23
	vmovaps	%zmm23, 31680(%rsp)             # 64-byte Spill
	vmovaps	6080(%rsp), %zmm23              # 64-byte Reload
	vfmadd231ps	%zmm0, %zmm23, %zmm3    # zmm3 = (zmm23 * zmm0) + zmm3
	vmovaps	%zmm3, 32384(%rsp)              # 64-byte Spill
	vfmadd231ps	4416(%rsp), %zmm0, %zmm2 # 64-byte Folded Reload
                                        # zmm2 = (zmm0 * mem) + zmm2
	vmovaps	%zmm2, 32960(%rsp)              # 64-byte Spill
	vfmadd231ps	4544(%rsp), %zmm0, %zmm30 # 64-byte Folded Reload
                                        # zmm30 = (zmm0 * mem) + zmm30
	vmovaps	%zmm30, 33472(%rsp)             # 64-byte Spill
	vfmadd231ps	4672(%rsp), %zmm0, %zmm1 # 64-byte Folded Reload
                                        # zmm1 = (zmm0 * mem) + zmm1
	vmovaps	%zmm1, 33984(%rsp)              # 64-byte Spill
	vfmadd231ps	3264(%rsp), %zmm0, %zmm18 # 64-byte Folded Reload
                                        # zmm18 = (zmm0 * mem) + zmm18
	vmovaps	%zmm18, 34624(%rsp)             # 64-byte Spill
	vfmadd231ps	3392(%rsp), %zmm0, %zmm11 # 64-byte Folded Reload
                                        # zmm11 = (zmm0 * mem) + zmm11
	vmovaps	%zmm11, 35392(%rsp)             # 64-byte Spill
	vfmadd231ps	3520(%rsp), %zmm0, %zmm7 # 64-byte Folded Reload
                                        # zmm7 = (zmm0 * mem) + zmm7
	vmovaps	%zmm7, 36096(%rsp)              # 64-byte Spill
	vfmadd231ps	3648(%rsp), %zmm0, %zmm19 # 64-byte Folded Reload
                                        # zmm19 = (zmm0 * mem) + zmm19
	vmovaps	%zmm19, 17216(%rsp)             # 64-byte Spill
	vfmadd231ps	4800(%rsp), %zmm0, %zmm13 # 64-byte Folded Reload
                                        # zmm13 = (zmm0 * mem) + zmm13
	vmovaps	%zmm13, 17664(%rsp)             # 64-byte Spill
	vfmadd231ps	6208(%rsp), %zmm0, %zmm12 # 64-byte Folded Reload
                                        # zmm12 = (zmm0 * mem) + zmm12
	vmovaps	%zmm12, 36992(%rsp)             # 64-byte Spill
	vfmadd231ps	6272(%rsp), %zmm0, %zmm24 # 64-byte Folded Reload
                                        # zmm24 = (zmm0 * mem) + zmm24
	vmovaps	%zmm24, 15552(%rsp)             # 64-byte Spill
	vfmadd231ps	6400(%rsp), %zmm0, %zmm15 # 64-byte Folded Reload
                                        # zmm15 = (zmm0 * mem) + zmm15
	vmovaps	%zmm15, 33856(%rsp)             # 64-byte Spill
	vfmadd231ps	6784(%rsp), %zmm0, %zmm25 # 64-byte Folded Reload
                                        # zmm25 = (zmm0 * mem) + zmm25
	vmovaps	%zmm25, 16640(%rsp)             # 64-byte Spill
	vfmadd231ps	6464(%rsp), %zmm0, %zmm21 # 64-byte Folded Reload
                                        # zmm21 = (zmm0 * mem) + zmm21
	vmovaps	%zmm21, 33408(%rsp)             # 64-byte Spill
	vfmadd231ps	7808(%rsp), %zmm0, %zmm27 # 64-byte Folded Reload
                                        # zmm27 = (zmm0 * mem) + zmm27
	vmovaps	%zmm27, 34816(%rsp)             # 64-byte Spill
	vfmadd231ps	7872(%rsp), %zmm0, %zmm26 # 64-byte Folded Reload
                                        # zmm26 = (zmm0 * mem) + zmm26
	vmovaps	%zmm26, 16896(%rsp)             # 64-byte Spill
	vmovaps	6656(%rsp), %zmm1               # 64-byte Reload
	vfmadd231ps	8576(%rsp), %zmm0, %zmm1 # 64-byte Folded Reload
                                        # zmm1 = (zmm0 * mem) + zmm1
	vmovaps	%zmm1, 6656(%rsp)               # 64-byte Spill
	vmovaps	64(%rsp), %zmm1                 # 64-byte Reload
	vfmadd231ps	6976(%rsp), %zmm0, %zmm1 # 64-byte Folded Reload
                                        # zmm1 = (zmm0 * mem) + zmm1
	vmovaps	%zmm1, 64(%rsp)                 # 64-byte Spill
	vmovaps	18944(%rsp), %zmm1              # 64-byte Reload
	vfmadd231ps	8000(%rsp), %zmm0, %zmm1 # 64-byte Folded Reload
                                        # zmm1 = (zmm0 * mem) + zmm1
	vmovaps	%zmm1, 18944(%rsp)              # 64-byte Spill
	vmovaps	18880(%rsp), %zmm1              # 64-byte Reload
	vfmadd231ps	8128(%rsp), %zmm0, %zmm1 # 64-byte Folded Reload
                                        # zmm1 = (zmm0 * mem) + zmm1
	vmovaps	%zmm1, 18880(%rsp)              # 64-byte Spill
	vfmadd231ps	4928(%rsp), %zmm0, %zmm14 # 64-byte Folded Reload
                                        # zmm14 = (zmm0 * mem) + zmm14
	vmovaps	%zmm14, 44672(%rsp)             # 64-byte Spill
	vfmadd231ps	4992(%rsp), %zmm0, %zmm22 # 64-byte Folded Reload
                                        # zmm22 = (zmm0 * mem) + zmm22
	vmovaps	%zmm22, 45056(%rsp)             # 64-byte Spill
	vfmadd231ps	5120(%rsp), %zmm0, %zmm16 # 64-byte Folded Reload
                                        # zmm16 = (zmm0 * mem) + zmm16
	vmovaps	%zmm16, 45440(%rsp)             # 64-byte Spill
	vfmadd231ps	5248(%rsp), %zmm0, %zmm17 # 64-byte Folded Reload
                                        # zmm17 = (zmm0 * mem) + zmm17
	vmovaps	%zmm17, 45824(%rsp)             # 64-byte Spill
	vfmadd231ps	5376(%rsp), %zmm0, %zmm20 # 64-byte Folded Reload
                                        # zmm20 = (zmm0 * mem) + zmm20
	vmovaps	%zmm20, 46208(%rsp)             # 64-byte Spill
	vfmadd231ps	7104(%rsp), %zmm0, %zmm10 # 64-byte Folded Reload
                                        # zmm10 = (zmm0 * mem) + zmm10
	vmovaps	%zmm10, 46528(%rsp)             # 64-byte Spill
	vmovaps	.LCPI0_93(%rip), %zmm0          # zmm0 = [0,1,2,3,4,5,6,7,21,u,u,u,u,u,u,u]
	vmovaps	8704(%rsp), %zmm8               # 64-byte Reload
	vpermt2ps	5824(%rsp), %zmm0, %zmm8 # 64-byte Folded Reload
	vmovaps	.LCPI0_101(%rip), %zmm0         # zmm0 = [0,1,2,3,4,5,6,7,8,21,u,u,u,u,u,u]
	vpermt2ps	7360(%rsp), %zmm0, %zmm8 # 64-byte Folded Reload
	vmovaps	.LCPI0_110(%rip), %zmm0         # zmm0 = [0,1,2,3,4,5,6,7,8,9,21,u,u,u,u,u]
	vpermt2ps	2304(%rsp), %zmm0, %zmm8 # 64-byte Folded Reload
	vmovaps	.LCPI0_120(%rip), %zmm0         # zmm0 = [0,1,2,3,4,5,6,7,8,9,10,21,u,u,u,u]
	vpermt2ps	7680(%rsp), %zmm0, %zmm8 # 64-byte Folded Reload
	vmovaps	.LCPI0_130(%rip), %zmm0         # zmm0 = [0,1,2,3,4,5,6,7,8,9,10,11,21,u,u,u]
	vpermt2ps	7616(%rsp), %zmm0, %zmm8 # 64-byte Folded Reload
	vmovaps	.LCPI0_142(%rip), %zmm0         # zmm0 = [0,1,2,3,4,5,6,7,8,9,10,11,12,21,u,u]
	vpermt2ps	7552(%rsp), %zmm0, %zmm8 # 64-byte Folded Reload
	vmovaps	.LCPI0_155(%rip), %zmm0         # zmm0 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,21,u]
	vpermt2ps	7488(%rsp), %zmm0, %zmm8 # 64-byte Folded Reload
	vmovaps	.LCPI0_169(%rip), %zmm0         # zmm0 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,21]
	vpermt2ps	7424(%rsp), %zmm0, %zmm8 # 64-byte Folded Reload
	vmulps	5888(%rsp), %zmm8, %zmm17       # 64-byte Folded Reload
	vmulps	3712(%rsp), %zmm8, %zmm3        # 64-byte Folded Reload
	vmulps	3840(%rsp), %zmm8, %zmm2        # 64-byte Folded Reload
	vmulps	3968(%rsp), %zmm8, %zmm10       # 64-byte Folded Reload
	vmulps	4096(%rsp), %zmm8, %zmm30       # 64-byte Folded Reload
	vmulps	4224(%rsp), %zmm8, %zmm5        # 64-byte Folded Reload
	vmovaps	6016(%rsp), %zmm24              # 64-byte Reload
	vmulps	%zmm8, %zmm24, %zmm18
	vmulps	4352(%rsp), %zmm8, %zmm13       # 64-byte Folded Reload
	vmulps	4480(%rsp), %zmm8, %zmm9        # 64-byte Folded Reload
	vmulps	4608(%rsp), %zmm8, %zmm11       # 64-byte Folded Reload
	vmulps	3200(%rsp), %zmm8, %zmm7        # 64-byte Folded Reload
	vmulps	3328(%rsp), %zmm8, %zmm26       # 64-byte Folded Reload
	vmulps	3456(%rsp), %zmm8, %zmm22       # 64-byte Folded Reload
	vmulps	3584(%rsp), %zmm8, %zmm19       # 64-byte Folded Reload
	vmulps	4736(%rsp), %zmm8, %zmm21       # 64-byte Folded Reload
	vmulps	6144(%rsp), %zmm8, %zmm16       # 64-byte Folded Reload
	vmulps	%zmm8, %zmm29, %zmm12
	vmulps	6336(%rsp), %zmm8, %zmm0        # 64-byte Folded Reload
	vmovaps	%zmm0, 10880(%rsp)              # 64-byte Spill
	vmulps	4864(%rsp), %zmm8, %zmm6        # 64-byte Folded Reload
	vmulps	6720(%rsp), %zmm8, %zmm4        # 64-byte Folded Reload
	vmulps	6848(%rsp), %zmm8, %zmm0        # 64-byte Folded Reload
	vmovaps	%zmm0, 10368(%rsp)              # 64-byte Spill
	vmulps	%zmm8, %zmm28, %zmm14
	vmulps	8512(%rsp), %zmm8, %zmm0        # 64-byte Folded Reload
	vmovaps	%zmm0, 18816(%rsp)              # 64-byte Spill
	vmulps	7936(%rsp), %zmm8, %zmm0        # 64-byte Folded Reload
	vmovaps	%zmm0, 10304(%rsp)              # 64-byte Spill
	vmulps	8640(%rsp), %zmm8, %zmm28       # 64-byte Folded Reload
	vmulps	8064(%rsp), %zmm8, %zmm27       # 64-byte Folded Reload
	vmulps	6528(%rsp), %zmm8, %zmm15       # 64-byte Folded Reload
	vmulps	6592(%rsp), %zmm8, %zmm25       # 64-byte Folded Reload
	vmulps	5056(%rsp), %zmm8, %zmm31       # 64-byte Folded Reload
	vmulps	5184(%rsp), %zmm8, %zmm29       # 64-byte Folded Reload
	vmulps	5312(%rsp), %zmm8, %zmm20       # 64-byte Folded Reload
	vmulps	7040(%rsp), %zmm8, %zmm1        # 64-byte Folded Reload
	vmovaps	1600(%rsp), %zmm0               # 64-byte Reload
	vmovaps	.LCPI0_93(%rip), %zmm8          # zmm8 = [0,1,2,3,4,5,6,7,21,u,u,u,u,u,u,u]
	vpermt2ps	8192(%rsp), %zmm8, %zmm0 # 64-byte Folded Reload
	vmovaps	.LCPI0_101(%rip), %zmm8         # zmm8 = [0,1,2,3,4,5,6,7,8,21,u,u,u,u,u,u]
	vpermt2ps	5760(%rsp), %zmm8, %zmm0 # 64-byte Folded Reload
	vmovaps	.LCPI0_110(%rip), %zmm8         # zmm8 = [0,1,2,3,4,5,6,7,8,9,21,u,u,u,u,u]
	vpermt2ps	7168(%rsp), %zmm8, %zmm0 # 64-byte Folded Reload
	vmovaps	.LCPI0_120(%rip), %zmm8         # zmm8 = [0,1,2,3,4,5,6,7,8,9,10,21,u,u,u,u]
	vpermt2ps	8256(%rsp), %zmm8, %zmm0 # 64-byte Folded Reload
	vmovaps	.LCPI0_130(%rip), %zmm8         # zmm8 = [0,1,2,3,4,5,6,7,8,9,10,11,21,u,u,u]
	vpermt2ps	7232(%rsp), %zmm8, %zmm0 # 64-byte Folded Reload
	vmovaps	.LCPI0_142(%rip), %zmm8         # zmm8 = [0,1,2,3,4,5,6,7,8,9,10,11,12,21,u,u]
	vpermt2ps	7296(%rsp), %zmm8, %zmm0 # 64-byte Folded Reload
	vmovaps	.LCPI0_155(%rip), %zmm8         # zmm8 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,21,u]
	vpermt2ps	1920(%rsp), %zmm8, %zmm0 # 64-byte Folded Reload
	vmovaps	.LCPI0_169(%rip), %zmm8         # zmm8 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,21]
	vpermt2ps	576(%rsp), %zmm8, %zmm0 # 64-byte Folded Reload
	vfmadd231ps	5952(%rsp), %zmm0, %zmm17 # 64-byte Folded Reload
                                        # zmm17 = (zmm0 * mem) + zmm17
	vmovaps	%zmm17, 32320(%rsp)             # 64-byte Spill
	vfmadd231ps	3776(%rsp), %zmm0, %zmm3 # 64-byte Folded Reload
                                        # zmm3 = (zmm0 * mem) + zmm3
	vmovaps	%zmm3, 9600(%rsp)               # 64-byte Spill
	vfmadd231ps	3904(%rsp), %zmm0, %zmm2 # 64-byte Folded Reload
                                        # zmm2 = (zmm0 * mem) + zmm2
	vmovaps	%zmm2, 12928(%rsp)              # 64-byte Spill
	vfmadd231ps	4032(%rsp), %zmm0, %zmm10 # 64-byte Folded Reload
                                        # zmm10 = (zmm0 * mem) + zmm10
	vmovaps	%zmm10, 31360(%rsp)             # 64-byte Spill
	vfmadd231ps	4160(%rsp), %zmm0, %zmm30 # 64-byte Folded Reload
                                        # zmm30 = (zmm0 * mem) + zmm30
	vmovaps	%zmm30, 13824(%rsp)             # 64-byte Spill
	vfmadd231ps	4288(%rsp), %zmm0, %zmm5 # 64-byte Folded Reload
                                        # zmm5 = (zmm0 * mem) + zmm5
	vmovaps	%zmm5, 31808(%rsp)              # 64-byte Spill
	vfmadd231ps	%zmm0, %zmm23, %zmm18   # zmm18 = (zmm23 * zmm0) + zmm18
	vmovaps	%zmm18, 32640(%rsp)             # 64-byte Spill
	vfmadd231ps	4416(%rsp), %zmm0, %zmm13 # 64-byte Folded Reload
                                        # zmm13 = (zmm0 * mem) + zmm13
	vmovaps	%zmm13, 33024(%rsp)             # 64-byte Spill
	vfmadd231ps	4544(%rsp), %zmm0, %zmm9 # 64-byte Folded Reload
                                        # zmm9 = (zmm0 * mem) + zmm9
	vmovaps	%zmm9, 33536(%rsp)              # 64-byte Spill
	vfmadd231ps	4672(%rsp), %zmm0, %zmm11 # 64-byte Folded Reload
                                        # zmm11 = (zmm0 * mem) + zmm11
	vmovaps	%zmm11, 34048(%rsp)             # 64-byte Spill
	vfmadd231ps	3264(%rsp), %zmm0, %zmm7 # 64-byte Folded Reload
                                        # zmm7 = (zmm0 * mem) + zmm7
	vmovaps	%zmm7, 34752(%rsp)              # 64-byte Spill
	vfmadd231ps	3392(%rsp), %zmm0, %zmm26 # 64-byte Folded Reload
                                        # zmm26 = (zmm0 * mem) + zmm26
	vmovaps	%zmm26, 35520(%rsp)             # 64-byte Spill
	vfmadd231ps	3520(%rsp), %zmm0, %zmm22 # 64-byte Folded Reload
                                        # zmm22 = (zmm0 * mem) + zmm22
	vmovaps	%zmm22, 16960(%rsp)             # 64-byte Spill
	vfmadd231ps	3648(%rsp), %zmm0, %zmm19 # 64-byte Folded Reload
                                        # zmm19 = (zmm0 * mem) + zmm19
	vmovaps	%zmm19, 17344(%rsp)             # 64-byte Spill
	vfmadd231ps	4800(%rsp), %zmm0, %zmm21 # 64-byte Folded Reload
                                        # zmm21 = (zmm0 * mem) + zmm21
	vmovaps	%zmm21, 17728(%rsp)             # 64-byte Spill
	vfmadd231ps	6208(%rsp), %zmm0, %zmm16 # 64-byte Folded Reload
                                        # zmm16 = (zmm0 * mem) + zmm16
	vmovaps	%zmm16, 17856(%rsp)             # 64-byte Spill
	vfmadd231ps	6272(%rsp), %zmm0, %zmm12 # 64-byte Folded Reload
                                        # zmm12 = (zmm0 * mem) + zmm12
	vmovaps	%zmm12, 32704(%rsp)             # 64-byte Spill
	vmovaps	10880(%rsp), %zmm2              # 64-byte Reload
	vfmadd231ps	6400(%rsp), %zmm0, %zmm2 # 64-byte Folded Reload
                                        # zmm2 = (zmm0 * mem) + zmm2
	vmovaps	%zmm2, 10880(%rsp)              # 64-byte Spill
	vfmadd231ps	6784(%rsp), %zmm0, %zmm6 # 64-byte Folded Reload
                                        # zmm6 = (zmm0 * mem) + zmm6
	vmovaps	%zmm6, 15744(%rsp)              # 64-byte Spill
	vfmadd231ps	6464(%rsp), %zmm0, %zmm4 # 64-byte Folded Reload
                                        # zmm4 = (zmm0 * mem) + zmm4
	vmovaps	%zmm4, 33920(%rsp)              # 64-byte Spill
	vmovaps	10368(%rsp), %zmm2              # 64-byte Reload
	vfmadd231ps	7808(%rsp), %zmm0, %zmm2 # 64-byte Folded Reload
                                        # zmm2 = (zmm0 * mem) + zmm2
	vmovaps	%zmm2, 10368(%rsp)              # 64-byte Spill
	vfmadd231ps	7872(%rsp), %zmm0, %zmm14 # 64-byte Folded Reload
                                        # zmm14 = (zmm0 * mem) + zmm14
	vmovaps	%zmm14, 35328(%rsp)             # 64-byte Spill
	vmovaps	18816(%rsp), %zmm2              # 64-byte Reload
	vfmadd231ps	8576(%rsp), %zmm0, %zmm2 # 64-byte Folded Reload
                                        # zmm2 = (zmm0 * mem) + zmm2
	vmovaps	%zmm2, 18816(%rsp)              # 64-byte Spill
	vmovaps	10304(%rsp), %zmm2              # 64-byte Reload
	vfmadd231ps	6976(%rsp), %zmm0, %zmm2 # 64-byte Folded Reload
                                        # zmm2 = (zmm0 * mem) + zmm2
	vmovaps	%zmm2, 10304(%rsp)              # 64-byte Spill
	vfmadd231ps	8000(%rsp), %zmm0, %zmm28 # 64-byte Folded Reload
                                        # zmm28 = (zmm0 * mem) + zmm28
	vmovaps	%zmm28, 36736(%rsp)             # 64-byte Spill
	vfmadd231ps	8128(%rsp), %zmm0, %zmm27 # 64-byte Folded Reload
                                        # zmm27 = (zmm0 * mem) + zmm27
	vmovaps	%zmm27, 44416(%rsp)             # 64-byte Spill
	vfmadd231ps	4928(%rsp), %zmm0, %zmm15 # 64-byte Folded Reload
                                        # zmm15 = (zmm0 * mem) + zmm15
	vmovaps	%zmm15, 44608(%rsp)             # 64-byte Spill
	vfmadd231ps	4992(%rsp), %zmm0, %zmm25 # 64-byte Folded Reload
                                        # zmm25 = (zmm0 * mem) + zmm25
	vmovaps	%zmm25, 44992(%rsp)             # 64-byte Spill
	vfmadd231ps	5120(%rsp), %zmm0, %zmm31 # 64-byte Folded Reload
                                        # zmm31 = (zmm0 * mem) + zmm31
	vmovaps	%zmm31, 45376(%rsp)             # 64-byte Spill
	vfmadd231ps	5248(%rsp), %zmm0, %zmm29 # 64-byte Folded Reload
                                        # zmm29 = (zmm0 * mem) + zmm29
	vmovaps	%zmm29, 45760(%rsp)             # 64-byte Spill
	vfmadd231ps	5376(%rsp), %zmm0, %zmm20 # 64-byte Folded Reload
                                        # zmm20 = (zmm0 * mem) + zmm20
	vmovaps	%zmm20, 46144(%rsp)             # 64-byte Spill
	vfmadd231ps	7104(%rsp), %zmm0, %zmm1 # 64-byte Folded Reload
                                        # zmm1 = (zmm0 * mem) + zmm1
	vmovaps	%zmm1, 46464(%rsp)              # 64-byte Spill
	vmovapd	.LCPI0_94(%rip), %zmm25         # zmm25 = [0,1,2,3,11,u,u,u]
	vmovapd	11008(%rsp), %zmm0              # 64-byte Reload
	vpermt2pd	5824(%rsp), %zmm25, %zmm0 # 64-byte Folded Reload
	vmovaps	.LCPI0_102(%rip), %zmm20        # zmm20 = [0,1,2,3,4,5,6,7,8,22,u,u,u,u,u,u]
	vpermt2ps	7360(%rsp), %zmm20, %zmm0 # 64-byte Folded Reload
	vmovapd	.LCPI0_111(%rip), %zmm14        # zmm14 = [0,1,2,3,4,11,u,u]
	vpermt2pd	2304(%rsp), %zmm14, %zmm0 # 64-byte Folded Reload
	vmovaps	.LCPI0_121(%rip), %zmm9         # zmm9 = [0,1,2,3,4,5,6,7,8,9,10,22,u,u,u,u]
	vpermt2ps	7680(%rsp), %zmm9, %zmm0 # 64-byte Folded Reload
	vmovapd	.LCPI0_131(%rip), %zmm1         # zmm1 = [0,1,2,3,4,5,11,u]
	vpermt2pd	7616(%rsp), %zmm1, %zmm0 # 64-byte Folded Reload
	vmovaps	.LCPI0_143(%rip), %zmm10        # zmm10 = [0,1,2,3,4,5,6,7,8,9,10,11,12,22,u,u]
	vpermt2ps	7552(%rsp), %zmm10, %zmm0 # 64-byte Folded Reload
	vmovapd	.LCPI0_156(%rip), %zmm22        # zmm22 = [0,1,2,3,4,5,6,11]
	vpermt2pd	7488(%rsp), %zmm22, %zmm0 # 64-byte Folded Reload
	vmovaps	.LCPI0_170(%rip), %zmm13        # zmm13 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,22]
	vpermt2ps	7424(%rsp), %zmm13, %zmm0 # 64-byte Folded Reload
	vmulps	5888(%rsp), %zmm0, %zmm7        # 64-byte Folded Reload
	vmulps	3712(%rsp), %zmm0, %zmm5        # 64-byte Folded Reload
	vmulps	3840(%rsp), %zmm0, %zmm4        # 64-byte Folded Reload
	vmulps	3968(%rsp), %zmm0, %zmm3        # 64-byte Folded Reload
	vmulps	4096(%rsp), %zmm0, %zmm21       # 64-byte Folded Reload
	vmulps	4224(%rsp), %zmm0, %zmm2        # 64-byte Folded Reload
	vmulps	%zmm0, %zmm24, %zmm1
	vmulps	4352(%rsp), %zmm0, %zmm6        # 64-byte Folded Reload
	vmulps	4480(%rsp), %zmm0, %zmm9        # 64-byte Folded Reload
	vmulps	4608(%rsp), %zmm0, %zmm10       # 64-byte Folded Reload
	vmulps	3200(%rsp), %zmm0, %zmm11       # 64-byte Folded Reload
	vmulps	3328(%rsp), %zmm0, %zmm16       # 64-byte Folded Reload
	vmulps	3456(%rsp), %zmm0, %zmm13       # 64-byte Folded Reload
	vmulps	3584(%rsp), %zmm0, %zmm14       # 64-byte Folded Reload
	vmulps	4736(%rsp), %zmm0, %zmm15       # 64-byte Folded Reload
	vmulps	6144(%rsp), %zmm0, %zmm12       # 64-byte Folded Reload
	vmulps	7744(%rsp), %zmm0, %zmm23       # 64-byte Folded Reload
	vmulps	6336(%rsp), %zmm0, %zmm26       # 64-byte Folded Reload
	vmulps	4864(%rsp), %zmm0, %zmm25       # 64-byte Folded Reload
	vmulps	6720(%rsp), %zmm0, %zmm24       # 64-byte Folded Reload
	vmulps	6848(%rsp), %zmm0, %zmm8        # 64-byte Folded Reload
	vmovaps	%zmm8, 1600(%rsp)               # 64-byte Spill
	vmulps	6912(%rsp), %zmm0, %zmm31       # 64-byte Folded Reload
	vmulps	8512(%rsp), %zmm0, %zmm30       # 64-byte Folded Reload
	vmulps	7936(%rsp), %zmm0, %zmm8        # 64-byte Folded Reload
	vmovaps	%zmm8, 10176(%rsp)              # 64-byte Spill
	vmulps	8640(%rsp), %zmm0, %zmm27       # 64-byte Folded Reload
	vmulps	8064(%rsp), %zmm0, %zmm28       # 64-byte Folded Reload
	vmulps	6528(%rsp), %zmm0, %zmm19       # 64-byte Folded Reload
	vmulps	6592(%rsp), %zmm0, %zmm18       # 64-byte Folded Reload
	vmulps	5056(%rsp), %zmm0, %zmm22       # 64-byte Folded Reload
	vmulps	5184(%rsp), %zmm0, %zmm20       # 64-byte Folded Reload
	vmulps	5312(%rsp), %zmm0, %zmm17       # 64-byte Folded Reload
	vmulps	7040(%rsp), %zmm0, %zmm8        # 64-byte Folded Reload
	vmovapd	1216(%rsp), %zmm0               # 64-byte Reload
	vmovapd	.LCPI0_94(%rip), %zmm29         # zmm29 = [0,1,2,3,11,u,u,u]
	vpermt2pd	8192(%rsp), %zmm29, %zmm0 # 64-byte Folded Reload
	vmovaps	.LCPI0_102(%rip), %zmm29        # zmm29 = [0,1,2,3,4,5,6,7,8,22,u,u,u,u,u,u]
	vpermt2ps	5760(%rsp), %zmm29, %zmm0 # 64-byte Folded Reload
	vmovapd	.LCPI0_111(%rip), %zmm29        # zmm29 = [0,1,2,3,4,11,u,u]
	vpermt2pd	7168(%rsp), %zmm29, %zmm0 # 64-byte Folded Reload
	vmovaps	.LCPI0_121(%rip), %zmm29        # zmm29 = [0,1,2,3,4,5,6,7,8,9,10,22,u,u,u,u]
	vpermt2ps	8256(%rsp), %zmm29, %zmm0 # 64-byte Folded Reload
	vmovapd	.LCPI0_131(%rip), %zmm29        # zmm29 = [0,1,2,3,4,5,11,u]
	vpermt2pd	7232(%rsp), %zmm29, %zmm0 # 64-byte Folded Reload
	vmovaps	.LCPI0_143(%rip), %zmm29        # zmm29 = [0,1,2,3,4,5,6,7,8,9,10,11,12,22,u,u]
	vpermt2ps	7296(%rsp), %zmm29, %zmm0 # 64-byte Folded Reload
	vmovapd	.LCPI0_156(%rip), %zmm29        # zmm29 = [0,1,2,3,4,5,6,11]
	vpermt2pd	1920(%rsp), %zmm29, %zmm0 # 64-byte Folded Reload
	vmovaps	.LCPI0_170(%rip), %zmm29        # zmm29 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,22]
	vpermt2ps	576(%rsp), %zmm29, %zmm0 # 64-byte Folded Reload
	vfmadd231ps	5952(%rsp), %zmm0, %zmm7 # 64-byte Folded Reload
                                        # zmm7 = (zmm0 * mem) + zmm7
	vmovaps	%zmm7, 15296(%rsp)              # 64-byte Spill
	vfmadd231ps	3776(%rsp), %zmm0, %zmm5 # 64-byte Folded Reload
                                        # zmm5 = (zmm0 * mem) + zmm5
	vmovaps	%zmm5, 32576(%rsp)              # 64-byte Spill
	vfmadd231ps	3904(%rsp), %zmm0, %zmm4 # 64-byte Folded Reload
                                        # zmm4 = (zmm0 * mem) + zmm4
	vmovaps	%zmm4, 12480(%rsp)              # 64-byte Spill
	vfmadd231ps	4032(%rsp), %zmm0, %zmm3 # 64-byte Folded Reload
                                        # zmm3 = (zmm0 * mem) + zmm3
	vmovaps	%zmm3, 9728(%rsp)               # 64-byte Spill
	vfmadd231ps	4160(%rsp), %zmm0, %zmm21 # 64-byte Folded Reload
                                        # zmm21 = (zmm0 * mem) + zmm21
	vmovaps	%zmm21, 13312(%rsp)             # 64-byte Spill
	vfmadd231ps	4288(%rsp), %zmm0, %zmm2 # 64-byte Folded Reload
                                        # zmm2 = (zmm0 * mem) + zmm2
	vmovaps	%zmm2, 13632(%rsp)              # 64-byte Spill
	vfmadd231ps	6080(%rsp), %zmm0, %zmm1 # 64-byte Folded Reload
                                        # zmm1 = (zmm0 * mem) + zmm1
	vmovaps	%zmm1, 14016(%rsp)              # 64-byte Spill
	vfmadd231ps	4416(%rsp), %zmm0, %zmm6 # 64-byte Folded Reload
                                        # zmm6 = (zmm0 * mem) + zmm6
	vmovaps	%zmm6, 14528(%rsp)              # 64-byte Spill
	vfmadd231ps	4544(%rsp), %zmm0, %zmm9 # 64-byte Folded Reload
                                        # zmm9 = (zmm0 * mem) + zmm9
	vmovaps	%zmm9, 14784(%rsp)              # 64-byte Spill
	vfmadd231ps	4672(%rsp), %zmm0, %zmm10 # 64-byte Folded Reload
                                        # zmm10 = (zmm0 * mem) + zmm10
	vmovaps	%zmm10, 15616(%rsp)             # 64-byte Spill
	vfmadd231ps	3264(%rsp), %zmm0, %zmm11 # 64-byte Folded Reload
                                        # zmm11 = (zmm0 * mem) + zmm11
	vmovaps	%zmm11, 16000(%rsp)             # 64-byte Spill
	vfmadd231ps	3392(%rsp), %zmm0, %zmm16 # 64-byte Folded Reload
                                        # zmm16 = (zmm0 * mem) + zmm16
	vmovaps	%zmm16, 34240(%rsp)             # 64-byte Spill
	vfmadd231ps	3520(%rsp), %zmm0, %zmm13 # 64-byte Folded Reload
                                        # zmm13 = (zmm0 * mem) + zmm13
	vmovaps	%zmm13, 35072(%rsp)             # 64-byte Spill
	vfmadd231ps	3648(%rsp), %zmm0, %zmm14 # 64-byte Folded Reload
                                        # zmm14 = (zmm0 * mem) + zmm14
	vmovaps	%zmm14, 35904(%rsp)             # 64-byte Spill
	vfmadd231ps	4800(%rsp), %zmm0, %zmm15 # 64-byte Folded Reload
                                        # zmm15 = (zmm0 * mem) + zmm15
	vmovaps	%zmm15, 17024(%rsp)             # 64-byte Spill
	vfmadd231ps	6208(%rsp), %zmm0, %zmm12 # 64-byte Folded Reload
                                        # zmm12 = (zmm0 * mem) + zmm12
	vmovaps	%zmm12, 32512(%rsp)             # 64-byte Spill
	vfmadd231ps	6272(%rsp), %zmm0, %zmm23 # 64-byte Folded Reload
                                        # zmm23 = (zmm0 * mem) + zmm23
	vmovaps	%zmm23, 33152(%rsp)             # 64-byte Spill
	vfmadd231ps	6400(%rsp), %zmm0, %zmm26 # 64-byte Folded Reload
                                        # zmm26 = (zmm0 * mem) + zmm26
	vmovaps	%zmm26, 34560(%rsp)             # 64-byte Spill
	vfmadd231ps	6784(%rsp), %zmm0, %zmm25 # 64-byte Folded Reload
                                        # zmm25 = (zmm0 * mem) + zmm25
	vmovaps	%zmm25, 16512(%rsp)             # 64-byte Spill
	vfmadd231ps	6464(%rsp), %zmm0, %zmm24 # 64-byte Folded Reload
                                        # zmm24 = (zmm0 * mem) + zmm24
	vmovaps	%zmm24, 15488(%rsp)             # 64-byte Spill
	vmovaps	1600(%rsp), %zmm1               # 64-byte Reload
	vfmadd231ps	7808(%rsp), %zmm0, %zmm1 # 64-byte Folded Reload
                                        # zmm1 = (zmm0 * mem) + zmm1
	vmovaps	%zmm1, 1600(%rsp)               # 64-byte Spill
	vfmadd231ps	7872(%rsp), %zmm0, %zmm31 # 64-byte Folded Reload
                                        # zmm31 = (zmm0 * mem) + zmm31
	vmovaps	%zmm31, 34880(%rsp)             # 64-byte Spill
	vfmadd231ps	8576(%rsp), %zmm0, %zmm30 # 64-byte Folded Reload
                                        # zmm30 = (zmm0 * mem) + zmm30
	vmovaps	%zmm30, 18112(%rsp)             # 64-byte Spill
	vmovaps	10176(%rsp), %zmm1              # 64-byte Reload
	vfmadd231ps	6976(%rsp), %zmm0, %zmm1 # 64-byte Folded Reload
                                        # zmm1 = (zmm0 * mem) + zmm1
	vmovaps	%zmm1, 10176(%rsp)              # 64-byte Spill
	vfmadd231ps	8000(%rsp), %zmm0, %zmm27 # 64-byte Folded Reload
                                        # zmm27 = (zmm0 * mem) + zmm27
	vmovaps	%zmm27, 36544(%rsp)             # 64-byte Spill
	vfmadd231ps	8128(%rsp), %zmm0, %zmm28 # 64-byte Folded Reload
                                        # zmm28 = (zmm0 * mem) + zmm28
	vmovaps	%zmm28, 18176(%rsp)             # 64-byte Spill
	vfmadd231ps	4928(%rsp), %zmm0, %zmm19 # 64-byte Folded Reload
                                        # zmm19 = (zmm0 * mem) + zmm19
	vmovaps	%zmm19, 44544(%rsp)             # 64-byte Spill
	vfmadd231ps	4992(%rsp), %zmm0, %zmm18 # 64-byte Folded Reload
                                        # zmm18 = (zmm0 * mem) + zmm18
	vmovaps	%zmm18, 44864(%rsp)             # 64-byte Spill
	vfmadd231ps	5120(%rsp), %zmm0, %zmm22 # 64-byte Folded Reload
                                        # zmm22 = (zmm0 * mem) + zmm22
	vmovaps	%zmm22, 45248(%rsp)             # 64-byte Spill
	vfmadd231ps	5248(%rsp), %zmm0, %zmm20 # 64-byte Folded Reload
                                        # zmm20 = (zmm0 * mem) + zmm20
	vmovaps	%zmm20, 45632(%rsp)             # 64-byte Spill
	vfmadd231ps	5376(%rsp), %zmm0, %zmm17 # 64-byte Folded Reload
                                        # zmm17 = (zmm0 * mem) + zmm17
	vmovaps	%zmm17, 46016(%rsp)             # 64-byte Spill
	vfmadd231ps	7104(%rsp), %zmm0, %zmm8 # 64-byte Folded Reload
                                        # zmm8 = (zmm0 * mem) + zmm8
	vmovaps	%zmm8, 46336(%rsp)              # 64-byte Spill
	vmovaps	.LCPI0_95(%rip), %zmm4          # zmm4 = [0,1,2,3,4,5,6,7,23,u,u,u,u,u,u,u]
	vmovaps	10944(%rsp), %zmm0              # 64-byte Reload
	vpermt2ps	5824(%rsp), %zmm4, %zmm0 # 64-byte Folded Reload
	vmovaps	.LCPI0_103(%rip), %zmm4         # zmm4 = [0,1,2,3,4,5,6,7,8,23,u,u,u,u,u,u]
	vpermt2ps	7360(%rsp), %zmm4, %zmm0 # 64-byte Folded Reload
	vmovaps	.LCPI0_112(%rip), %zmm4         # zmm4 = [0,1,2,3,4,5,6,7,8,9,23,u,u,u,u,u]
	vpermt2ps	2304(%rsp), %zmm4, %zmm0 # 64-byte Folded Reload
	vmovaps	.LCPI0_122(%rip), %zmm12        # zmm12 = [0,1,2,3,4,5,6,7,8,9,10,23,u,u,u,u]
	vpermt2ps	7680(%rsp), %zmm12, %zmm0 # 64-byte Folded Reload
	vmovaps	.LCPI0_132(%rip), %zmm4         # zmm4 = [0,1,2,3,4,5,6,7,8,9,10,11,23,u,u,u]
	vpermt2ps	7616(%rsp), %zmm4, %zmm0 # 64-byte Folded Reload
	vmovaps	.LCPI0_144(%rip), %zmm4         # zmm4 = [0,1,2,3,4,5,6,7,8,9,10,11,12,23,u,u]
	vpermt2ps	7552(%rsp), %zmm4, %zmm0 # 64-byte Folded Reload
	vmovaps	.LCPI0_157(%rip), %zmm4         # zmm4 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,23,u]
	vpermt2ps	7488(%rsp), %zmm4, %zmm0 # 64-byte Folded Reload
	vmovaps	.LCPI0_171(%rip), %zmm4         # zmm4 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,23]
	vpermt2ps	7424(%rsp), %zmm4, %zmm0 # 64-byte Folded Reload
	vmulps	5888(%rsp), %zmm0, %zmm1        # 64-byte Folded Reload
	vmulps	3712(%rsp), %zmm0, %zmm3        # 64-byte Folded Reload
	vmulps	3840(%rsp), %zmm0, %zmm23       # 64-byte Folded Reload
	vmulps	3968(%rsp), %zmm0, %zmm2        # 64-byte Folded Reload
	vmulps	4096(%rsp), %zmm0, %zmm15       # 64-byte Folded Reload
	vmulps	4224(%rsp), %zmm0, %zmm13       # 64-byte Folded Reload
	vmulps	6016(%rsp), %zmm0, %zmm27       # 64-byte Folded Reload
	vmulps	4352(%rsp), %zmm0, %zmm26       # 64-byte Folded Reload
	vmulps	4480(%rsp), %zmm0, %zmm10       # 64-byte Folded Reload
	vmulps	4608(%rsp), %zmm0, %zmm9        # 64-byte Folded Reload
	vmulps	3200(%rsp), %zmm0, %zmm20       # 64-byte Folded Reload
	vmulps	3328(%rsp), %zmm0, %zmm8        # 64-byte Folded Reload
	vmulps	3456(%rsp), %zmm0, %zmm18       # 64-byte Folded Reload
	vmulps	3584(%rsp), %zmm0, %zmm17       # 64-byte Folded Reload
	vmulps	4736(%rsp), %zmm0, %zmm12       # 64-byte Folded Reload
	vmulps	6144(%rsp), %zmm0, %zmm4        # 64-byte Folded Reload
	vmulps	7744(%rsp), %zmm0, %zmm7        # 64-byte Folded Reload
	vmulps	6336(%rsp), %zmm0, %zmm11       # 64-byte Folded Reload
	vmulps	4864(%rsp), %zmm0, %zmm6        # 64-byte Folded Reload
	vmulps	6720(%rsp), %zmm0, %zmm5        # 64-byte Folded Reload
	vmulps	6848(%rsp), %zmm0, %zmm21       # 64-byte Folded Reload
	vmulps	6912(%rsp), %zmm0, %zmm16       # 64-byte Folded Reload
	vmulps	8512(%rsp), %zmm0, %zmm30       # 64-byte Folded Reload
	vmulps	7936(%rsp), %zmm0, %zmm31       # 64-byte Folded Reload
	vmulps	8640(%rsp), %zmm0, %zmm14       # 64-byte Folded Reload
	vmovaps	%zmm14, 10240(%rsp)             # 64-byte Spill
	vmulps	8064(%rsp), %zmm0, %zmm14       # 64-byte Folded Reload
	vmovaps	%zmm14, 18752(%rsp)             # 64-byte Spill
	vmulps	6528(%rsp), %zmm0, %zmm24       # 64-byte Folded Reload
	vmulps	6592(%rsp), %zmm0, %zmm14       # 64-byte Folded Reload
	vmulps	5056(%rsp), %zmm0, %zmm22       # 64-byte Folded Reload
	vmulps	5184(%rsp), %zmm0, %zmm25       # 64-byte Folded Reload
	vmulps	5312(%rsp), %zmm0, %zmm19       # 64-byte Folded Reload
	vmulps	7040(%rsp), %zmm0, %zmm29       # 64-byte Folded Reload
	vmovaps	1280(%rsp), %zmm0               # 64-byte Reload
	vmovaps	.LCPI0_95(%rip), %zmm28         # zmm28 = [0,1,2,3,4,5,6,7,23,u,u,u,u,u,u,u]
	vpermt2ps	8192(%rsp), %zmm28, %zmm0 # 64-byte Folded Reload
	vmovaps	.LCPI0_103(%rip), %zmm28        # zmm28 = [0,1,2,3,4,5,6,7,8,23,u,u,u,u,u,u]
	vpermt2ps	5760(%rsp), %zmm28, %zmm0 # 64-byte Folded Reload
	vmovaps	.LCPI0_112(%rip), %zmm28        # zmm28 = [0,1,2,3,4,5,6,7,8,9,23,u,u,u,u,u]
	vpermt2ps	7168(%rsp), %zmm28, %zmm0 # 64-byte Folded Reload
	vmovaps	.LCPI0_122(%rip), %zmm28        # zmm28 = [0,1,2,3,4,5,6,7,8,9,10,23,u,u,u,u]
	vpermt2ps	8256(%rsp), %zmm28, %zmm0 # 64-byte Folded Reload
	vmovaps	.LCPI0_132(%rip), %zmm28        # zmm28 = [0,1,2,3,4,5,6,7,8,9,10,11,23,u,u,u]
	vpermt2ps	7232(%rsp), %zmm28, %zmm0 # 64-byte Folded Reload
	vmovaps	.LCPI0_144(%rip), %zmm28        # zmm28 = [0,1,2,3,4,5,6,7,8,9,10,11,12,23,u,u]
	vpermt2ps	7296(%rsp), %zmm28, %zmm0 # 64-byte Folded Reload
	vmovaps	.LCPI0_157(%rip), %zmm28        # zmm28 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,23,u]
	vpermt2ps	1920(%rsp), %zmm28, %zmm0 # 64-byte Folded Reload
	vmovaps	.LCPI0_171(%rip), %zmm28        # zmm28 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,23]
	vpermt2ps	576(%rsp), %zmm28, %zmm0 # 64-byte Folded Reload
	vfmadd231ps	5952(%rsp), %zmm0, %zmm1 # 64-byte Folded Reload
                                        # zmm1 = (zmm0 * mem) + zmm1
	vmovaps	%zmm1, 32192(%rsp)              # 64-byte Spill
	vfmadd231ps	3776(%rsp), %zmm0, %zmm3 # 64-byte Folded Reload
                                        # zmm3 = (zmm0 * mem) + zmm3
	vmovaps	%zmm3, 31168(%rsp)              # 64-byte Spill
	vfmadd231ps	3904(%rsp), %zmm0, %zmm23 # 64-byte Folded Reload
                                        # zmm23 = (zmm0 * mem) + zmm23
	vmovaps	%zmm23, 12736(%rsp)             # 64-byte Spill
	vfmadd231ps	4032(%rsp), %zmm0, %zmm2 # 64-byte Folded Reload
                                        # zmm2 = (zmm0 * mem) + zmm2
	vmovaps	%zmm2, 13184(%rsp)              # 64-byte Spill
	vfmadd231ps	4160(%rsp), %zmm0, %zmm15 # 64-byte Folded Reload
                                        # zmm15 = (zmm0 * mem) + zmm15
	vmovaps	%zmm15, 13504(%rsp)             # 64-byte Spill
	vfmadd231ps	4288(%rsp), %zmm0, %zmm13 # 64-byte Folded Reload
                                        # zmm13 = (zmm0 * mem) + zmm13
	vmovaps	%zmm13, 13952(%rsp)             # 64-byte Spill
	vfmadd231ps	6080(%rsp), %zmm0, %zmm27 # 64-byte Folded Reload
                                        # zmm27 = (zmm0 * mem) + zmm27
	vmovaps	%zmm27, 14400(%rsp)             # 64-byte Spill
	vfmadd231ps	4416(%rsp), %zmm0, %zmm26 # 64-byte Folded Reload
                                        # zmm26 = (zmm0 * mem) + zmm26
	vmovaps	%zmm26, 14656(%rsp)             # 64-byte Spill
	vfmadd231ps	4544(%rsp), %zmm0, %zmm10 # 64-byte Folded Reload
                                        # zmm10 = (zmm0 * mem) + zmm10
	vmovaps	%zmm10, 15360(%rsp)             # 64-byte Spill
	vfmadd231ps	4672(%rsp), %zmm0, %zmm9 # 64-byte Folded Reload
                                        # zmm9 = (zmm0 * mem) + zmm9
	vmovaps	%zmm9, 15936(%rsp)              # 64-byte Spill
	vfmadd231ps	3264(%rsp), %zmm0, %zmm20 # 64-byte Folded Reload
                                        # zmm20 = (zmm0 * mem) + zmm20
	vmovaps	%zmm20, 16128(%rsp)             # 64-byte Spill
	vfmadd231ps	3392(%rsp), %zmm0, %zmm8 # 64-byte Folded Reload
                                        # zmm8 = (zmm0 * mem) + zmm8
	vmovaps	%zmm8, 35008(%rsp)              # 64-byte Spill
	vfmadd231ps	3520(%rsp), %zmm0, %zmm18 # 64-byte Folded Reload
                                        # zmm18 = (zmm0 * mem) + zmm18
	vmovaps	%zmm18, 35840(%rsp)             # 64-byte Spill
	vfmadd231ps	3648(%rsp), %zmm0, %zmm17 # 64-byte Folded Reload
                                        # zmm17 = (zmm0 * mem) + zmm17
	vmovaps	%zmm17, 36288(%rsp)             # 64-byte Spill
	vfmadd231ps	4800(%rsp), %zmm0, %zmm12 # 64-byte Folded Reload
                                        # zmm12 = (zmm0 * mem) + zmm12
	vmovaps	%zmm12, 17472(%rsp)             # 64-byte Spill
	vfmadd231ps	6208(%rsp), %zmm0, %zmm4 # 64-byte Folded Reload
                                        # zmm4 = (zmm0 * mem) + zmm4
	vmovaps	%zmm4, 32256(%rsp)              # 64-byte Spill
	vfmadd231ps	6272(%rsp), %zmm0, %zmm7 # 64-byte Folded Reload
                                        # zmm7 = (zmm0 * mem) + zmm7
	vmovaps	%zmm7, 32448(%rsp)              # 64-byte Spill
	vfmadd231ps	6400(%rsp), %zmm0, %zmm11 # 64-byte Folded Reload
                                        # zmm11 = (zmm0 * mem) + zmm11
	vmovaps	%zmm11, 15168(%rsp)             # 64-byte Spill
	vfmadd231ps	6784(%rsp), %zmm0, %zmm6 # 64-byte Folded Reload
                                        # zmm6 = (zmm0 * mem) + zmm6
	vmovaps	%zmm6, 33216(%rsp)              # 64-byte Spill
	vfmadd231ps	6464(%rsp), %zmm0, %zmm5 # 64-byte Folded Reload
                                        # zmm5 = (zmm0 * mem) + zmm5
	vmovaps	%zmm5, 33344(%rsp)              # 64-byte Spill
	vfmadd231ps	7808(%rsp), %zmm0, %zmm21 # 64-byte Folded Reload
                                        # zmm21 = (zmm0 * mem) + zmm21
	vmovaps	%zmm21, 17280(%rsp)             # 64-byte Spill
	vfmadd231ps	7872(%rsp), %zmm0, %zmm16 # 64-byte Folded Reload
                                        # zmm16 = (zmm0 * mem) + zmm16
	vmovaps	%zmm16, 16576(%rsp)             # 64-byte Spill
	vfmadd231ps	8576(%rsp), %zmm0, %zmm30 # 64-byte Folded Reload
                                        # zmm30 = (zmm0 * mem) + zmm30
	vmovaps	%zmm30, 36928(%rsp)             # 64-byte Spill
	vfmadd231ps	6976(%rsp), %zmm0, %zmm31 # 64-byte Folded Reload
                                        # zmm31 = (zmm0 * mem) + zmm31
	vmovaps	%zmm31, 17536(%rsp)             # 64-byte Spill
	vmovaps	10240(%rsp), %zmm1              # 64-byte Reload
	vfmadd231ps	8000(%rsp), %zmm0, %zmm1 # 64-byte Folded Reload
                                        # zmm1 = (zmm0 * mem) + zmm1
	vmovaps	%zmm1, 10240(%rsp)              # 64-byte Spill
	vmovaps	18752(%rsp), %zmm1              # 64-byte Reload
	vmovaps	8128(%rsp), %zmm15              # 64-byte Reload
	vfmadd231ps	%zmm0, %zmm15, %zmm1    # zmm1 = (zmm15 * zmm0) + zmm1
	vmovaps	%zmm1, 18752(%rsp)              # 64-byte Spill
	vfmadd231ps	4928(%rsp), %zmm0, %zmm24 # 64-byte Folded Reload
                                        # zmm24 = (zmm0 * mem) + zmm24
	vmovaps	%zmm24, 44480(%rsp)             # 64-byte Spill
	vfmadd231ps	4992(%rsp), %zmm0, %zmm14 # 64-byte Folded Reload
                                        # zmm14 = (zmm0 * mem) + zmm14
	vmovaps	%zmm14, 44736(%rsp)             # 64-byte Spill
	vfmadd231ps	5120(%rsp), %zmm0, %zmm22 # 64-byte Folded Reload
                                        # zmm22 = (zmm0 * mem) + zmm22
	vmovaps	%zmm22, 45120(%rsp)             # 64-byte Spill
	vfmadd231ps	5248(%rsp), %zmm0, %zmm25 # 64-byte Folded Reload
                                        # zmm25 = (zmm0 * mem) + zmm25
	vmovaps	%zmm25, 45504(%rsp)             # 64-byte Spill
	vfmadd231ps	5376(%rsp), %zmm0, %zmm19 # 64-byte Folded Reload
                                        # zmm19 = (zmm0 * mem) + zmm19
	vmovaps	%zmm19, 45888(%rsp)             # 64-byte Spill
	vfmadd231ps	7104(%rsp), %zmm0, %zmm29 # 64-byte Folded Reload
                                        # zmm29 = (zmm0 * mem) + zmm29
	vmovaps	%zmm29, 46272(%rsp)             # 64-byte Spill
	vmovaps	.LCPI0_72(%rip), %zmm26         # zmm26 = [0,1,2,3,4,5,6,7,31,u,u,u,u,u,u,u]
	vmovaps	960(%rsp), %zmm4                # 64-byte Reload
	vmovaps	8192(%rsp), %zmm25              # 64-byte Reload
	vpermt2ps	%zmm25, %zmm26, %zmm4
	vmovaps	.LCPI0_73(%rip), %zmm29         # zmm29 = [0,1,2,3,4,5,6,7,8,31,u,u,u,u,u,u]
	vpermt2ps	5760(%rsp), %zmm29, %zmm4 # 64-byte Folded Reload
	vmovaps	.LCPI0_74(%rip), %zmm31         # zmm31 = [0,1,2,3,4,5,6,7,8,9,31,u,u,u,u,u]
	vpermt2ps	7168(%rsp), %zmm31, %zmm4 # 64-byte Folded Reload
	vmovaps	.LCPI0_75(%rip), %zmm17         # zmm17 = [0,1,2,3,4,5,6,7,8,9,10,31,u,u,u,u]
	vmovaps	8256(%rsp), %zmm22              # 64-byte Reload
	vpermt2ps	%zmm22, %zmm17, %zmm4
	vmovaps	.LCPI0_76(%rip), %zmm0          # zmm0 = [0,1,2,3,4,5,6,7,8,9,10,11,31,u,u,u]
	vpermt2ps	7232(%rsp), %zmm0, %zmm4 # 64-byte Folded Reload
	vmovaps	.LCPI0_77(%rip), %zmm2          # zmm2 = [0,1,2,3,4,5,6,7,8,9,10,11,12,31,u,u]
	vpermt2ps	7296(%rsp), %zmm2, %zmm4 # 64-byte Folded Reload
	vmovaps	.LCPI0_78(%rip), %zmm3          # zmm3 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,31,u]
	vpermt2ps	1920(%rsp), %zmm3, %zmm4 # 64-byte Folded Reload
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	576(%rsp), %zmm4 {%k7}          # 64-byte Folded Reload
	vmovaps	%zmm4, 960(%rsp)                # 64-byte Spill
	vmovaps	832(%rsp), %zmm1                # 64-byte Reload
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vpermt2ps	5824(%rsp), %zmm26, %zmm1 # 64-byte Folded Reload
	vpermt2ps	7360(%rsp), %zmm29, %zmm1 # 64-byte Folded Reload
	vpermt2ps	2304(%rsp), %zmm31, %zmm1 # 64-byte Folded Reload
	vpermt2ps	7680(%rsp), %zmm17, %zmm1 # 64-byte Folded Reload
	vpermt2ps	7616(%rsp), %zmm0, %zmm1 # 64-byte Folded Reload
	vpermt2ps	7552(%rsp), %zmm2, %zmm1 # 64-byte Folded Reload
	vpermt2ps	7488(%rsp), %zmm3, %zmm1 # 64-byte Folded Reload
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	7424(%rsp), %zmm1 {%k7}         # 64-byte Folded Reload
	vmovaps	%zmm1, 832(%rsp)                # 64-byte Spill
	vmovaps	8064(%rsp), %zmm28              # 64-byte Reload
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vmulps	%zmm1, %zmm28, %zmm3
	vfmadd231ps	%zmm4, %zmm15, %zmm3    # zmm3 = (zmm15 * zmm4) + zmm3
	vmovaps	%zmm15, %zmm19
	vextractf64x4	$1, %zmm3, %ymm4
	vaddps	%zmm4, %zmm3, %zmm3
	vextractf128	$1, %ymm3, %xmm4
	vaddps	%xmm4, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm4         # xmm4 = xmm3[1,0]
	vaddps	%xmm4, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm4            # xmm4 = xmm3[1,1,3,3]
	vaddss	%xmm4, %xmm3, %xmm0
	vmovss	%xmm0, 37248(%rsp)              # 4-byte Spill
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	25152(%rsp), %zmm24
	vmovaps	192(%rsp), %zmm15               # 64-byte Reload
	vmovaps	.LCPI0_90(%rip), %zmm4          # zmm4 = [0,1,2,3,4,5,6,7,17,u,u,u,u,u,u,u]
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vpermt2ps	%zmm24, %zmm4, %zmm15
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	25280(%rsp), %zmm16
	vmovaps	.LCPI0_97(%rip), %zmm5          # zmm5 = [0,1,2,3,4,5,6,7,8,17,u,u,u,u,u,u]
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vpermt2ps	%zmm16, %zmm5, %zmm15
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	25408(%rsp), %zmm31
	vmovaps	.LCPI0_106(%rip), %zmm12        # zmm12 = [0,1,2,3,4,5,6,7,8,9,17,u,u,u,u,u]
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vpermt2ps	%zmm31, %zmm12, %zmm15
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	25536(%rsp), %zmm20
	vmovaps	.LCPI0_116(%rip), %zmm10        # zmm10 = [0,1,2,3,4,5,6,7,8,9,10,17,u,u,u,u]
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vpermt2ps	%zmm20, %zmm10, %zmm15
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	25664(%rsp), %zmm17
	vmovaps	.LCPI0_126(%rip), %zmm11        # zmm11 = [0,1,2,3,4,5,6,7,8,9,10,11,17,u,u,u]
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vpermt2ps	%zmm17, %zmm11, %zmm15
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	25792(%rsp), %zmm29
	vmovaps	.LCPI0_138(%rip), %zmm1         # zmm1 = [0,1,2,3,4,5,6,7,8,9,10,11,12,17,u,u]
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vpermt2ps	%zmm29, %zmm1, %zmm15
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	25920(%rsp), %zmm26
	vmovaps	.LCPI0_151(%rip), %zmm2         # zmm2 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,17,u]
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vpermt2ps	%zmm26, %zmm2, %zmm15
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	26048(%rsp), %zmm27
	vmovaps	.LCPI0_165(%rip), %zmm6         # zmm6 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,17]
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vpermt2ps	%zmm27, %zmm6, %zmm15
	vmovaps	%zmm15, 192(%rsp)               # 64-byte Spill
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	27200(%rsp), %zmm0
	vmovaps	128(%rsp), %zmm3                # 64-byte Reload
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vpermt2ps	%zmm0, %zmm4, %zmm3
	vmovaps	%zmm0, %zmm7
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	27328(%rsp), %zmm0
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vpermt2ps	%zmm0, %zmm5, %zmm3
	vmovaps	%zmm0, %zmm8
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	27456(%rsp), %zmm0
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vpermt2ps	%zmm0, %zmm12, %zmm3
	vmovaps	%zmm0, %zmm9
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	27584(%rsp), %zmm0
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vpermt2ps	%zmm0, %zmm10, %zmm3
	vmovaps	%zmm0, %zmm10
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	27712(%rsp), %zmm0
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vpermt2ps	%zmm0, %zmm11, %zmm3
	vmovaps	%zmm0, %zmm13
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	27840(%rsp), %zmm0
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vpermt2ps	%zmm0, %zmm1, %zmm3
	vmovaps	%zmm0, %zmm14
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	27968(%rsp), %zmm18
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vpermt2ps	%zmm18, %zmm2, %zmm3
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	28096(%rsp), %zmm0
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vpermt2ps	%zmm0, %zmm6, %zmm3
	vmovaps	%zmm3, 128(%rsp)                # 64-byte Spill
	vmovaps	%zmm0, %zmm30
	vmulps	%zmm3, %zmm28, %zmm4
	vmovaps	%zmm19, %zmm1
	vfmadd231ps	%zmm15, %zmm19, %zmm4   # zmm4 = (zmm19 * zmm15) + zmm4
	vextractf64x4	$1, %zmm4, %ymm5
	vaddps	%zmm5, %zmm4, %zmm4
	vextractf128	$1, %ymm4, %xmm5
	vaddps	%xmm5, %xmm4, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm5         # xmm5 = xmm4[1,0]
	vaddps	%xmm5, %xmm4, %xmm4
	vmovshdup	%xmm4, %xmm5            # xmm5 = xmm4[1,1,3,3]
	vaddss	%xmm5, %xmm4, %xmm0
	vmovss	%xmm0, 37312(%rsp)              # 4-byte Spill
	vmovapd	640(%rsp), %zmm0                # 64-byte Reload
	vmovapd	.LCPI0_91(%rip), %zmm11         # zmm11 = [0,1,2,3,9,u,u,u]
	vpermt2pd	%zmm24, %zmm11, %zmm0
	vmovaps	.LCPI0_98(%rip), %zmm6          # zmm6 = [0,1,2,3,4,5,6,7,8,18,u,u,u,u,u,u]
	vpermt2ps	%zmm16, %zmm6, %zmm0
	vmovapd	.LCPI0_107(%rip), %zmm4         # zmm4 = [0,1,2,3,4,9,u,u]
	vpermt2pd	%zmm31, %zmm4, %zmm0
	vmovaps	.LCPI0_117(%rip), %zmm23        # zmm23 = [0,1,2,3,4,5,6,7,8,9,10,18,u,u,u,u]
	vpermt2ps	%zmm20, %zmm23, %zmm0
	vmovapd	.LCPI0_127(%rip), %zmm12        # zmm12 = [0,1,2,3,4,5,9,u]
	vpermt2pd	%zmm17, %zmm12, %zmm0
	vmovaps	.LCPI0_139(%rip), %zmm15        # zmm15 = [0,1,2,3,4,5,6,7,8,9,10,11,12,18,u,u]
	vpermt2ps	%zmm29, %zmm15, %zmm0
	vmovapd	.LCPI0_152(%rip), %zmm19        # zmm19 = [0,1,2,3,4,5,6,9]
	vpermt2pd	%zmm26, %zmm19, %zmm0
	vmovaps	.LCPI0_166(%rip), %zmm21        # zmm21 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,18]
	vpermt2ps	%zmm27, %zmm21, %zmm0
	vmovaps	%zmm0, %zmm2
	vmovaps	%zmm0, 640(%rsp)                # 64-byte Spill
	vmovapd	1664(%rsp), %zmm0               # 64-byte Reload
	vpermt2pd	%zmm7, %zmm11, %zmm0
	vpermt2ps	%zmm8, %zmm6, %zmm0
	vpermt2pd	%zmm9, %zmm4, %zmm0
	vpermt2ps	%zmm10, %zmm23, %zmm0
	vpermt2pd	%zmm13, %zmm12, %zmm0
	vpermt2ps	%zmm14, %zmm15, %zmm0
	vpermt2pd	%zmm18, %zmm19, %zmm0
	vmovaps	%zmm30, %zmm11
	vpermt2ps	%zmm30, %zmm21, %zmm0
	vmovaps	%zmm0, 1664(%rsp)               # 64-byte Spill
	vmulps	%zmm0, %zmm28, %zmm4
	vfmadd231ps	%zmm2, %zmm1, %zmm4     # zmm4 = (zmm1 * zmm2) + zmm4
	vmovaps	%zmm1, %zmm30
	vextractf64x4	$1, %zmm4, %ymm6
	vaddps	%zmm6, %zmm4, %zmm4
	vextractf128	$1, %ymm4, %xmm6
	vaddps	%xmm6, %xmm4, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm6         # xmm6 = xmm4[1,0]
	vaddps	%xmm6, %xmm4, %xmm4
	vmovshdup	%xmm4, %xmm6            # xmm6 = xmm4[1,1,3,3]
	vaddss	%xmm6, %xmm4, %xmm0
	vmovss	%xmm0, 37184(%rsp)              # 4-byte Spill
	vmovaps	9024(%rsp), %zmm0               # 64-byte Reload
	vmovaps	.LCPI0_92(%rip), %zmm4          # zmm4 = [0,1,2,3,4,5,6,7,19,u,u,u,u,u,u,u]
	vpermt2ps	%zmm24, %zmm4, %zmm0
	vmovaps	.LCPI0_99(%rip), %zmm12         # zmm12 = [0,1,2,3,4,5,6,7,8,19,u,u,u,u,u,u]
	vpermt2ps	%zmm16, %zmm12, %zmm0
	vmovaps	.LCPI0_108(%rip), %zmm19        # zmm19 = [0,1,2,3,4,5,6,7,8,9,19,u,u,u,u,u]
	vpermt2ps	%zmm31, %zmm19, %zmm0
	vmovaps	.LCPI0_118(%rip), %zmm15        # zmm15 = [0,1,2,3,4,5,6,7,8,9,10,19,u,u,u,u]
	vpermt2ps	%zmm20, %zmm15, %zmm0
	vmovaps	.LCPI0_128(%rip), %zmm21        # zmm21 = [0,1,2,3,4,5,6,7,8,9,10,11,19,u,u,u]
	vpermt2ps	%zmm17, %zmm21, %zmm0
	vmovaps	.LCPI0_140(%rip), %zmm23        # zmm23 = [0,1,2,3,4,5,6,7,8,9,10,11,12,19,u,u]
	vpermt2ps	%zmm29, %zmm23, %zmm0
	vmovaps	.LCPI0_153(%rip), %zmm3         # zmm3 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,19,u]
	vpermt2ps	%zmm26, %zmm3, %zmm0
	vmovaps	.LCPI0_167(%rip), %zmm5         # zmm5 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,19]
	vpermt2ps	%zmm27, %zmm5, %zmm0
	vmovaps	%zmm0, %zmm1
	vmovaps	%zmm0, 9024(%rsp)               # 64-byte Spill
	vmovaps	11136(%rsp), %zmm0              # 64-byte Reload
	vmovapd	%zmm7, %zmm2
	vpermt2ps	%zmm7, %zmm4, %zmm0
	vpermt2ps	%zmm8, %zmm12, %zmm0
	vpermt2ps	%zmm9, %zmm19, %zmm0
	vpermt2ps	%zmm10, %zmm15, %zmm0
	vpermt2ps	%zmm13, %zmm21, %zmm0
	vpermt2ps	%zmm14, %zmm23, %zmm0
	vpermt2ps	%zmm18, %zmm3, %zmm0
	vpermt2ps	%zmm11, %zmm5, %zmm0
	vmovaps	%zmm11, %zmm3
	vmovaps	%zmm0, 11136(%rsp)              # 64-byte Spill
	vmulps	%zmm0, %zmm28, %zmm4
	vfmadd231ps	%zmm1, %zmm30, %zmm4    # zmm4 = (zmm30 * zmm1) + zmm4
	vextractf64x4	$1, %zmm4, %ymm7
	vaddps	%zmm7, %zmm4, %zmm4
	vextractf128	$1, %ymm4, %xmm7
	vaddps	%xmm7, %xmm4, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm7         # xmm7 = xmm4[1,0]
	vaddps	%xmm7, %xmm4, %xmm4
	vmovshdup	%xmm4, %xmm7            # xmm7 = xmm4[1,1,3,3]
	vaddss	%xmm7, %xmm4, %xmm0
	vmovss	%xmm0, 37120(%rsp)              # 4-byte Spill
	vmovapd	896(%rsp), %zmm0                # 64-byte Reload
	vshuff64x2	$212, %zmm24, %zmm0, %zmm1 # zmm1 = zmm0[0,1,2,3],zmm24[2,3,6,7]
	vmovapd	37376(%rsp), %zmm0              # 64-byte Reload
	vshuff64x2	$212, %zmm2, %zmm0, %zmm0 # zmm0 = zmm0[0,1,2,3],zmm2[2,3,6,7]
	vmovapd	%zmm2, %zmm5
	vmovapd	%zmm2, 19008(%rsp)              # 64-byte Spill
	vmovaps	.LCPI0_100(%rip), %zmm15        # zmm15 = [0,1,2,3,4,5,6,7,8,20,u,u,u,u,u,u]
	vpermt2ps	%zmm16, %zmm15, %zmm1
	vmovapd	.LCPI0_109(%rip), %zmm23        # zmm23 = [0,1,2,3,4,10,u,u]
	vpermt2pd	%zmm31, %zmm23, %zmm1
	vmovaps	.LCPI0_119(%rip), %zmm19        # zmm19 = [0,1,2,3,4,5,6,7,8,9,10,20,u,u,u,u]
	vpermt2ps	%zmm20, %zmm19, %zmm1
	vmovapd	.LCPI0_129(%rip), %zmm2         # zmm2 = [0,1,2,3,4,5,10,u]
	vpermt2pd	%zmm17, %zmm2, %zmm1
	vmovapd	%zmm2, %zmm4
	vmovaps	.LCPI0_141(%rip), %zmm21        # zmm21 = [0,1,2,3,4,5,6,7,8,9,10,11,12,20,u,u]
	vpermt2ps	%zmm29, %zmm21, %zmm1
	vmovapd	.LCPI0_154(%rip), %zmm11        # zmm11 = [0,1,2,3,4,5,6,10]
	vpermt2pd	%zmm26, %zmm11, %zmm1
	vmovaps	.LCPI0_168(%rip), %zmm12        # zmm12 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,20]
	vpermt2ps	%zmm27, %zmm12, %zmm1
	vmovaps	%zmm1, 1216(%rsp)               # 64-byte Spill
	vmovaps	%zmm8, %zmm2
	vmovaps	%zmm8, 10432(%rsp)              # 64-byte Spill
	vpermt2ps	%zmm8, %zmm15, %zmm0
	vmovaps	%zmm9, 10496(%rsp)              # 64-byte Spill
	vpermt2pd	%zmm9, %zmm23, %zmm0
	vmovaps	%zmm10, 10624(%rsp)             # 64-byte Spill
	vpermt2ps	%zmm10, %zmm19, %zmm0
	vmovaps	%zmm13, 10688(%rsp)             # 64-byte Spill
	vpermt2pd	%zmm13, %zmm4, %zmm0
	vmovaps	%zmm14, 10560(%rsp)             # 64-byte Spill
	vpermt2ps	%zmm14, %zmm21, %zmm0
	vmovaps	%zmm18, 10752(%rsp)             # 64-byte Spill
	vpermt2pd	%zmm18, %zmm11, %zmm0
	vmovaps	%zmm3, 10816(%rsp)              # 64-byte Spill
	vpermt2ps	%zmm3, %zmm12, %zmm0
	vmovaps	%zmm0, 1280(%rsp)               # 64-byte Spill
	vmulps	%zmm0, %zmm28, %zmm4
	vfmadd231ps	%zmm1, %zmm30, %zmm4    # zmm4 = (zmm30 * zmm1) + zmm4
	vextractf64x4	$1, %zmm4, %ymm8
	vaddps	%zmm8, %zmm4, %zmm4
	vextractf128	$1, %ymm4, %xmm8
	vaddps	%xmm4, %xmm8, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm8         # xmm8 = xmm4[1,0]
	vaddps	%xmm4, %xmm8, %xmm4
	vmovshdup	%xmm4, %xmm8            # xmm8 = xmm4[1,1,3,3]
	vaddss	%xmm4, %xmm8, %xmm7
	vmovapd	8448(%rsp), %zmm8               # 64-byte Reload
	vmovapd	.LCPI0_94(%rip), %zmm11         # zmm11 = [0,1,2,3,11,u,u,u]
	vpermt2pd	%zmm24, %zmm11, %zmm8
	vmovaps	.LCPI0_102(%rip), %zmm12        # zmm12 = [0,1,2,3,4,5,6,7,8,22,u,u,u,u,u,u]
	vpermt2ps	%zmm16, %zmm12, %zmm8
	vmovapd	.LCPI0_111(%rip), %zmm15        # zmm15 = [0,1,2,3,4,11,u,u]
	vpermt2pd	%zmm31, %zmm15, %zmm8
	vmovaps	.LCPI0_121(%rip), %zmm19        # zmm19 = [0,1,2,3,4,5,6,7,8,9,10,22,u,u,u,u]
	vpermt2ps	%zmm20, %zmm19, %zmm8
	vmovapd	.LCPI0_131(%rip), %zmm21        # zmm21 = [0,1,2,3,4,5,11,u]
	vpermt2pd	%zmm17, %zmm21, %zmm8
	vmovaps	.LCPI0_143(%rip), %zmm23        # zmm23 = [0,1,2,3,4,5,6,7,8,9,10,11,12,22,u,u]
	vpermt2ps	%zmm29, %zmm23, %zmm8
	vmovapd	.LCPI0_156(%rip), %zmm4         # zmm4 = [0,1,2,3,4,5,6,11]
	vpermt2pd	%zmm26, %zmm4, %zmm8
	vmovaps	.LCPI0_170(%rip), %zmm6         # zmm6 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,22]
	vpermt2ps	%zmm27, %zmm6, %zmm8
	vmovaps	%zmm8, %zmm1
	vmovaps	%zmm8, 8448(%rsp)               # 64-byte Spill
	vmovapd	8320(%rsp), %zmm0               # 64-byte Reload
	vpermt2pd	%zmm5, %zmm11, %zmm0
	vpermt2ps	%zmm2, %zmm12, %zmm0
	vpermt2pd	%zmm9, %zmm15, %zmm0
	vpermt2ps	%zmm10, %zmm19, %zmm0
	vpermt2pd	%zmm13, %zmm21, %zmm0
	vpermt2ps	%zmm14, %zmm23, %zmm0
	vpermt2pd	%zmm18, %zmm4, %zmm0
	vpermt2ps	%zmm3, %zmm6, %zmm0
	vmovaps	%zmm0, 8320(%rsp)               # 64-byte Spill
	vmulps	%zmm0, %zmm28, %zmm8
	vfmadd231ps	%zmm1, %zmm30, %zmm8    # zmm8 = (zmm30 * zmm1) + zmm8
	vextractf64x4	$1, %zmm8, %ymm9
	vaddps	%zmm9, %zmm8, %zmm8
	vextractf128	$1, %ymm8, %xmm9
	vaddps	%xmm9, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm9         # xmm9 = xmm8[1,0]
	vaddps	%xmm9, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm9            # xmm9 = xmm8[1,1,3,3]
	vaddss	%xmm9, %xmm8, %xmm8
	vmovapd	1344(%rsp), %zmm0               # 64-byte Reload
	vshuff64x2	$212, %zmm25, %zmm0, %zmm13 # zmm13 = zmm0[0,1,2,3],zmm25[2,3,6,7]
	vmovapd	18496(%rsp), %zmm0              # 64-byte Reload
	vshuff64x2	$244, %zmm24, %zmm0, %zmm0 # zmm0 = zmm0[0,1,2,3],zmm24[6,7,6,7]
	vmovaps	.LCPI0_37(%rip), %zmm30         # zmm30 = [0,1,2,3,4,5,6,7,8,28,u,u,u,u,u,u]
	vpermt2ps	%zmm16, %zmm30, %zmm0
	vmovapd	.LCPI0_38(%rip), %zmm10         # zmm10 = [0,1,2,3,4,14,u,u]
	vpermt2pd	%zmm31, %zmm10, %zmm0
	vmovaps	.LCPI0_39(%rip), %zmm14         # zmm14 = [0,1,2,3,4,5,6,7,8,9,10,28,u,u,u,u]
	vmovaps	%zmm20, %zmm19
	vpermt2ps	%zmm20, %zmm14, %zmm0
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovapd	%zmm17, %zmm0 {%k4}
	vmovapd	%zmm0, %zmm9
	vmovapd	768(%rsp), %zmm0                # 64-byte Reload
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vshuff64x2	$244, %zmm25, %zmm0, %zmm0 # zmm0 = zmm0[0,1,2,3],zmm25[6,7,6,7]
	vmovapd	%zmm25, %zmm4
	vmovaps	5760(%rsp), %zmm2               # 64-byte Reload
	vpermt2ps	%zmm2, %zmm30, %zmm0
	vmovapd	7168(%rsp), %zmm12              # 64-byte Reload
	vpermt2pd	%zmm12, %zmm10, %zmm0
	vmovaps	%zmm22, %zmm11
	vpermt2ps	%zmm22, %zmm14, %zmm0
	vmovapd	7232(%rsp), %zmm15              # 64-byte Reload
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovapd	%zmm15, %zmm0 {%k4}
	vmovapd	.LCPI0_59(%rip), %zmm3          # zmm3 = [0,1,2,3,15,u,u,u]
	vmovapd	1728(%rsp), %zmm1               # 64-byte Reload
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vpermt2pd	%zmm24, %zmm3, %zmm1
	vmovaps	.LCPI0_60(%rip), %zmm14         # zmm14 = [0,1,2,3,4,5,6,7,8,30,u,u,u,u,u,u]
	vpermt2ps	%zmm16, %zmm14, %zmm1
	vmovapd	.LCPI0_61(%rip), %zmm18         # zmm18 = [0,1,2,3,4,15,u,u]
	vpermt2pd	%zmm31, %zmm18, %zmm1
	vmovaps	.LCPI0_62(%rip), %zmm20         # zmm20 = [0,1,2,3,4,5,6,7,8,9,10,30,u,u,u,u]
	vpermt2ps	%zmm19, %zmm20, %zmm1
	vmovapd	.LCPI0_63(%rip), %zmm22         # zmm22 = [0,1,2,3,4,5,15,u]
	vpermt2pd	%zmm17, %zmm22, %zmm1
	vmovaps	.LCPI0_64(%rip), %zmm25         # zmm25 = [0,1,2,3,4,5,6,7,8,9,10,11,12,30,u,u]
	vpermt2ps	%zmm29, %zmm25, %zmm1
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovapd	%zmm26, %zmm1 {%k6}
	vmovapd	%zmm1, %zmm10
	vmovapd	1024(%rsp), %zmm1               # 64-byte Reload
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vpermt2pd	%zmm4, %zmm3, %zmm1
	vpermt2ps	%zmm2, %zmm14, %zmm1
	vmovapd	%zmm12, %zmm5
	vpermt2pd	%zmm12, %zmm18, %zmm1
	vpermt2ps	%zmm11, %zmm20, %zmm1
	vmovaps	%zmm11, %zmm3
	vpermt2pd	%zmm15, %zmm22, %zmm1
	vmovapd	%zmm15, %zmm6
	vmovaps	7296(%rsp), %zmm14              # 64-byte Reload
	vpermt2ps	%zmm14, %zmm25, %zmm1
	vmovapd	1920(%rsp), %zmm11              # 64-byte Reload
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovapd	%zmm11, %zmm1 {%k6}
	vmovapd	%zmm1, %zmm12
	vmovaps	1792(%rsp), %zmm1               # 64-byte Reload
	vmovaps	%zmm24, 11008(%rsp)             # 64-byte Spill
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vmovaps	.LCPI0_72(%rip), %zmm15         # zmm15 = [0,1,2,3,4,5,6,7,31,u,u,u,u,u,u,u]
	vpermt2ps	%zmm24, %zmm15, %zmm1
	vmovaps	%zmm16, 8832(%rsp)              # 64-byte Spill
	vmovaps	.LCPI0_73(%rip), %zmm15         # zmm15 = [0,1,2,3,4,5,6,7,8,31,u,u,u,u,u,u]
	vpermt2ps	%zmm16, %zmm15, %zmm1
	vmovaps	%zmm31, 10944(%rsp)             # 64-byte Spill
	vmovaps	.LCPI0_74(%rip), %zmm15         # zmm15 = [0,1,2,3,4,5,6,7,8,9,31,u,u,u,u,u]
	vpermt2ps	%zmm31, %zmm15, %zmm1
	vmovaps	%zmm19, %zmm15
	vmovaps	%zmm19, 8768(%rsp)              # 64-byte Spill
	vmovaps	.LCPI0_75(%rip), %zmm19         # zmm19 = [0,1,2,3,4,5,6,7,8,9,10,31,u,u,u,u]
	vpermt2ps	%zmm15, %zmm19, %zmm1
	vmovaps	%zmm17, 8704(%rsp)              # 64-byte Spill
	vmovaps	.LCPI0_76(%rip), %zmm19         # zmm19 = [0,1,2,3,4,5,6,7,8,9,10,11,31,u,u,u]
	vpermt2ps	%zmm17, %zmm19, %zmm1
	vmovaps	.LCPI0_77(%rip), %zmm19         # zmm19 = [0,1,2,3,4,5,6,7,8,9,10,11,12,31,u,u]
	vpermt2ps	%zmm29, %zmm19, %zmm1
	vmovaps	.LCPI0_78(%rip), %zmm19         # zmm19 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,31,u]
	vpermt2ps	%zmm26, %zmm19, %zmm1
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	%zmm27, %zmm1 {%k7}
	vmovaps	%zmm1, 1792(%rsp)               # 64-byte Spill
	vmovaps	1536(%rsp), %zmm1               # 64-byte Reload
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vmovaps	.LCPI0_95(%rip), %zmm19         # zmm19 = [0,1,2,3,4,5,6,7,23,u,u,u,u,u,u,u]
	vpermt2ps	%zmm24, %zmm19, %zmm1
	vmovaps	.LCPI0_100(%rip), %zmm4         # zmm4 = [0,1,2,3,4,5,6,7,8,20,u,u,u,u,u,u]
	vpermt2ps	%zmm2, %zmm4, %zmm13
	vmovaps	.LCPI0_103(%rip), %zmm2         # zmm2 = [0,1,2,3,4,5,6,7,8,23,u,u,u,u,u,u]
	vpermt2ps	%zmm16, %zmm2, %zmm1
	vmovapd	.LCPI0_109(%rip), %zmm2         # zmm2 = [0,1,2,3,4,10,u,u]
	vpermt2pd	%zmm5, %zmm2, %zmm13
	vmovaps	.LCPI0_112(%rip), %zmm2         # zmm2 = [0,1,2,3,4,5,6,7,8,9,23,u,u,u,u,u]
	vpermt2ps	%zmm31, %zmm2, %zmm1
	vmovaps	.LCPI0_119(%rip), %zmm2         # zmm2 = [0,1,2,3,4,5,6,7,8,9,10,20,u,u,u,u]
	vpermt2ps	%zmm3, %zmm2, %zmm13
	vmovaps	.LCPI0_122(%rip), %zmm2         # zmm2 = [0,1,2,3,4,5,6,7,8,9,10,23,u,u,u,u]
	vpermt2ps	%zmm15, %zmm2, %zmm1
	vmovapd	.LCPI0_129(%rip), %zmm2         # zmm2 = [0,1,2,3,4,5,10,u]
	vpermt2pd	%zmm6, %zmm2, %zmm13
	vmovaps	.LCPI0_132(%rip), %zmm2         # zmm2 = [0,1,2,3,4,5,6,7,8,9,10,11,23,u,u,u]
	vpermt2ps	%zmm17, %zmm2, %zmm1
	vmovaps	.LCPI0_141(%rip), %zmm3         # zmm3 = [0,1,2,3,4,5,6,7,8,9,10,11,12,20,u,u]
	vpermt2ps	%zmm14, %zmm3, %zmm13
	vmovaps	.LCPI0_149(%rip), %zmm30        # zmm30 = [0,1,2,3,4,5,6,7,8,9,10,11,12,28,u,u]
	vpermt2ps	%zmm14, %zmm30, %zmm0
	vmovaps	%zmm29, 19200(%rsp)             # 64-byte Spill
	vmovaps	.LCPI0_144(%rip), %zmm2         # zmm2 = [0,1,2,3,4,5,6,7,8,9,10,11,12,23,u,u]
	vpermt2ps	%zmm29, %zmm2, %zmm1
	vpermt2ps	%zmm29, %zmm30, %zmm9
	vmovapd	.LCPI0_154(%rip), %zmm3         # zmm3 = [0,1,2,3,4,5,6,10]
	vpermt2pd	%zmm11, %zmm3, %zmm13
	vmovapd	.LCPI0_162(%rip), %zmm30        # zmm30 = [0,1,2,3,4,5,6,14]
	vpermt2pd	%zmm11, %zmm30, %zmm0
	vmovaps	%zmm26, 19136(%rsp)             # 64-byte Spill
	vmovaps	.LCPI0_157(%rip), %zmm2         # zmm2 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,23,u]
	vpermt2ps	%zmm26, %zmm2, %zmm1
	vmovaps	%zmm1, %zmm2
	vpermt2pd	%zmm26, %zmm30, %zmm9
	vmovaps	576(%rsp), %zmm1                # 64-byte Reload
	vmovaps	.LCPI0_168(%rip), %zmm3         # zmm3 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,20]
	vpermt2ps	%zmm1, %zmm3, %zmm13
	vmovaps	%zmm13, 768(%rsp)               # 64-byte Spill
	vmovaps	.LCPI0_176(%rip), %zmm30        # zmm30 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,28]
	vpermt2ps	%zmm1, %zmm30, %zmm0
	vmovaps	%zmm0, 896(%rsp)                # 64-byte Spill
	vmovaps	.LCPI0_178(%rip), %zmm13        # zmm13 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,30]
	vpermt2ps	%zmm1, %zmm13, %zmm12
	vmovaps	%zmm12, 1024(%rsp)              # 64-byte Spill
	vmovaps	%zmm27, 19072(%rsp)             # 64-byte Spill
	vmovaps	.LCPI0_171(%rip), %zmm0         # zmm0 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,23]
	vpermt2ps	%zmm27, %zmm0, %zmm2
	vmovaps	%zmm2, 1536(%rsp)               # 64-byte Spill
	vpermt2ps	%zmm27, %zmm30, %zmm9
	vmovaps	%zmm9, 1344(%rsp)               # 64-byte Spill
	vpermt2ps	%zmm27, %zmm13, %zmm10
	vmovaps	%zmm10, 1728(%rsp)              # 64-byte Spill
	vxorps	%xmm1, %xmm1, %xmm1
	vaddss	37248(%rsp), %xmm1, %xmm0       # 4-byte Folded Reload
	vmovaps	%xmm0, 11936(%rsp)              # 16-byte Spill
	vaddss	37312(%rsp), %xmm1, %xmm0       # 4-byte Folded Reload
	vmovaps	%xmm0, 11984(%rsp)              # 16-byte Spill
	vaddss	37184(%rsp), %xmm1, %xmm0       # 4-byte Folded Reload
	vmovaps	%xmm0, 11968(%rsp)              # 16-byte Spill
	vaddss	37120(%rsp), %xmm1, %xmm0       # 4-byte Folded Reload
	vmovaps	%xmm0, 11952(%rsp)              # 16-byte Spill
	vaddss	%xmm1, %xmm7, %xmm0
	vmovaps	%xmm0, 12016(%rsp)              # 16-byte Spill
	vaddss	%xmm1, %xmm8, %xmm0
	vmovaps	%xmm0, 12000(%rsp)              # 16-byte Spill
	vmovaps	128(%rsp), %zmm0                # 64-byte Reload
	vmulps	5888(%rsp), %zmm0, %zmm8        # 64-byte Folded Reload
	vmulps	3712(%rsp), %zmm0, %zmm9        # 64-byte Folded Reload
	vmulps	3840(%rsp), %zmm0, %zmm10       # 64-byte Folded Reload
	vmulps	3968(%rsp), %zmm0, %zmm19       # 64-byte Folded Reload
	vmulps	4096(%rsp), %zmm0, %zmm17       # 64-byte Folded Reload
	vmulps	4224(%rsp), %zmm0, %zmm15       # 64-byte Folded Reload
	vmulps	6016(%rsp), %zmm0, %zmm13       # 64-byte Folded Reload
	vmulps	4352(%rsp), %zmm0, %zmm12       # 64-byte Folded Reload
	vmulps	4480(%rsp), %zmm0, %zmm11       # 64-byte Folded Reload
	vmulps	4608(%rsp), %zmm0, %zmm14       # 64-byte Folded Reload
	vmulps	3200(%rsp), %zmm0, %zmm29       # 64-byte Folded Reload
	vmulps	3328(%rsp), %zmm0, %zmm20       # 64-byte Folded Reload
	vmulps	3456(%rsp), %zmm0, %zmm18       # 64-byte Folded Reload
	vmulps	3584(%rsp), %zmm0, %zmm16       # 64-byte Folded Reload
	vmulps	4736(%rsp), %zmm0, %zmm28       # 64-byte Folded Reload
	vmulps	6144(%rsp), %zmm0, %zmm27       # 64-byte Folded Reload
	vmulps	7744(%rsp), %zmm0, %zmm25       # 64-byte Folded Reload
	vmulps	6336(%rsp), %zmm0, %zmm7        # 64-byte Folded Reload
	vmulps	4864(%rsp), %zmm0, %zmm30       # 64-byte Folded Reload
	vmulps	6720(%rsp), %zmm0, %zmm2        # 64-byte Folded Reload
	vmulps	6848(%rsp), %zmm0, %zmm3        # 64-byte Folded Reload
	vmulps	6912(%rsp), %zmm0, %zmm6        # 64-byte Folded Reload
	vmovaps	8512(%rsp), %zmm31              # 64-byte Reload
	vmulps	%zmm0, %zmm31, %zmm1
	vmovaps	%zmm1, 18496(%rsp)              # 64-byte Spill
	vmulps	7936(%rsp), %zmm0, %zmm5        # 64-byte Folded Reload
	vmulps	8640(%rsp), %zmm0, %zmm4        # 64-byte Folded Reload
	vmulps	6528(%rsp), %zmm0, %zmm26       # 64-byte Folded Reload
	vmulps	6592(%rsp), %zmm0, %zmm24       # 64-byte Folded Reload
	vmulps	5056(%rsp), %zmm0, %zmm23       # 64-byte Folded Reload
	vmulps	5184(%rsp), %zmm0, %zmm22       # 64-byte Folded Reload
	vmulps	5312(%rsp), %zmm0, %zmm21       # 64-byte Folded Reload
	vmulps	7040(%rsp), %zmm0, %zmm1        # 64-byte Folded Reload
	vmovaps	192(%rsp), %zmm0                # 64-byte Reload
	vfmadd231ps	5952(%rsp), %zmm0, %zmm8 # 64-byte Folded Reload
                                        # zmm8 = (zmm0 * mem) + zmm8
	vmovaps	%zmm8, 30720(%rsp)              # 64-byte Spill
	vfmadd231ps	3776(%rsp), %zmm0, %zmm9 # 64-byte Folded Reload
                                        # zmm9 = (zmm0 * mem) + zmm9
	vmovaps	%zmm9, 30976(%rsp)              # 64-byte Spill
	vfmadd231ps	3904(%rsp), %zmm0, %zmm10 # 64-byte Folded Reload
                                        # zmm10 = (zmm0 * mem) + zmm10
	vmovaps	%zmm10, 31104(%rsp)             # 64-byte Spill
	vfmadd231ps	4032(%rsp), %zmm0, %zmm19 # 64-byte Folded Reload
                                        # zmm19 = (zmm0 * mem) + zmm19
	vmovaps	%zmm19, 9664(%rsp)              # 64-byte Spill
	vfmadd231ps	4160(%rsp), %zmm0, %zmm17 # 64-byte Folded Reload
                                        # zmm17 = (zmm0 * mem) + zmm17
	vmovaps	%zmm17, 13120(%rsp)             # 64-byte Spill
	vfmadd231ps	4288(%rsp), %zmm0, %zmm15 # 64-byte Folded Reload
                                        # zmm15 = (zmm0 * mem) + zmm15
	vmovaps	%zmm15, 13440(%rsp)             # 64-byte Spill
	vfmadd231ps	6080(%rsp), %zmm0, %zmm13 # 64-byte Folded Reload
                                        # zmm13 = (zmm0 * mem) + zmm13
	vmovaps	%zmm13, 13888(%rsp)             # 64-byte Spill
	vfmadd231ps	4416(%rsp), %zmm0, %zmm12 # 64-byte Folded Reload
                                        # zmm12 = (zmm0 * mem) + zmm12
	vmovaps	%zmm12, 14336(%rsp)             # 64-byte Spill
	vfmadd231ps	4544(%rsp), %zmm0, %zmm11 # 64-byte Folded Reload
                                        # zmm11 = (zmm0 * mem) + zmm11
	vmovaps	%zmm11, 14592(%rsp)             # 64-byte Spill
	vfmadd231ps	4672(%rsp), %zmm0, %zmm14 # 64-byte Folded Reload
                                        # zmm14 = (zmm0 * mem) + zmm14
	vmovaps	%zmm14, 15232(%rsp)             # 64-byte Spill
	vfmadd231ps	3264(%rsp), %zmm0, %zmm29 # 64-byte Folded Reload
                                        # zmm29 = (zmm0 * mem) + zmm29
	vmovaps	%zmm29, 15872(%rsp)             # 64-byte Spill
	vfmadd231ps	3392(%rsp), %zmm0, %zmm20 # 64-byte Folded Reload
                                        # zmm20 = (zmm0 * mem) + zmm20
	vmovaps	%zmm20, 16064(%rsp)             # 64-byte Spill
	vfmadd231ps	3520(%rsp), %zmm0, %zmm18 # 64-byte Folded Reload
                                        # zmm18 = (zmm0 * mem) + zmm18
	vmovaps	%zmm18, 16704(%rsp)             # 64-byte Spill
	vfmadd231ps	3648(%rsp), %zmm0, %zmm16 # 64-byte Folded Reload
                                        # zmm16 = (zmm0 * mem) + zmm16
	vmovaps	%zmm16, 35648(%rsp)             # 64-byte Spill
	vfmadd231ps	4800(%rsp), %zmm0, %zmm28 # 64-byte Folded Reload
                                        # zmm28 = (zmm0 * mem) + zmm28
	vmovaps	%zmm28, 36224(%rsp)             # 64-byte Spill
	vfmadd231ps	6208(%rsp), %zmm0, %zmm27 # 64-byte Folded Reload
                                        # zmm27 = (zmm0 * mem) + zmm27
	vmovaps	%zmm27, 36032(%rsp)             # 64-byte Spill
	vfmadd231ps	6272(%rsp), %zmm0, %zmm25 # 64-byte Folded Reload
                                        # zmm25 = (zmm0 * mem) + zmm25
	vmovaps	%zmm25, 32128(%rsp)             # 64-byte Spill
	vfmadd231ps	6400(%rsp), %zmm0, %zmm7 # 64-byte Folded Reload
                                        # zmm7 = (zmm0 * mem) + zmm7
	vmovaps	%zmm7, 15040(%rsp)              # 64-byte Spill
	vfmadd231ps	6784(%rsp), %zmm0, %zmm30 # 64-byte Folded Reload
                                        # zmm30 = (zmm0 * mem) + zmm30
	vmovaps	%zmm30, 15104(%rsp)             # 64-byte Spill
	vfmadd231ps	6464(%rsp), %zmm0, %zmm2 # 64-byte Folded Reload
                                        # zmm2 = (zmm0 * mem) + zmm2
	vmovaps	%zmm2, 34368(%rsp)              # 64-byte Spill
	vfmadd231ps	7808(%rsp), %zmm0, %zmm3 # 64-byte Folded Reload
                                        # zmm3 = (zmm0 * mem) + zmm3
	vmovaps	%zmm3, 34496(%rsp)              # 64-byte Spill
	vfmadd231ps	7872(%rsp), %zmm0, %zmm6 # 64-byte Folded Reload
                                        # zmm6 = (zmm0 * mem) + zmm6
	vmovaps	%zmm6, 34688(%rsp)              # 64-byte Spill
	vmovaps	8576(%rsp), %zmm6               # 64-byte Reload
	vmovaps	18496(%rsp), %zmm13             # 64-byte Reload
	vfmadd231ps	%zmm0, %zmm6, %zmm13    # zmm13 = (zmm6 * zmm0) + zmm13
	vfmadd231ps	6976(%rsp), %zmm0, %zmm5 # 64-byte Folded Reload
                                        # zmm5 = (zmm0 * mem) + zmm5
	vmovaps	%zmm5, 36416(%rsp)              # 64-byte Spill
	vfmadd231ps	8000(%rsp), %zmm0, %zmm4 # 64-byte Folded Reload
                                        # zmm4 = (zmm0 * mem) + zmm4
	vmovaps	%zmm4, 36480(%rsp)              # 64-byte Spill
	vfmadd231ps	4928(%rsp), %zmm0, %zmm26 # 64-byte Folded Reload
                                        # zmm26 = (zmm0 * mem) + zmm26
	vmovaps	%zmm26, 37120(%rsp)             # 64-byte Spill
	vfmadd231ps	4992(%rsp), %zmm0, %zmm24 # 64-byte Folded Reload
                                        # zmm24 = (zmm0 * mem) + zmm24
	vmovaps	%zmm24, 37184(%rsp)             # 64-byte Spill
	vfmadd231ps	5120(%rsp), %zmm0, %zmm23 # 64-byte Folded Reload
                                        # zmm23 = (zmm0 * mem) + zmm23
	vmovaps	%zmm23, 37248(%rsp)             # 64-byte Spill
	vfmadd231ps	5248(%rsp), %zmm0, %zmm22 # 64-byte Folded Reload
                                        # zmm22 = (zmm0 * mem) + zmm22
	vmovaps	%zmm22, 37312(%rsp)             # 64-byte Spill
	vfmadd231ps	5376(%rsp), %zmm0, %zmm21 # 64-byte Folded Reload
                                        # zmm21 = (zmm0 * mem) + zmm21
	vmovaps	%zmm21, 37376(%rsp)             # 64-byte Spill
	vfmadd231ps	7104(%rsp), %zmm0, %zmm1 # 64-byte Folded Reload
                                        # zmm1 = (zmm0 * mem) + zmm1
	vmovaps	%zmm1, 18496(%rsp)              # 64-byte Spill
	vmovaps	18240(%rsp), %zmm0              # 64-byte Reload
	vextractf64x4	$1, %zmm0, %ymm3
	vaddps	%zmm3, %zmm0, %zmm1
	vextractf128	$1, %ymm1, %xmm3
	vaddps	%xmm3, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm3         # xmm3 = xmm1[1,0]
	vaddps	%xmm3, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm3            # xmm3 = xmm1[1,1,3,3]
	vaddss	%xmm3, %xmm1, %xmm0
	vmovss	%xmm0, 2896(%rsp)               # 4-byte Spill
	vmovaps	1152(%rsp), %zmm0               # 64-byte Reload
	vextractf64x4	$1, %zmm0, %ymm1
	vaddps	%zmm1, %zmm0, %zmm1
	vextractf128	$1, %ymm1, %xmm3
	vaddps	%xmm3, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm3         # xmm3 = xmm1[1,0]
	vaddps	%xmm3, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm3            # xmm3 = xmm1[1,1,3,3]
	vaddss	%xmm3, %xmm1, %xmm0
	vmovss	%xmm0, 18240(%rsp)              # 4-byte Spill
	vmovaps	6656(%rsp), %zmm0               # 64-byte Reload
	vextractf64x4	$1, %zmm0, %ymm1
	vaddps	%zmm1, %zmm0, %zmm1
	vextractf128	$1, %ymm1, %xmm3
	vaddps	%xmm3, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm3         # xmm3 = xmm1[1,0]
	vaddps	%xmm3, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm3            # xmm3 = xmm1[1,1,3,3]
	vaddss	%xmm3, %xmm1, %xmm0
	vmovss	%xmm0, 6656(%rsp)               # 4-byte Spill
	vmovapd	18432(%rsp), %zmm0              # 64-byte Reload
	vmovapd	5824(%rsp), %zmm8               # 64-byte Reload
	vshuff64x2	$212, %zmm8, %zmm0, %zmm0 # zmm0 = zmm0[0,1,2,3],zmm8[2,3,6,7]
	vmovaps	7360(%rsp), %zmm7               # 64-byte Reload
	vmovaps	.LCPI0_100(%rip), %zmm1         # zmm1 = [0,1,2,3,4,5,6,7,8,20,u,u,u,u,u,u]
	vpermt2ps	%zmm7, %zmm1, %zmm0
	vmovapd	2304(%rsp), %zmm4               # 64-byte Reload
	vmovapd	.LCPI0_109(%rip), %zmm1         # zmm1 = [0,1,2,3,4,10,u,u]
	vpermt2pd	%zmm4, %zmm1, %zmm0
	vmovaps	7680(%rsp), %zmm9               # 64-byte Reload
	vmovaps	.LCPI0_119(%rip), %zmm1         # zmm1 = [0,1,2,3,4,5,6,7,8,9,10,20,u,u,u,u]
	vpermt2ps	%zmm9, %zmm1, %zmm0
	vmovapd	7616(%rsp), %zmm5               # 64-byte Reload
	vmovapd	.LCPI0_129(%rip), %zmm1         # zmm1 = [0,1,2,3,4,5,10,u]
	vpermt2pd	%zmm5, %zmm1, %zmm0
	vmovaps	7552(%rsp), %zmm11              # 64-byte Reload
	vmovaps	.LCPI0_141(%rip), %zmm1         # zmm1 = [0,1,2,3,4,5,6,7,8,9,10,11,12,20,u,u]
	vpermt2ps	%zmm11, %zmm1, %zmm0
	vmovapd	7488(%rsp), %zmm10              # 64-byte Reload
	vmovapd	.LCPI0_154(%rip), %zmm1         # zmm1 = [0,1,2,3,4,5,6,10]
	vpermt2pd	%zmm10, %zmm1, %zmm0
	vmovaps	7424(%rsp), %zmm12              # 64-byte Reload
	vmovaps	.LCPI0_168(%rip), %zmm1         # zmm1 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,20]
	vpermt2ps	%zmm12, %zmm1, %zmm0
	vmovaps	%zmm0, 1152(%rsp)               # 64-byte Spill
	vmulps	%zmm0, %zmm31, %zmm1
	vfmadd231ps	768(%rsp), %zmm6, %zmm1 # 64-byte Folded Reload
                                        # zmm1 = (zmm6 * mem) + zmm1
	vextractf64x4	$1, %zmm1, %ymm3
	vaddps	%zmm3, %zmm1, %zmm1
	vextractf128	$1, %ymm1, %xmm3
	vaddps	%xmm3, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm3         # xmm3 = xmm1[1,0]
	vaddps	%xmm3, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm3            # xmm3 = xmm1[1,1,3,3]
	vaddss	%xmm3, %xmm1, %xmm0
	vmovss	%xmm0, 18432(%rsp)              # 4-byte Spill
	vmovaps	18112(%rsp), %zmm0              # 64-byte Reload
	vextractf64x4	$1, %zmm0, %ymm1
	vaddps	%zmm1, %zmm0, %zmm0
	vextractf128	$1, %ymm0, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm1         # xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm0
	vmovss	%xmm0, 18112(%rsp)              # 4-byte Spill
	vmovapd	1984(%rsp), %zmm0               # 64-byte Reload
	vmovapd	.LCPI0_59(%rip), %zmm1          # zmm1 = [0,1,2,3,15,u,u,u]
	vpermt2pd	%zmm8, %zmm1, %zmm0
	vmovaps	.LCPI0_60(%rip), %zmm1          # zmm1 = [0,1,2,3,4,5,6,7,8,30,u,u,u,u,u,u]
	vpermt2ps	%zmm7, %zmm1, %zmm0
	vmovapd	.LCPI0_61(%rip), %zmm1          # zmm1 = [0,1,2,3,4,15,u,u]
	vpermt2pd	%zmm4, %zmm1, %zmm0
	vmovaps	.LCPI0_62(%rip), %zmm1          # zmm1 = [0,1,2,3,4,5,6,7,8,9,10,30,u,u,u,u]
	vpermt2ps	%zmm9, %zmm1, %zmm0
	vmovapd	.LCPI0_63(%rip), %zmm1          # zmm1 = [0,1,2,3,4,5,15,u]
	vpermt2pd	%zmm5, %zmm1, %zmm0
	vmovaps	.LCPI0_64(%rip), %zmm1          # zmm1 = [0,1,2,3,4,5,6,7,8,9,10,11,12,30,u,u]
	vpermt2ps	%zmm11, %zmm1, %zmm0
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovapd	%zmm10, %zmm0 {%k6}
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vmovaps	.LCPI0_178(%rip), %zmm1         # zmm1 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,30]
	vpermt2ps	%zmm12, %zmm1, %zmm0
	vmovaps	%zmm0, 1984(%rsp)               # 64-byte Spill
	vmulps	%zmm0, %zmm31, %zmm0
	vfmadd231ps	1024(%rsp), %zmm6, %zmm0 # 64-byte Folded Reload
                                        # zmm0 = (zmm6 * mem) + zmm0
	vextractf64x4	$1, %zmm0, %ymm1
	vaddps	%zmm1, %zmm0, %zmm1
	vmovaps	8384(%rsp), %zmm0               # 64-byte Reload
	vmovaps	.LCPI0_95(%rip), %zmm2          # zmm2 = [0,1,2,3,4,5,6,7,23,u,u,u,u,u,u,u]
	vmovaps	19008(%rsp), %zmm9              # 64-byte Reload
	vpermt2ps	%zmm9, %zmm2, %zmm0
	vmovaps	.LCPI0_103(%rip), %zmm2         # zmm2 = [0,1,2,3,4,5,6,7,8,23,u,u,u,u,u,u]
	vmovaps	10432(%rsp), %zmm29             # 64-byte Reload
	vpermt2ps	%zmm29, %zmm2, %zmm0
	vmovaps	.LCPI0_112(%rip), %zmm2         # zmm2 = [0,1,2,3,4,5,6,7,8,9,23,u,u,u,u,u]
	vmovaps	10496(%rsp), %zmm10             # 64-byte Reload
	vpermt2ps	%zmm10, %zmm2, %zmm0
	vmovaps	.LCPI0_122(%rip), %zmm2         # zmm2 = [0,1,2,3,4,5,6,7,8,9,10,23,u,u,u,u]
	vmovaps	10624(%rsp), %zmm11             # 64-byte Reload
	vpermt2ps	%zmm11, %zmm2, %zmm0
	vmovaps	.LCPI0_132(%rip), %zmm2         # zmm2 = [0,1,2,3,4,5,6,7,8,9,10,11,23,u,u,u]
	vmovaps	10688(%rsp), %zmm14             # 64-byte Reload
	vpermt2ps	%zmm14, %zmm2, %zmm0
	vmovaps	.LCPI0_144(%rip), %zmm2         # zmm2 = [0,1,2,3,4,5,6,7,8,9,10,11,12,23,u,u]
	vmovaps	10560(%rsp), %zmm15             # 64-byte Reload
	vpermt2ps	%zmm15, %zmm2, %zmm0
	vmovaps	.LCPI0_157(%rip), %zmm2         # zmm2 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,23,u]
	vmovaps	10752(%rsp), %zmm8              # 64-byte Reload
	vpermt2ps	%zmm8, %zmm2, %zmm0
	vmovaps	.LCPI0_171(%rip), %zmm2         # zmm2 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,23]
	vmovaps	10816(%rsp), %zmm7              # 64-byte Reload
	vpermt2ps	%zmm7, %zmm2, %zmm0
	vmovaps	%zmm0, 8384(%rsp)               # 64-byte Spill
	vmulps	%zmm0, %zmm31, %zmm0
	vfmadd231ps	1536(%rsp), %zmm6, %zmm0 # 64-byte Folded Reload
                                        # zmm0 = (zmm6 * mem) + zmm0
	vmovaps	%zmm6, %zmm5
	vextractf64x4	$1, %zmm0, %ymm3
	vaddps	%zmm3, %zmm0, %zmm0
	vextractf128	$1, %ymm0, %xmm3
	vaddps	%xmm3, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm3         # xmm3 = xmm0[1,0]
	vaddps	%xmm3, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm3            # xmm3 = xmm0[1,1,3,3]
	vaddss	%xmm3, %xmm0, %xmm0
	vxorps	%xmm6, %xmm6, %xmm6
	vaddss	%xmm6, %xmm0, %xmm0
	vmovaps	%xmm0, 192(%rsp)                # 16-byte Spill
	vextractf128	$1, %ymm1, %xmm3
	vaddps	%xmm3, %xmm1, %xmm1
	vmulps	8320(%rsp), %zmm31, %zmm3       # 64-byte Folded Reload
	vfmadd231ps	8448(%rsp), %zmm5, %zmm3 # 64-byte Folded Reload
                                        # zmm3 = (zmm5 * mem) + zmm3
	vextractf64x4	$1, %zmm3, %ymm4
	vaddps	%zmm4, %zmm3, %zmm3
	vextractf128	$1, %ymm3, %xmm4
	vaddps	%xmm4, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm4         # xmm4 = xmm3[1,0]
	vaddps	%xmm4, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm4            # xmm4 = xmm3[1,1,3,3]
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm6, %xmm3, %xmm0
	vmovaps	%xmm0, 128(%rsp)                # 16-byte Spill
	vmulps	1280(%rsp), %zmm31, %zmm4       # 64-byte Folded Reload
	vmovaps	%zmm31, %zmm19
	vmovaps	%zmm5, %zmm2
	vfmadd231ps	1216(%rsp), %zmm5, %zmm4 # 64-byte Folded Reload
                                        # zmm4 = (zmm5 * mem) + zmm4
	vextractf64x4	$1, %zmm4, %ymm5
	vaddps	%zmm5, %zmm4, %zmm4
	vextractf128	$1, %ymm4, %xmm5
	vaddps	%xmm5, %xmm4, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm5         # xmm5 = xmm4[1,0]
	vaddps	%xmm5, %xmm4, %xmm4
	vmovshdup	%xmm4, %xmm5            # xmm5 = xmm4[1,1,3,3]
	vaddss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm6, %xmm4, %xmm4
	vshufpd	$1, %xmm1, %xmm1, %xmm5         # xmm5 = xmm1[1,0]
	vaddps	%xmm5, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm5            # xmm5 = xmm1[1,1,3,3]
	vaddss	%xmm5, %xmm1, %xmm0
	vmovss	%xmm0, 5632(%rsp)               # 4-byte Spill
	vmulps	832(%rsp), %zmm31, %zmm1        # 64-byte Folded Reload
	vfmadd231ps	960(%rsp), %zmm2, %zmm1 # 64-byte Folded Reload
                                        # zmm1 = (zmm2 * mem) + zmm1
	vmovaps	%zmm2, %zmm21
	vextractf64x4	$1, %zmm1, %ymm5
	vaddps	%zmm5, %zmm1, %zmm1
	vextractf128	$1, %ymm1, %xmm5
	vaddps	%xmm5, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm5         # xmm5 = xmm1[1,0]
	vaddps	%xmm5, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm5            # xmm5 = xmm1[1,1,3,3]
	vaddss	%xmm5, %xmm1, %xmm0
	vmovss	%xmm0, 5616(%rsp)               # 4-byte Spill
	vextractf64x4	$1, %zmm13, %ymm1
	vaddps	%zmm1, %zmm13, %zmm1
	vmovaps	8960(%rsp), %zmm0               # 64-byte Reload
	vmovaps	.LCPI0_93(%rip), %zmm12         # zmm12 = [0,1,2,3,4,5,6,7,21,u,u,u,u,u,u,u]
	vpermt2ps	11008(%rsp), %zmm12, %zmm0 # 64-byte Folded Reload
	vmovaps	.LCPI0_101(%rip), %zmm13        # zmm13 = [0,1,2,3,4,5,6,7,8,21,u,u,u,u,u,u]
	vpermt2ps	8832(%rsp), %zmm13, %zmm0 # 64-byte Folded Reload
	vmovaps	.LCPI0_110(%rip), %zmm17        # zmm17 = [0,1,2,3,4,5,6,7,8,9,21,u,u,u,u,u]
	vpermt2ps	10944(%rsp), %zmm17, %zmm0 # 64-byte Folded Reload
	vmovaps	.LCPI0_120(%rip), %zmm20        # zmm20 = [0,1,2,3,4,5,6,7,8,9,10,21,u,u,u,u]
	vpermt2ps	8768(%rsp), %zmm20, %zmm0 # 64-byte Folded Reload
	vmovaps	.LCPI0_130(%rip), %zmm23        # zmm23 = [0,1,2,3,4,5,6,7,8,9,10,11,21,u,u,u]
	vpermt2ps	8704(%rsp), %zmm23, %zmm0 # 64-byte Folded Reload
	vmovaps	.LCPI0_142(%rip), %zmm25        # zmm25 = [0,1,2,3,4,5,6,7,8,9,10,11,12,21,u,u]
	vpermt2ps	19200(%rsp), %zmm25, %zmm0 # 64-byte Folded Reload
	vmovaps	.LCPI0_155(%rip), %zmm26        # zmm26 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,21,u]
	vpermt2ps	19136(%rsp), %zmm26, %zmm0 # 64-byte Folded Reload
	vmovaps	.LCPI0_169(%rip), %zmm3         # zmm3 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,21]
	vpermt2ps	19072(%rsp), %zmm3, %zmm0 # 64-byte Folded Reload
	vmovaps	%zmm0, 8960(%rsp)               # 64-byte Spill
	vextractf128	$1, %ymm1, %xmm5
	vaddps	%xmm5, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm5         # xmm5 = xmm1[1,0]
	vaddps	%xmm5, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm5            # xmm5 = xmm1[1,1,3,3]
	vaddss	%xmm5, %xmm1, %xmm1
	vmovss	%xmm1, 5600(%rsp)               # 4-byte Spill
	vmulps	1664(%rsp), %zmm31, %zmm1       # 64-byte Folded Reload
	vfmadd231ps	640(%rsp), %zmm2, %zmm1 # 64-byte Folded Reload
                                        # zmm1 = (zmm2 * mem) + zmm1
	vextractf64x4	$1, %zmm1, %ymm5
	vaddps	%zmm5, %zmm1, %zmm1
	vmovaps	.LCPI0_27(%rip), %zmm5          # zmm5 = [0,1,2,3,4,5,6,7,27,u,u,u,u,u,u,u]
	vmovaps	1408(%rsp), %zmm6               # 64-byte Reload
	vmovaps	8192(%rsp), %zmm18              # 64-byte Reload
	vpermt2ps	%zmm18, %zmm5, %zmm6
	vmovaps	.LCPI0_28(%rip), %zmm5          # zmm5 = [0,1,2,3,4,5,6,7,8,27,u,u,u,u,u,u]
	vmovaps	5760(%rsp), %zmm22              # 64-byte Reload
	vpermt2ps	%zmm22, %zmm5, %zmm6
	vmovaps	.LCPI0_29(%rip), %zmm5          # zmm5 = [0,1,2,3,4,5,6,7,8,9,27,u,u,u,u,u]
	vmovaps	7168(%rsp), %zmm16              # 64-byte Reload
	vpermt2ps	%zmm16, %zmm5, %zmm6
	vmovaps	8256(%rsp), %zmm24              # 64-byte Reload
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	%zmm24, %zmm6 {%k3}
	vmovaps	.LCPI0_136(%rip), %zmm2         # zmm2 = [0,1,2,3,4,5,6,7,8,9,10,11,27,u,u,u]
	vmovaps	7232(%rsp), %zmm27              # 64-byte Reload
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vpermt2ps	%zmm27, %zmm2, %zmm6
	vmovaps	.LCPI0_148(%rip), %zmm2         # zmm2 = [0,1,2,3,4,5,6,7,8,9,10,11,12,27,u,u]
	vmovaps	7296(%rsp), %zmm30              # 64-byte Reload
	vpermt2ps	%zmm30, %zmm2, %zmm6
	vmovaps	.LCPI0_161(%rip), %zmm2         # zmm2 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,27,u]
	vmovaps	1920(%rsp), %zmm31              # 64-byte Reload
	vpermt2ps	%zmm31, %zmm2, %zmm6
	vmovaps	.LCPI0_175(%rip), %zmm2         # zmm2 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,27]
	vpermt2ps	576(%rsp), %zmm2, %zmm6 # 64-byte Folded Reload
	vmovaps	%zmm6, 1408(%rsp)               # 64-byte Spill
	vextractf128	$1, %ymm1, %xmm5
	vaddps	%xmm5, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm5         # xmm5 = xmm1[1,0]
	vaddps	%xmm5, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm5            # xmm5 = xmm1[1,1,3,3]
	vaddss	%xmm5, %xmm1, %xmm1
	vmovss	%xmm1, 2880(%rsp)               # 4-byte Spill
	vmulps	11136(%rsp), %zmm19, %zmm1      # 64-byte Folded Reload
	vfmadd231ps	9024(%rsp), %zmm21, %zmm1 # 64-byte Folded Reload
                                        # zmm1 = (zmm21 * mem) + zmm1
	vextractf64x4	$1, %zmm1, %ymm5
	vaddps	%zmm5, %zmm1, %zmm1
	vextractf128	$1, %ymm1, %xmm5
	vaddps	%xmm5, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm5         # xmm5 = xmm1[1,0]
	vaddps	%xmm5, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm5            # xmm5 = xmm1[1,1,3,3]
	vaddss	%xmm5, %xmm1, %xmm1
	vmovss	%xmm1, 2864(%rsp)               # 4-byte Spill
	vmovaps	8896(%rsp), %zmm1               # 64-byte Reload
	vpermt2ps	%zmm9, %zmm12, %zmm1
	vpermt2ps	%zmm29, %zmm13, %zmm1
	vpermt2ps	%zmm10, %zmm17, %zmm1
	vpermt2ps	%zmm11, %zmm20, %zmm1
	vpermt2ps	%zmm14, %zmm23, %zmm1
	vpermt2ps	%zmm15, %zmm25, %zmm1
	vpermt2ps	%zmm8, %zmm26, %zmm1
	vpermt2ps	%zmm7, %zmm3, %zmm1
	vmovaps	%zmm1, 8896(%rsp)               # 64-byte Spill
	vmulps	%zmm1, %zmm19, %zmm1
	vfmadd231ps	%zmm0, %zmm21, %zmm1    # zmm1 = (zmm21 * zmm0) + zmm1
	vextractf64x4	$1, %zmm1, %ymm5
	vaddps	%zmm5, %zmm1, %zmm1
	vextractf128	$1, %ymm1, %xmm5
	vaddps	%xmm5, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm5         # xmm5 = xmm1[1,0]
	vaddps	%xmm5, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm5            # xmm5 = xmm1[1,1,3,3]
	vaddss	%xmm5, %xmm1, %xmm1
	vxorps	%xmm23, %xmm23, %xmm23
	vaddss	%xmm23, %xmm1, %xmm1
	vinsertps	$16, %xmm1, %xmm4, %xmm1 # xmm1 = xmm4[0],xmm1[0],xmm4[2,3]
	vinsertps	$32, 128(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 192(%rsp), %xmm1, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm1[0,1,2],mem[0]
	vmovaps	%xmm0, 2752(%rsp)               # 16-byte Spill
	vmovapd	18368(%rsp), %zmm0              # 64-byte Reload
	vshuff64x2	$244, %zmm9, %zmm0, %zmm0 # zmm0 = zmm0[0,1,2,3],zmm9[6,7,6,7]
	vmovaps	.LCPI0_37(%rip), %zmm1          # zmm1 = [0,1,2,3,4,5,6,7,8,28,u,u,u,u,u,u]
	vpermt2ps	%zmm29, %zmm1, %zmm0
	vmovapd	.LCPI0_38(%rip), %zmm1          # zmm1 = [0,1,2,3,4,14,u,u]
	vpermt2pd	%zmm10, %zmm1, %zmm0
	vmovaps	.LCPI0_39(%rip), %zmm1          # zmm1 = [0,1,2,3,4,5,6,7,8,9,10,28,u,u,u,u]
	vpermt2ps	%zmm11, %zmm1, %zmm0
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovapd	%zmm14, %zmm0 {%k4}
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vmovaps	.LCPI0_149(%rip), %zmm1         # zmm1 = [0,1,2,3,4,5,6,7,8,9,10,11,12,28,u,u]
	vpermt2ps	%zmm15, %zmm1, %zmm0
	vmovapd	.LCPI0_162(%rip), %zmm1         # zmm1 = [0,1,2,3,4,5,6,14]
	vpermt2pd	%zmm8, %zmm1, %zmm0
	vmovaps	.LCPI0_176(%rip), %zmm1         # zmm1 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,28]
	vpermt2ps	%zmm7, %zmm1, %zmm0
	vmovaps	%zmm0, 192(%rsp)                # 64-byte Spill
	vmulps	%zmm0, %zmm19, %zmm0
	vfmadd231ps	1344(%rsp), %zmm21, %zmm0 # 64-byte Folded Reload
                                        # zmm0 = (zmm21 * mem) + zmm0
	vextractf64x4	$1, %zmm0, %ymm1
	vaddps	%zmm1, %zmm0, %zmm0
	vextractf128	$1, %ymm0, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm1         # xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm0
	vmovss	%xmm0, 18368(%rsp)              # 4-byte Spill
	vmovapd	2176(%rsp), %zmm0               # 64-byte Reload
	vmovapd	.LCPI0_59(%rip), %zmm1          # zmm1 = [0,1,2,3,15,u,u,u]
	vpermt2pd	%zmm9, %zmm1, %zmm0
	vmovaps	.LCPI0_60(%rip), %zmm1          # zmm1 = [0,1,2,3,4,5,6,7,8,30,u,u,u,u,u,u]
	vpermt2ps	%zmm29, %zmm1, %zmm0
	vmovapd	.LCPI0_61(%rip), %zmm1          # zmm1 = [0,1,2,3,4,15,u,u]
	vpermt2pd	%zmm10, %zmm1, %zmm0
	vmovaps	.LCPI0_62(%rip), %zmm1          # zmm1 = [0,1,2,3,4,5,6,7,8,9,10,30,u,u,u,u]
	vpermt2ps	%zmm11, %zmm1, %zmm0
	vmovapd	.LCPI0_63(%rip), %zmm1          # zmm1 = [0,1,2,3,4,5,15,u]
	vpermt2pd	%zmm14, %zmm1, %zmm0
	vmovaps	.LCPI0_64(%rip), %zmm1          # zmm1 = [0,1,2,3,4,5,6,7,8,9,10,11,12,30,u,u]
	vpermt2ps	%zmm15, %zmm1, %zmm0
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovapd	%zmm8, %zmm0 {%k6}
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vmovaps	.LCPI0_178(%rip), %zmm1         # zmm1 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,30]
	vpermt2ps	%zmm7, %zmm1, %zmm0
	vmovaps	%zmm0, 2176(%rsp)               # 64-byte Spill
	vmulps	%zmm0, %zmm19, %zmm0
	vfmadd231ps	1728(%rsp), %zmm21, %zmm0 # 64-byte Folded Reload
                                        # zmm0 = (zmm21 * mem) + zmm0
	vextractf64x4	$1, %zmm0, %ymm1
	vaddps	%zmm1, %zmm0, %zmm0
	vextractf128	$1, %ymm0, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm1         # xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm0
	vmovss	%xmm0, 17152(%rsp)              # 4-byte Spill
	vmovaps	2432(%rsp), %zmm0               # 64-byte Reload
	vmovaps	.LCPI0_72(%rip), %zmm1          # zmm1 = [0,1,2,3,4,5,6,7,31,u,u,u,u,u,u,u]
	vpermt2ps	%zmm9, %zmm1, %zmm0
	vmovaps	.LCPI0_73(%rip), %zmm1          # zmm1 = [0,1,2,3,4,5,6,7,8,31,u,u,u,u,u,u]
	vpermt2ps	%zmm29, %zmm1, %zmm0
	vmovaps	.LCPI0_74(%rip), %zmm1          # zmm1 = [0,1,2,3,4,5,6,7,8,9,31,u,u,u,u,u]
	vpermt2ps	%zmm10, %zmm1, %zmm0
	vmovaps	.LCPI0_75(%rip), %zmm1          # zmm1 = [0,1,2,3,4,5,6,7,8,9,10,31,u,u,u,u]
	vpermt2ps	%zmm11, %zmm1, %zmm0
	vmovaps	.LCPI0_76(%rip), %zmm1          # zmm1 = [0,1,2,3,4,5,6,7,8,9,10,11,31,u,u,u]
	vpermt2ps	%zmm14, %zmm1, %zmm0
	vmovaps	.LCPI0_77(%rip), %zmm1          # zmm1 = [0,1,2,3,4,5,6,7,8,9,10,11,12,31,u,u]
	vpermt2ps	%zmm15, %zmm1, %zmm0
	vmovaps	.LCPI0_78(%rip), %zmm1          # zmm1 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,31,u]
	vpermt2ps	%zmm8, %zmm1, %zmm0
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	%zmm7, %zmm0 {%k7}
	vmovaps	%zmm0, 2432(%rsp)               # 64-byte Spill
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vmulps	%zmm0, %zmm19, %zmm0
	vfmadd231ps	1792(%rsp), %zmm21, %zmm0 # 64-byte Folded Reload
                                        # zmm0 = (zmm21 * mem) + zmm0
	vextractf64x4	$1, %zmm0, %ymm1
	vaddps	%zmm1, %zmm0, %zmm0
	vextractf128	$1, %ymm0, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm1         # xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm0
	vmovss	%xmm0, 2848(%rsp)               # 4-byte Spill
	vmovaps	37056(%rsp), %zmm1              # 64-byte Reload
	vextractf64x4	$1, %zmm1, %ymm0
	vaddps	%zmm0, %zmm1, %zmm0
	vextractf128	$1, %ymm0, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm1         # xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm0
	vmovss	%xmm0, 2832(%rsp)               # 4-byte Spill
	vmovaps	18304(%rsp), %zmm1              # 64-byte Reload
	vextractf64x4	$1, %zmm1, %ymm0
	vaddps	%zmm0, %zmm1, %zmm0
	vextractf128	$1, %ymm0, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm1         # xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm0
	vmovss	%xmm0, 18304(%rsp)              # 4-byte Spill
	vmovaps	64(%rsp), %zmm1                 # 64-byte Reload
	vextractf64x4	$1, %zmm1, %ymm0
	vaddps	%zmm0, %zmm1, %zmm0
	vextractf128	$1, %ymm0, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vmovapd	.LCPI0_18(%rip), %zmm26         # zmm26 = [0,1,2,3,13,u,u,u]
	vmovapd	704(%rsp), %zmm2                # 64-byte Reload
	vmovaps	%zmm18, %zmm3
	vpermt2pd	%zmm18, %zmm26, %zmm2
	vmovapd	%zmm26, %zmm18
	vmovaps	.LCPI0_19(%rip), %zmm28         # zmm28 = [0,1,2,3,4,5,6,7,8,26,u,u,u,u,u,u]
	vmovaps	%zmm22, %zmm8
	vpermt2ps	%zmm22, %zmm28, %zmm2
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovapd	%zmm16, %zmm2 {%k2}
	vmovaps	.LCPI0_125(%rip), %zmm1         # zmm1 = [0,1,2,3,4,5,6,7,8,9,10,26,u,u,u,u]
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vpermt2ps	%zmm24, %zmm1, %zmm2
	vmovapd	.LCPI0_135(%rip), %zmm1         # zmm1 = [0,1,2,3,4,5,13,u]
	vmovaps	%zmm27, %zmm4
	vpermt2pd	%zmm27, %zmm1, %zmm2
	vmovapd	%zmm1, %zmm27
	vmovaps	.LCPI0_147(%rip), %zmm1         # zmm1 = [0,1,2,3,4,5,6,7,8,9,10,11,12,26,u,u]
	vpermt2ps	%zmm30, %zmm1, %zmm2
	vmovaps	%zmm1, %zmm12
	vmovapd	.LCPI0_160(%rip), %zmm1         # zmm1 = [0,1,2,3,4,5,6,13]
	vmovaps	%zmm31, %zmm15
	vpermt2pd	%zmm31, %zmm1, %zmm2
	vmovapd	%zmm1, %zmm31
	vmovaps	.LCPI0_174(%rip), %zmm14        # zmm14 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,26]
	vmovaps	576(%rsp), %zmm17               # 64-byte Reload
	vpermt2ps	%zmm17, %zmm14, %zmm2
	vmovaps	%zmm2, %zmm22
	vmovaps	%zmm2, 704(%rsp)                # 64-byte Spill
	vshufpd	$1, %xmm0, %xmm0, %xmm1         # xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm0
	vmovss	%xmm0, 2736(%rsp)               # 4-byte Spill
	vmovaps	1152(%rsp), %zmm5               # 64-byte Reload
	vmovaps	7936(%rsp), %zmm25              # 64-byte Reload
	vmulps	%zmm5, %zmm25, %zmm0
	vmovaps	768(%rsp), %zmm10               # 64-byte Reload
	vfmadd231ps	6976(%rsp), %zmm10, %zmm0 # 64-byte Folded Reload
                                        # zmm0 = (zmm10 * mem) + zmm0
	vextractf64x4	$1, %zmm0, %ymm1
	vaddps	%zmm1, %zmm0, %zmm0
	vextractf128	$1, %ymm0, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm1         # xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm0
	vmovss	%xmm0, 2816(%rsp)               # 4-byte Spill
	vmovaps	10176(%rsp), %zmm1              # 64-byte Reload
	vextractf64x4	$1, %zmm1, %ymm0
	vaddps	%zmm0, %zmm1, %zmm0
	vmovaps	.LCPI0_10(%rip), %zmm20         # zmm20 = [0,1,2,3,4,5,6,7,25,u,u,u,u,u,u,u]
	vmovaps	512(%rsp), %zmm2                # 64-byte Reload
	vpermt2ps	%zmm3, %zmm20, %zmm2
	vmovaps	%zmm20, %zmm7
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	%zmm8, %zmm2 {%k1}
	vmovaps	.LCPI0_114(%rip), %zmm1         # zmm1 = [0,1,2,3,4,5,6,7,8,9,25,u,u,u,u,u]
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vpermt2ps	%zmm16, %zmm1, %zmm2
	vmovaps	%zmm1, %zmm8
	vmovaps	.LCPI0_124(%rip), %zmm1         # zmm1 = [0,1,2,3,4,5,6,7,8,9,10,25,u,u,u,u]
	vpermt2ps	%zmm24, %zmm1, %zmm2
	vmovaps	%zmm1, %zmm9
	vmovaps	.LCPI0_134(%rip), %zmm1         # zmm1 = [0,1,2,3,4,5,6,7,8,9,10,11,25,u,u,u]
	vpermt2ps	%zmm4, %zmm1, %zmm2
	vmovaps	%zmm1, %zmm11
	vmovaps	.LCPI0_146(%rip), %zmm1         # zmm1 = [0,1,2,3,4,5,6,7,8,9,10,11,12,25,u,u]
	vpermt2ps	%zmm30, %zmm1, %zmm2
	vmovaps	%zmm1, %zmm13
	vmovaps	.LCPI0_159(%rip), %zmm1         # zmm1 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,25,u]
	vpermt2ps	%zmm15, %zmm1, %zmm2
	vmovaps	%zmm1, %zmm16
	vmovaps	.LCPI0_173(%rip), %zmm1         # zmm1 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,25]
	vpermt2ps	%zmm17, %zmm1, %zmm2
	vmovaps	%zmm1, %zmm17
	vmovaps	%zmm2, %zmm20
	vmovaps	%zmm2, 512(%rsp)                # 64-byte Spill
	vextractf128	$1, %ymm0, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vmovaps	8064(%rsp), %zmm3               # 64-byte Reload
	vmulps	1984(%rsp), %zmm3, %zmm1        # 64-byte Folded Reload
	vmovaps	8128(%rsp), %zmm4               # 64-byte Reload
	vfmadd231ps	1024(%rsp), %zmm4, %zmm1 # 64-byte Folded Reload
                                        # zmm1 = (zmm4 * mem) + zmm1
	vextractf64x4	$1, %zmm1, %ymm2
	vaddps	%zmm2, %zmm1, %zmm1
	vextractf128	$1, %ymm1, %xmm2
	vaddps	%xmm2, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm2         # xmm2 = xmm1[1,0]
	vaddps	%xmm2, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm2            # xmm2 = xmm1[1,1,3,3]
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm23, %xmm1, %xmm1
	vmovaps	%xmm1, 37056(%rsp)              # 16-byte Spill
	vshufpd	$1, %xmm0, %xmm0, %xmm1         # xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm0
	vmovss	%xmm0, 10176(%rsp)              # 4-byte Spill
	vmulps	%zmm5, %zmm3, %zmm0
	vmovaps	%zmm3, %zmm26
	vfmadd231ps	%zmm10, %zmm4, %zmm0    # zmm0 = (zmm4 * zmm10) + zmm0
	vmovaps	%zmm4, %zmm29
	vextractf64x4	$1, %zmm0, %ymm1
	vaddps	%zmm1, %zmm0, %zmm0
	vextractf128	$1, %ymm0, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm1         # xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm0
	vmovss	%xmm0, 16768(%rsp)              # 4-byte Spill
	vmovaps	18176(%rsp), %zmm1              # 64-byte Reload
	vextractf64x4	$1, %zmm1, %ymm0
	vaddps	%zmm0, %zmm1, %zmm0
	vextractf128	$1, %ymm0, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm1         # xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm0
	vmovss	%xmm0, 18176(%rsp)              # 4-byte Spill
	vmovaps	18752(%rsp), %zmm1              # 64-byte Reload
	vextractf64x4	$1, %zmm1, %ymm0
	vaddps	%zmm0, %zmm1, %zmm0
	vextractf128	$1, %ymm0, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm1         # xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm19
	vmovaps	448(%rsp), %zmm0                # 64-byte Reload
	vmovaps	5824(%rsp), %zmm6               # 64-byte Reload
	vpermt2ps	%zmm6, %zmm7, %zmm0
	vmovaps	%zmm7, %zmm24
	vmovaps	7360(%rsp), %zmm5               # 64-byte Reload
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	%zmm5, %zmm0 {%k1}
	vmovaps	2304(%rsp), %zmm7               # 64-byte Reload
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vpermt2ps	%zmm7, %zmm8, %zmm0
	vmovaps	7680(%rsp), %zmm8               # 64-byte Reload
	vpermt2ps	%zmm8, %zmm9, %zmm0
	vmovaps	7616(%rsp), %zmm10              # 64-byte Reload
	vpermt2ps	%zmm10, %zmm11, %zmm0
	vmovaps	7552(%rsp), %zmm11              # 64-byte Reload
	vpermt2ps	%zmm11, %zmm13, %zmm0
	vmovaps	7488(%rsp), %zmm15              # 64-byte Reload
	vpermt2ps	%zmm15, %zmm16, %zmm0
	vmovaps	7424(%rsp), %zmm16              # 64-byte Reload
	vpermt2ps	%zmm16, %zmm17, %zmm0
	vmovaps	%zmm0, 448(%rsp)                # 64-byte Spill
	vmulps	%zmm0, %zmm3, %zmm0
	vfmadd231ps	%zmm20, %zmm4, %zmm0    # zmm0 = (zmm4 * zmm20) + zmm0
	vextractf64x4	$1, %zmm0, %ymm1
	vaddps	%zmm1, %zmm0, %zmm0
	vextractf128	$1, %ymm0, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm1         # xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm17
	vmovapd	2048(%rsp), %zmm0               # 64-byte Reload
	vmovapd	%zmm18, %zmm21
	vpermt2pd	%zmm6, %zmm18, %zmm0
	vmovaps	%zmm6, %zmm18
	vmovaps	%zmm28, %zmm30
	vpermt2ps	%zmm5, %zmm28, %zmm0
	vmovaps	%zmm5, %zmm6
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovapd	%zmm7, %zmm0 {%k2}
	vmovaps	.LCPI0_125(%rip), %zmm4         # zmm4 = [0,1,2,3,4,5,6,7,8,9,10,26,u,u,u,u]
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vpermt2ps	%zmm8, %zmm4, %zmm0
	vmovapd	%zmm27, %zmm9
	vpermt2pd	%zmm10, %zmm27, %zmm0
	vpermt2ps	%zmm11, %zmm12, %zmm0
	vmovapd	%zmm31, %zmm12
	vpermt2pd	%zmm15, %zmm31, %zmm0
	vpermt2ps	%zmm16, %zmm14, %zmm0
	vmovaps	%zmm0, 2048(%rsp)               # 64-byte Spill
	vmulps	%zmm0, %zmm3, %zmm0
	vfmadd231ps	%zmm22, %zmm29, %zmm0   # zmm0 = (zmm29 * zmm22) + zmm0
	vextractf64x4	$1, %zmm0, %ymm1
	vaddps	%zmm1, %zmm0, %zmm0
	vextractf128	$1, %ymm0, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm1         # xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm13
	vmovaps	256(%rsp), %zmm1                # 64-byte Reload
	vmovaps	.LCPI0_27(%rip), %zmm2          # zmm2 = [0,1,2,3,4,5,6,7,27,u,u,u,u,u,u,u]
	vpermt2ps	%zmm18, %zmm2, %zmm1
	vmovaps	.LCPI0_28(%rip), %zmm0          # zmm0 = [0,1,2,3,4,5,6,7,8,27,u,u,u,u,u,u]
	vpermt2ps	%zmm5, %zmm0, %zmm1
	vmovaps	.LCPI0_29(%rip), %zmm20         # zmm20 = [0,1,2,3,4,5,6,7,8,9,27,u,u,u,u,u]
	vpermt2ps	%zmm7, %zmm20, %zmm1
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	%zmm8, %zmm1 {%k3}
	vmovaps	.LCPI0_136(%rip), %zmm3         # zmm3 = [0,1,2,3,4,5,6,7,8,9,10,11,27,u,u,u]
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vpermt2ps	%zmm10, %zmm3, %zmm1
	vmovaps	.LCPI0_148(%rip), %zmm27        # zmm27 = [0,1,2,3,4,5,6,7,8,9,10,11,12,27,u,u]
	vpermt2ps	%zmm11, %zmm27, %zmm1
	vmovaps	.LCPI0_161(%rip), %zmm23        # zmm23 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,27,u]
	vpermt2ps	%zmm15, %zmm23, %zmm1
	vmovaps	.LCPI0_175(%rip), %zmm22        # zmm22 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,27]
	vpermt2ps	%zmm16, %zmm22, %zmm1
	vmovaps	%zmm1, 256(%rsp)                # 64-byte Spill
	vmulps	%zmm1, %zmm26, %zmm1
	vfmadd231ps	1408(%rsp), %zmm29, %zmm1 # 64-byte Folded Reload
                                        # zmm1 = (zmm29 * mem) + zmm1
	vextractf64x4	$1, %zmm1, %ymm5
	vaddps	%zmm5, %zmm1, %zmm1
	vextractf128	$1, %ymm1, %xmm5
	vaddps	%xmm5, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm5         # xmm5 = xmm1[1,0]
	vaddps	%xmm5, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm5            # xmm5 = xmm1[1,1,3,3]
	vaddss	%xmm5, %xmm1, %xmm28
	vmovapd	37440(%rsp), %zmm1              # 64-byte Reload
	vshuff64x2	$244, %zmm18, %zmm1, %zmm1 # zmm1 = zmm1[0,1,2,3],zmm18[6,7,6,7]
	vmovaps	.LCPI0_37(%rip), %zmm5          # zmm5 = [0,1,2,3,4,5,6,7,8,28,u,u,u,u,u,u]
	vpermt2ps	%zmm6, %zmm5, %zmm1
	vmovapd	.LCPI0_38(%rip), %zmm5          # zmm5 = [0,1,2,3,4,14,u,u]
	vpermt2pd	%zmm7, %zmm5, %zmm1
	vmovaps	.LCPI0_39(%rip), %zmm5          # zmm5 = [0,1,2,3,4,5,6,7,8,9,10,28,u,u,u,u]
	vpermt2ps	%zmm8, %zmm5, %zmm1
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovapd	%zmm10, %zmm1 {%k4}
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vmovaps	.LCPI0_149(%rip), %zmm5         # zmm5 = [0,1,2,3,4,5,6,7,8,9,10,11,12,28,u,u]
	vpermt2ps	%zmm11, %zmm5, %zmm1
	vmovapd	.LCPI0_162(%rip), %zmm5         # zmm5 = [0,1,2,3,4,5,6,14]
	vpermt2pd	%zmm15, %zmm5, %zmm1
	vmovaps	.LCPI0_176(%rip), %zmm5         # zmm5 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,28]
	vpermt2ps	%zmm16, %zmm5, %zmm1
	vmovaps	%zmm1, 128(%rsp)                # 64-byte Spill
	vmulps	%zmm1, %zmm26, %zmm5
	vfmadd231ps	896(%rsp), %zmm29, %zmm5 # 64-byte Folded Reload
                                        # zmm5 = (zmm29 * mem) + zmm5
	vextractf64x4	$1, %zmm5, %ymm16
	vaddps	%zmm16, %zmm5, %zmm5
	vextractf32x4	$1, %ymm5, %xmm16
	vaddps	%xmm16, %xmm5, %xmm5
	vshufpd	$1, %xmm5, %xmm5, %xmm16        # xmm16 = xmm5[1,0]
	vaddps	%xmm16, %xmm5, %xmm5
	vmovshdup	%xmm5, %xmm16           # xmm16 = xmm5[1,1,3,3]
	vaddss	%xmm16, %xmm5, %xmm26
	vmovaps	1472(%rsp), %zmm15              # 64-byte Reload
	vmovaps	11008(%rsp), %zmm31             # 64-byte Reload
	vpermt2ps	%zmm31, %zmm24, %zmm15
	vmovaps	8832(%rsp), %zmm5               # 64-byte Reload
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	%zmm5, %zmm15 {%k1}
	vmovapd	2496(%rsp), %zmm1               # 64-byte Reload
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vpermt2pd	%zmm31, %zmm21, %zmm1
	vpermt2ps	%zmm5, %zmm30, %zmm1
	vmovapd	10944(%rsp), %zmm21             # 64-byte Reload
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovapd	%zmm21, %zmm1 {%k2}
	vmovapd	%zmm1, %zmm24
	vmovaps	1856(%rsp), %zmm1               # 64-byte Reload
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vpermt2ps	%zmm31, %zmm2, %zmm1
	vpermt2ps	%zmm5, %zmm0, %zmm1
	vpermt2ps	%zmm21, %zmm20, %zmm1
	vmovaps	8768(%rsp), %zmm7               # 64-byte Reload
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	%zmm7, %zmm1 {%k3}
	vmovaps	%zmm1, %zmm11
	vmovaps	.LCPI0_47(%rip), %zmm0          # zmm0 = [0,1,2,3,4,5,6,7,29,u,u,u,u,u,u,u]
	vmovaps	1088(%rsp), %zmm1               # 64-byte Reload
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vpermt2ps	%zmm31, %zmm0, %zmm1
	vmovaps	.LCPI0_48(%rip), %zmm10         # zmm10 = [0,1,2,3,4,5,6,7,8,29,u,u,u,u,u,u]
	vpermt2ps	%zmm5, %zmm10, %zmm1
	vmovaps	%zmm5, %zmm6
	vmovaps	.LCPI0_49(%rip), %zmm16         # zmm16 = [0,1,2,3,4,5,6,7,8,9,29,u,u,u,u,u]
	vpermt2ps	%zmm21, %zmm16, %zmm1
	vmovaps	.LCPI0_50(%rip), %zmm18         # zmm18 = [0,1,2,3,4,5,6,7,8,9,10,29,u,u,u,u]
	vpermt2ps	%zmm7, %zmm18, %zmm1
	vmovaps	%zmm7, %zmm10
	vmovaps	.LCPI0_51(%rip), %zmm20         # zmm20 = [0,1,2,3,4,5,6,7,8,9,10,11,29,u,u,u]
	vmovaps	8704(%rsp), %zmm0               # 64-byte Reload
	vpermt2ps	%zmm0, %zmm20, %zmm1
	vmovaps	19200(%rsp), %zmm16             # 64-byte Reload
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	%zmm16, %zmm1 {%k5}
	vmovaps	%zmm1, %zmm8
	vmovaps	37568(%rsp), %zmm1              # 64-byte Reload
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vinsertf64x4	$1, %ymm31, %zmm1, %zmm5
	vmovaps	.LCPI0_96(%rip), %zmm7          # zmm7 = [0,1,2,3,4,5,6,7,8,16,u,u,u,u,u,u]
	vpermt2ps	%zmm6, %zmm7, %zmm5
	vmovapd	.LCPI0_105(%rip), %zmm6         # zmm6 = [0,1,2,3,4,8,u,u]
	vpermt2pd	%zmm21, %zmm6, %zmm5
	vmovaps	.LCPI0_114(%rip), %zmm1         # zmm1 = [0,1,2,3,4,5,6,7,8,9,25,u,u,u,u,u]
	vpermt2ps	%zmm21, %zmm1, %zmm15
	vmovaps	.LCPI0_115(%rip), %zmm2         # zmm2 = [0,1,2,3,4,5,6,7,8,9,10,16,u,u,u,u]
	vpermt2ps	%zmm10, %zmm2, %zmm5
	vmovaps	.LCPI0_124(%rip), %zmm2         # zmm2 = [0,1,2,3,4,5,6,7,8,9,10,25,u,u,u,u]
	vpermt2ps	%zmm10, %zmm2, %zmm15
	vpermt2ps	%zmm10, %zmm4, %zmm24
	vmovaps	%zmm0, %zmm1
	vinsertf32x4	$3, %xmm1, %zmm5, %zmm2
	vmovaps	.LCPI0_134(%rip), %zmm0         # zmm0 = [0,1,2,3,4,5,6,7,8,9,10,11,25,u,u,u]
	vpermt2ps	%zmm1, %zmm0, %zmm15
	vpermt2pd	%zmm1, %zmm9, %zmm24
	vpermt2ps	%zmm1, %zmm3, %zmm11
	vmovaps	.LCPI0_137(%rip), %zmm0         # zmm0 = [0,1,2,3,4,5,6,7,8,9,10,11,12,16,u,u]
	vpermt2ps	%zmm16, %zmm0, %zmm2
	vmovaps	.LCPI0_146(%rip), %zmm0         # zmm0 = [0,1,2,3,4,5,6,7,8,9,10,11,12,25,u,u]
	vpermt2ps	%zmm16, %zmm0, %zmm15
	vmovaps	.LCPI0_147(%rip), %zmm0         # zmm0 = [0,1,2,3,4,5,6,7,8,9,10,11,12,26,u,u]
	vpermt2ps	%zmm16, %zmm0, %zmm24
	vpermt2ps	%zmm16, %zmm27, %zmm11
	vmovapd	.LCPI0_150(%rip), %zmm0         # zmm0 = [0,1,2,3,4,5,6,8]
	vmovapd	19136(%rsp), %zmm1              # 64-byte Reload
	vpermt2pd	%zmm1, %zmm0, %zmm2
	vmovaps	.LCPI0_159(%rip), %zmm0         # zmm0 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,25,u]
	vpermt2ps	%zmm1, %zmm0, %zmm15
	vpermt2pd	%zmm1, %zmm12, %zmm24
	vpermt2ps	%zmm1, %zmm23, %zmm11
	vmovaps	.LCPI0_163(%rip), %zmm16        # zmm16 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,29,u]
	vpermt2ps	%zmm1, %zmm16, %zmm8
	vmovaps	.LCPI0_164(%rip), %zmm0         # zmm0 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,16]
	vmovaps	19072(%rsp), %zmm1              # 64-byte Reload
	vpermt2ps	%zmm1, %zmm0, %zmm2
	vmovaps	%zmm2, 64(%rsp)                 # 64-byte Spill
	vmovaps	.LCPI0_173(%rip), %zmm0         # zmm0 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,25]
	vpermt2ps	%zmm1, %zmm0, %zmm15
	vmovaps	%zmm15, 1472(%rsp)              # 64-byte Spill
	vpermt2ps	%zmm1, %zmm14, %zmm24
	vmovaps	%zmm24, 2496(%rsp)              # 64-byte Spill
	vpermt2ps	%zmm1, %zmm22, %zmm11
	vmovaps	%zmm11, 1856(%rsp)              # 64-byte Spill
	vmovaps	.LCPI0_177(%rip), %zmm18        # zmm18 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,29]
	vpermt2ps	%zmm1, %zmm18, %zmm8
	vmovaps	%zmm8, 1088(%rsp)               # 64-byte Spill
	vxorps	%xmm5, %xmm5, %xmm5
	vaddss	18240(%rsp), %xmm5, %xmm0       # 4-byte Folded Reload
	vmovaps	%xmm0, 368(%rsp)                # 16-byte Spill
	vaddss	6656(%rsp), %xmm5, %xmm0        # 4-byte Folded Reload
	vmovaps	%xmm0, 384(%rsp)                # 16-byte Spill
	vaddss	18432(%rsp), %xmm5, %xmm0       # 4-byte Folded Reload
	vmovaps	%xmm0, 14912(%rsp)              # 16-byte Spill
	vaddss	18112(%rsp), %xmm5, %xmm0       # 4-byte Folded Reload
	vmovaps	%xmm0, 15424(%rsp)              # 16-byte Spill
	vaddss	5632(%rsp), %xmm5, %xmm0        # 4-byte Folded Reload
	vmovaps	%xmm0, 2688(%rsp)               # 16-byte Spill
	vaddss	5616(%rsp), %xmm5, %xmm0        # 4-byte Folded Reload
	vmovaps	%xmm0, 16192(%rsp)              # 16-byte Spill
	vaddss	5600(%rsp), %xmm5, %xmm0        # 4-byte Folded Reload
	vmovaps	%xmm0, 2800(%rsp)               # 16-byte Spill
	vaddss	2880(%rsp), %xmm5, %xmm0        # 4-byte Folded Reload
	vmovaps	%xmm0, 2784(%rsp)               # 16-byte Spill
	vaddss	2864(%rsp), %xmm5, %xmm0        # 4-byte Folded Reload
	vmovaps	%xmm0, 2768(%rsp)               # 16-byte Spill
	vaddss	18368(%rsp), %xmm5, %xmm0       # 4-byte Folded Reload
	vmovaps	%xmm0, 5520(%rsp)               # 16-byte Spill
	vaddss	17152(%rsp), %xmm5, %xmm0       # 4-byte Folded Reload
	vmovaps	%xmm0, 400(%rsp)                # 16-byte Spill
	vaddss	2848(%rsp), %xmm5, %xmm0        # 4-byte Folded Reload
	vmovaps	%xmm0, 2704(%rsp)               # 16-byte Spill
	vaddss	2832(%rsp), %xmm5, %xmm0        # 4-byte Folded Reload
	vmovaps	%xmm0, 5568(%rsp)               # 16-byte Spill
	vaddss	18304(%rsp), %xmm5, %xmm0       # 4-byte Folded Reload
	vmovaps	%xmm0, 5552(%rsp)               # 16-byte Spill
	vaddss	2736(%rsp), %xmm5, %xmm0        # 4-byte Folded Reload
	vmovaps	%xmm0, 5536(%rsp)               # 16-byte Spill
	vaddss	2816(%rsp), %xmm5, %xmm0        # 4-byte Folded Reload
	vmovaps	%xmm0, 2736(%rsp)               # 16-byte Spill
	vaddss	10176(%rsp), %xmm5, %xmm0       # 4-byte Folded Reload
	vmovaps	%xmm0, 5584(%rsp)               # 16-byte Spill
	vaddss	16768(%rsp), %xmm5, %xmm1       # 4-byte Folded Reload
	vmovaps	%xmm1, 5632(%rsp)               # 16-byte Spill
	vaddss	%xmm5, %xmm28, %xmm1
	vmovaps	%xmm1, 18240(%rsp)              # 16-byte Spill
	vaddss	%xmm5, %xmm26, %xmm1
	vmovaps	%xmm1, 18304(%rsp)              # 16-byte Spill
	vaddss	18176(%rsp), %xmm5, %xmm1       # 4-byte Folded Reload
	vmovaps	%xmm1, 5616(%rsp)               # 16-byte Spill
	vaddss	%xmm5, %xmm19, %xmm1
	vmovaps	%xmm1, 5600(%rsp)               # 16-byte Spill
	vaddss	%xmm5, %xmm17, %xmm1
	vmovaps	%xmm1, 18176(%rsp)              # 16-byte Spill
	vaddss	%xmm5, %xmm13, %xmm0
	vmovaps	%xmm0, 18112(%rsp)              # 16-byte Spill
	vmovaps	1664(%rsp), %zmm0               # 64-byte Reload
	vmulps	5888(%rsp), %zmm0, %zmm17       # 64-byte Folded Reload
	vmulps	3712(%rsp), %zmm0, %zmm16       # 64-byte Folded Reload
	vmulps	3840(%rsp), %zmm0, %zmm15       # 64-byte Folded Reload
	vmulps	3968(%rsp), %zmm0, %zmm14       # 64-byte Folded Reload
	vmulps	4096(%rsp), %zmm0, %zmm13       # 64-byte Folded Reload
	vmulps	4224(%rsp), %zmm0, %zmm12       # 64-byte Folded Reload
	vmulps	6016(%rsp), %zmm0, %zmm11       # 64-byte Folded Reload
	vmulps	4352(%rsp), %zmm0, %zmm10       # 64-byte Folded Reload
	vmulps	4480(%rsp), %zmm0, %zmm9        # 64-byte Folded Reload
	vmulps	4608(%rsp), %zmm0, %zmm8        # 64-byte Folded Reload
	vmulps	3200(%rsp), %zmm0, %zmm7        # 64-byte Folded Reload
	vmulps	3328(%rsp), %zmm0, %zmm6        # 64-byte Folded Reload
	vmulps	3456(%rsp), %zmm0, %zmm5        # 64-byte Folded Reload
	vmulps	3584(%rsp), %zmm0, %zmm21       # 64-byte Folded Reload
	vmulps	4736(%rsp), %zmm0, %zmm20       # 64-byte Folded Reload
	vmulps	6144(%rsp), %zmm0, %zmm30       # 64-byte Folded Reload
	vmulps	7744(%rsp), %zmm0, %zmm18       # 64-byte Folded Reload
	vmulps	6336(%rsp), %zmm0, %zmm22       # 64-byte Folded Reload
	vmulps	4864(%rsp), %zmm0, %zmm19       # 64-byte Folded Reload
	vmulps	6720(%rsp), %zmm0, %zmm4        # 64-byte Folded Reload
	vmovaps	6848(%rsp), %zmm29              # 64-byte Reload
	vmulps	%zmm0, %zmm29, %zmm1
	vmulps	6912(%rsp), %zmm0, %zmm31       # 64-byte Folded Reload
	vmulps	%zmm0, %zmm25, %zmm2
	vmovaps	%zmm2, 6656(%rsp)               # 64-byte Spill
	vmulps	8640(%rsp), %zmm0, %zmm28       # 64-byte Folded Reload
	vmulps	6528(%rsp), %zmm0, %zmm23       # 64-byte Folded Reload
	vmulps	6592(%rsp), %zmm0, %zmm27       # 64-byte Folded Reload
	vmulps	5056(%rsp), %zmm0, %zmm24       # 64-byte Folded Reload
	vmulps	5184(%rsp), %zmm0, %zmm25       # 64-byte Folded Reload
	vmulps	5312(%rsp), %zmm0, %zmm26       # 64-byte Folded Reload
	vmulps	7040(%rsp), %zmm0, %zmm3        # 64-byte Folded Reload
	vmovaps	640(%rsp), %zmm2                # 64-byte Reload
	vfmadd231ps	5952(%rsp), %zmm2, %zmm17 # 64-byte Folded Reload
                                        # zmm17 = (zmm2 * mem) + zmm17
	vmovaps	%zmm17, 30208(%rsp)             # 64-byte Spill
	vfmadd231ps	3776(%rsp), %zmm2, %zmm16 # 64-byte Folded Reload
                                        # zmm16 = (zmm2 * mem) + zmm16
	vmovaps	%zmm16, 30464(%rsp)             # 64-byte Spill
	vfmadd231ps	3904(%rsp), %zmm2, %zmm15 # 64-byte Folded Reload
                                        # zmm15 = (zmm2 * mem) + zmm15
	vmovaps	%zmm15, 30784(%rsp)             # 64-byte Spill
	vfmadd231ps	4032(%rsp), %zmm2, %zmm14 # 64-byte Folded Reload
                                        # zmm14 = (zmm2 * mem) + zmm14
	vmovaps	%zmm14, 12288(%rsp)             # 64-byte Spill
	vfmadd231ps	4160(%rsp), %zmm2, %zmm13 # 64-byte Folded Reload
                                        # zmm13 = (zmm2 * mem) + zmm13
	vmovaps	%zmm13, 9216(%rsp)              # 64-byte Spill
	vfmadd231ps	4288(%rsp), %zmm2, %zmm12 # 64-byte Folded Reload
                                        # zmm12 = (zmm2 * mem) + zmm12
	vmovaps	%zmm12, 12864(%rsp)             # 64-byte Spill
	vfmadd231ps	6080(%rsp), %zmm2, %zmm11 # 64-byte Folded Reload
                                        # zmm11 = (zmm2 * mem) + zmm11
	vmovaps	%zmm11, 13376(%rsp)             # 64-byte Spill
	vfmadd231ps	4416(%rsp), %zmm2, %zmm10 # 64-byte Folded Reload
                                        # zmm10 = (zmm2 * mem) + zmm10
	vmovaps	%zmm10, 13696(%rsp)             # 64-byte Spill
	vfmadd231ps	4544(%rsp), %zmm2, %zmm9 # 64-byte Folded Reload
                                        # zmm9 = (zmm2 * mem) + zmm9
	vmovaps	%zmm9, 14080(%rsp)              # 64-byte Spill
	vfmadd231ps	4672(%rsp), %zmm2, %zmm8 # 64-byte Folded Reload
                                        # zmm8 = (zmm2 * mem) + zmm8
	vmovaps	%zmm8, 14464(%rsp)              # 64-byte Spill
	vfmadd231ps	3264(%rsp), %zmm2, %zmm7 # 64-byte Folded Reload
                                        # zmm7 = (zmm2 * mem) + zmm7
	vmovaps	%zmm7, 14720(%rsp)              # 64-byte Spill
	vfmadd231ps	3392(%rsp), %zmm2, %zmm6 # 64-byte Folded Reload
                                        # zmm6 = (zmm2 * mem) + zmm6
	vmovaps	%zmm6, 33280(%rsp)              # 64-byte Spill
	vfmadd231ps	3520(%rsp), %zmm2, %zmm5 # 64-byte Folded Reload
                                        # zmm5 = (zmm2 * mem) + zmm5
	vmovaps	%zmm5, 33728(%rsp)              # 64-byte Spill
	vfmadd231ps	3648(%rsp), %zmm2, %zmm21 # 64-byte Folded Reload
                                        # zmm21 = (zmm2 * mem) + zmm21
	vmovaps	%zmm21, 34176(%rsp)             # 64-byte Spill
	vfmadd231ps	4800(%rsp), %zmm2, %zmm20 # 64-byte Folded Reload
                                        # zmm20 = (zmm2 * mem) + zmm20
	vmovaps	%zmm20, 16768(%rsp)             # 64-byte Spill
	vfmadd231ps	6208(%rsp), %zmm2, %zmm30 # 64-byte Folded Reload
                                        # zmm30 = (zmm2 * mem) + zmm30
	vmovaps	%zmm30, 31872(%rsp)             # 64-byte Spill
	vfmadd231ps	6272(%rsp), %zmm2, %zmm18 # 64-byte Folded Reload
                                        # zmm18 = (zmm2 * mem) + zmm18
	vmovaps	%zmm18, 32000(%rsp)             # 64-byte Spill
	vfmadd231ps	6400(%rsp), %zmm2, %zmm22 # 64-byte Folded Reload
                                        # zmm22 = (zmm2 * mem) + zmm22
	vmovaps	%zmm22, 16256(%rsp)             # 64-byte Spill
	vfmadd231ps	6784(%rsp), %zmm2, %zmm19 # 64-byte Folded Reload
                                        # zmm19 = (zmm2 * mem) + zmm19
	vmovaps	%zmm19, 14976(%rsp)             # 64-byte Spill
	vfmadd231ps	6464(%rsp), %zmm2, %zmm4 # 64-byte Folded Reload
                                        # zmm4 = (zmm2 * mem) + zmm4
	vmovaps	%zmm4, 16384(%rsp)              # 64-byte Spill
	vmovaps	7808(%rsp), %zmm4               # 64-byte Reload
	vfmadd231ps	%zmm2, %zmm4, %zmm1     # zmm1 = (zmm4 * zmm2) + zmm1
	vmovaps	%zmm1, 16448(%rsp)              # 64-byte Spill
	vmovaps	7872(%rsp), %zmm13              # 64-byte Reload
	vfmadd231ps	%zmm2, %zmm13, %zmm31   # zmm31 = (zmm13 * zmm2) + zmm31
	vmovaps	%zmm31, 34304(%rsp)             # 64-byte Spill
	vxorps	%xmm18, %xmm18, %xmm18
	vaddss	2896(%rsp), %xmm18, %xmm0       # 4-byte Folded Reload
	vmovaps	6656(%rsp), %zmm1               # 64-byte Reload
	vfmadd231ps	6976(%rsp), %zmm2, %zmm1 # 64-byte Folded Reload
                                        # zmm1 = (zmm2 * mem) + zmm1
	vmovaps	%zmm1, 6656(%rsp)               # 64-byte Spill
	vfmadd231ps	8000(%rsp), %zmm2, %zmm28 # 64-byte Folded Reload
                                        # zmm28 = (zmm2 * mem) + zmm28
	vmovaps	%zmm28, 17152(%rsp)             # 64-byte Spill
	vfmadd231ps	4928(%rsp), %zmm2, %zmm23 # 64-byte Folded Reload
                                        # zmm23 = (zmm2 * mem) + zmm23
	vmovaps	%zmm23, 18368(%rsp)             # 64-byte Spill
	vfmadd231ps	4992(%rsp), %zmm2, %zmm27 # 64-byte Folded Reload
                                        # zmm27 = (zmm2 * mem) + zmm27
	vmovaps	%zmm27, 18432(%rsp)             # 64-byte Spill
	vfmadd231ps	5120(%rsp), %zmm2, %zmm24 # 64-byte Folded Reload
                                        # zmm24 = (zmm2 * mem) + zmm24
	vmovaps	%zmm24, 37440(%rsp)             # 64-byte Spill
	vfmadd231ps	5248(%rsp), %zmm2, %zmm25 # 64-byte Folded Reload
                                        # zmm25 = (zmm2 * mem) + zmm25
	vmovaps	%zmm25, 37568(%rsp)             # 64-byte Spill
	vfmadd231ps	5376(%rsp), %zmm2, %zmm26 # 64-byte Folded Reload
                                        # zmm26 = (zmm2 * mem) + zmm26
	vmovaps	%zmm26, 10176(%rsp)             # 64-byte Spill
	vfmadd231ps	7104(%rsp), %zmm2, %zmm3 # 64-byte Folded Reload
                                        # zmm3 = (zmm2 * mem) + zmm3
	vmovaps	%zmm3, 18752(%rsp)              # 64-byte Spill
	vmulps	1152(%rsp), %zmm29, %zmm2       # 64-byte Folded Reload
	vfmadd231ps	768(%rsp), %zmm4, %zmm2 # 64-byte Folded Reload
                                        # zmm2 = (zmm4 * mem) + zmm2
	vextractf64x4	$1, %zmm2, %ymm3
	vaddps	%zmm3, %zmm2, %zmm2
	vextractf128	$1, %ymm2, %xmm3
	vaddps	%xmm3, %xmm2, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm3         # xmm3 = xmm2[1,0]
	vaddps	%xmm3, %xmm2, %xmm2
	vmovshdup	%xmm2, %xmm3            # xmm3 = xmm2[1,1,3,3]
	vaddss	%xmm3, %xmm2, %xmm1
	vmovss	%xmm1, 2720(%rsp)               # 4-byte Spill
	vmovaps	1600(%rsp), %zmm1               # 64-byte Reload
	vextractf64x4	$1, %zmm1, %ymm2
	vaddps	%zmm2, %zmm1, %zmm2
	vextractf128	$1, %ymm2, %xmm3
	vaddps	%xmm3, %xmm2, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm3         # xmm3 = xmm2[1,0]
	vaddps	%xmm3, %xmm2, %xmm2
	vmovshdup	%xmm2, %xmm3            # xmm3 = xmm2[1,1,3,3]
	vaddss	%xmm3, %xmm2, %xmm1
	vmovss	%xmm1, 2896(%rsp)               # 4-byte Spill
	vmovaps	17280(%rsp), %zmm1              # 64-byte Reload
	vextractf64x4	$1, %zmm1, %ymm2
	vaddps	%zmm2, %zmm1, %zmm2
	vextractf128	$1, %ymm2, %xmm3
	vaddps	%xmm3, %xmm2, %xmm2
	vmovaps	18624(%rsp), %zmm3              # 64-byte Reload
	vmovaps	8192(%rsp), %zmm12              # 64-byte Reload
	vinsertf64x4	$1, %ymm12, %zmm3, %zmm3
	vmovaps	5760(%rsp), %zmm11              # 64-byte Reload
	vmovaps	.LCPI0_96(%rip), %zmm14         # zmm14 = [0,1,2,3,4,5,6,7,8,16,u,u,u,u,u,u]
	vpermt2ps	%zmm11, %zmm14, %zmm3
	vmovapd	7168(%rsp), %zmm10              # 64-byte Reload
	vmovapd	.LCPI0_105(%rip), %zmm16        # zmm16 = [0,1,2,3,4,8,u,u]
	vpermt2pd	%zmm10, %zmm16, %zmm3
	vmovaps	8256(%rsp), %zmm6               # 64-byte Reload
	vmovaps	.LCPI0_115(%rip), %zmm17        # zmm17 = [0,1,2,3,4,5,6,7,8,9,10,16,u,u,u,u]
	vpermt2ps	%zmm6, %zmm17, %zmm3
	vmovaps	7232(%rsp), %zmm5               # 64-byte Reload
	vinsertf32x4	$3, %xmm5, %zmm3, %zmm1
	vmovaps	7296(%rsp), %zmm9               # 64-byte Reload
	vmovaps	.LCPI0_137(%rip), %zmm24        # zmm24 = [0,1,2,3,4,5,6,7,8,9,10,11,12,16,u,u]
	vpermt2ps	%zmm9, %zmm24, %zmm1
	vmovapd	1920(%rsp), %zmm8               # 64-byte Reload
	vmovapd	.LCPI0_150(%rip), %zmm25        # zmm25 = [0,1,2,3,4,5,6,8]
	vpermt2pd	%zmm8, %zmm25, %zmm1
	vmovaps	576(%rsp), %zmm7                # 64-byte Reload
	vmovaps	.LCPI0_164(%rip), %zmm26        # zmm26 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,16]
	vpermt2ps	%zmm7, %zmm26, %zmm1
	vmovaps	%zmm1, %zmm15
	vmovaps	%zmm1, 1664(%rsp)               # 64-byte Spill
	vshufpd	$1, %xmm2, %xmm2, %xmm3         # xmm3 = xmm2[1,0]
	vaddps	%xmm3, %xmm2, %xmm3
	vmovaps	8512(%rsp), %zmm21              # 64-byte Reload
	vmulps	128(%rsp), %zmm21, %zmm2        # 64-byte Folded Reload
	vmovaps	8576(%rsp), %zmm23              # 64-byte Reload
	vfmadd231ps	896(%rsp), %zmm23, %zmm2 # 64-byte Folded Reload
                                        # zmm2 = (zmm23 * mem) + zmm2
	vextractf64x4	$1, %zmm2, %ymm4
	vaddps	%zmm4, %zmm2, %zmm2
	vextractf128	$1, %ymm2, %xmm4
	vaddps	%xmm4, %xmm2, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm4         # xmm4 = xmm2[1,0]
	vaddps	%xmm4, %xmm2, %xmm2
	vmovshdup	%xmm2, %xmm4            # xmm4 = xmm2[1,1,3,3]
	vaddss	%xmm4, %xmm2, %xmm2
	vaddss	%xmm18, %xmm2, %xmm30
	vmovshdup	%xmm3, %xmm4            # xmm4 = xmm3[1,1,3,3]
	vaddss	%xmm4, %xmm3, %xmm1
	vmovss	%xmm1, 2880(%rsp)               # 4-byte Spill
	vmovaps	6912(%rsp), %zmm1               # 64-byte Reload
	vmulps	832(%rsp), %zmm1, %zmm3         # 64-byte Folded Reload
	vfmadd231ps	960(%rsp), %zmm13, %zmm3 # 64-byte Folded Reload
                                        # zmm3 = (zmm13 * mem) + zmm3
	vextractf64x4	$1, %zmm3, %ymm4
	vaddps	%zmm4, %zmm3, %zmm3
	vextractf128	$1, %ymm3, %xmm4
	vaddps	%xmm4, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm4         # xmm4 = xmm3[1,0]
	vaddps	%xmm4, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm4            # xmm4 = xmm3[1,1,3,3]
	vaddss	%xmm4, %xmm3, %xmm1
	vmovss	%xmm1, 2864(%rsp)               # 4-byte Spill
	vmovaps	18560(%rsp), %zmm3              # 64-byte Reload
	vmovaps	5824(%rsp), %zmm22              # 64-byte Reload
	vinsertf64x4	$1, %ymm22, %zmm3, %zmm3
	vmovaps	7360(%rsp), %zmm20              # 64-byte Reload
	vpermt2ps	%zmm20, %zmm14, %zmm3
	vmovapd	2304(%rsp), %zmm19              # 64-byte Reload
	vpermt2pd	%zmm19, %zmm16, %zmm3
	vmovaps	7680(%rsp), %zmm18              # 64-byte Reload
	vpermt2ps	%zmm18, %zmm17, %zmm3
	vmovaps	7616(%rsp), %zmm17              # 64-byte Reload
	vinsertf32x4	$3, %xmm17, %zmm3, %zmm1
	vmovaps	7552(%rsp), %zmm16              # 64-byte Reload
	vpermt2ps	%zmm16, %zmm24, %zmm1
	vmovapd	7488(%rsp), %zmm14              # 64-byte Reload
	vpermt2pd	%zmm14, %zmm25, %zmm1
	vmovaps	7424(%rsp), %zmm13              # 64-byte Reload
	vpermt2ps	%zmm13, %zmm26, %zmm1
	vmulps	%zmm1, %zmm21, %zmm3
	vmovaps	%zmm1, %zmm31
	vfmadd231ps	%zmm15, %zmm23, %zmm3   # zmm3 = (zmm23 * zmm15) + zmm3
	vextractf64x4	$1, %zmm3, %ymm4
	vaddps	%zmm4, %zmm3, %zmm3
	vextractf128	$1, %ymm3, %xmm4
	vaddps	%xmm4, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm4         # xmm4 = xmm3[1,0]
	vaddps	%xmm4, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm4            # xmm4 = xmm3[1,1,3,3]
	vaddss	%xmm4, %xmm3, %xmm3
	vxorps	%xmm2, %xmm2, %xmm2
	vaddss	%xmm2, %xmm3, %xmm3
	vinsertps	$16, %xmm0, %xmm3, %xmm0 # xmm0 = xmm3[0],xmm0[0],xmm3[2,3]
	vinsertps	$32, 368(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, 384(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%ymm0, 18624(%rsp)              # 32-byte Spill
	vmovaps	18816(%rsp), %zmm1              # 64-byte Reload
	vextractf64x4	$1, %zmm1, %ymm0
	vaddps	%zmm0, %zmm1, %zmm0
	vextractf128	$1, %ymm0, %xmm3
	vaddps	%xmm3, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm3         # xmm3 = xmm0[1,0]
	vaddps	%xmm3, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm3            # xmm3 = xmm0[1,1,3,3]
	vaddss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	vmovaps	14912(%rsp), %xmm1              # 16-byte Reload
	vinsertps	$16, %xmm0, %xmm1, %xmm0 # xmm0 = xmm1[0],xmm0[0],xmm1[2,3]
	vinsertps	$32, 15424(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vmovaps	36928(%rsp), %zmm1              # 64-byte Reload
	vextractf64x4	$1, %zmm1, %ymm3
	vaddps	%zmm3, %zmm1, %zmm3
	vextractf128	$1, %ymm3, %xmm4
	vaddps	%xmm4, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm4         # xmm4 = xmm3[1,0]
	vaddps	%xmm4, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm4            # xmm4 = xmm3[1,1,3,3]
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm2, %xmm3, %xmm3
	vinsertps	$48, %xmm3, %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],xmm3[0]
	vmovaps	%xmm0, 18560(%rsp)              # 16-byte Spill
	vmovaps	448(%rsp), %zmm26               # 64-byte Reload
	vmovaps	%zmm21, %zmm0
	vmulps	%zmm26, %zmm21, %zmm3
	vmovaps	512(%rsp), %zmm27               # 64-byte Reload
	vmovaps	%zmm23, %zmm21
	vfmadd231ps	%zmm27, %zmm23, %zmm3   # zmm3 = (zmm23 * zmm27) + zmm3
	vextractf64x4	$1, %zmm3, %ymm4
	vaddps	%zmm4, %zmm3, %zmm3
	vextractf128	$1, %ymm3, %xmm4
	vaddps	%xmm4, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm4         # xmm4 = xmm3[1,0]
	vaddps	%xmm4, %xmm3, %xmm3
	vmovaps	2560(%rsp), %zmm4               # 64-byte Reload
	vmovaps	.LCPI0_47(%rip), %zmm29         # zmm29 = [0,1,2,3,4,5,6,7,29,u,u,u,u,u,u,u]
	vpermt2ps	%zmm12, %zmm29, %zmm4
	vmovaps	.LCPI0_48(%rip), %zmm12         # zmm12 = [0,1,2,3,4,5,6,7,8,29,u,u,u,u,u,u]
	vpermt2ps	%zmm11, %zmm12, %zmm4
	vmovaps	.LCPI0_49(%rip), %zmm2          # zmm2 = [0,1,2,3,4,5,6,7,8,9,29,u,u,u,u,u]
	vpermt2ps	%zmm10, %zmm2, %zmm4
	vmovaps	.LCPI0_50(%rip), %zmm10         # zmm10 = [0,1,2,3,4,5,6,7,8,9,10,29,u,u,u,u]
	vpermt2ps	%zmm6, %zmm10, %zmm4
	vmovaps	.LCPI0_51(%rip), %zmm11         # zmm11 = [0,1,2,3,4,5,6,7,8,9,10,11,29,u,u,u]
	vpermt2ps	%zmm5, %zmm11, %zmm4
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	%zmm9, %zmm4 {%k5}
	vmovaps	.LCPI0_163(%rip), %zmm15        # zmm15 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,29,u]
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vpermt2ps	%zmm8, %zmm15, %zmm4
	vmovaps	.LCPI0_177(%rip), %zmm8         # zmm8 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,29]
	vpermt2ps	%zmm7, %zmm8, %zmm4
	vmovaps	%zmm4, %zmm6
	vmovaps	%zmm4, 2560(%rsp)               # 64-byte Spill
	vmovshdup	%xmm3, %xmm4            # xmm4 = xmm3[1,1,3,3]
	vaddss	%xmm4, %xmm3, %xmm1
	vmovss	%xmm1, 2848(%rsp)               # 4-byte Spill
	vmovaps	2048(%rsp), %zmm23              # 64-byte Reload
	vmulps	%zmm23, %zmm0, %zmm3
	vmovaps	%zmm0, %zmm9
	vmovaps	704(%rsp), %zmm24               # 64-byte Reload
	vfmadd231ps	%zmm24, %zmm21, %zmm3   # zmm3 = (zmm21 * zmm24) + zmm3
	vextractf64x4	$1, %zmm3, %ymm4
	vaddps	%zmm4, %zmm3, %zmm3
	vextractf128	$1, %ymm3, %xmm4
	vaddps	%xmm4, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm4         # xmm4 = xmm3[1,0]
	vaddps	%xmm4, %xmm3, %xmm3
	vmovaps	18880(%rsp), %zmm1              # 64-byte Reload
	vextractf64x4	$1, %zmm1, %ymm4
	vaddps	%zmm4, %zmm1, %zmm4
	vextractf128	$1, %ymm4, %xmm5
	vaddps	%xmm5, %xmm4, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm5         # xmm5 = xmm4[1,0]
	vaddps	%xmm5, %xmm4, %xmm4
	vmovshdup	%xmm4, %xmm5            # xmm5 = xmm4[1,1,3,3]
	vaddss	%xmm5, %xmm4, %xmm4
	vxorps	%xmm0, %xmm0, %xmm0
	vaddss	%xmm0, %xmm4, %xmm1
	vmovaps	%xmm1, 18880(%rsp)              # 16-byte Spill
	vmovshdup	%xmm3, %xmm4            # xmm4 = xmm3[1,1,3,3]
	vaddss	%xmm4, %xmm3, %xmm1
	vmovss	%xmm1, 2832(%rsp)               # 4-byte Spill
	vmovaps	256(%rsp), %zmm25               # 64-byte Reload
	vmulps	%zmm25, %zmm9, %zmm3
	vmovaps	1408(%rsp), %zmm28              # 64-byte Reload
	vfmadd231ps	%zmm28, %zmm21, %zmm3   # zmm3 = (zmm21 * zmm28) + zmm3
	vextractf64x4	$1, %zmm3, %ymm4
	vaddps	%zmm4, %zmm3, %zmm3
	vextractf128	$1, %ymm3, %xmm4
	vaddps	%xmm4, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm4         # xmm4 = xmm3[1,0]
	vaddps	%xmm4, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm4            # xmm4 = xmm3[1,1,3,3]
	vaddss	%xmm4, %xmm3, %xmm1
	vmovss	%xmm1, 2816(%rsp)               # 4-byte Spill
	vmovaps	2368(%rsp), %zmm4               # 64-byte Reload
	vpermt2ps	%zmm22, %zmm29, %zmm4
	vpermt2ps	%zmm20, %zmm12, %zmm4
	vpermt2ps	%zmm19, %zmm2, %zmm4
	vpermt2ps	%zmm18, %zmm10, %zmm4
	vpermt2ps	%zmm17, %zmm11, %zmm4
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	%zmm16, %zmm4 {%k5}
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vpermt2ps	%zmm14, %zmm15, %zmm4
	vpermt2ps	%zmm13, %zmm8, %zmm4
	vmulps	%zmm4, %zmm9, %zmm3
	vmovaps	%zmm4, %zmm20
	vmovaps	%zmm4, 2368(%rsp)               # 64-byte Spill
	vfmadd231ps	%zmm6, %zmm21, %zmm3    # zmm3 = (zmm21 * zmm6) + zmm3
	vextractf64x4	$1, %zmm3, %ymm4
	vaddps	%zmm4, %zmm3, %zmm3
	vextractf128	$1, %ymm3, %xmm4
	vaddps	%xmm4, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm4         # xmm4 = xmm3[1,0]
	vaddps	%xmm4, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm4            # xmm4 = xmm3[1,1,3,3]
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm0, %xmm3, %xmm3
	vxorps	%xmm10, %xmm10, %xmm10
	vinsertps	$16, %xmm3, %xmm30, %xmm2 # xmm2 = xmm30[0],xmm3[0],xmm30[2,3]
	vinsertps	$32, 2688(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, 16192(%rsp), %xmm2, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm2[0,1,2],mem[0]
	vmovaps	%xmm1, 18816(%rsp)              # 16-byte Spill
	vmovaps	37504(%rsp), %zmm2              # 64-byte Reload
	vmovaps	19008(%rsp), %zmm16             # 64-byte Reload
	vinsertf64x4	$1, %ymm16, %zmm2, %zmm2
	vmovaps	10432(%rsp), %zmm19             # 64-byte Reload
	vmovaps	.LCPI0_96(%rip), %zmm0          # zmm0 = [0,1,2,3,4,5,6,7,8,16,u,u,u,u,u,u]
	vpermt2ps	%zmm19, %zmm0, %zmm2
	vmovapd	10496(%rsp), %zmm7              # 64-byte Reload
	vmovapd	.LCPI0_105(%rip), %zmm0         # zmm0 = [0,1,2,3,4,8,u,u]
	vpermt2pd	%zmm7, %zmm0, %zmm2
	vmovaps	10624(%rsp), %zmm0              # 64-byte Reload
	vmovaps	.LCPI0_115(%rip), %zmm1         # zmm1 = [0,1,2,3,4,5,6,7,8,9,10,16,u,u,u,u]
	vpermt2ps	%zmm0, %zmm1, %zmm2
	vmovaps	10688(%rsp), %zmm6              # 64-byte Reload
	vinsertf32x4	$3, %xmm6, %zmm2, %zmm1
	vmovaps	10560(%rsp), %zmm29             # 64-byte Reload
	vmovaps	.LCPI0_137(%rip), %zmm2         # zmm2 = [0,1,2,3,4,5,6,7,8,9,10,11,12,16,u,u]
	vpermt2ps	%zmm29, %zmm2, %zmm1
	vmovapd	10752(%rsp), %zmm5              # 64-byte Reload
	vmovapd	.LCPI0_150(%rip), %zmm2         # zmm2 = [0,1,2,3,4,5,6,8]
	vpermt2pd	%zmm5, %zmm2, %zmm1
	vmovaps	10816(%rsp), %zmm4              # 64-byte Reload
	vmovaps	.LCPI0_164(%rip), %zmm2         # zmm2 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,16]
	vpermt2ps	%zmm4, %zmm2, %zmm1
	vmulps	%zmm1, %zmm9, %zmm2
	vmovaps	%zmm1, %zmm30
	vmovaps	64(%rsp), %zmm18                # 64-byte Reload
	vfmadd231ps	%zmm18, %zmm21, %zmm2   # zmm2 = (zmm21 * zmm18) + zmm2
	vextractf64x4	$1, %zmm2, %ymm3
	vaddps	%zmm3, %zmm2, %zmm2
	vextractf128	$1, %ymm2, %xmm3
	vaddps	%xmm3, %xmm2, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm3         # xmm3 = xmm2[1,0]
	vaddps	%xmm3, %xmm2, %xmm2
	vmovshdup	%xmm2, %xmm3            # xmm3 = xmm2[1,1,3,3]
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm2, %xmm10, %xmm2
	vinsertps	$16, 2800(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$32, 2784(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, 2768(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[0,1,2],mem[0]
	vmovaps	3072(%rsp), %zmm1               # 64-byte Reload
	vmovaps	.LCPI0_10(%rip), %zmm3          # zmm3 = [0,1,2,3,4,5,6,7,25,u,u,u,u,u,u,u]
	vpermt2ps	%zmm16, %zmm3, %zmm1
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	%zmm19, %zmm1 {%k1}
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vmovaps	.LCPI0_114(%rip), %zmm3         # zmm3 = [0,1,2,3,4,5,6,7,8,9,25,u,u,u,u,u]
	vpermt2ps	%zmm7, %zmm3, %zmm1
	vmovapd	%zmm7, %zmm14
	vmovaps	.LCPI0_124(%rip), %zmm3         # zmm3 = [0,1,2,3,4,5,6,7,8,9,10,25,u,u,u,u]
	vpermt2ps	%zmm0, %zmm3, %zmm1
	vmovaps	.LCPI0_134(%rip), %zmm3         # zmm3 = [0,1,2,3,4,5,6,7,8,9,10,11,25,u,u,u]
	vpermt2ps	%zmm6, %zmm3, %zmm1
	vmovaps	%zmm6, %zmm13
	vmovaps	.LCPI0_146(%rip), %zmm3         # zmm3 = [0,1,2,3,4,5,6,7,8,9,10,11,12,25,u,u]
	vpermt2ps	%zmm29, %zmm3, %zmm1
	vmovaps	.LCPI0_159(%rip), %zmm3         # zmm3 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,25,u]
	vpermt2ps	%zmm5, %zmm3, %zmm1
	vmovaps	.LCPI0_173(%rip), %zmm3         # zmm3 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,25]
	vpermt2ps	%zmm4, %zmm3, %zmm1
	vmovaps	%zmm4, %zmm8
	vmulps	%zmm1, %zmm9, %zmm3
	vmovaps	%zmm1, %zmm22
	vmovaps	%zmm1, 3072(%rsp)               # 64-byte Spill
	vmovaps	1472(%rsp), %zmm17              # 64-byte Reload
	vfmadd231ps	%zmm17, %zmm21, %zmm3   # zmm3 = (zmm21 * zmm17) + zmm3
	vextractf64x4	$1, %zmm3, %ymm4
	vaddps	%zmm4, %zmm3, %zmm3
	vextractf128	$1, %ymm3, %xmm4
	vaddps	%xmm4, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm4         # xmm4 = xmm3[1,0]
	vaddps	%xmm4, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm4            # xmm4 = xmm3[1,1,3,3]
	vaddss	%xmm4, %xmm3, %xmm1
	vmovss	%xmm1, 2800(%rsp)               # 4-byte Spill
	vmovapd	2112(%rsp), %zmm1               # 64-byte Reload
	vmovapd	.LCPI0_18(%rip), %zmm3          # zmm3 = [0,1,2,3,13,u,u,u]
	vpermt2pd	%zmm16, %zmm3, %zmm1
	vmovaps	%zmm19, %zmm7
	vmovaps	.LCPI0_19(%rip), %zmm3          # zmm3 = [0,1,2,3,4,5,6,7,8,26,u,u,u,u,u,u]
	vpermt2ps	%zmm19, %zmm3, %zmm1
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovapd	%zmm14, %zmm1 {%k2}
	vmovaps	%zmm0, %zmm6
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vmovaps	.LCPI0_125(%rip), %zmm0         # zmm0 = [0,1,2,3,4,5,6,7,8,9,10,26,u,u,u,u]
	vpermt2ps	%zmm6, %zmm0, %zmm1
	vmovapd	.LCPI0_135(%rip), %zmm0         # zmm0 = [0,1,2,3,4,5,13,u]
	vpermt2pd	%zmm13, %zmm0, %zmm1
	vmovaps	.LCPI0_147(%rip), %zmm0         # zmm0 = [0,1,2,3,4,5,6,7,8,9,10,11,12,26,u,u]
	vpermt2ps	%zmm29, %zmm0, %zmm1
	vmovapd	.LCPI0_160(%rip), %zmm0         # zmm0 = [0,1,2,3,4,5,6,13]
	vpermt2pd	%zmm5, %zmm0, %zmm1
	vmovaps	.LCPI0_174(%rip), %zmm0         # zmm0 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,26]
	vpermt2ps	%zmm8, %zmm0, %zmm1
	vmovaps	%zmm1, 2112(%rsp)               # 64-byte Spill
	vmulps	%zmm1, %zmm9, %zmm3
	vmovaps	2496(%rsp), %zmm19              # 64-byte Reload
	vfmadd231ps	%zmm19, %zmm21, %zmm3   # zmm3 = (zmm21 * zmm19) + zmm3
	vextractf64x4	$1, %zmm3, %ymm4
	vaddps	%zmm4, %zmm3, %zmm3
	vextractf128	$1, %ymm3, %xmm4
	vaddps	%xmm4, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm4         # xmm4 = xmm3[1,0]
	vaddps	%xmm4, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm4            # xmm4 = xmm3[1,1,3,3]
	vaddss	%xmm4, %xmm3, %xmm1
	vmovss	%xmm1, 2784(%rsp)               # 4-byte Spill
	vmovaps	3136(%rsp), %zmm0               # 64-byte Reload
	vmovaps	.LCPI0_27(%rip), %zmm1          # zmm1 = [0,1,2,3,4,5,6,7,27,u,u,u,u,u,u,u]
	vpermt2ps	%zmm16, %zmm1, %zmm0
	vmovaps	.LCPI0_28(%rip), %zmm1          # zmm1 = [0,1,2,3,4,5,6,7,8,27,u,u,u,u,u,u]
	vpermt2ps	%zmm7, %zmm1, %zmm0
	vmovaps	.LCPI0_29(%rip), %zmm1          # zmm1 = [0,1,2,3,4,5,6,7,8,9,27,u,u,u,u,u]
	vpermt2ps	%zmm14, %zmm1, %zmm0
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	%zmm6, %zmm0 {%k3}
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vmovaps	.LCPI0_136(%rip), %zmm1         # zmm1 = [0,1,2,3,4,5,6,7,8,9,10,11,27,u,u,u]
	vpermt2ps	%zmm13, %zmm1, %zmm0
	vmovaps	.LCPI0_148(%rip), %zmm1         # zmm1 = [0,1,2,3,4,5,6,7,8,9,10,11,12,27,u,u]
	vpermt2ps	%zmm29, %zmm1, %zmm0
	vmovaps	.LCPI0_161(%rip), %zmm1         # zmm1 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,27,u]
	vpermt2ps	%zmm5, %zmm1, %zmm0
	vmovaps	.LCPI0_175(%rip), %zmm1         # zmm1 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,27]
	vpermt2ps	%zmm8, %zmm1, %zmm0
	vmovaps	%zmm0, 3136(%rsp)               # 64-byte Spill
	vmulps	%zmm0, %zmm9, %zmm3
	vfmadd231ps	1856(%rsp), %zmm21, %zmm3 # 64-byte Folded Reload
                                        # zmm3 = (zmm21 * mem) + zmm3
	vextractf64x4	$1, %zmm3, %ymm4
	vaddps	%zmm4, %zmm3, %zmm3
	vextractf128	$1, %ymm3, %xmm4
	vaddps	%xmm4, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm4         # xmm4 = xmm3[1,0]
	vaddps	%xmm4, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm4            # xmm4 = xmm3[1,1,3,3]
	vaddss	%xmm4, %xmm3, %xmm1
	vmovss	%xmm1, 2768(%rsp)               # 4-byte Spill
	vmovaps	2240(%rsp), %zmm0               # 64-byte Reload
	vmovaps	.LCPI0_47(%rip), %zmm1          # zmm1 = [0,1,2,3,4,5,6,7,29,u,u,u,u,u,u,u]
	vpermt2ps	%zmm16, %zmm1, %zmm0
	vmovaps	.LCPI0_48(%rip), %zmm1          # zmm1 = [0,1,2,3,4,5,6,7,8,29,u,u,u,u,u,u]
	vpermt2ps	%zmm7, %zmm1, %zmm0
	vmovaps	.LCPI0_49(%rip), %zmm1          # zmm1 = [0,1,2,3,4,5,6,7,8,9,29,u,u,u,u,u]
	vpermt2ps	%zmm14, %zmm1, %zmm0
	vmovaps	.LCPI0_50(%rip), %zmm1          # zmm1 = [0,1,2,3,4,5,6,7,8,9,10,29,u,u,u,u]
	vpermt2ps	%zmm6, %zmm1, %zmm0
	vpermt2ps	%zmm13, %zmm11, %zmm0
	.loc	1 224 20                        # 03-matrix-multiplication-cpu.py:224:20
	vmovaps	%zmm29, %zmm0 {%k5}
	.loc	1 226 35                        # 03-matrix-multiplication-cpu.py:226:35
	vpermt2ps	%zmm5, %zmm15, %zmm0
	vmovaps	.LCPI0_177(%rip), %zmm1         # zmm1 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,29]
	vpermt2ps	%zmm8, %zmm1, %zmm0
	vmovaps	%zmm0, 2240(%rsp)               # 64-byte Spill
	vmulps	%zmm0, %zmm9, %zmm3
	vfmadd231ps	1088(%rsp), %zmm21, %zmm3 # 64-byte Folded Reload
                                        # zmm3 = (zmm21 * mem) + zmm3
	vextractf64x4	$1, %zmm3, %ymm4
	vaddps	%zmm4, %zmm3, %zmm3
	vextractf128	$1, %ymm3, %xmm4
	vaddps	%xmm4, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm4         # xmm4 = xmm3[1,0]
	vaddps	%xmm4, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm4            # xmm4 = xmm3[1,1,3,3]
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm3, %xmm10, %xmm3
	vmovaps	5520(%rsp), %xmm0               # 16-byte Reload
	vinsertps	$16, %xmm3, %xmm0, %xmm3 # xmm3 = xmm0[0],xmm3[0],xmm0[2,3]
	vinsertps	$32, 400(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, 2704(%rsp), %xmm3, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm3[0,1,2],mem[0]
	vmovaps	%xmm3, 37504(%rsp)              # 16-byte Spill
	vinsertf128	$1, 2752(%rsp), %ymm2, %ymm2 # 16-byte Folded Reload
	vmovaps	%zmm2, 36928(%rsp)              # 64-byte Spill
	vmovaps	18624(%rsp), %ymm0              # 32-byte Reload
	vinsertf128	$1, 18560(%rsp), %ymm0, %ymm0 # 16-byte Folded Reload
	vmovaps	%zmm0, 18560(%rsp)              # 64-byte Spill
	vmovaps	7936(%rsp), %zmm12              # 64-byte Reload
	vmulps	%zmm31, %zmm12, %zmm0
	vmovaps	%zmm31, %zmm7
	vmovaps	%zmm31, 1600(%rsp)              # 64-byte Spill
	vmovaps	6976(%rsp), %zmm13              # 64-byte Reload
	vmovaps	1664(%rsp), %zmm8               # 64-byte Reload
	vfmadd231ps	%zmm8, %zmm13, %zmm0    # zmm0 = (zmm13 * zmm8) + zmm0
	vextractf64x4	$1, %zmm0, %ymm2
	vaddps	%zmm2, %zmm0, %zmm0
	vextractf128	$1, %ymm0, %xmm2
	vaddps	%xmm2, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm2         # xmm2 = xmm0[1,0]
	vaddps	%xmm2, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm2            # xmm2 = xmm0[1,1,3,3]
	vaddss	%xmm2, %xmm0, %xmm0
	vxorps	%xmm2, %xmm2, %xmm2
	vaddss	%xmm2, %xmm0, %xmm0
	vinsertps	$16, 5568(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, 5552(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, 5536(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%ymm0, 17280(%rsp)              # 32-byte Spill
	vmovaps	10304(%rsp), %zmm1              # 64-byte Reload
	vextractf64x4	$1, %zmm1, %ymm0
	vaddps	%zmm0, %zmm1, %zmm0
	vextractf128	$1, %ymm0, %xmm3
	vaddps	%xmm3, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm3         # xmm3 = xmm0[1,0]
	vaddps	%xmm3, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm3            # xmm3 = xmm0[1,1,3,3]
	vaddss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	vmovaps	2736(%rsp), %xmm1               # 16-byte Reload
	vinsertps	$16, %xmm0, %xmm1, %xmm0 # xmm0 = xmm1[0],xmm0[0],xmm1[2,3]
	vinsertps	$32, 5584(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vmovaps	17536(%rsp), %zmm1              # 64-byte Reload
	vextractf64x4	$1, %zmm1, %ymm3
	vaddps	%zmm3, %zmm1, %zmm3
	vextractf128	$1, %ymm3, %xmm4
	vaddps	%xmm4, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm4         # xmm4 = xmm3[1,0]
	vaddps	%xmm4, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm4            # xmm4 = xmm3[1,1,3,3]
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm2, %xmm3, %xmm3
	vxorps	%xmm9, %xmm9, %xmm9
	vinsertps	$48, %xmm3, %xmm0, %xmm2 # xmm2 = xmm0[0,1,2],xmm3[0]
	vmulps	%zmm26, %zmm12, %zmm0
	vfmadd231ps	%zmm27, %zmm13, %zmm0   # zmm0 = (zmm13 * zmm27) + zmm0
	vextractf64x4	$1, %zmm0, %ymm3
	vaddps	%zmm3, %zmm0, %zmm0
	vextractf128	$1, %ymm0, %xmm3
	vaddps	%xmm3, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm3         # xmm3 = xmm0[1,0]
	vaddps	%xmm3, %xmm0, %xmm0
	vmovaps	36608(%rsp), %zmm1              # 64-byte Reload
	vextractf64x4	$1, %zmm1, %ymm3
	vaddps	%zmm3, %zmm1, %zmm3
	vextractf128	$1, %ymm3, %xmm4
	vaddps	%xmm4, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm4         # xmm4 = xmm3[1,0]
	vaddps	%xmm4, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm4            # xmm4 = xmm3[1,1,3,3]
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm3, %xmm9, %xmm3
	vmovaps	%xmm3, 18624(%rsp)              # 16-byte Spill
	vmovshdup	%xmm0, %xmm3            # xmm3 = xmm0[1,1,3,3]
	vaddss	%xmm3, %xmm0, %xmm0
	vmovss	%xmm0, 10304(%rsp)              # 4-byte Spill
	vmulps	%zmm23, %zmm12, %zmm0
	vfmadd231ps	%zmm24, %zmm13, %zmm0   # zmm0 = (zmm13 * zmm24) + zmm0
	vextractf64x4	$1, %zmm0, %ymm3
	vaddps	%zmm3, %zmm0, %zmm0
	vextractf128	$1, %ymm0, %xmm3
	vaddps	%xmm3, %xmm0, %xmm0
	vmovaps	36672(%rsp), %zmm1              # 64-byte Reload
	vextractf64x4	$1, %zmm1, %ymm3
	vaddps	%zmm3, %zmm1, %zmm3
	vextractf128	$1, %ymm3, %xmm5
	vaddps	%xmm5, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm5         # xmm5 = xmm3[1,0]
	vaddps	%xmm5, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm5            # xmm5 = xmm3[1,1,3,3]
	vaddss	%xmm5, %xmm3, %xmm3
	vaddss	%xmm3, %xmm9, %xmm1
	vmovaps	%xmm1, 17536(%rsp)              # 16-byte Spill
	vshufpd	$1, %xmm0, %xmm0, %xmm3         # xmm3 = xmm0[1,0]
	vaddps	%xmm3, %xmm0, %xmm0
	vmulps	8384(%rsp), %zmm12, %zmm3       # 64-byte Folded Reload
	vfmadd231ps	1536(%rsp), %zmm13, %zmm3 # 64-byte Folded Reload
                                        # zmm3 = (zmm13 * mem) + zmm3
	vextractf64x4	$1, %zmm3, %ymm5
	vaddps	%zmm5, %zmm3, %zmm3
	vextractf128	$1, %ymm3, %xmm5
	vaddps	%xmm5, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm5         # xmm5 = xmm3[1,0]
	vaddps	%xmm5, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm5            # xmm5 = xmm3[1,1,3,3]
	vaddss	%xmm5, %xmm3, %xmm3
	vaddss	%xmm3, %xmm9, %xmm3
	vmulps	8320(%rsp), %zmm12, %zmm5       # 64-byte Folded Reload
	vfmadd231ps	8448(%rsp), %zmm13, %zmm5 # 64-byte Folded Reload
                                        # zmm5 = (zmm13 * mem) + zmm5
	vextractf64x4	$1, %zmm5, %ymm6
	vaddps	%zmm6, %zmm5, %zmm5
	vextractf128	$1, %ymm5, %xmm6
	vaddps	%xmm6, %xmm5, %xmm5
	vshufpd	$1, %xmm5, %xmm5, %xmm6         # xmm6 = xmm5[1,0]
	vaddps	%xmm6, %xmm5, %xmm5
	vmovshdup	%xmm5, %xmm6            # xmm6 = xmm5[1,1,3,3]
	vaddss	%xmm6, %xmm5, %xmm5
	vaddss	%xmm5, %xmm9, %xmm5
	vmovshdup	%xmm0, %xmm6            # xmm6 = xmm0[1,1,3,3]
	vaddss	%xmm6, %xmm0, %xmm0
	vmovss	%xmm0, 2752(%rsp)               # 4-byte Spill
	vmulps	1280(%rsp), %zmm12, %zmm0       # 64-byte Folded Reload
	vfmadd231ps	1216(%rsp), %zmm13, %zmm0 # 64-byte Folded Reload
                                        # zmm0 = (zmm13 * mem) + zmm0
	vextractf64x4	$1, %zmm0, %ymm6
	vaddps	%zmm6, %zmm0, %zmm0
	vextractf128	$1, %ymm0, %xmm6
	vaddps	%xmm6, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm6         # xmm6 = xmm0[1,0]
	vaddps	%xmm6, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm6            # xmm6 = xmm0[1,1,3,3]
	vaddss	%xmm6, %xmm0, %xmm0
	vmovaps	36864(%rsp), %zmm9              # 64-byte Reload
	vextractf64x4	$1, %zmm9, %ymm6
	vaddps	%zmm6, %zmm9, %zmm6
	vextractf128	$1, %ymm6, %xmm10
	vaddps	%xmm6, %xmm10, %xmm6
	vshufpd	$1, %xmm6, %xmm6, %xmm10        # xmm10 = xmm6[1,0]
	vaddps	%xmm6, %xmm10, %xmm6
	vxorps	%xmm1, %xmm1, %xmm1
	vaddss	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm6, %xmm10           # xmm10 = xmm6[1,1,3,3]
	vaddss	%xmm6, %xmm10, %xmm6
	vmulps	%zmm25, %zmm12, %zmm11
	vfmadd231ps	%zmm28, %zmm13, %zmm11  # zmm11 = (zmm13 * zmm28) + zmm11
	vaddss	%xmm1, %xmm6, %xmm10
	vxorps	%xmm4, %xmm4, %xmm4
	vextractf64x4	$1, %zmm11, %ymm6
	vaddps	%zmm6, %zmm11, %zmm6
	vextractf128	$1, %ymm6, %xmm11
	vaddps	%xmm6, %xmm11, %xmm6
	vmulps	8896(%rsp), %zmm12, %zmm11      # 64-byte Folded Reload
	vmovaps	%zmm12, %zmm9
	vfmadd231ps	8960(%rsp), %zmm13, %zmm11 # 64-byte Folded Reload
                                        # zmm11 = (zmm13 * mem) + zmm11
	vextractf64x4	$1, %zmm11, %ymm12
	vaddps	%zmm12, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm12
	vaddps	%xmm12, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm12      # xmm12 = xmm11[1,0]
	vaddps	%xmm12, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm12          # xmm12 = xmm11[1,1,3,3]
	vaddss	%xmm12, %xmm11, %xmm11
	vaddss	%xmm4, %xmm11, %xmm11
	vinsertps	$16, %xmm11, %xmm0, %xmm0 # xmm0 = xmm0[0],xmm11[0],xmm0[2,3]
	vinsertps	$32, %xmm5, %xmm0, %xmm0 # xmm0 = xmm0[0,1],xmm5[0],xmm0[3]
	vinsertps	$48, %xmm3, %xmm0, %xmm11 # xmm11 = xmm0[0,1,2],xmm3[0]
	vshufpd	$1, %xmm6, %xmm6, %xmm0         # xmm0 = xmm6[1,0]
	vaddps	%xmm0, %xmm6, %xmm0
	vmovaps	2432(%rsp), %zmm31              # 64-byte Reload
	vmulps	%zmm31, %zmm9, %zmm3
	vfmadd231ps	1792(%rsp), %zmm13, %zmm3 # 64-byte Folded Reload
                                        # zmm3 = (zmm13 * mem) + zmm3
	vextractf64x4	$1, %zmm3, %ymm5
	vaddps	%zmm5, %zmm3, %zmm3
	vextractf128	$1, %ymm3, %xmm5
	vaddps	%xmm5, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm5         # xmm5 = xmm3[1,0]
	vaddps	%xmm5, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm5            # xmm5 = xmm3[1,1,3,3]
	vaddss	%xmm5, %xmm3, %xmm3
	vaddss	%xmm4, %xmm3, %xmm12
	vmovshdup	%xmm0, %xmm3            # xmm3 = xmm0[1,1,3,3]
	vaddss	%xmm3, %xmm0, %xmm0
	vmovss	%xmm0, 400(%rsp)                # 4-byte Spill
	vmulps	128(%rsp), %zmm9, %zmm0         # 64-byte Folded Reload
	vmovaps	896(%rsp), %zmm26               # 64-byte Reload
	vfmadd231ps	%zmm26, %zmm13, %zmm0   # zmm0 = (zmm13 * zmm26) + zmm0
	vmovaps	%zmm13, %zmm16
	vextractf64x4	$1, %zmm0, %ymm3
	vaddps	%zmm3, %zmm0, %zmm0
	vextractf128	$1, %ymm0, %xmm3
	vaddps	%xmm3, %xmm0, %xmm0
	vmovaps	36416(%rsp), %zmm1              # 64-byte Reload
	vextractf64x4	$1, %zmm1, %ymm3
	vaddps	%zmm3, %zmm1, %zmm3
	vextractf128	$1, %ymm3, %xmm6
	vaddps	%xmm6, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm6         # xmm6 = xmm3[1,0]
	vaddps	%xmm6, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm6            # xmm6 = xmm3[1,1,3,3]
	vaddss	%xmm6, %xmm3, %xmm3
	vaddss	%xmm4, %xmm3, %xmm3
	vshufpd	$1, %xmm0, %xmm0, %xmm6         # xmm6 = xmm0[1,0]
	vaddps	%xmm6, %xmm0, %xmm0
	vmulps	2176(%rsp), %zmm9, %zmm6        # 64-byte Folded Reload
	vmovaps	1728(%rsp), %zmm29              # 64-byte Reload
	vfmadd231ps	%zmm29, %zmm13, %zmm6   # zmm6 = (zmm13 * zmm29) + zmm6
	vextractf64x4	$1, %zmm6, %ymm13
	vaddps	%zmm13, %zmm6, %zmm6
	vextractf128	$1, %ymm6, %xmm13
	vaddps	%xmm6, %xmm13, %xmm6
	vshufpd	$1, %xmm6, %xmm6, %xmm13        # xmm13 = xmm6[1,0]
	vaddps	%xmm6, %xmm13, %xmm6
	vmovshdup	%xmm6, %xmm13           # xmm13 = xmm6[1,1,3,3]
	vaddss	%xmm6, %xmm13, %xmm6
	vaddss	%xmm4, %xmm6, %xmm13
	vmovshdup	%xmm0, %xmm6            # xmm6 = xmm0[1,1,3,3]
	vaddss	%xmm6, %xmm0, %xmm0
	vmulps	%zmm20, %zmm9, %zmm6
	vmovaps	2560(%rsp), %zmm5               # 64-byte Reload
	vfmadd231ps	%zmm5, %zmm16, %zmm6    # zmm6 = (zmm16 * zmm5) + zmm6
	vextractf64x4	$1, %zmm6, %ymm15
	vaddps	%zmm15, %zmm6, %zmm6
	vextractf128	$1, %ymm6, %xmm15
	vaddps	%xmm6, %xmm15, %xmm6
	vshufpd	$1, %xmm6, %xmm6, %xmm15        # xmm15 = xmm6[1,0]
	vaddps	%xmm6, %xmm15, %xmm6
	vmovshdup	%xmm6, %xmm15           # xmm15 = xmm6[1,1,3,3]
	vaddss	%xmm6, %xmm15, %xmm6
	vxorps	%xmm1, %xmm1, %xmm1
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm1, %xmm6, %xmm6
	vinsertps	$16, %xmm6, %xmm0, %xmm0 # xmm0 = xmm0[0],xmm6[0],xmm0[2,3]
	vmovaps	1984(%rsp), %zmm14              # 64-byte Reload
	vmulps	%zmm14, %zmm9, %zmm6
	vmovaps	1024(%rsp), %zmm1               # 64-byte Reload
	vfmadd231ps	%zmm1, %zmm16, %zmm6    # zmm6 = (zmm16 * zmm1) + zmm6
	vextractf64x4	$1, %zmm6, %ymm15
	vaddps	%zmm15, %zmm6, %zmm6
	vextractf128	$1, %ymm6, %xmm15
	vaddps	%xmm6, %xmm15, %xmm6
	vshufpd	$1, %xmm6, %xmm6, %xmm15        # xmm15 = xmm6[1,0]
	vaddps	%xmm6, %xmm15, %xmm6
	vmovshdup	%xmm6, %xmm15           # xmm15 = xmm6[1,1,3,3]
	vaddss	%xmm6, %xmm15, %xmm6
	vaddss	%xmm4, %xmm6, %xmm6
	vinsertps	$32, %xmm6, %xmm0, %xmm0 # xmm0 = xmm0[0,1],xmm6[0],xmm0[3]
	vmovaps	832(%rsp), %zmm25               # 64-byte Reload
	vmulps	%zmm25, %zmm9, %zmm6
	vmovaps	960(%rsp), %zmm21               # 64-byte Reload
	vfmadd231ps	%zmm21, %zmm16, %zmm6   # zmm6 = (zmm16 * zmm21) + zmm6
	vextractf64x4	$1, %zmm6, %ymm15
	vaddps	%zmm15, %zmm6, %zmm6
	vextractf128	$1, %ymm6, %xmm15
	vaddps	%xmm6, %xmm15, %xmm6
	vshufpd	$1, %xmm6, %xmm6, %xmm15        # xmm15 = xmm6[1,0]
	vaddps	%xmm6, %xmm15, %xmm6
	vmovshdup	%xmm6, %xmm15           # xmm15 = xmm6[1,1,3,3]
	vaddss	%xmm6, %xmm15, %xmm6
	vaddss	%xmm4, %xmm6, %xmm6
	vinsertps	$48, %xmm6, %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],xmm6[0]
	vmovaps	%xmm0, 36864(%rsp)              # 16-byte Spill
	vmovaps	%zmm30, 640(%rsp)               # 64-byte Spill
	vmulps	%zmm30, %zmm9, %zmm0
	vfmadd231ps	%zmm18, %zmm16, %zmm0   # zmm0 = (zmm16 * zmm18) + zmm0
	vextractf64x4	$1, %zmm0, %ymm6
	vaddps	%zmm6, %zmm0, %zmm0
	vextractf128	$1, %ymm0, %xmm6
	vaddps	%xmm6, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm6         # xmm6 = xmm0[1,0]
	vaddps	%xmm6, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm6            # xmm6 = xmm0[1,1,3,3]
	vaddss	%xmm6, %xmm0, %xmm0
	vaddss	%xmm4, %xmm0, %xmm0
	vinsertps	$16, %xmm3, %xmm0, %xmm0 # xmm0 = xmm0[0],xmm3[0],xmm0[2,3]
	vmovaps	6656(%rsp), %zmm6               # 64-byte Reload
	vextractf64x4	$1, %zmm6, %ymm3
	vaddps	%zmm3, %zmm6, %zmm3
	vextractf128	$1, %ymm3, %xmm6
	vaddps	%xmm6, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm6         # xmm6 = xmm3[1,0]
	vaddps	%xmm6, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm6            # xmm6 = xmm3[1,1,3,3]
	vaddss	%xmm6, %xmm3, %xmm3
	vaddss	%xmm4, %xmm3, %xmm3
	vinsertps	$32, %xmm3, %xmm0, %xmm0 # xmm0 = xmm0[0,1],xmm3[0],xmm0[3]
	vmulps	11136(%rsp), %zmm9, %zmm3       # 64-byte Folded Reload
	vfmadd231ps	9024(%rsp), %zmm16, %zmm3 # 64-byte Folded Reload
                                        # zmm3 = (zmm16 * mem) + zmm3
	vextractf64x4	$1, %zmm3, %ymm6
	vaddps	%zmm6, %zmm3, %zmm3
	vextractf128	$1, %ymm3, %xmm6
	vaddps	%xmm6, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm6         # xmm6 = xmm3[1,0]
	vaddps	%xmm6, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm6            # xmm6 = xmm3[1,1,3,3]
	vaddss	%xmm6, %xmm3, %xmm3
	vaddss	%xmm4, %xmm3, %xmm3
	vinsertps	$48, %xmm3, %xmm0, %xmm15 # xmm15 = xmm0[0,1,2],xmm3[0]
	vmulps	%zmm22, %zmm9, %zmm0
	vfmadd231ps	%zmm17, %zmm16, %zmm0   # zmm0 = (zmm16 * zmm17) + zmm0
	vextractf64x4	$1, %zmm0, %ymm3
	vaddps	%zmm3, %zmm0, %zmm0
	vextractf128	$1, %ymm0, %xmm3
	vaddps	%xmm3, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm3         # xmm3 = xmm0[1,0]
	vaddps	%xmm3, %xmm0, %xmm0
	vmovaps	10240(%rsp), %zmm6              # 64-byte Reload
	vextractf64x4	$1, %zmm6, %ymm3
	vaddps	%zmm3, %zmm6, %zmm3
	vextractf128	$1, %ymm3, %xmm6
	vaddps	%xmm6, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm6         # xmm6 = xmm3[1,0]
	vaddps	%xmm6, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm6            # xmm6 = xmm3[1,1,3,3]
	vaddss	%xmm6, %xmm3, %xmm3
	vaddss	%xmm4, %xmm3, %xmm6
	vmovshdup	%xmm0, %xmm3            # xmm3 = xmm0[1,1,3,3]
	vaddss	%xmm3, %xmm0, %xmm0
	vmovss	%xmm0, 10240(%rsp)              # 4-byte Spill
	vmulps	2112(%rsp), %zmm9, %zmm3        # 64-byte Folded Reload
	vfmadd231ps	%zmm19, %zmm16, %zmm3   # zmm3 = (zmm16 * zmm19) + zmm3
	vmovaps	%zmm16, %zmm18
	vextractf64x4	$1, %zmm3, %ymm16
	vaddps	%zmm16, %zmm3, %zmm3
	vextractf32x4	$1, %ymm3, %xmm16
	vaddps	%xmm16, %xmm3, %xmm3
	vmovaps	36544(%rsp), %zmm0              # 64-byte Reload
	vextractf64x4	$1, %zmm0, %ymm16
	vaddps	%zmm16, %zmm0, %zmm16
	vextractf32x4	$1, %ymm16, %xmm17
	vaddps	%xmm17, %xmm16, %xmm16
	vshufpd	$1, %xmm16, %xmm16, %xmm17      # xmm17 = xmm16[1,0]
	vaddps	%xmm17, %xmm16, %xmm16
	vmovshdup	%xmm16, %xmm17          # xmm17 = xmm16[1,1,3,3]
	vaddss	%xmm17, %xmm16, %xmm16
	vaddss	%xmm4, %xmm16, %xmm16
	vshufpd	$1, %xmm3, %xmm3, %xmm17        # xmm17 = xmm3[1,0]
	vaddps	%xmm17, %xmm3, %xmm3
	vmovaps	8640(%rsp), %zmm0               # 64-byte Reload
	vmulps	1152(%rsp), %zmm0, %zmm17       # 64-byte Folded Reload
	vmovaps	8000(%rsp), %zmm20              # 64-byte Reload
	vfmadd231ps	768(%rsp), %zmm20, %zmm17 # 64-byte Folded Reload
                                        # zmm17 = (zmm20 * mem) + zmm17
	vextractf64x4	$1, %zmm17, %ymm19
	vaddps	%zmm19, %zmm17, %zmm17
	vextractf32x4	$1, %ymm17, %xmm19
	vaddps	%xmm19, %xmm17, %xmm17
	vshufpd	$1, %xmm17, %xmm17, %xmm19      # xmm19 = xmm17[1,0]
	vaddps	%xmm19, %xmm17, %xmm17
	vmovshdup	%xmm17, %xmm19          # xmm19 = xmm17[1,1,3,3]
	vaddss	%xmm19, %xmm17, %xmm17
	vaddss	%xmm4, %xmm17, %xmm17
	vmovaps	192(%rsp), %zmm27               # 64-byte Reload
	vmulps	%zmm27, %zmm9, %zmm19
	vmovaps	1344(%rsp), %zmm28              # 64-byte Reload
	vfmadd231ps	%zmm28, %zmm18, %zmm19  # zmm19 = (zmm18 * zmm28) + zmm19
	vextractf64x4	$1, %zmm19, %ymm22
	vaddps	%zmm22, %zmm19, %zmm19
	vextractf32x4	$1, %ymm19, %xmm22
	vaddps	%xmm22, %xmm19, %xmm19
	vshufpd	$1, %xmm19, %xmm19, %xmm22      # xmm22 = xmm19[1,0]
	vaddps	%xmm22, %xmm19, %xmm19
	vmovshdup	%xmm19, %xmm22          # xmm22 = xmm19[1,1,3,3]
	vaddss	%xmm22, %xmm19, %xmm19
	vaddss	%xmm4, %xmm19, %xmm19
	vmovshdup	%xmm3, %xmm22           # xmm22 = xmm3[1,1,3,3]
	vaddss	%xmm22, %xmm3, %xmm3
	vmovss	%xmm3, 6656(%rsp)               # 4-byte Spill
	vmovaps	3136(%rsp), %zmm30              # 64-byte Reload
	vmulps	%zmm30, %zmm9, %zmm22
	vmovaps	1856(%rsp), %zmm3               # 64-byte Reload
	vfmadd231ps	%zmm3, %zmm18, %zmm22   # zmm22 = (zmm18 * zmm3) + zmm22
	vextractf64x4	$1, %zmm22, %ymm23
	vaddps	%zmm23, %zmm22, %zmm22
	vmovaps	36736(%rsp), %zmm24             # 64-byte Reload
	vextractf64x4	$1, %zmm24, %ymm23
	vaddps	%zmm23, %zmm24, %zmm23
	vextractf32x4	$1, %ymm23, %xmm24
	vaddps	%xmm24, %xmm23, %xmm23
	vshufpd	$1, %xmm23, %xmm23, %xmm24      # xmm24 = xmm23[1,0]
	vaddps	%xmm24, %xmm23, %xmm23
	vmovshdup	%xmm23, %xmm24          # xmm24 = xmm23[1,1,3,3]
	vaddss	%xmm24, %xmm23, %xmm23
	vaddss	%xmm4, %xmm23, %xmm23
	vinsertps	$16, %xmm23, %xmm17, %xmm17 # xmm17 = xmm17[0],xmm23[0],xmm17[2,3]
	vinsertps	$32, %xmm16, %xmm17, %xmm16 # xmm16 = xmm17[0,1],xmm16[0],xmm17[3]
	vinsertps	$48, %xmm6, %xmm16, %xmm16 # xmm16 = xmm16[0,1,2],xmm6[0]
	vextractf32x4	$1, %ymm22, %xmm6
	vaddps	%xmm6, %xmm22, %xmm6
	vshufpd	$1, %xmm6, %xmm6, %xmm17        # xmm17 = xmm6[1,0]
	vaddps	%xmm17, %xmm6, %xmm6
	vmovshdup	%xmm6, %xmm17           # xmm17 = xmm6[1,1,3,3]
	vaddss	%xmm17, %xmm6, %xmm6
	vmovaps	2240(%rsp), %zmm24              # 64-byte Reload
	vmulps	%zmm24, %zmm9, %zmm17
	vmovaps	1088(%rsp), %zmm23              # 64-byte Reload
	vfmadd231ps	%zmm23, %zmm18, %zmm17  # zmm17 = (zmm18 * zmm23) + zmm17
	vextractf64x4	$1, %zmm17, %ymm22
	vaddps	%zmm22, %zmm17, %zmm17
	vextractf32x4	$1, %ymm17, %xmm22
	vaddps	%xmm22, %xmm17, %xmm17
	vshufpd	$1, %xmm17, %xmm17, %xmm22      # xmm22 = xmm17[1,0]
	vaddps	%xmm22, %xmm17, %xmm17
	vmovshdup	%xmm17, %xmm22          # xmm22 = xmm17[1,1,3,3]
	vaddss	%xmm22, %xmm17, %xmm17
	vaddss	%xmm4, %xmm17, %xmm17
	vinsertps	$16, %xmm17, %xmm19, %xmm17 # xmm17 = xmm19[0],xmm17[0],xmm19[2,3]
	vinsertps	$32, %xmm13, %xmm17, %xmm13 # xmm13 = xmm17[0,1],xmm13[0],xmm17[3]
	vinsertps	$48, %xmm12, %xmm13, %xmm12 # xmm12 = xmm13[0,1,2],xmm12[0]
	vmovaps	%xmm12, 36672(%rsp)             # 16-byte Spill
	vinsertf128	$1, %xmm11, %ymm15, %ymm11
	vmovaps	%zmm11, 36608(%rsp)             # 64-byte Spill
	vmovaps	17280(%rsp), %ymm9              # 32-byte Reload
	vinsertf128	$1, %xmm2, %ymm9, %ymm2
	vmovaps	%zmm2, 36736(%rsp)              # 64-byte Spill
	vmovaps	%zmm0, %zmm9
	vmulps	%zmm7, %zmm0, %zmm2
	vfmadd231ps	%zmm8, %zmm20, %zmm2    # zmm2 = (zmm20 * zmm8) + zmm2
	vextractf64x4	$1, %zmm2, %ymm7
	vaddps	%zmm7, %zmm2, %zmm2
	vextractf128	$1, %ymm2, %xmm7
	vaddps	%xmm7, %xmm2, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm7         # xmm7 = xmm2[1,0]
	vaddps	%xmm7, %xmm2, %xmm2
	vmovshdup	%xmm2, %xmm7            # xmm7 = xmm2[1,1,3,3]
	vaddss	%xmm7, %xmm2, %xmm2
	vaddss	%xmm4, %xmm2, %xmm2
	vinsertps	$16, %xmm10, %xmm2, %xmm2 # xmm2 = xmm2[0],xmm10[0],xmm2[2,3]
	vinsertps	$32, 17536(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vmovaps	18944(%rsp), %zmm0              # 64-byte Reload
	vextractf64x4	$1, %zmm0, %ymm7
	vaddps	%zmm7, %zmm0, %zmm7
	vextractf128	$1, %ymm7, %xmm8
	vaddps	%xmm7, %xmm8, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm8         # xmm8 = xmm7[1,0]
	vaddps	%xmm7, %xmm8, %xmm7
	vmovshdup	%xmm7, %xmm8            # xmm8 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm4, %xmm7, %xmm7
	vinsertps	$48, %xmm7, %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],xmm7[0]
	vmulps	448(%rsp), %zmm9, %zmm7         # 64-byte Folded Reload
	vfmadd231ps	512(%rsp), %zmm20, %zmm7 # 64-byte Folded Reload
                                        # zmm7 = (zmm20 * mem) + zmm7
	vextractf64x4	$1, %zmm7, %ymm8
	vaddps	%zmm8, %zmm7, %zmm7
	vextractf128	$1, %ymm7, %xmm8
	vaddps	%xmm7, %xmm8, %xmm7
	vmovaps	36800(%rsp), %zmm10             # 64-byte Reload
	vextractf64x4	$1, %zmm10, %ymm8
	vaddps	%zmm8, %zmm10, %zmm8
	vextractf128	$1, %ymm8, %xmm10
	vaddps	%xmm10, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm10        # xmm10 = xmm8[1,0]
	vaddps	%xmm10, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm10           # xmm10 = xmm8[1,1,3,3]
	vaddss	%xmm10, %xmm8, %xmm8
	vaddss	%xmm4, %xmm8, %xmm8
	vmovaps	%xmm8, 36800(%rsp)              # 16-byte Spill
	vshufpd	$1, %xmm7, %xmm7, %xmm8         # xmm8 = xmm7[1,0]
	vaddps	%xmm7, %xmm8, %xmm7
	vmovshdup	%xmm7, %xmm8            # xmm8 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm8, %xmm7
	vmulps	128(%rsp), %zmm9, %zmm8         # 64-byte Folded Reload
	vfmadd231ps	%zmm26, %zmm20, %zmm8   # zmm8 = (zmm20 * zmm26) + zmm8
	vextractf64x4	$1, %zmm8, %ymm10
	vaddps	%zmm10, %zmm8, %zmm8
	vextractf128	$1, %ymm8, %xmm10
	vaddps	%xmm10, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm10        # xmm10 = xmm8[1,0]
	vaddps	%xmm10, %xmm8, %xmm8
	vinsertf32x4	$1, %xmm16, %ymm2, %ymm2
	vmovaps	%zmm2, 36544(%rsp)              # 64-byte Spill
	vmovshdup	%xmm8, %xmm2            # xmm2 = xmm8[1,1,3,3]
	vaddss	%xmm2, %xmm8, %xmm2
	vmulps	2368(%rsp), %zmm9, %zmm8        # 64-byte Folded Reload
	vfmadd231ps	%zmm5, %zmm20, %zmm8    # zmm8 = (zmm20 * zmm5) + zmm8
	vextractf64x4	$1, %zmm8, %ymm10
	vaddps	%zmm10, %zmm8, %zmm8
	vextractf128	$1, %ymm8, %xmm10
	vaddps	%xmm10, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm10        # xmm10 = xmm8[1,0]
	vaddps	%xmm10, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm10           # xmm10 = xmm8[1,1,3,3]
	vaddss	%xmm10, %xmm8, %xmm8
	vaddss	%xmm4, %xmm2, %xmm2
	vaddss	%xmm4, %xmm8, %xmm8
	vinsertps	$16, %xmm8, %xmm2, %xmm2 # xmm2 = xmm2[0],xmm8[0],xmm2[2,3]
	vmulps	%zmm14, %zmm9, %zmm8
	vfmadd231ps	%zmm1, %zmm20, %zmm8    # zmm8 = (zmm20 * zmm1) + zmm8
	vextractf64x4	$1, %zmm8, %ymm10
	vaddps	%zmm10, %zmm8, %zmm8
	vextractf128	$1, %ymm8, %xmm10
	vaddps	%xmm10, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm10        # xmm10 = xmm8[1,0]
	vaddps	%xmm10, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm10           # xmm10 = xmm8[1,1,3,3]
	vaddss	%xmm10, %xmm8, %xmm8
	vaddss	%xmm4, %xmm8, %xmm8
	vinsertps	$32, %xmm8, %xmm2, %xmm2 # xmm2 = xmm2[0,1],xmm8[0],xmm2[3]
	vmulps	%zmm25, %zmm9, %zmm8
	vfmadd231ps	%zmm21, %zmm20, %zmm8   # zmm8 = (zmm20 * zmm21) + zmm8
	vextractf64x4	$1, %zmm8, %ymm10
	vaddps	%zmm10, %zmm8, %zmm8
	vextractf128	$1, %ymm8, %xmm10
	vaddps	%xmm10, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm10        # xmm10 = xmm8[1,0]
	vaddps	%xmm10, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm10           # xmm10 = xmm8[1,1,3,3]
	vaddss	%xmm10, %xmm8, %xmm8
	vaddss	%xmm4, %xmm8, %xmm8
	vinsertps	$48, %xmm8, %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],xmm8[0]
	vmovaps	%xmm2, 17536(%rsp)              # 16-byte Spill
	vmulps	2048(%rsp), %zmm9, %zmm2        # 64-byte Folded Reload
	vfmadd231ps	704(%rsp), %zmm20, %zmm2 # 64-byte Folded Reload
                                        # zmm2 = (zmm20 * mem) + zmm2
	vextractf64x4	$1, %zmm2, %ymm8
	vaddps	%zmm8, %zmm2, %zmm8
	vmulps	%zmm31, %zmm9, %zmm2
	vmovaps	%zmm9, %zmm18
	vfmadd231ps	1792(%rsp), %zmm20, %zmm2 # 64-byte Folded Reload
                                        # zmm2 = (zmm20 * mem) + zmm2
	vextractf64x4	$1, %zmm2, %ymm10
	vaddps	%zmm10, %zmm2, %zmm2
	vextractf128	$1, %ymm2, %xmm10
	vaddps	%xmm2, %xmm10, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm10        # xmm10 = xmm2[1,0]
	vaddps	%xmm2, %xmm10, %xmm2
	vmovshdup	%xmm2, %xmm10           # xmm10 = xmm2[1,1,3,3]
	vaddss	%xmm2, %xmm10, %xmm2
	vaddss	%xmm4, %xmm2, %xmm2
	vmulps	2176(%rsp), %zmm9, %zmm10       # 64-byte Folded Reload
	vfmadd231ps	%zmm29, %zmm20, %zmm10  # zmm10 = (zmm20 * zmm29) + zmm10
	vextractf64x4	$1, %zmm10, %ymm11
	vaddps	%zmm11, %zmm10, %zmm10
	vextractf128	$1, %ymm10, %xmm11
	vaddps	%xmm11, %xmm10, %xmm10
	vshufpd	$1, %xmm10, %xmm10, %xmm11      # xmm11 = xmm10[1,0]
	vaddps	%xmm11, %xmm10, %xmm10
	vmovshdup	%xmm10, %xmm11          # xmm11 = xmm10[1,1,3,3]
	vaddss	%xmm11, %xmm10, %xmm10
	vaddss	%xmm4, %xmm10, %xmm10
	vextractf128	$1, %ymm8, %xmm11
	vaddps	%xmm11, %xmm8, %xmm8
	vmulps	8384(%rsp), %zmm9, %zmm11       # 64-byte Folded Reload
	vfmadd231ps	1536(%rsp), %zmm20, %zmm11 # 64-byte Folded Reload
                                        # zmm11 = (zmm20 * mem) + zmm11
	vextractf64x4	$1, %zmm11, %ymm12
	vaddps	%zmm12, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm12
	vaddps	%xmm12, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm12      # xmm12 = xmm11[1,0]
	vaddps	%xmm12, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm12          # xmm12 = xmm11[1,1,3,3]
	vaddss	%xmm12, %xmm11, %xmm11
	vaddss	%xmm4, %xmm11, %xmm12
	vmulps	8320(%rsp), %zmm9, %zmm11       # 64-byte Folded Reload
	vfmadd231ps	8448(%rsp), %zmm20, %zmm11 # 64-byte Folded Reload
                                        # zmm11 = (zmm20 * mem) + zmm11
	vextractf64x4	$1, %zmm11, %ymm13
	vaddps	%zmm13, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm13
	vaddps	%xmm13, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm13      # xmm13 = xmm11[1,0]
	vaddps	%xmm13, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm13          # xmm13 = xmm11[1,1,3,3]
	vaddss	%xmm13, %xmm11, %xmm11
	vaddss	%xmm4, %xmm11, %xmm13
	vshufpd	$1, %xmm8, %xmm8, %xmm11        # xmm11 = xmm8[1,0]
	vaddps	%xmm11, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm11           # xmm11 = xmm8[1,1,3,3]
	vaddss	%xmm11, %xmm8, %xmm8
	vmulps	256(%rsp), %zmm9, %zmm11        # 64-byte Folded Reload
	vfmadd231ps	1408(%rsp), %zmm20, %zmm11 # 64-byte Folded Reload
                                        # zmm11 = (zmm20 * mem) + zmm11
	vextractf64x4	$1, %zmm11, %ymm15
	vaddps	%zmm15, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm15
	vaddps	%xmm15, %xmm11, %xmm11
	vmovaps	36480(%rsp), %zmm0              # 64-byte Reload
	vextractf64x4	$1, %zmm0, %ymm15
	vaddps	%zmm15, %zmm0, %zmm15
	vextractf32x4	$1, %ymm15, %xmm16
	vaddps	%xmm16, %xmm15, %xmm15
	vshufpd	$1, %xmm15, %xmm15, %xmm16      # xmm16 = xmm15[1,0]
	vaddps	%xmm16, %xmm15, %xmm15
	vmovshdup	%xmm15, %xmm16          # xmm16 = xmm15[1,1,3,3]
	vaddss	%xmm16, %xmm15, %xmm15
	vaddss	%xmm4, %xmm15, %xmm15
	vmulps	1280(%rsp), %zmm9, %zmm16       # 64-byte Folded Reload
	vfmadd231ps	1216(%rsp), %zmm20, %zmm16 # 64-byte Folded Reload
                                        # zmm16 = (zmm20 * mem) + zmm16
	vextractf64x4	$1, %zmm16, %ymm17
	vaddps	%zmm17, %zmm16, %zmm16
	vextractf32x4	$1, %ymm16, %xmm17
	vaddps	%xmm17, %xmm16, %xmm16
	vshufpd	$1, %xmm16, %xmm16, %xmm17      # xmm17 = xmm16[1,0]
	vaddps	%xmm17, %xmm16, %xmm16
	vmovshdup	%xmm16, %xmm17          # xmm17 = xmm16[1,1,3,3]
	vaddss	%xmm17, %xmm16, %xmm16
	vaddss	%xmm4, %xmm16, %xmm16
	vshufpd	$1, %xmm11, %xmm11, %xmm17      # xmm17 = xmm11[1,0]
	vaddps	%xmm17, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm17          # xmm17 = xmm11[1,1,3,3]
	vaddss	%xmm17, %xmm11, %xmm11
	vmulps	640(%rsp), %zmm9, %zmm17        # 64-byte Folded Reload
	vfmadd231ps	64(%rsp), %zmm20, %zmm17 # 64-byte Folded Reload
                                        # zmm17 = (zmm20 * mem) + zmm17
	vextractf64x4	$1, %zmm17, %ymm19
	vaddps	%zmm19, %zmm17, %zmm17
	vextractf32x4	$1, %ymm17, %xmm19
	vaddps	%xmm19, %xmm17, %xmm17
	vshufpd	$1, %xmm17, %xmm17, %xmm19      # xmm19 = xmm17[1,0]
	vaddps	%xmm19, %xmm17, %xmm17
	vmovshdup	%xmm17, %xmm19          # xmm19 = xmm17[1,1,3,3]
	vaddss	%xmm19, %xmm17, %xmm17
	vaddss	%xmm4, %xmm17, %xmm17
	vinsertps	$16, %xmm15, %xmm17, %xmm15 # xmm15 = xmm17[0],xmm15[0],xmm17[2,3]
	vmovaps	17152(%rsp), %zmm0              # 64-byte Reload
	vextractf64x4	$1, %zmm0, %ymm17
	vaddps	%zmm17, %zmm0, %zmm9
	vextractf32x4	$1, %ymm9, %xmm17
	vaddps	%xmm17, %xmm9, %xmm9
	vshufpd	$1, %xmm9, %xmm9, %xmm17        # xmm17 = xmm9[1,0]
	vaddps	%xmm17, %xmm9, %xmm9
	vmovshdup	%xmm9, %xmm17           # xmm17 = xmm9[1,1,3,3]
	vaddss	%xmm17, %xmm9, %xmm9
	vaddss	%xmm4, %xmm9, %xmm9
	vinsertps	$32, %xmm9, %xmm15, %xmm9 # xmm9 = xmm15[0,1],xmm9[0],xmm15[3]
	vmovaps	11136(%rsp), %zmm5              # 64-byte Reload
	vmulps	%zmm5, %zmm18, %zmm15
	vfmadd231ps	9024(%rsp), %zmm20, %zmm15 # 64-byte Folded Reload
                                        # zmm15 = (zmm20 * mem) + zmm15
	vextractf64x4	$1, %zmm15, %ymm17
	vaddps	%zmm17, %zmm15, %zmm15
	vextractf32x4	$1, %ymm15, %xmm17
	vaddps	%xmm17, %xmm15, %xmm15
	vshufpd	$1, %xmm15, %xmm15, %xmm17      # xmm17 = xmm15[1,0]
	vaddps	%xmm17, %xmm15, %xmm15
	vmovshdup	%xmm15, %xmm17          # xmm17 = xmm15[1,1,3,3]
	vaddss	%xmm17, %xmm15, %xmm15
	vaddss	%xmm4, %xmm15, %xmm15
	vinsertps	$48, %xmm15, %xmm9, %xmm9 # xmm9 = xmm9[0,1,2],xmm15[0]
	vmulps	%zmm27, %zmm18, %zmm15
	vfmadd231ps	%zmm28, %zmm20, %zmm15  # zmm15 = (zmm20 * zmm28) + zmm15
	vextractf64x4	$1, %zmm15, %ymm17
	vaddps	%zmm17, %zmm15, %zmm15
	vextractf32x4	$1, %ymm15, %xmm17
	vaddps	%xmm17, %xmm15, %xmm15
	vshufpd	$1, %xmm15, %xmm15, %xmm17      # xmm17 = xmm15[1,0]
	vaddps	%xmm17, %xmm15, %xmm15
	vmovshdup	%xmm15, %xmm17          # xmm17 = xmm15[1,1,3,3]
	vaddss	%xmm17, %xmm15, %xmm15
	vaddss	%xmm4, %xmm15, %xmm15
	vmulps	8896(%rsp), %zmm18, %zmm17      # 64-byte Folded Reload
	vfmadd231ps	8960(%rsp), %zmm20, %zmm17 # 64-byte Folded Reload
                                        # zmm17 = (zmm20 * mem) + zmm17
	vextractf64x4	$1, %zmm17, %ymm19
	vaddps	%zmm19, %zmm17, %zmm17
	vextractf32x4	$1, %ymm17, %xmm19
	vaddps	%xmm19, %xmm17, %xmm17
	vshufpd	$1, %xmm17, %xmm17, %xmm19      # xmm19 = xmm17[1,0]
	vaddps	%xmm19, %xmm17, %xmm17
	vmovshdup	%xmm17, %xmm19          # xmm19 = xmm17[1,1,3,3]
	vaddss	%xmm19, %xmm17, %xmm17
	vaddss	%xmm4, %xmm17, %xmm17
	vinsertps	$16, %xmm17, %xmm16, %xmm16 # xmm16 = xmm16[0],xmm17[0],xmm16[2,3]
	vinsertps	$32, %xmm13, %xmm16, %xmm13 # xmm13 = xmm16[0,1],xmm13[0],xmm16[3]
	vinsertps	$48, %xmm12, %xmm13, %xmm12 # xmm12 = xmm13[0,1,2],xmm12[0]
	vinsertf128	$1, %xmm12, %ymm9, %ymm9
	vmovaps	%zmm9, 36480(%rsp)              # 64-byte Spill
	vmulps	3072(%rsp), %zmm18, %zmm9       # 64-byte Folded Reload
	vfmadd231ps	1472(%rsp), %zmm20, %zmm9 # 64-byte Folded Reload
                                        # zmm9 = (zmm20 * mem) + zmm9
	vextractf64x4	$1, %zmm9, %ymm12
	vaddps	%zmm12, %zmm9, %zmm9
	vextractf128	$1, %ymm9, %xmm12
	vaddps	%xmm12, %xmm9, %xmm9
	vmulps	%zmm24, %zmm18, %zmm12
	vfmadd231ps	%zmm23, %zmm20, %zmm12  # zmm12 = (zmm20 * zmm23) + zmm12
	vextractf64x4	$1, %zmm12, %ymm13
	vaddps	%zmm13, %zmm12, %zmm12
	vextractf128	$1, %ymm12, %xmm13
	vaddps	%xmm13, %xmm12, %xmm12
	vshufpd	$1, %xmm12, %xmm12, %xmm13      # xmm13 = xmm12[1,0]
	vaddps	%xmm13, %xmm12, %xmm12
	vmovshdup	%xmm12, %xmm13          # xmm13 = xmm12[1,1,3,3]
	vaddss	%xmm13, %xmm12, %xmm12
	vaddss	%xmm4, %xmm12, %xmm12
	vinsertps	$16, %xmm12, %xmm15, %xmm12 # xmm12 = xmm15[0],xmm12[0],xmm15[2,3]
	vinsertps	$32, %xmm10, %xmm12, %xmm10 # xmm10 = xmm12[0,1],xmm10[0],xmm12[3]
	vinsertps	$48, %xmm2, %xmm10, %xmm2 # xmm2 = xmm10[0,1,2],xmm2[0]
	vmovaps	%xmm2, 36416(%rsp)              # 16-byte Spill
	vshufpd	$1, %xmm9, %xmm9, %xmm2         # xmm2 = xmm9[1,0]
	vaddps	%xmm2, %xmm9, %xmm2
	vmulps	%zmm30, %zmm18, %zmm9
	vfmadd231ps	%zmm3, %zmm20, %zmm9    # zmm9 = (zmm20 * zmm3) + zmm9
	vextractf64x4	$1, %zmm9, %ymm10
	vaddps	%zmm10, %zmm9, %zmm9
	vextractf128	$1, %ymm9, %xmm10
	vaddps	%xmm10, %xmm9, %xmm9
	vshufpd	$1, %xmm9, %xmm9, %xmm10        # xmm10 = xmm9[1,0]
	vaddps	%xmm10, %xmm9, %xmm9
	vmovshdup	%xmm9, %xmm10           # xmm10 = xmm9[1,1,3,3]
	vaddss	%xmm10, %xmm9, %xmm9
	vaddss	%xmm4, %xmm9, %xmm9
	vmovaps	%xmm9, 17280(%rsp)              # 16-byte Spill
	vmovshdup	%xmm2, %xmm9            # xmm9 = xmm2[1,1,3,3]
	vaddss	%xmm2, %xmm9, %xmm9
	vmulps	2112(%rsp), %zmm18, %zmm2       # 64-byte Folded Reload
	vfmadd231ps	2496(%rsp), %zmm20, %zmm2 # 64-byte Folded Reload
                                        # zmm2 = (zmm20 * mem) + zmm2
	vextractf64x4	$1, %zmm2, %ymm10
	vaddps	%zmm10, %zmm2, %zmm2
	vextractf128	$1, %ymm2, %xmm10
	vaddps	%xmm2, %xmm10, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm10        # xmm10 = xmm2[1,0]
	vaddps	%xmm2, %xmm10, %xmm2
	vmovshdup	%xmm2, %xmm10           # xmm10 = xmm2[1,1,3,3]
	vaddss	%xmm2, %xmm10, %xmm10
	vxorps	%xmm2, %xmm2, %xmm2
	vaddss	2896(%rsp), %xmm2, %xmm0        # 4-byte Folded Reload
	vmovaps	%xmm0, 336(%rsp)                # 16-byte Spill
	vaddss	2880(%rsp), %xmm2, %xmm0        # 4-byte Folded Reload
	vmovaps	%xmm0, 14144(%rsp)              # 16-byte Spill
	vaddss	2864(%rsp), %xmm2, %xmm0        # 4-byte Folded Reload
	vmovaps	%xmm0, 352(%rsp)                # 16-byte Spill
	vaddss	2848(%rsp), %xmm2, %xmm12       # 4-byte Folded Reload
	vmovaps	%xmm12, 5552(%rsp)              # 16-byte Spill
	vaddss	2832(%rsp), %xmm2, %xmm12       # 4-byte Folded Reload
	vmovaps	%xmm12, 5536(%rsp)              # 16-byte Spill
	vaddss	2816(%rsp), %xmm2, %xmm12       # 4-byte Folded Reload
	vmovaps	%xmm12, 5520(%rsp)              # 16-byte Spill
	vaddss	2800(%rsp), %xmm2, %xmm12       # 4-byte Folded Reload
	vmovaps	%xmm12, 2736(%rsp)              # 16-byte Spill
	vaddss	2784(%rsp), %xmm2, %xmm12       # 4-byte Folded Reload
	vmovaps	%xmm12, 5584(%rsp)              # 16-byte Spill
	vaddss	2768(%rsp), %xmm2, %xmm1        # 4-byte Folded Reload
	vmovaps	%xmm1, 5568(%rsp)               # 16-byte Spill
	vaddss	10304(%rsp), %xmm2, %xmm1       # 4-byte Folded Reload
	vmovaps	%xmm1, 2784(%rsp)               # 16-byte Spill
	vaddss	2752(%rsp), %xmm2, %xmm1        # 4-byte Folded Reload
	vmovaps	%xmm1, 2768(%rsp)               # 16-byte Spill
	vaddss	400(%rsp), %xmm2, %xmm1         # 4-byte Folded Reload
	vmovaps	%xmm1, 2752(%rsp)               # 16-byte Spill
	vaddss	10240(%rsp), %xmm2, %xmm0       # 4-byte Folded Reload
	vmovaps	%xmm0, 2832(%rsp)               # 16-byte Spill
	vaddss	6656(%rsp), %xmm2, %xmm0        # 4-byte Folded Reload
	vmovaps	%xmm0, 2816(%rsp)               # 16-byte Spill
	vaddss	%xmm2, %xmm8, %xmm0
	vmovaps	%xmm0, 2864(%rsp)               # 16-byte Spill
	vaddss	%xmm2, %xmm11, %xmm0
	vmovaps	%xmm0, 17152(%rsp)              # 16-byte Spill
	vaddss	%xmm2, %xmm9, %xmm0
	vmovaps	%xmm0, 2896(%rsp)               # 16-byte Spill
	vaddss	%xmm2, %xmm10, %xmm0
	vmovaps	%xmm0, 2880(%rsp)               # 16-byte Spill
	vaddss	%xmm2, %xmm6, %xmm0
	vmovaps	%xmm0, 2800(%rsp)               # 16-byte Spill
	vaddss	%xmm2, %xmm7, %xmm0
	vxorps	%xmm27, %xmm27, %xmm27
	vmovaps	%xmm0, 2848(%rsp)               # 16-byte Spill
	vmovaps	%zmm5, %zmm1
	vmulps	5888(%rsp), %zmm5, %zmm29       # 64-byte Folded Reload
	vmulps	3712(%rsp), %zmm5, %zmm0        # 64-byte Folded Reload
	vmulps	3840(%rsp), %zmm5, %zmm26       # 64-byte Folded Reload
	vmulps	3968(%rsp), %zmm5, %zmm28       # 64-byte Folded Reload
	vmulps	4096(%rsp), %zmm5, %zmm24       # 64-byte Folded Reload
	vmulps	4224(%rsp), %zmm5, %zmm23       # 64-byte Folded Reload
	vmulps	6016(%rsp), %zmm5, %zmm22       # 64-byte Folded Reload
	vmulps	4352(%rsp), %zmm5, %zmm21       # 64-byte Folded Reload
	vmulps	4480(%rsp), %zmm5, %zmm20       # 64-byte Folded Reload
	vmulps	4608(%rsp), %zmm5, %zmm18       # 64-byte Folded Reload
	vmulps	3200(%rsp), %zmm5, %zmm17       # 64-byte Folded Reload
	vmulps	3328(%rsp), %zmm5, %zmm15       # 64-byte Folded Reload
	vmulps	3456(%rsp), %zmm5, %zmm14       # 64-byte Folded Reload
	vmulps	3584(%rsp), %zmm5, %zmm13       # 64-byte Folded Reload
	vmulps	4736(%rsp), %zmm5, %zmm5        # 64-byte Folded Reload
	vmulps	6144(%rsp), %zmm1, %zmm2        # 64-byte Folded Reload
	vmulps	7744(%rsp), %zmm1, %zmm19       # 64-byte Folded Reload
	vmovaps	6336(%rsp), %zmm9               # 64-byte Reload
	vmulps	%zmm1, %zmm9, %zmm6
	vmulps	4864(%rsp), %zmm1, %zmm31       # 64-byte Folded Reload
	vmovaps	6720(%rsp), %zmm16              # 64-byte Reload
	vmulps	%zmm1, %zmm16, %zmm25
	vmulps	6848(%rsp), %zmm1, %zmm3        # 64-byte Folded Reload
	vmulps	6912(%rsp), %zmm1, %zmm30       # 64-byte Folded Reload
	vmulps	6528(%rsp), %zmm1, %zmm12       # 64-byte Folded Reload
	vmulps	6592(%rsp), %zmm1, %zmm11       # 64-byte Folded Reload
	vmulps	5056(%rsp), %zmm1, %zmm10       # 64-byte Folded Reload
	vmulps	5184(%rsp), %zmm1, %zmm8        # 64-byte Folded Reload
	vmulps	5312(%rsp), %zmm1, %zmm7        # 64-byte Folded Reload
	vmulps	7040(%rsp), %zmm1, %zmm4        # 64-byte Folded Reload
	vmovaps	9024(%rsp), %zmm1               # 64-byte Reload
	vfmadd231ps	5952(%rsp), %zmm1, %zmm29 # 64-byte Folded Reload
                                        # zmm29 = (zmm1 * mem) + zmm29
	vmovaps	%zmm29, 29504(%rsp)             # 64-byte Spill
	vfmadd231ps	3776(%rsp), %zmm1, %zmm0 # 64-byte Folded Reload
                                        # zmm0 = (zmm1 * mem) + zmm0
	vmovaps	%zmm0, 29696(%rsp)              # 64-byte Spill
	vfmadd231ps	3904(%rsp), %zmm1, %zmm26 # 64-byte Folded Reload
                                        # zmm26 = (zmm1 * mem) + zmm26
	vmovaps	%zmm26, 29952(%rsp)             # 64-byte Spill
	vfmadd231ps	4032(%rsp), %zmm1, %zmm28 # 64-byte Folded Reload
                                        # zmm28 = (zmm1 * mem) + zmm28
	vmovaps	%zmm28, 30144(%rsp)             # 64-byte Spill
	vfmadd231ps	4160(%rsp), %zmm1, %zmm24 # 64-byte Folded Reload
                                        # zmm24 = (zmm1 * mem) + zmm24
	vmovaps	%zmm24, 30400(%rsp)             # 64-byte Spill
	vfmadd231ps	4288(%rsp), %zmm1, %zmm23 # 64-byte Folded Reload
                                        # zmm23 = (zmm1 * mem) + zmm23
	vmovaps	%zmm23, 30656(%rsp)             # 64-byte Spill
	vfmadd231ps	6080(%rsp), %zmm1, %zmm22 # 64-byte Folded Reload
                                        # zmm22 = (zmm1 * mem) + zmm22
	vmovaps	%zmm22, 31040(%rsp)             # 64-byte Spill
	vfmadd231ps	4416(%rsp), %zmm1, %zmm21 # 64-byte Folded Reload
                                        # zmm21 = (zmm1 * mem) + zmm21
	vmovaps	%zmm21, 9152(%rsp)              # 64-byte Spill
	vfmadd231ps	4544(%rsp), %zmm1, %zmm20 # 64-byte Folded Reload
                                        # zmm20 = (zmm1 * mem) + zmm20
	vmovaps	%zmm20, 12800(%rsp)             # 64-byte Spill
	vfmadd231ps	4672(%rsp), %zmm1, %zmm18 # 64-byte Folded Reload
                                        # zmm18 = (zmm1 * mem) + zmm18
	vmovaps	%zmm18, 13248(%rsp)             # 64-byte Spill
	vfmadd231ps	3264(%rsp), %zmm1, %zmm17 # 64-byte Folded Reload
                                        # zmm17 = (zmm1 * mem) + zmm17
	vmovaps	%zmm17, 13568(%rsp)             # 64-byte Spill
	vfmadd231ps	3392(%rsp), %zmm1, %zmm15 # 64-byte Folded Reload
                                        # zmm15 = (zmm1 * mem) + zmm15
	vmovaps	%zmm15, 31616(%rsp)             # 64-byte Spill
	vfmadd231ps	3520(%rsp), %zmm1, %zmm14 # 64-byte Folded Reload
                                        # zmm14 = (zmm1 * mem) + zmm14
	vmovaps	%zmm14, 32064(%rsp)             # 64-byte Spill
	vfmadd231ps	3648(%rsp), %zmm1, %zmm13 # 64-byte Folded Reload
                                        # zmm13 = (zmm1 * mem) + zmm13
	vmovaps	%zmm13, 32832(%rsp)             # 64-byte Spill
	vfmadd231ps	4800(%rsp), %zmm1, %zmm5 # 64-byte Folded Reload
                                        # zmm5 = (zmm1 * mem) + zmm5
	vmovaps	%zmm5, 15424(%rsp)              # 64-byte Spill
	vfmadd231ps	6208(%rsp), %zmm1, %zmm2 # 64-byte Folded Reload
                                        # zmm2 = (zmm1 * mem) + zmm2
	vmovaps	%zmm2, 14272(%rsp)              # 64-byte Spill
	vfmadd231ps	6272(%rsp), %zmm1, %zmm19 # 64-byte Folded Reload
                                        # zmm19 = (zmm1 * mem) + zmm19
	vmovaps	%zmm19, 31744(%rsp)             # 64-byte Spill
	vmovaps	6400(%rsp), %zmm13              # 64-byte Reload
	vfmadd231ps	%zmm1, %zmm13, %zmm6    # zmm6 = (zmm13 * zmm1) + zmm6
	vmovaps	6784(%rsp), %zmm15              # 64-byte Reload
	vfmadd231ps	%zmm1, %zmm15, %zmm31   # zmm31 = (zmm15 * zmm1) + zmm31
	vmovaps	%zmm31, 14912(%rsp)             # 64-byte Spill
	vmovaps	6464(%rsp), %zmm0               # 64-byte Reload
	vfmadd231ps	%zmm1, %zmm0, %zmm25    # zmm25 = (zmm0 * zmm1) + zmm25
	vaddss	2720(%rsp), %xmm27, %xmm2       # 4-byte Folded Reload
	vmovaps	%xmm2, 13760(%rsp)              # 16-byte Spill
	vmovaps	7808(%rsp), %zmm31              # 64-byte Reload
	vfmadd231ps	%zmm1, %zmm31, %zmm3    # zmm3 = (zmm31 * zmm1) + zmm3
	vmovaps	%zmm3, 10048(%rsp)              # 64-byte Spill
	vmovaps	7872(%rsp), %zmm23              # 64-byte Reload
	vfmadd231ps	%zmm1, %zmm23, %zmm30   # zmm30 = (zmm23 * zmm1) + zmm30
	vmovaps	%zmm30, 16192(%rsp)             # 64-byte Spill
	vfmadd231ps	4928(%rsp), %zmm1, %zmm12 # 64-byte Folded Reload
                                        # zmm12 = (zmm1 * mem) + zmm12
	vmovaps	%zmm12, 6656(%rsp)              # 64-byte Spill
	vfmadd231ps	4992(%rsp), %zmm1, %zmm11 # 64-byte Folded Reload
                                        # zmm11 = (zmm1 * mem) + zmm11
	vmovaps	%zmm11, 10240(%rsp)             # 64-byte Spill
	vfmadd231ps	5120(%rsp), %zmm1, %zmm10 # 64-byte Folded Reload
                                        # zmm10 = (zmm1 * mem) + zmm10
	vmovaps	%zmm10, 10304(%rsp)             # 64-byte Spill
	vfmadd231ps	5248(%rsp), %zmm1, %zmm8 # 64-byte Folded Reload
                                        # zmm8 = (zmm1 * mem) + zmm8
	vmovaps	%zmm8, 18944(%rsp)              # 64-byte Spill
	vfmadd231ps	5376(%rsp), %zmm1, %zmm7 # 64-byte Folded Reload
                                        # zmm7 = (zmm1 * mem) + zmm7
	vmovaps	%zmm7, 11136(%rsp)              # 64-byte Spill
	vfmadd231ps	7104(%rsp), %zmm1, %zmm4 # 64-byte Folded Reload
                                        # zmm4 = (zmm1 * mem) + zmm4
	vmovaps	%zmm4, 9024(%rsp)               # 64-byte Spill
	vmovaps	1152(%rsp), %zmm19              # 64-byte Reload
	vmulps	%zmm19, %zmm9, %zmm4
	vmovaps	768(%rsp), %zmm21               # 64-byte Reload
	vfmadd231ps	%zmm21, %zmm13, %zmm4   # zmm4 = (zmm13 * zmm21) + zmm4
	vextractf64x4	$1, %zmm4, %ymm7
	vaddps	%zmm7, %zmm4, %zmm4
	vextractf128	$1, %ymm4, %xmm7
	vaddps	%xmm7, %xmm4, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm7         # xmm7 = xmm4[1,0]
	vaddps	%xmm7, %xmm4, %xmm4
	vmovshdup	%xmm4, %xmm7            # xmm7 = xmm4[1,1,3,3]
	vaddss	%xmm7, %xmm4, %xmm2
	vmovss	%xmm2, 384(%rsp)                # 4-byte Spill
	vmovaps	34560(%rsp), %zmm5              # 64-byte Reload
	vextractf64x4	$1, %zmm5, %ymm4
	vaddps	%zmm4, %zmm5, %zmm4
	vextractf128	$1, %ymm4, %xmm7
	vaddps	%xmm7, %xmm4, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm7         # xmm7 = xmm4[1,0]
	vaddps	%xmm7, %xmm4, %xmm4
	vmovshdup	%xmm4, %xmm7            # xmm7 = xmm4[1,1,3,3]
	vaddss	%xmm7, %xmm4, %xmm2
	vmovss	%xmm2, 2720(%rsp)               # 4-byte Spill
	vmovaps	16256(%rsp), %zmm2              # 64-byte Reload
	vextractf64x4	$1, %zmm2, %ymm4
	vaddps	%zmm4, %zmm2, %zmm4
	vextractf128	$1, %ymm4, %xmm7
	vaddps	%xmm7, %xmm4, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm7         # xmm7 = xmm4[1,0]
	vaddps	%xmm7, %xmm4, %xmm4
	vmovshdup	%xmm4, %xmm7            # xmm7 = xmm4[1,1,3,3]
	vaddss	%xmm7, %xmm4, %xmm2
	vmovss	%xmm2, 16256(%rsp)              # 4-byte Spill
	vextractf64x4	$1, %zmm6, %ymm4
	vaddps	%zmm4, %zmm6, %zmm3
	vextractf128	$1, %ymm3, %xmm4
	vaddps	%xmm4, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm4         # xmm4 = xmm3[1,0]
	vaddps	%xmm4, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm4            # xmm4 = xmm3[1,1,3,3]
	vaddss	%xmm4, %xmm3, %xmm2
	vmovss	%xmm2, 400(%rsp)                # 4-byte Spill
	vmovaps	34816(%rsp), %zmm4              # 64-byte Reload
	vextractf64x4	$1, %zmm4, %ymm3
	vaddps	%zmm3, %zmm4, %zmm3
	vextractf128	$1, %ymm3, %xmm4
	vaddps	%xmm4, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm4         # xmm4 = xmm3[1,0]
	vaddps	%xmm4, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm4            # xmm4 = xmm3[1,1,3,3]
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm27, %xmm3, %xmm30
	vmovaps	1280(%rsp), %zmm20              # 64-byte Reload
	vmulps	%zmm20, %zmm9, %zmm3
	vmovaps	1216(%rsp), %zmm22              # 64-byte Reload
	vmovaps	%zmm13, %zmm2
	vfmadd231ps	%zmm22, %zmm13, %zmm3   # zmm3 = (zmm13 * zmm22) + zmm3
	vextractf64x4	$1, %zmm3, %ymm4
	vaddps	%zmm4, %zmm3, %zmm3
	vextractf128	$1, %ymm3, %xmm4
	vaddps	%xmm4, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm4         # xmm4 = xmm3[1,0]
	vaddps	%xmm4, %xmm3, %xmm3
	vmovaps	35136(%rsp), %zmm8              # 64-byte Reload
	vextractf64x4	$1, %zmm8, %ymm4
	vaddps	%zmm4, %zmm8, %zmm4
	vextractf128	$1, %ymm4, %xmm8
	vaddps	%xmm4, %xmm8, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm8         # xmm8 = xmm4[1,0]
	vaddps	%xmm4, %xmm8, %xmm4
	vmovshdup	%xmm4, %xmm8            # xmm8 = xmm4[1,1,3,3]
	vaddss	%xmm4, %xmm8, %xmm4
	vaddss	%xmm27, %xmm4, %xmm8
	vmovshdup	%xmm3, %xmm4            # xmm4 = xmm3[1,1,3,3]
	vaddss	%xmm4, %xmm3, %xmm3
	vmovss	%xmm3, 2704(%rsp)               # 4-byte Spill
	vmovaps	8320(%rsp), %zmm13              # 64-byte Reload
	vmulps	%zmm13, %zmm9, %zmm3
	vmovaps	%zmm9, %zmm12
	vmovaps	8448(%rsp), %zmm5               # 64-byte Reload
	vfmadd231ps	%zmm5, %zmm2, %zmm3     # zmm3 = (zmm2 * zmm5) + zmm3
	vmovaps	%zmm2, %zmm14
	vextractf64x4	$1, %zmm3, %ymm4
	vaddps	%zmm4, %zmm3, %zmm3
	vextractf128	$1, %ymm3, %xmm4
	vaddps	%xmm4, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm4         # xmm4 = xmm3[1,0]
	vaddps	%xmm4, %xmm3, %xmm3
	vmovaps	35456(%rsp), %zmm1              # 64-byte Reload
	vextractf64x4	$1, %zmm1, %ymm4
	vaddps	%zmm4, %zmm1, %zmm4
	vextractf128	$1, %ymm4, %xmm9
	vaddps	%xmm4, %xmm9, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm9         # xmm9 = xmm4[1,0]
	vaddps	%xmm4, %xmm9, %xmm4
	vmovshdup	%xmm4, %xmm9            # xmm9 = xmm4[1,1,3,3]
	vaddss	%xmm4, %xmm9, %xmm4
	vaddss	%xmm27, %xmm4, %xmm9
	vmulps	2432(%rsp), %zmm16, %zmm4       # 64-byte Folded Reload
	vmovaps	%zmm0, %zmm1
	vfmadd231ps	1792(%rsp), %zmm0, %zmm4 # 64-byte Folded Reload
                                        # zmm4 = (zmm0 * mem) + zmm4
	vextractf64x4	$1, %zmm4, %ymm10
	vaddps	%zmm10, %zmm4, %zmm4
	vextractf128	$1, %ymm4, %xmm10
	vaddps	%xmm4, %xmm10, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm10        # xmm10 = xmm4[1,0]
	vaddps	%xmm4, %xmm10, %xmm4
	vmovshdup	%xmm4, %xmm10           # xmm10 = xmm4[1,1,3,3]
	vaddss	%xmm4, %xmm10, %xmm4
	vaddss	%xmm27, %xmm4, %xmm10
	vmovshdup	%xmm3, %xmm4            # xmm4 = xmm3[1,1,3,3]
	vaddss	%xmm4, %xmm3, %xmm0
	vmovss	%xmm0, 2688(%rsp)               # 4-byte Spill
	vmulps	2176(%rsp), %zmm16, %zmm3       # 64-byte Folded Reload
	vmovaps	%zmm16, %zmm24
	vfmadd231ps	1728(%rsp), %zmm1, %zmm3 # 64-byte Folded Reload
                                        # zmm3 = (zmm1 * mem) + zmm3
	vmovaps	%zmm1, %zmm28
	vextractf64x4	$1, %zmm3, %ymm4
	vaddps	%zmm4, %zmm3, %zmm3
	vextractf128	$1, %ymm3, %xmm4
	vaddps	%xmm4, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm4         # xmm4 = xmm3[1,0]
	vaddps	%xmm4, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm4            # xmm4 = xmm3[1,1,3,3]
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm27, %xmm3, %xmm11
	vmovaps	8384(%rsp), %zmm7               # 64-byte Reload
	vmulps	%zmm7, %zmm12, %zmm3
	vmovaps	1536(%rsp), %zmm2               # 64-byte Reload
	vfmadd231ps	%zmm2, %zmm14, %zmm3    # zmm3 = (zmm14 * zmm2) + zmm3
	vextractf64x4	$1, %zmm3, %ymm4
	vaddps	%zmm4, %zmm3, %zmm3
	vextractf128	$1, %ymm3, %xmm4
	vaddps	%xmm4, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm4         # xmm4 = xmm3[1,0]
	vaddps	%xmm4, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm4            # xmm4 = xmm3[1,1,3,3]
	vaddss	%xmm4, %xmm3, %xmm0
	vmovss	%xmm0, 368(%rsp)                # 4-byte Spill
	vmovaps	35776(%rsp), %zmm1              # 64-byte Reload
	vextractf64x4	$1, %zmm1, %ymm3
	vaddps	%zmm3, %zmm1, %zmm3
	vextractf128	$1, %ymm3, %xmm4
	vaddps	%xmm4, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm4         # xmm4 = xmm3[1,0]
	vaddps	%xmm4, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm4            # xmm4 = xmm3[1,1,3,3]
	vaddss	%xmm4, %xmm3, %xmm0
	vmovss	%xmm0, 2672(%rsp)               # 4-byte Spill
	vmovaps	16832(%rsp), %zmm0              # 64-byte Reload
	vextractf64x4	$1, %zmm0, %ymm3
	vaddps	%zmm3, %zmm0, %zmm3
	vextractf128	$1, %ymm3, %xmm4
	vaddps	%xmm4, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm4         # xmm4 = xmm3[1,0]
	vaddps	%xmm4, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm4            # xmm4 = xmm3[1,1,3,3]
	vaddss	%xmm4, %xmm3, %xmm0
	vmovss	%xmm0, 14208(%rsp)              # 4-byte Spill
	vmovaps	16640(%rsp), %zmm0              # 64-byte Reload
	vextractf64x4	$1, %zmm0, %ymm3
	vaddps	%zmm3, %zmm0, %zmm3
	vextractf128	$1, %ymm3, %xmm4
	vaddps	%xmm4, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm4         # xmm4 = xmm3[1,0]
	vaddps	%xmm4, %xmm3, %xmm3
	vmovaps	16384(%rsp), %zmm0              # 64-byte Reload
	vextractf64x4	$1, %zmm0, %ymm4
	vaddps	%zmm4, %zmm0, %zmm4
	vextractf128	$1, %ymm4, %xmm12
	vaddps	%xmm4, %xmm12, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm12        # xmm12 = xmm4[1,0]
	vaddps	%xmm4, %xmm12, %xmm4
	vmovshdup	%xmm4, %xmm12           # xmm12 = xmm4[1,1,3,3]
	vaddss	%xmm4, %xmm12, %xmm4
	vaddss	%xmm27, %xmm4, %xmm14
	vmovshdup	%xmm3, %xmm4            # xmm4 = xmm3[1,1,3,3]
	vaddss	%xmm4, %xmm3, %xmm0
	vmovss	%xmm0, 16384(%rsp)              # 4-byte Spill
	vmulps	4864(%rsp), %zmm19, %zmm3       # 64-byte Folded Reload
	vfmadd231ps	%zmm21, %zmm15, %zmm3   # zmm3 = (zmm15 * zmm21) + zmm3
	vextractf64x4	$1, %zmm3, %ymm4
	vaddps	%zmm4, %zmm3, %zmm3
	vextractf128	$1, %ymm3, %xmm4
	vaddps	%xmm4, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm4         # xmm4 = xmm3[1,0]
	vaddps	%xmm4, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm4            # xmm4 = xmm3[1,1,3,3]
	vaddss	%xmm4, %xmm3, %xmm0
	vmovss	%xmm0, 10112(%rsp)              # 4-byte Spill
	vmovaps	16512(%rsp), %zmm0              # 64-byte Reload
	vextractf64x4	$1, %zmm0, %ymm3
	vaddps	%zmm3, %zmm0, %zmm3
	vextractf128	$1, %ymm3, %xmm12
	vaddps	%xmm3, %xmm12, %xmm3
	vmovaps	34368(%rsp), %zmm0              # 64-byte Reload
	vextractf64x4	$1, %zmm0, %ymm12
	vaddps	%zmm12, %zmm0, %zmm12
	vextractf128	$1, %ymm12, %xmm15
	vaddps	%xmm15, %xmm12, %xmm12
	vshufpd	$1, %xmm12, %xmm12, %xmm15      # xmm15 = xmm12[1,0]
	vaddps	%xmm15, %xmm12, %xmm12
	vmovshdup	%xmm12, %xmm15          # xmm15 = xmm12[1,1,3,3]
	vaddss	%xmm15, %xmm12, %xmm12
	vaddss	%xmm27, %xmm12, %xmm15
	vmulps	192(%rsp), %zmm16, %zmm12       # 64-byte Folded Reload
	vfmadd231ps	1344(%rsp), %zmm28, %zmm12 # 64-byte Folded Reload
                                        # zmm12 = (zmm28 * mem) + zmm12
	vextractf64x4	$1, %zmm12, %ymm17
	vaddps	%zmm17, %zmm12, %zmm12
	vextractf32x4	$1, %ymm12, %xmm17
	vaddps	%xmm17, %xmm12, %xmm12
	vshufpd	$1, %xmm12, %xmm12, %xmm17      # xmm17 = xmm12[1,0]
	vaddps	%xmm17, %xmm12, %xmm12
	vmovshdup	%xmm12, %xmm17          # xmm17 = xmm12[1,1,3,3]
	vaddss	%xmm17, %xmm12, %xmm12
	vaddss	%xmm27, %xmm12, %xmm12
	vshufpd	$1, %xmm3, %xmm3, %xmm17        # xmm17 = xmm3[1,0]
	vaddps	%xmm17, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm17           # xmm17 = xmm3[1,1,3,3]
	vaddss	%xmm17, %xmm3, %xmm0
	vmovss	%xmm0, 2656(%rsp)               # 4-byte Spill
	vmovaps	640(%rsp), %zmm29               # 64-byte Reload
	vmulps	%zmm29, %zmm16, %zmm17
	vmovaps	64(%rsp), %zmm26                # 64-byte Reload
	vfmadd231ps	%zmm26, %zmm28, %zmm17  # zmm17 = (zmm28 * zmm26) + zmm17
	vextractf64x4	$1, %zmm17, %ymm18
	vaddps	%zmm18, %zmm17, %zmm17
	vextractf32x4	$1, %ymm17, %xmm18
	vaddps	%xmm18, %xmm17, %xmm17
	vshufpd	$1, %xmm17, %xmm17, %xmm18      # xmm18 = xmm17[1,0]
	vaddps	%xmm18, %xmm17, %xmm17
	vmovshdup	%xmm17, %xmm18          # xmm18 = xmm17[1,1,3,3]
	vaddss	%xmm18, %xmm17, %xmm17
	vaddss	%xmm27, %xmm17, %xmm17
	vinsertps	$16, %xmm15, %xmm17, %xmm15 # xmm15 = xmm17[0],xmm15[0],xmm17[2,3]
	vinsertps	$32, %xmm14, %xmm15, %xmm14 # xmm14 = xmm15[0,1],xmm14[0],xmm15[3]
	vextractf64x4	$1, %zmm25, %ymm15
	vaddps	%zmm15, %zmm25, %zmm6
	vextractf128	$1, %ymm6, %xmm15
	vaddps	%xmm6, %xmm15, %xmm6
	vshufpd	$1, %xmm6, %xmm6, %xmm15        # xmm15 = xmm6[1,0]
	vaddps	%xmm6, %xmm15, %xmm6
	vmovshdup	%xmm6, %xmm15           # xmm15 = xmm6[1,1,3,3]
	vaddss	%xmm6, %xmm15, %xmm6
	vaddss	%xmm27, %xmm6, %xmm6
	vinsertps	$48, %xmm6, %xmm14, %xmm14 # xmm14 = xmm14[0,1,2],xmm6[0]
	vmulps	%zmm20, %zmm16, %zmm6
	vmovaps	%zmm20, %zmm25
	vfmadd231ps	%zmm22, %zmm28, %zmm6   # zmm6 = (zmm28 * zmm22) + zmm6
	vmovaps	%zmm22, %zmm16
	vextractf64x4	$1, %zmm6, %ymm15
	vaddps	%zmm15, %zmm6, %zmm6
	vextractf128	$1, %ymm6, %xmm15
	vaddps	%xmm6, %xmm15, %xmm15
	vmovaps	35712(%rsp), %zmm0              # 64-byte Reload
	vextractf64x4	$1, %zmm0, %ymm6
	vaddps	%zmm6, %zmm0, %zmm6
	vextractf32x4	$1, %ymm6, %xmm17
	vaddps	%xmm17, %xmm6, %xmm6
	vshufpd	$1, %xmm6, %xmm6, %xmm17        # xmm17 = xmm6[1,0]
	vaddps	%xmm17, %xmm6, %xmm6
	vmovshdup	%xmm6, %xmm17           # xmm17 = xmm6[1,1,3,3]
	vaddss	%xmm17, %xmm6, %xmm6
	vaddss	%xmm27, %xmm6, %xmm0
	vmovaps	%xmm0, 16832(%rsp)              # 16-byte Spill
	vshufpd	$1, %xmm15, %xmm15, %xmm17      # xmm17 = xmm15[1,0]
	vaddps	%xmm17, %xmm15, %xmm15
	vmovshdup	%xmm15, %xmm17          # xmm17 = xmm15[1,1,3,3]
	vaddss	%xmm17, %xmm15, %xmm15
	vmulps	8896(%rsp), %zmm24, %zmm17      # 64-byte Folded Reload
	vmovaps	8960(%rsp), %zmm6               # 64-byte Reload
	vfmadd231ps	%zmm6, %zmm28, %zmm17   # zmm17 = (zmm28 * zmm6) + zmm17
	vextractf64x4	$1, %zmm17, %ymm18
	vaddps	%zmm18, %zmm17, %zmm17
	vextractf32x4	$1, %ymm17, %xmm18
	vaddps	%xmm18, %xmm17, %xmm17
	vshufpd	$1, %xmm17, %xmm17, %xmm18      # xmm18 = xmm17[1,0]
	vaddps	%xmm18, %xmm17, %xmm17
	vmovshdup	%xmm17, %xmm18          # xmm18 = xmm17[1,1,3,3]
	vaddss	%xmm18, %xmm17, %xmm17
	vaddss	%xmm27, %xmm15, %xmm15
	vaddss	%xmm27, %xmm17, %xmm17
	vinsertps	$16, %xmm17, %xmm15, %xmm15 # xmm15 = xmm15[0],xmm17[0],xmm15[2,3]
	vmulps	%zmm13, %zmm24, %zmm17
	vfmadd231ps	%zmm5, %zmm28, %zmm17   # zmm17 = (zmm28 * zmm5) + zmm17
	vextractf64x4	$1, %zmm17, %ymm18
	vaddps	%zmm18, %zmm17, %zmm17
	vextractf32x4	$1, %ymm17, %xmm18
	vaddps	%xmm18, %xmm17, %xmm17
	vshufpd	$1, %xmm17, %xmm17, %xmm18      # xmm18 = xmm17[1,0]
	vaddps	%xmm18, %xmm17, %xmm17
	vmovshdup	%xmm17, %xmm18          # xmm18 = xmm17[1,1,3,3]
	vaddss	%xmm18, %xmm17, %xmm17
	vaddss	%xmm27, %xmm17, %xmm17
	vinsertps	$32, %xmm17, %xmm15, %xmm15 # xmm15 = xmm15[0,1],xmm17[0],xmm15[3]
	vmulps	%zmm7, %zmm24, %zmm17
	vfmadd231ps	%zmm2, %zmm28, %zmm17   # zmm17 = (zmm28 * zmm2) + zmm17
	vextractf64x4	$1, %zmm17, %ymm18
	vaddps	%zmm18, %zmm17, %zmm17
	vextractf32x4	$1, %ymm17, %xmm18
	vaddps	%xmm18, %xmm17, %xmm17
	vshufpd	$1, %xmm17, %xmm17, %xmm18      # xmm18 = xmm17[1,0]
	vaddps	%xmm18, %xmm17, %xmm17
	vmovshdup	%xmm17, %xmm18          # xmm18 = xmm17[1,1,3,3]
	vaddss	%xmm18, %xmm17, %xmm17
	vaddss	%xmm27, %xmm17, %xmm17
	vinsertps	$48, %xmm17, %xmm15, %xmm15 # xmm15 = xmm15[0,1,2],xmm17[0]
	vmulps	2240(%rsp), %zmm24, %zmm17      # 64-byte Folded Reload
	vfmadd231ps	1088(%rsp), %zmm28, %zmm17 # 64-byte Folded Reload
                                        # zmm17 = (zmm28 * mem) + zmm17
	vextractf64x4	$1, %zmm17, %ymm18
	vaddps	%zmm18, %zmm17, %zmm17
	vextractf32x4	$1, %ymm17, %xmm18
	vaddps	%xmm18, %xmm17, %xmm17
	vshufpd	$1, %xmm17, %xmm17, %xmm18      # xmm18 = xmm17[1,0]
	vaddps	%xmm18, %xmm17, %xmm17
	vmovshdup	%xmm17, %xmm18          # xmm18 = xmm17[1,1,3,3]
	vaddss	%xmm18, %xmm17, %xmm17
	vaddss	%xmm27, %xmm17, %xmm17
	vinsertps	$16, %xmm17, %xmm12, %xmm12 # xmm12 = xmm12[0],xmm17[0],xmm12[2,3]
	vinsertps	$32, %xmm11, %xmm12, %xmm11 # xmm11 = xmm12[0,1],xmm11[0],xmm12[3]
	vinsertps	$48, %xmm10, %xmm11, %xmm1 # xmm1 = xmm11[0,1,2],xmm10[0]
	vmovaps	%xmm1, 35776(%rsp)              # 16-byte Spill
	vinsertf128	$1, %xmm15, %ymm14, %ymm1
	vmovaps	%zmm1, 35712(%rsp)              # 64-byte Spill
	vmovaps	6848(%rsp), %zmm22              # 64-byte Reload
	vmovaps	1600(%rsp), %zmm4               # 64-byte Reload
	vmulps	%zmm4, %zmm22, %zmm10
	vmovaps	1664(%rsp), %zmm24              # 64-byte Reload
	vfmadd231ps	%zmm24, %zmm31, %zmm10  # zmm10 = (zmm31 * zmm24) + zmm10
	vextractf64x4	$1, %zmm10, %ymm11
	vaddps	%zmm11, %zmm10, %zmm10
	vextractf128	$1, %ymm10, %xmm11
	vaddps	%xmm11, %xmm10, %xmm10
	vshufpd	$1, %xmm10, %xmm10, %xmm11      # xmm11 = xmm10[1,0]
	vaddps	%xmm11, %xmm10, %xmm10
	vmovshdup	%xmm10, %xmm11          # xmm11 = xmm10[1,1,3,3]
	vaddss	%xmm11, %xmm10, %xmm10
	vaddss	%xmm27, %xmm10, %xmm10
	vinsertps	$16, %xmm9, %xmm10, %xmm9 # xmm9 = xmm10[0],xmm9[0],xmm10[2,3]
	vinsertps	$32, %xmm8, %xmm9, %xmm8 # xmm8 = xmm9[0,1],xmm8[0],xmm9[3]
	vinsertps	$48, %xmm30, %xmm8, %xmm0 # xmm0 = xmm8[0,1,2],xmm30[0]
	vmovaps	%ymm0, 16640(%rsp)              # 32-byte Spill
	vmovaps	10368(%rsp), %zmm0              # 64-byte Reload
	vextractf64x4	$1, %zmm0, %ymm7
	vaddps	%zmm7, %zmm0, %zmm7
	vextractf128	$1, %ymm7, %xmm9
	vaddps	%xmm7, %xmm9, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm9         # xmm9 = xmm7[1,0]
	vaddps	%xmm7, %xmm9, %xmm7
	vmovshdup	%xmm7, %xmm9            # xmm9 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm9, %xmm7
	vaddss	%xmm27, %xmm7, %xmm7
	vmovaps	13760(%rsp), %xmm0              # 16-byte Reload
	vinsertps	$16, %xmm7, %xmm0, %xmm5 # xmm5 = xmm0[0],xmm7[0],xmm0[2,3]
	vinsertps	$32, 336(%rsp), %xmm5, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, 14144(%rsp), %xmm2, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm2[0,1,2],mem[0]
	vmovaps	%xmm0, 16512(%rsp)              # 16-byte Spill
	vmovaps	16576(%rsp), %zmm0              # 64-byte Reload
	vextractf64x4	$1, %zmm0, %ymm2
	vaddps	%zmm2, %zmm0, %zmm2
	vextractf128	$1, %ymm2, %xmm5
	vaddps	%xmm5, %xmm2, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm5         # xmm5 = xmm2[1,0]
	vaddps	%xmm5, %xmm2, %xmm2
	vmovshdup	%xmm2, %xmm5            # xmm5 = xmm2[1,1,3,3]
	vaddss	%xmm5, %xmm2, %xmm2
	vaddss	%xmm27, %xmm2, %xmm7
	vmulps	448(%rsp), %zmm22, %zmm2        # 64-byte Folded Reload
	vfmadd231ps	512(%rsp), %zmm31, %zmm2 # 64-byte Folded Reload
                                        # zmm2 = (zmm31 * mem) + zmm2
	vextractf64x4	$1, %zmm2, %ymm5
	vaddps	%zmm5, %zmm2, %zmm2
	vextractf128	$1, %ymm2, %xmm5
	vaddps	%xmm5, %xmm2, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm5         # xmm5 = xmm2[1,0]
	vaddps	%xmm5, %xmm2, %xmm2
	vmovaps	34880(%rsp), %zmm0              # 64-byte Reload
	vextractf64x4	$1, %zmm0, %ymm5
	vaddps	%zmm5, %zmm0, %zmm5
	vextractf128	$1, %ymm5, %xmm9
	vaddps	%xmm5, %xmm9, %xmm5
	vshufpd	$1, %xmm5, %xmm5, %xmm9         # xmm9 = xmm5[1,0]
	vaddps	%xmm5, %xmm9, %xmm5
	vmovshdup	%xmm5, %xmm9            # xmm9 = xmm5[1,1,3,3]
	vaddss	%xmm5, %xmm9, %xmm5
	vaddss	%xmm27, %xmm5, %xmm9
	vmovshdup	%xmm2, %xmm5            # xmm5 = xmm2[1,1,3,3]
	vaddss	%xmm5, %xmm2, %xmm0
	vmovss	%xmm0, 10368(%rsp)              # 4-byte Spill
	vmovaps	6912(%rsp), %zmm28              # 64-byte Reload
	vmulps	%zmm19, %zmm28, %zmm2
	vfmadd231ps	%zmm21, %zmm23, %zmm2   # zmm2 = (zmm23 * zmm21) + zmm2
	vextractf64x4	$1, %zmm2, %ymm10
	vaddps	%zmm10, %zmm2, %zmm2
	vextractf128	$1, %ymm2, %xmm10
	vaddps	%xmm2, %xmm10, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm10        # xmm10 = xmm2[1,0]
	vaddps	%xmm2, %xmm10, %xmm2
	vmovshdup	%xmm2, %xmm10           # xmm10 = xmm2[1,1,3,3]
	vaddss	%xmm2, %xmm10, %xmm2
	vaddss	%xmm27, %xmm2, %xmm11
	vmulps	2048(%rsp), %zmm22, %zmm2       # 64-byte Folded Reload
	vfmadd231ps	704(%rsp), %zmm31, %zmm2 # 64-byte Folded Reload
                                        # zmm2 = (zmm31 * mem) + zmm2
	vextractf64x4	$1, %zmm2, %ymm10
	vaddps	%zmm10, %zmm2, %zmm2
	vextractf128	$1, %ymm2, %xmm10
	vaddps	%xmm2, %xmm10, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm10        # xmm10 = xmm2[1,0]
	vaddps	%xmm2, %xmm10, %xmm2
	vmovaps	16448(%rsp), %zmm0              # 64-byte Reload
	vextractf64x4	$1, %zmm0, %ymm10
	vaddps	%zmm10, %zmm0, %zmm10
	vextractf128	$1, %ymm10, %xmm12
	vaddps	%xmm12, %xmm10, %xmm10
	vshufpd	$1, %xmm10, %xmm10, %xmm12      # xmm12 = xmm10[1,0]
	vaddps	%xmm12, %xmm10, %xmm10
	vmovshdup	%xmm10, %xmm12          # xmm12 = xmm10[1,1,3,3]
	vaddss	%xmm12, %xmm10, %xmm10
	vaddss	%xmm27, %xmm10, %xmm12
	vmovshdup	%xmm2, %xmm10           # xmm10 = xmm2[1,1,3,3]
	vaddss	%xmm2, %xmm10, %xmm0
	vmovss	%xmm0, 16576(%rsp)              # 4-byte Spill
	vmulps	256(%rsp), %zmm22, %zmm2        # 64-byte Folded Reload
	vfmadd231ps	1408(%rsp), %zmm31, %zmm2 # 64-byte Folded Reload
                                        # zmm2 = (zmm31 * mem) + zmm2
	vextractf64x4	$1, %zmm2, %ymm10
	vaddps	%zmm10, %zmm2, %zmm2
	vextractf128	$1, %ymm2, %xmm10
	vaddps	%xmm2, %xmm10, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm10        # xmm10 = xmm2[1,0]
	vaddps	%xmm2, %xmm10, %xmm2
	vmovaps	34496(%rsp), %zmm0              # 64-byte Reload
	vextractf64x4	$1, %zmm0, %ymm10
	vaddps	%zmm10, %zmm0, %zmm10
	vextractf128	$1, %ymm10, %xmm14
	vaddps	%xmm14, %xmm10, %xmm10
	vshufpd	$1, %xmm10, %xmm10, %xmm14      # xmm14 = xmm10[1,0]
	vaddps	%xmm14, %xmm10, %xmm10
	vmovshdup	%xmm10, %xmm14          # xmm14 = xmm10[1,1,3,3]
	vaddss	%xmm14, %xmm10, %xmm10
	vaddss	%xmm27, %xmm10, %xmm14
	vmovshdup	%xmm2, %xmm10           # xmm10 = xmm2[1,1,3,3]
	vaddss	%xmm2, %xmm10, %xmm0
	vmovss	%xmm0, 16448(%rsp)              # 4-byte Spill
	vmovaps	128(%rsp), %zmm13               # 64-byte Reload
	vmulps	%zmm13, %zmm22, %zmm10
	vmovaps	896(%rsp), %zmm5                # 64-byte Reload
	vfmadd231ps	%zmm5, %zmm31, %zmm10   # zmm10 = (zmm31 * zmm5) + zmm10
	vextractf64x4	$1, %zmm10, %ymm15
	vaddps	%zmm15, %zmm10, %zmm10
	vextractf128	$1, %ymm10, %xmm15
	vaddps	%xmm15, %xmm10, %xmm15
	vmovaps	17088(%rsp), %zmm1              # 64-byte Reload
	vextractf64x4	$1, %zmm1, %ymm10
	vaddps	%zmm10, %zmm1, %zmm10
	vextractf32x4	$1, %ymm10, %xmm17
	vaddps	%xmm17, %xmm10, %xmm10
	vshufpd	$1, %xmm10, %xmm10, %xmm17      # xmm17 = xmm10[1,0]
	vaddps	%xmm17, %xmm10, %xmm10
	vmovshdup	%xmm10, %xmm17          # xmm17 = xmm10[1,1,3,3]
	vaddss	%xmm17, %xmm10, %xmm10
	vaddss	%xmm27, %xmm10, %xmm10
	vshufpd	$1, %xmm15, %xmm15, %xmm17      # xmm17 = xmm15[1,0]
	vaddps	%xmm17, %xmm15, %xmm15
	vmovshdup	%xmm15, %xmm17          # xmm17 = xmm15[1,1,3,3]
	vaddss	%xmm17, %xmm15, %xmm15
	vmovaps	2368(%rsp), %zmm19              # 64-byte Reload
	vmulps	%zmm19, %zmm22, %zmm17
	vmovaps	2560(%rsp), %zmm2               # 64-byte Reload
	vfmadd231ps	%zmm2, %zmm31, %zmm17   # zmm17 = (zmm31 * zmm2) + zmm17
	vextractf64x4	$1, %zmm17, %ymm18
	vaddps	%zmm18, %zmm17, %zmm17
	vextractf32x4	$1, %ymm17, %xmm18
	vaddps	%xmm18, %xmm17, %xmm17
	vshufpd	$1, %xmm17, %xmm17, %xmm18      # xmm18 = xmm17[1,0]
	vaddps	%xmm18, %xmm17, %xmm17
	vmovshdup	%xmm17, %xmm18          # xmm18 = xmm17[1,1,3,3]
	vaddss	%xmm18, %xmm17, %xmm17
	vaddss	%xmm27, %xmm15, %xmm15
	vaddss	%xmm27, %xmm17, %xmm17
	vinsertps	$16, %xmm17, %xmm15, %xmm15 # xmm15 = xmm15[0],xmm17[0],xmm15[2,3]
	vmovaps	1984(%rsp), %zmm20              # 64-byte Reload
	vmulps	%zmm20, %zmm22, %zmm17
	vmovaps	1024(%rsp), %zmm1               # 64-byte Reload
	vfmadd231ps	%zmm1, %zmm31, %zmm17   # zmm17 = (zmm31 * zmm1) + zmm17
	vextractf64x4	$1, %zmm17, %ymm18
	vaddps	%zmm18, %zmm17, %zmm17
	vextractf32x4	$1, %ymm17, %xmm18
	vaddps	%xmm18, %xmm17, %xmm17
	vshufpd	$1, %xmm17, %xmm17, %xmm18      # xmm18 = xmm17[1,0]
	vaddps	%xmm18, %xmm17, %xmm17
	vmovshdup	%xmm17, %xmm18          # xmm18 = xmm17[1,1,3,3]
	vaddss	%xmm18, %xmm17, %xmm17
	vaddss	%xmm27, %xmm17, %xmm17
	vinsertps	$32, %xmm17, %xmm15, %xmm15 # xmm15 = xmm15[0,1],xmm17[0],xmm15[3]
	vmulps	832(%rsp), %zmm22, %zmm17       # 64-byte Folded Reload
	vfmadd231ps	960(%rsp), %zmm31, %zmm17 # 64-byte Folded Reload
                                        # zmm17 = (zmm31 * mem) + zmm17
	vextractf64x4	$1, %zmm17, %ymm18
	vaddps	%zmm18, %zmm17, %zmm17
	vextractf32x4	$1, %ymm17, %xmm18
	vaddps	%xmm18, %xmm17, %xmm17
	vshufpd	$1, %xmm17, %xmm17, %xmm18      # xmm18 = xmm17[1,0]
	vaddps	%xmm18, %xmm17, %xmm17
	vmovshdup	%xmm17, %xmm18          # xmm18 = xmm17[1,1,3,3]
	vaddss	%xmm18, %xmm17, %xmm17
	vaddss	%xmm27, %xmm17, %xmm17
	vinsertps	$48, %xmm17, %xmm15, %xmm3 # xmm3 = xmm15[0,1,2],xmm17[0]
	vmovaps	%xmm3, 35456(%rsp)              # 16-byte Spill
	vmulps	%zmm29, %zmm22, %zmm15
	vfmadd231ps	%zmm26, %zmm31, %zmm15  # zmm15 = (zmm31 * zmm26) + zmm15
	vextractf64x4	$1, %zmm15, %ymm17
	vaddps	%zmm17, %zmm15, %zmm15
	vextractf32x4	$1, %ymm15, %xmm17
	vaddps	%xmm17, %xmm15, %xmm15
	vshufpd	$1, %xmm15, %xmm15, %xmm17      # xmm17 = xmm15[1,0]
	vaddps	%xmm17, %xmm15, %xmm15
	vmovshdup	%xmm15, %xmm17          # xmm17 = xmm15[1,1,3,3]
	vaddss	%xmm17, %xmm15, %xmm15
	vaddss	%xmm27, %xmm15, %xmm15
	vinsertps	$16, %xmm14, %xmm15, %xmm14 # xmm14 = xmm15[0],xmm14[0],xmm15[2,3]
	vinsertps	$32, %xmm12, %xmm14, %xmm12 # xmm12 = xmm14[0,1],xmm12[0],xmm14[3]
	vmovaps	10048(%rsp), %zmm0              # 64-byte Reload
	vextractf64x4	$1, %zmm0, %ymm14
	vaddps	%zmm14, %zmm0, %zmm0
	vextractf128	$1, %ymm0, %xmm14
	vaddps	%xmm0, %xmm14, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm14        # xmm14 = xmm0[1,0]
	vaddps	%xmm0, %xmm14, %xmm0
	vmovshdup	%xmm0, %xmm14           # xmm14 = xmm0[1,1,3,3]
	vaddss	%xmm0, %xmm14, %xmm0
	vaddss	%xmm27, %xmm0, %xmm0
	vinsertps	$48, %xmm0, %xmm12, %xmm0 # xmm0 = xmm12[0,1,2],xmm0[0]
	vmovaps	%ymm0, 17088(%rsp)              # 32-byte Spill
	vmovaps	%zmm25, %zmm21
	vmulps	%zmm25, %zmm22, %zmm12
	vfmadd231ps	%zmm16, %zmm31, %zmm12  # zmm12 = (zmm31 * zmm16) + zmm12
	vextractf64x4	$1, %zmm12, %ymm14
	vaddps	%zmm14, %zmm12, %zmm12
	vextractf128	$1, %ymm12, %xmm14
	vaddps	%xmm14, %xmm12, %xmm12
	vshufpd	$1, %xmm12, %xmm12, %xmm14      # xmm14 = xmm12[1,0]
	vaddps	%xmm14, %xmm12, %xmm12
	vmovaps	35328(%rsp), %zmm0              # 64-byte Reload
	vextractf64x4	$1, %zmm0, %ymm14
	vaddps	%zmm14, %zmm0, %zmm14
	vextractf128	$1, %ymm14, %xmm15
	vaddps	%xmm15, %xmm14, %xmm14
	vshufpd	$1, %xmm14, %xmm14, %xmm15      # xmm15 = xmm14[1,0]
	vaddps	%xmm15, %xmm14, %xmm14
	vmovshdup	%xmm14, %xmm15          # xmm15 = xmm14[1,1,3,3]
	vaddss	%xmm15, %xmm14, %xmm14
	vaddss	%xmm27, %xmm14, %xmm14
	vinsertps	$16, %xmm14, %xmm11, %xmm11 # xmm11 = xmm11[0],xmm14[0],xmm11[2,3]
	vinsertps	$32, %xmm9, %xmm11, %xmm9 # xmm9 = xmm11[0,1],xmm9[0],xmm11[3]
	vinsertps	$48, %xmm7, %xmm9, %xmm7 # xmm7 = xmm9[0,1,2],xmm7[0]
	vmovshdup	%xmm12, %xmm9           # xmm9 = xmm12[1,1,3,3]
	vaddss	%xmm9, %xmm12, %xmm9
	vmulps	8896(%rsp), %zmm22, %zmm11      # 64-byte Folded Reload
	vfmadd231ps	%zmm6, %zmm31, %zmm11   # zmm11 = (zmm31 * zmm6) + zmm11
	vextractf64x4	$1, %zmm11, %ymm12
	vaddps	%zmm12, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm12
	vaddps	%xmm12, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm12      # xmm12 = xmm11[1,0]
	vaddps	%xmm12, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm12          # xmm12 = xmm11[1,1,3,3]
	vaddss	%xmm12, %xmm11, %xmm11
	vaddss	%xmm27, %xmm9, %xmm9
	vaddss	%xmm27, %xmm11, %xmm11
	vinsertps	$16, %xmm11, %xmm9, %xmm9 # xmm9 = xmm9[0],xmm11[0],xmm9[2,3]
	vmovaps	8320(%rsp), %zmm14              # 64-byte Reload
	vmulps	%zmm14, %zmm22, %zmm11
	vmovaps	8448(%rsp), %zmm0               # 64-byte Reload
	vfmadd231ps	%zmm0, %zmm31, %zmm11   # zmm11 = (zmm31 * zmm0) + zmm11
	vextractf64x4	$1, %zmm11, %ymm12
	vaddps	%zmm12, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm12
	vaddps	%xmm12, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm12      # xmm12 = xmm11[1,0]
	vaddps	%xmm12, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm12          # xmm12 = xmm11[1,1,3,3]
	vaddss	%xmm12, %xmm11, %xmm11
	vaddss	%xmm27, %xmm11, %xmm11
	vinsertps	$32, %xmm11, %xmm9, %xmm9 # xmm9 = xmm9[0,1],xmm11[0],xmm9[3]
	vmovaps	8384(%rsp), %zmm15              # 64-byte Reload
	vmulps	%zmm15, %zmm22, %zmm11
	vmovaps	1536(%rsp), %zmm3               # 64-byte Reload
	vfmadd231ps	%zmm3, %zmm31, %zmm11   # zmm11 = (zmm31 * zmm3) + zmm11
	vextractf64x4	$1, %zmm11, %ymm12
	vaddps	%zmm12, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm12
	vaddps	%xmm12, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm12      # xmm12 = xmm11[1,0]
	vaddps	%xmm12, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm12          # xmm12 = xmm11[1,1,3,3]
	vaddss	%xmm12, %xmm11, %xmm11
	vaddss	%xmm27, %xmm11, %xmm11
	vinsertps	$48, %xmm11, %xmm9, %xmm9 # xmm9 = xmm9[0,1,2],xmm11[0]
	vmulps	%zmm4, %zmm28, %zmm11
	vfmadd231ps	%zmm24, %zmm23, %zmm11  # zmm11 = (zmm23 * zmm24) + zmm11
	vextractf64x4	$1, %zmm11, %ymm12
	vaddps	%zmm12, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm12
	vaddps	%xmm12, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm12      # xmm12 = xmm11[1,0]
	vaddps	%xmm12, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm12          # xmm12 = xmm11[1,1,3,3]
	vaddss	%xmm12, %xmm11, %xmm11
	vaddss	%xmm27, %xmm11, %xmm11
	vinsertps	$16, %xmm10, %xmm11, %xmm10 # xmm10 = xmm11[0],xmm10[0],xmm11[2,3]
	vinsertps	$32, 16832(%rsp), %xmm10, %xmm6 # 16-byte Folded Reload
                                        # xmm6 = xmm10[0,1],mem[0],xmm10[3]
	vmovaps	16896(%rsp), %zmm11             # 64-byte Reload
	vextractf64x4	$1, %zmm11, %ymm10
	vaddps	%zmm10, %zmm11, %zmm10
	vextractf128	$1, %ymm10, %xmm11
	vaddps	%xmm11, %xmm10, %xmm10
	vshufpd	$1, %xmm10, %xmm10, %xmm11      # xmm11 = xmm10[1,0]
	vaddps	%xmm11, %xmm10, %xmm10
	vmovshdup	%xmm10, %xmm11          # xmm11 = xmm10[1,1,3,3]
	vaddss	%xmm11, %xmm10, %xmm10
	vaddss	%xmm27, %xmm10, %xmm10
	vinsertps	$48, %xmm10, %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],xmm10[0]
	vmulps	%zmm20, %zmm28, %zmm10
	vfmadd231ps	%zmm1, %zmm23, %zmm10   # zmm10 = (zmm23 * zmm1) + zmm10
	vextractf64x4	$1, %zmm10, %ymm11
	vaddps	%zmm11, %zmm10, %zmm10
	vextractf128	$1, %ymm10, %xmm11
	vaddps	%xmm11, %xmm10, %xmm10
	vshufpd	$1, %xmm10, %xmm10, %xmm11      # xmm11 = xmm10[1,0]
	vaddps	%xmm11, %xmm10, %xmm10
	vmovshdup	%xmm10, %xmm11          # xmm11 = xmm10[1,1,3,3]
	vaddss	%xmm11, %xmm10, %xmm10
	vaddss	%xmm27, %xmm10, %xmm10
	vmulps	%zmm13, %zmm28, %zmm11
	vfmadd231ps	%zmm5, %zmm23, %zmm11   # zmm11 = (zmm23 * zmm5) + zmm11
	vmovaps	%zmm23, %zmm4
	vextractf64x4	$1, %zmm11, %ymm12
	vaddps	%zmm12, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm12
	vaddps	%xmm12, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm12      # xmm12 = xmm11[1,0]
	vaddps	%xmm12, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm12          # xmm12 = xmm11[1,1,3,3]
	vaddss	%xmm12, %xmm11, %xmm11
	vinsertf128	$1, %xmm7, %ymm6, %ymm1
	vmovaps	%zmm1, 16832(%rsp)              # 64-byte Spill
	vaddss	%xmm27, %xmm11, %xmm6
	vmovaps	3072(%rsp), %zmm25              # 64-byte Reload
	vmulps	%zmm25, %zmm22, %zmm7
	vmovaps	1472(%rsp), %zmm26              # 64-byte Reload
	vfmadd231ps	%zmm26, %zmm31, %zmm7   # zmm7 = (zmm31 * zmm26) + zmm7
	vextractf64x4	$1, %zmm7, %ymm11
	vaddps	%zmm11, %zmm7, %zmm7
	vmulps	%zmm19, %zmm28, %zmm11
	vfmadd231ps	%zmm2, %zmm23, %zmm11   # zmm11 = (zmm23 * zmm2) + zmm11
	vextractf64x4	$1, %zmm11, %ymm12
	vaddps	%zmm12, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm12
	vaddps	%xmm12, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm12      # xmm12 = xmm11[1,0]
	vaddps	%xmm12, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm12          # xmm12 = xmm11[1,1,3,3]
	vaddss	%xmm12, %xmm11, %xmm11
	vaddss	%xmm27, %xmm11, %xmm11
	vinsertps	$16, %xmm11, %xmm6, %xmm6 # xmm6 = xmm6[0],xmm11[0],xmm6[2,3]
	vinsertps	$32, %xmm10, %xmm6, %xmm6 # xmm6 = xmm6[0,1],xmm10[0],xmm6[3]
	vinsertps	$48, 352(%rsp), %xmm6, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm6[0,1,2],mem[0]
	vmovaps	%xmm1, 35136(%rsp)              # 16-byte Spill
	vmovaps	2432(%rsp), %zmm18              # 64-byte Reload
	vmulps	%zmm18, %zmm28, %zmm6
	vmovaps	1792(%rsp), %zmm5               # 64-byte Reload
	vfmadd231ps	%zmm23, %zmm5, %zmm6    # zmm6 = (zmm5 * zmm23) + zmm6
	vextractf64x4	$1, %zmm6, %ymm10
	vaddps	%zmm10, %zmm6, %zmm6
	vextractf128	$1, %ymm6, %xmm10
	vaddps	%xmm6, %xmm10, %xmm6
	vshufpd	$1, %xmm6, %xmm6, %xmm10        # xmm10 = xmm6[1,0]
	vaddps	%xmm6, %xmm10, %xmm6
	vmovshdup	%xmm6, %xmm10           # xmm10 = xmm6[1,1,3,3]
	vaddss	%xmm6, %xmm10, %xmm6
	vaddss	%xmm27, %xmm6, %xmm19
	vextractf128	$1, %ymm7, %xmm10
	vaddps	%xmm7, %xmm10, %xmm11
	vmovaps	2176(%rsp), %zmm2               # 64-byte Reload
	vmulps	%zmm2, %zmm28, %zmm7
	vmovaps	1728(%rsp), %zmm8               # 64-byte Reload
	vfmadd231ps	%zmm8, %zmm23, %zmm7    # zmm7 = (zmm23 * zmm8) + zmm7
	vextractf64x4	$1, %zmm7, %ymm10
	vaddps	%zmm10, %zmm7, %zmm7
	vextractf128	$1, %ymm7, %xmm10
	vaddps	%xmm7, %xmm10, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm10        # xmm10 = xmm7[1,0]
	vaddps	%xmm7, %xmm10, %xmm7
	vmovshdup	%xmm7, %xmm10           # xmm10 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm10, %xmm7
	vaddss	%xmm27, %xmm7, %xmm7
	vmulps	%zmm15, %zmm28, %zmm10
	vfmadd231ps	%zmm3, %zmm23, %zmm10   # zmm10 = (zmm23 * zmm3) + zmm10
	vextractf64x4	$1, %zmm10, %ymm12
	vaddps	%zmm12, %zmm10, %zmm10
	vextractf128	$1, %ymm10, %xmm12
	vaddps	%xmm12, %xmm10, %xmm10
	vshufpd	$1, %xmm10, %xmm10, %xmm12      # xmm12 = xmm10[1,0]
	vaddps	%xmm12, %xmm10, %xmm10
	vmovshdup	%xmm10, %xmm12          # xmm12 = xmm10[1,1,3,3]
	vaddss	%xmm12, %xmm10, %xmm10
	vaddss	%xmm27, %xmm10, %xmm10
	vshufpd	$1, %xmm11, %xmm11, %xmm12      # xmm12 = xmm11[1,0]
	vaddps	%xmm12, %xmm11, %xmm12
	vmulps	%zmm14, %zmm28, %zmm11
	vfmadd231ps	%zmm0, %zmm23, %zmm11   # zmm11 = (zmm23 * zmm0) + zmm11
	vextractf64x4	$1, %zmm11, %ymm14
	vaddps	%zmm14, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm14
	vaddps	%xmm14, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm14      # xmm14 = xmm11[1,0]
	vaddps	%xmm14, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm14          # xmm14 = xmm11[1,1,3,3]
	vaddss	%xmm14, %xmm11, %xmm11
	vaddss	%xmm27, %xmm11, %xmm6
	vmovshdup	%xmm12, %xmm14          # xmm14 = xmm12[1,1,3,3]
	vaddss	%xmm14, %xmm12, %xmm0
	vmovss	%xmm0, 16896(%rsp)              # 4-byte Spill
	vmovaps	2112(%rsp), %zmm13              # 64-byte Reload
	vmulps	%zmm13, %zmm22, %zmm14
	vmovaps	2496(%rsp), %zmm30              # 64-byte Reload
	vfmadd231ps	%zmm30, %zmm31, %zmm14  # zmm14 = (zmm31 * zmm30) + zmm14
	vextractf64x4	$1, %zmm14, %ymm15
	vaddps	%zmm15, %zmm14, %zmm14
	vmulps	%zmm18, %zmm22, %zmm15
	vfmadd231ps	%zmm31, %zmm5, %zmm15   # zmm15 = (zmm5 * zmm31) + zmm15
	vextractf64x4	$1, %zmm15, %ymm17
	vaddps	%zmm17, %zmm15, %zmm15
	vextractf32x4	$1, %ymm15, %xmm17
	vaddps	%xmm17, %xmm15, %xmm15
	vshufpd	$1, %xmm15, %xmm15, %xmm17      # xmm17 = xmm15[1,0]
	vaddps	%xmm17, %xmm15, %xmm15
	vmovshdup	%xmm15, %xmm17          # xmm17 = xmm15[1,1,3,3]
	vaddss	%xmm17, %xmm15, %xmm15
	vaddss	%xmm27, %xmm15, %xmm15
	vextractf32x4	$1, %ymm14, %xmm17
	vaddps	%xmm17, %xmm14, %xmm14
	vmulps	%zmm2, %zmm22, %zmm17
	vfmadd231ps	%zmm8, %zmm31, %zmm17   # zmm17 = (zmm31 * zmm8) + zmm17
	vextractf64x4	$1, %zmm17, %ymm18
	vaddps	%zmm18, %zmm17, %zmm17
	vextractf32x4	$1, %ymm17, %xmm18
	vaddps	%xmm18, %xmm17, %xmm17
	vshufpd	$1, %xmm17, %xmm17, %xmm18      # xmm18 = xmm17[1,0]
	vaddps	%xmm18, %xmm17, %xmm17
	vmovshdup	%xmm17, %xmm18          # xmm18 = xmm17[1,1,3,3]
	vaddss	%xmm18, %xmm17, %xmm17
	vaddss	%xmm27, %xmm17, %xmm18
	vmovaps	192(%rsp), %zmm5                # 64-byte Reload
	vmulps	%zmm5, %zmm22, %zmm17
	vmovaps	%zmm22, %zmm2
	vmovaps	1344(%rsp), %zmm1               # 64-byte Reload
	vfmadd231ps	%zmm1, %zmm31, %zmm17   # zmm17 = (zmm31 * zmm1) + zmm17
	vextractf64x4	$1, %zmm17, %ymm20
	vaddps	%zmm20, %zmm17, %zmm17
	vextractf32x4	$1, %ymm17, %xmm20
	vaddps	%xmm20, %xmm17, %xmm17
	vshufpd	$1, %xmm17, %xmm17, %xmm20      # xmm20 = xmm17[1,0]
	vaddps	%xmm20, %xmm17, %xmm17
	vmovshdup	%xmm17, %xmm20          # xmm20 = xmm17[1,1,3,3]
	vaddss	%xmm20, %xmm17, %xmm17
	vaddss	%xmm27, %xmm17, %xmm22
	vshufpd	$1, %xmm14, %xmm14, %xmm17      # xmm17 = xmm14[1,0]
	vaddps	%xmm17, %xmm14, %xmm14
	vmovshdup	%xmm14, %xmm17          # xmm17 = xmm14[1,1,3,3]
	vaddss	%xmm17, %xmm14, %xmm17
	vmovaps	3136(%rsp), %zmm12              # 64-byte Reload
	vmulps	%zmm12, %zmm2, %zmm14
	vmovaps	1856(%rsp), %zmm11              # 64-byte Reload
	vfmadd231ps	%zmm11, %zmm31, %zmm14  # zmm14 = (zmm31 * zmm11) + zmm14
	vextractf64x4	$1, %zmm14, %ymm20
	vaddps	%zmm20, %zmm14, %zmm14
	vextractf32x4	$1, %ymm14, %xmm20
	vaddps	%xmm20, %xmm14, %xmm14
	vmovaps	34304(%rsp), %zmm0              # 64-byte Reload
	vextractf64x4	$1, %zmm0, %ymm20
	vaddps	%zmm20, %zmm0, %zmm20
	vextractf32x4	$1, %ymm20, %xmm23
	vaddps	%xmm23, %xmm20, %xmm20
	vshufpd	$1, %xmm20, %xmm20, %xmm23      # xmm23 = xmm20[1,0]
	vaddps	%xmm23, %xmm20, %xmm20
	vmovshdup	%xmm20, %xmm23          # xmm23 = xmm20[1,1,3,3]
	vaddss	%xmm23, %xmm20, %xmm20
	vaddss	%xmm27, %xmm20, %xmm23
	vshufpd	$1, %xmm14, %xmm14, %xmm20      # xmm20 = xmm14[1,0]
	vaddps	%xmm20, %xmm14, %xmm14
	vmovshdup	%xmm14, %xmm20          # xmm20 = xmm14[1,1,3,3]
	vaddss	%xmm20, %xmm14, %xmm20
	vmovaps	2240(%rsp), %zmm3               # 64-byte Reload
	vmulps	%zmm3, %zmm2, %zmm14
	vmovaps	1088(%rsp), %zmm2               # 64-byte Reload
	vfmadd231ps	%zmm2, %zmm31, %zmm14   # zmm14 = (zmm31 * zmm2) + zmm14
	vextractf64x4	$1, %zmm14, %ymm24
	vaddps	%zmm24, %zmm14, %zmm14
	vextractf32x4	$1, %ymm14, %xmm24
	vaddps	%xmm24, %xmm14, %xmm14
	vshufpd	$1, %xmm14, %xmm14, %xmm24      # xmm24 = xmm14[1,0]
	vaddps	%xmm24, %xmm14, %xmm14
	vmovshdup	%xmm14, %xmm24          # xmm24 = xmm14[1,1,3,3]
	vaddss	%xmm24, %xmm14, %xmm14
	vaddss	%xmm27, %xmm14, %xmm14
	vinsertps	$16, %xmm14, %xmm22, %xmm14 # xmm14 = xmm22[0],xmm14[0],xmm22[2,3]
	vinsertps	$32, %xmm18, %xmm14, %xmm14 # xmm14 = xmm14[0,1],xmm18[0],xmm14[3]
	vinsertps	$48, %xmm15, %xmm14, %xmm14 # xmm14 = xmm14[0,1,2],xmm15[0]
	vmovaps	%xmm14, 34880(%rsp)             # 16-byte Spill
	vmovaps	17088(%rsp), %ymm0              # 32-byte Reload
	vinsertf128	$1, %xmm9, %ymm0, %ymm0
	vmovaps	%zmm0, 34816(%rsp)              # 64-byte Spill
	vmovaps	16640(%rsp), %ymm0              # 32-byte Reload
	vinsertf128	$1, 16512(%rsp), %ymm0, %ymm0 # 16-byte Folded Reload
	vmovaps	%zmm0, 16640(%rsp)              # 64-byte Spill
	vmulps	448(%rsp), %zmm28, %zmm0        # 64-byte Folded Reload
	vfmadd231ps	512(%rsp), %zmm4, %zmm0 # 64-byte Folded Reload
                                        # zmm0 = (zmm4 * mem) + zmm0
	vextractf64x4	$1, %zmm0, %ymm8
	vaddps	%zmm8, %zmm0, %zmm0
	vextractf128	$1, %ymm0, %xmm8
	vaddps	%xmm0, %xmm8, %xmm0
	vmovaps	34688(%rsp), %zmm9              # 64-byte Reload
	vextractf64x4	$1, %zmm9, %ymm8
	vaddps	%zmm8, %zmm9, %zmm8
	vextractf128	$1, %ymm8, %xmm9
	vaddps	%xmm9, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm9         # xmm9 = xmm8[1,0]
	vaddps	%xmm9, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm9            # xmm9 = xmm8[1,1,3,3]
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm27, %xmm8, %xmm8
	vshufpd	$1, %xmm0, %xmm0, %xmm9         # xmm9 = xmm0[1,0]
	vaddps	%xmm0, %xmm9, %xmm0
	vmulps	%zmm21, %zmm28, %zmm9
	vfmadd231ps	%zmm16, %zmm4, %zmm9    # zmm9 = (zmm4 * zmm16) + zmm9
	vextractf64x4	$1, %zmm9, %ymm14
	vaddps	%zmm14, %zmm9, %zmm9
	vextractf128	$1, %ymm9, %xmm14
	vaddps	%xmm14, %xmm9, %xmm9
	vshufpd	$1, %xmm9, %xmm9, %xmm14        # xmm14 = xmm9[1,0]
	vaddps	%xmm14, %xmm9, %xmm9
	vmovshdup	%xmm9, %xmm14           # xmm14 = xmm9[1,1,3,3]
	vaddss	%xmm14, %xmm9, %xmm9
	vaddss	%xmm27, %xmm9, %xmm9
	vmovshdup	%xmm0, %xmm14           # xmm14 = xmm0[1,1,3,3]
	vaddss	%xmm0, %xmm14, %xmm0
	vmulps	%zmm29, %zmm28, %zmm14
	vfmadd231ps	64(%rsp), %zmm4, %zmm14 # 64-byte Folded Reload
                                        # zmm14 = (zmm4 * mem) + zmm14
	vextractf64x4	$1, %zmm14, %ymm15
	vaddps	%zmm15, %zmm14, %zmm14
	vextractf128	$1, %ymm14, %xmm15
	vaddps	%xmm15, %xmm14, %xmm14
	vshufpd	$1, %xmm14, %xmm14, %xmm15      # xmm15 = xmm14[1,0]
	vaddps	%xmm15, %xmm14, %xmm14
	vmovshdup	%xmm14, %xmm15          # xmm15 = xmm14[1,1,3,3]
	vaddss	%xmm15, %xmm14, %xmm14
	vaddss	%xmm27, %xmm14, %xmm14
	vinsertps	$16, %xmm8, %xmm14, %xmm8 # xmm8 = xmm14[0],xmm8[0],xmm14[2,3]
	vinsertps	$32, %xmm23, %xmm8, %xmm8 # xmm8 = xmm8[0,1],xmm23[0],xmm8[3]
	vmovaps	16192(%rsp), %zmm15             # 64-byte Reload
	vextractf64x4	$1, %zmm15, %ymm14
	vaddps	%zmm14, %zmm15, %zmm14
	vextractf128	$1, %ymm14, %xmm15
	vaddps	%xmm15, %xmm14, %xmm14
	vshufpd	$1, %xmm14, %xmm14, %xmm15      # xmm15 = xmm14[1,0]
	vaddps	%xmm15, %xmm14, %xmm14
	vmovshdup	%xmm14, %xmm15          # xmm15 = xmm14[1,1,3,3]
	vaddss	%xmm15, %xmm14, %xmm14
	vaddss	%xmm27, %xmm14, %xmm14
	vinsertps	$48, %xmm14, %xmm8, %xmm8 # xmm8 = xmm8[0,1,2],xmm14[0]
	vmulps	%zmm5, %zmm28, %zmm14
	vfmadd231ps	%zmm1, %zmm4, %zmm14    # zmm14 = (zmm4 * zmm1) + zmm14
	vextractf64x4	$1, %zmm14, %ymm15
	vaddps	%zmm15, %zmm14, %zmm14
	vextractf128	$1, %ymm14, %xmm15
	vaddps	%xmm15, %xmm14, %xmm14
	vshufpd	$1, %xmm14, %xmm14, %xmm15      # xmm15 = xmm14[1,0]
	vaddps	%xmm15, %xmm14, %xmm14
	vmovshdup	%xmm14, %xmm15          # xmm15 = xmm14[1,1,3,3]
	vaddss	%xmm15, %xmm14, %xmm14
	vaddss	%xmm27, %xmm14, %xmm14
	vmovaps	8896(%rsp), %zmm5               # 64-byte Reload
	vmulps	%zmm5, %zmm28, %zmm15
	vfmadd231ps	8960(%rsp), %zmm4, %zmm15 # 64-byte Folded Reload
                                        # zmm15 = (zmm4 * mem) + zmm15
	vextractf64x4	$1, %zmm15, %ymm16
	vaddps	%zmm16, %zmm15, %zmm15
	vextractf32x4	$1, %ymm15, %xmm16
	vaddps	%xmm16, %xmm15, %xmm15
	vshufpd	$1, %xmm15, %xmm15, %xmm16      # xmm16 = xmm15[1,0]
	vaddps	%xmm16, %xmm15, %xmm15
	vmovshdup	%xmm15, %xmm16          # xmm16 = xmm15[1,1,3,3]
	vaddss	%xmm16, %xmm15, %xmm15
	vaddss	%xmm27, %xmm15, %xmm15
	vinsertps	$16, %xmm15, %xmm9, %xmm9 # xmm9 = xmm9[0],xmm15[0],xmm9[2,3]
	vinsertps	$32, %xmm6, %xmm9, %xmm9 # xmm9 = xmm9[0,1],xmm6[0],xmm9[3]
	vinsertps	$48, %xmm10, %xmm9, %xmm9 # xmm9 = xmm9[0,1,2],xmm10[0]
	vinsertf128	$1, %xmm9, %ymm8, %ymm1
	vmovaps	%zmm1, 34688(%rsp)              # 64-byte Spill
	vmulps	2048(%rsp), %zmm28, %zmm8       # 64-byte Folded Reload
	vfmadd231ps	704(%rsp), %zmm4, %zmm8 # 64-byte Folded Reload
                                        # zmm8 = (zmm4 * mem) + zmm8
	vextractf64x4	$1, %zmm8, %ymm9
	vaddps	%zmm9, %zmm8, %zmm8
	vextractf128	$1, %ymm8, %xmm9
	vaddps	%xmm9, %xmm8, %xmm8
	vmulps	%zmm3, %zmm28, %zmm9
	vfmadd231ps	%zmm2, %zmm4, %zmm9     # zmm9 = (zmm4 * zmm2) + zmm9
	vextractf64x4	$1, %zmm9, %ymm10
	vaddps	%zmm10, %zmm9, %zmm9
	vextractf128	$1, %ymm9, %xmm10
	vaddps	%xmm10, %xmm9, %xmm9
	vshufpd	$1, %xmm9, %xmm9, %xmm10        # xmm10 = xmm9[1,0]
	vaddps	%xmm10, %xmm9, %xmm9
	vmovshdup	%xmm9, %xmm10           # xmm10 = xmm9[1,1,3,3]
	vaddss	%xmm10, %xmm9, %xmm9
	vaddss	%xmm27, %xmm9, %xmm9
	vinsertps	$16, %xmm9, %xmm14, %xmm9 # xmm9 = xmm14[0],xmm9[0],xmm14[2,3]
	vinsertps	$32, %xmm7, %xmm9, %xmm7 # xmm7 = xmm9[0,1],xmm7[0],xmm9[3]
	vinsertps	$48, %xmm19, %xmm7, %xmm1 # xmm1 = xmm7[0,1,2],xmm19[0]
	vmovaps	%xmm1, 16512(%rsp)              # 16-byte Spill
	vshufpd	$1, %xmm8, %xmm8, %xmm6         # xmm6 = xmm8[1,0]
	vaddps	%xmm6, %xmm8, %xmm6
	vmulps	%zmm12, %zmm28, %zmm7
	vfmadd231ps	%zmm11, %zmm4, %zmm7    # zmm7 = (zmm4 * zmm11) + zmm7
	vextractf64x4	$1, %zmm7, %ymm8
	vaddps	%zmm8, %zmm7, %zmm7
	vextractf128	$1, %ymm7, %xmm8
	vaddps	%xmm7, %xmm8, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm8         # xmm8 = xmm7[1,0]
	vaddps	%xmm7, %xmm8, %xmm7
	vmovshdup	%xmm7, %xmm8            # xmm8 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm27, %xmm7, %xmm1
	vmovaps	%xmm1, 34560(%rsp)              # 16-byte Spill
	vmulps	%zmm13, %zmm28, %zmm7
	vfmadd231ps	%zmm30, %zmm4, %zmm7    # zmm7 = (zmm4 * zmm30) + zmm7
	vextractf64x4	$1, %zmm7, %ymm8
	vaddps	%zmm8, %zmm7, %zmm7
	vextractf128	$1, %ymm7, %xmm8
	vaddps	%xmm7, %xmm8, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm8         # xmm8 = xmm7[1,0]
	vaddps	%xmm7, %xmm8, %xmm7
	vmovshdup	%xmm7, %xmm8            # xmm8 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm27, %xmm7, %xmm1
	vmovaps	%xmm1, 34496(%rsp)              # 16-byte Spill
	vmovshdup	%xmm6, %xmm7            # xmm7 = xmm6[1,1,3,3]
	vaddss	%xmm7, %xmm6, %xmm19
	vmulps	%zmm25, %zmm28, %zmm6
	vfmadd231ps	%zmm26, %zmm4, %zmm6    # zmm6 = (zmm4 * zmm26) + zmm6
	vextractf64x4	$1, %zmm6, %ymm7
	vaddps	%zmm7, %zmm6, %zmm6
	vextractf128	$1, %ymm6, %xmm7
	vaddps	%xmm7, %xmm6, %xmm6
	vshufpd	$1, %xmm6, %xmm6, %xmm7         # xmm7 = xmm6[1,0]
	vaddps	%xmm7, %xmm6, %xmm6
	vmovshdup	%xmm6, %xmm7            # xmm7 = xmm6[1,1,3,3]
	vaddss	%xmm7, %xmm6, %xmm6
	vaddss	%xmm27, %xmm6, %xmm1
	vmovaps	%xmm1, 34368(%rsp)              # 16-byte Spill
	vmulps	256(%rsp), %zmm28, %zmm6        # 64-byte Folded Reload
	vfmadd231ps	1408(%rsp), %zmm4, %zmm6 # 64-byte Folded Reload
                                        # zmm6 = (zmm4 * mem) + zmm6
	vextractf64x4	$1, %zmm6, %ymm7
	vaddps	%zmm7, %zmm6, %zmm6
	vextractf128	$1, %ymm6, %xmm7
	vaddps	%xmm7, %xmm6, %xmm6
	vshufpd	$1, %xmm6, %xmm6, %xmm7         # xmm7 = xmm6[1,0]
	vaddps	%xmm7, %xmm6, %xmm6
	vmovshdup	%xmm6, %xmm7            # xmm7 = xmm6[1,1,3,3]
	vaddss	%xmm7, %xmm6, %xmm22
	vxorps	%xmm2, %xmm2, %xmm2
	vaddss	2720(%rsp), %xmm2, %xmm1        # 4-byte Folded Reload
	vmovaps	%xmm1, 9088(%rsp)               # 16-byte Spill
	vaddss	16256(%rsp), %xmm2, %xmm1       # 4-byte Folded Reload
	vmovaps	%xmm1, 9280(%rsp)               # 16-byte Spill
	vaddss	400(%rsp), %xmm2, %xmm1         # 4-byte Folded Reload
	vmovaps	%xmm1, 12416(%rsp)              # 16-byte Spill
	vaddss	2704(%rsp), %xmm2, %xmm1        # 4-byte Folded Reload
	vmovaps	%xmm1, 9472(%rsp)               # 16-byte Spill
	vaddss	2688(%rsp), %xmm2, %xmm1        # 4-byte Folded Reload
	vmovaps	%xmm1, 9408(%rsp)               # 16-byte Spill
	vaddss	368(%rsp), %xmm2, %xmm1         # 4-byte Folded Reload
	vmovaps	%xmm1, 9344(%rsp)               # 16-byte Spill
	vaddss	2672(%rsp), %xmm2, %xmm1        # 4-byte Folded Reload
	vmovaps	%xmm1, 9792(%rsp)               # 16-byte Spill
	vaddss	14208(%rsp), %xmm2, %xmm1       # 4-byte Folded Reload
	vmovaps	%xmm1, 12672(%rsp)              # 16-byte Spill
	vaddss	16384(%rsp), %xmm2, %xmm1       # 4-byte Folded Reload
	vmovaps	%xmm1, 9536(%rsp)               # 16-byte Spill
	vaddss	10112(%rsp), %xmm2, %xmm1       # 4-byte Folded Reload
	vmovaps	%xmm1, 13056(%rsp)              # 16-byte Spill
	vaddss	2656(%rsp), %xmm2, %xmm1        # 4-byte Folded Reload
	vmovaps	%xmm1, 12992(%rsp)              # 16-byte Spill
	vaddss	10368(%rsp), %xmm2, %xmm1       # 4-byte Folded Reload
	vmovaps	%xmm1, 2704(%rsp)               # 16-byte Spill
	vaddss	16576(%rsp), %xmm2, %xmm1       # 4-byte Folded Reload
	vmovaps	%xmm1, 2688(%rsp)               # 16-byte Spill
	vaddss	16448(%rsp), %xmm2, %xmm1       # 4-byte Folded Reload
	vmovaps	%xmm1, 16192(%rsp)              # 16-byte Spill
	vaddss	16896(%rsp), %xmm2, %xmm1       # 4-byte Folded Reload
	vmovaps	%xmm1, 16256(%rsp)              # 16-byte Spill
	vaddss	%xmm2, %xmm17, %xmm1
	vmovaps	%xmm1, 2720(%rsp)               # 16-byte Spill
	vaddss	%xmm2, %xmm20, %xmm1
	vmovaps	%xmm1, 400(%rsp)                # 16-byte Spill
	vaddss	%xmm2, %xmm0, %xmm0
	vmovaps	%xmm0, 34304(%rsp)              # 16-byte Spill
	vaddss	%xmm2, %xmm19, %xmm0
	vmovaps	%xmm0, 16448(%rsp)              # 16-byte Spill
	vaddss	%xmm2, %xmm22, %xmm0
	vmovaps	%xmm0, 16384(%rsp)              # 16-byte Spill
	vmovaps	5888(%rsp), %zmm17              # 64-byte Reload
	vmovaps	%zmm5, %zmm2
	vmulps	%zmm5, %zmm17, %zmm10
	vmulps	3712(%rsp), %zmm5, %zmm9        # 64-byte Folded Reload
	vmulps	3840(%rsp), %zmm5, %zmm21       # 64-byte Folded Reload
	vmulps	3968(%rsp), %zmm5, %zmm8        # 64-byte Folded Reload
	vmulps	4096(%rsp), %zmm5, %zmm7        # 64-byte Folded Reload
	vmulps	4224(%rsp), %zmm5, %zmm6        # 64-byte Folded Reload
	vmulps	6016(%rsp), %zmm5, %zmm5        # 64-byte Folded Reload
	vmulps	4352(%rsp), %zmm2, %zmm4        # 64-byte Folded Reload
	vmulps	4480(%rsp), %zmm2, %zmm1        # 64-byte Folded Reload
	vmulps	4608(%rsp), %zmm2, %zmm26       # 64-byte Folded Reload
	vmulps	3200(%rsp), %zmm2, %zmm12       # 64-byte Folded Reload
	vmulps	3328(%rsp), %zmm2, %zmm30       # 64-byte Folded Reload
	vmulps	3456(%rsp), %zmm2, %zmm25       # 64-byte Folded Reload
	vmulps	3584(%rsp), %zmm2, %zmm24       # 64-byte Folded Reload
	vmulps	4736(%rsp), %zmm2, %zmm22       # 64-byte Folded Reload
	vmulps	6144(%rsp), %zmm2, %zmm0        # 64-byte Folded Reload
	vmovaps	7744(%rsp), %zmm29              # 64-byte Reload
	vmulps	%zmm2, %zmm29, %zmm11
	vmovaps	6336(%rsp), %zmm18              # 64-byte Reload
	vmulps	%zmm2, %zmm18, %zmm28
	vmulps	4864(%rsp), %zmm2, %zmm16       # 64-byte Folded Reload
	vmulps	8064(%rsp), %zmm2, %zmm15       # 64-byte Folded Reload
	vmulps	6528(%rsp), %zmm2, %zmm23       # 64-byte Folded Reload
	vmulps	6592(%rsp), %zmm2, %zmm19       # 64-byte Folded Reload
	vmulps	5056(%rsp), %zmm2, %zmm13       # 64-byte Folded Reload
	vmulps	5184(%rsp), %zmm2, %zmm20       # 64-byte Folded Reload
	vmulps	5312(%rsp), %zmm2, %zmm14       # 64-byte Folded Reload
	vmulps	7040(%rsp), %zmm2, %zmm3        # 64-byte Folded Reload
	vmovaps	8960(%rsp), %zmm2               # 64-byte Reload
	vmovaps	5952(%rsp), %zmm31              # 64-byte Reload
	vfmadd231ps	%zmm2, %zmm31, %zmm10   # zmm10 = (zmm31 * zmm2) + zmm10
	vmovaps	%zmm10, 28928(%rsp)             # 64-byte Spill
	vfmadd231ps	3776(%rsp), %zmm2, %zmm9 # 64-byte Folded Reload
                                        # zmm9 = (zmm2 * mem) + zmm9
	vmovaps	%zmm9, 29120(%rsp)              # 64-byte Spill
	vfmadd231ps	3904(%rsp), %zmm2, %zmm21 # 64-byte Folded Reload
                                        # zmm21 = (zmm2 * mem) + zmm21
	vmovaps	%zmm21, 29312(%rsp)             # 64-byte Spill
	vfmadd231ps	4032(%rsp), %zmm2, %zmm8 # 64-byte Folded Reload
                                        # zmm8 = (zmm2 * mem) + zmm8
	vmovaps	%zmm8, 29568(%rsp)              # 64-byte Spill
	vfmadd231ps	4160(%rsp), %zmm2, %zmm7 # 64-byte Folded Reload
                                        # zmm7 = (zmm2 * mem) + zmm7
	vmovaps	%zmm7, 29824(%rsp)              # 64-byte Spill
	vfmadd231ps	4288(%rsp), %zmm2, %zmm6 # 64-byte Folded Reload
                                        # zmm6 = (zmm2 * mem) + zmm6
	vmovaps	%zmm6, 30016(%rsp)              # 64-byte Spill
	vfmadd231ps	6080(%rsp), %zmm2, %zmm5 # 64-byte Folded Reload
                                        # zmm5 = (zmm2 * mem) + zmm5
	vmovaps	%zmm5, 30272(%rsp)              # 64-byte Spill
	vfmadd231ps	4416(%rsp), %zmm2, %zmm4 # 64-byte Folded Reload
                                        # zmm4 = (zmm2 * mem) + zmm4
	vmovaps	%zmm4, 30528(%rsp)              # 64-byte Spill
	vfmadd231ps	4544(%rsp), %zmm2, %zmm1 # 64-byte Folded Reload
                                        # zmm1 = (zmm2 * mem) + zmm1
	vmovaps	%zmm1, 30848(%rsp)              # 64-byte Spill
	vfmadd231ps	4672(%rsp), %zmm2, %zmm26 # 64-byte Folded Reload
                                        # zmm26 = (zmm2 * mem) + zmm26
	vmovaps	%zmm26, 12352(%rsp)             # 64-byte Spill
	vfmadd231ps	3264(%rsp), %zmm2, %zmm12 # 64-byte Folded Reload
                                        # zmm12 = (zmm2 * mem) + zmm12
	vmovaps	%zmm12, 12608(%rsp)             # 64-byte Spill
	vfmadd231ps	3392(%rsp), %zmm2, %zmm30 # 64-byte Folded Reload
                                        # zmm30 = (zmm2 * mem) + zmm30
	vmovaps	%zmm30, 31232(%rsp)             # 64-byte Spill
	vfmadd231ps	3520(%rsp), %zmm2, %zmm25 # 64-byte Folded Reload
                                        # zmm25 = (zmm2 * mem) + zmm25
	vmovaps	%zmm25, 31296(%rsp)             # 64-byte Spill
	vfmadd231ps	3648(%rsp), %zmm2, %zmm24 # 64-byte Folded Reload
                                        # zmm24 = (zmm2 * mem) + zmm24
	vmovaps	%zmm24, 13760(%rsp)             # 64-byte Spill
	vfmadd231ps	4800(%rsp), %zmm2, %zmm22 # 64-byte Folded Reload
                                        # zmm22 = (zmm2 * mem) + zmm22
	vmovaps	%zmm22, 14144(%rsp)             # 64-byte Spill
	vfmadd231ps	6208(%rsp), %zmm2, %zmm0 # 64-byte Folded Reload
                                        # zmm0 = (zmm2 * mem) + zmm0
	vmovaps	%zmm0, 10112(%rsp)              # 64-byte Spill
	vmovaps	6272(%rsp), %zmm0               # 64-byte Reload
	vfmadd231ps	%zmm2, %zmm0, %zmm11    # zmm11 = (zmm0 * zmm2) + zmm11
	vmovaps	%zmm11, 14208(%rsp)             # 64-byte Spill
	vaddss	384(%rsp), %xmm27, %xmm22       # 4-byte Folded Reload
	vmovaps	6400(%rsp), %zmm5               # 64-byte Reload
	vfmadd231ps	%zmm2, %zmm5, %zmm28    # zmm28 = (zmm5 * zmm2) + zmm28
	vmovaps	6784(%rsp), %zmm6               # 64-byte Reload
	vfmadd231ps	%zmm2, %zmm6, %zmm16    # zmm16 = (zmm6 * zmm2) + zmm16
	vmovaps	%zmm16, %zmm26
	vfmadd231ps	8128(%rsp), %zmm2, %zmm15 # 64-byte Folded Reload
                                        # zmm15 = (zmm2 * mem) + zmm15
	vmovaps	%zmm15, 16576(%rsp)             # 64-byte Spill
	vfmadd231ps	4928(%rsp), %zmm2, %zmm23 # 64-byte Folded Reload
                                        # zmm23 = (zmm2 * mem) + zmm23
	vmovaps	%zmm23, 35328(%rsp)             # 64-byte Spill
	vfmadd231ps	4992(%rsp), %zmm2, %zmm19 # 64-byte Folded Reload
                                        # zmm19 = (zmm2 * mem) + zmm19
	vmovaps	%zmm19, 16896(%rsp)             # 64-byte Spill
	vfmadd231ps	5120(%rsp), %zmm2, %zmm13 # 64-byte Folded Reload
                                        # zmm13 = (zmm2 * mem) + zmm13
	vmovaps	%zmm13, 17088(%rsp)             # 64-byte Spill
	vfmadd231ps	5248(%rsp), %zmm2, %zmm20 # 64-byte Folded Reload
                                        # zmm20 = (zmm2 * mem) + zmm20
	vmovaps	%zmm20, 10368(%rsp)             # 64-byte Spill
	vfmadd231ps	5376(%rsp), %zmm2, %zmm14 # 64-byte Folded Reload
                                        # zmm14 = (zmm2 * mem) + zmm14
	vmovaps	%zmm14, 8896(%rsp)              # 64-byte Spill
	vfmadd231ps	7104(%rsp), %zmm2, %zmm3 # 64-byte Folded Reload
                                        # zmm3 = (zmm2 * mem) + zmm3
	vmovaps	%zmm3, 8960(%rsp)               # 64-byte Spill
	vmovaps	1152(%rsp), %zmm4               # 64-byte Reload
	vmulps	%zmm4, %zmm17, %zmm2
	vmovaps	768(%rsp), %zmm1                # 64-byte Reload
	vfmadd231ps	%zmm1, %zmm31, %zmm2    # zmm2 = (zmm31 * zmm1) + zmm2
	vextractf64x4	$1, %zmm2, %ymm3
	vaddps	%zmm3, %zmm2, %zmm2
	vextractf128	$1, %ymm2, %xmm3
	vaddps	%xmm3, %xmm2, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm3         # xmm3 = xmm2[1,0]
	vaddps	%xmm3, %xmm2, %xmm2
	vmovshdup	%xmm2, %xmm3            # xmm3 = xmm2[1,1,3,3]
	vaddss	%xmm3, %xmm2, %xmm2
	vmovss	%xmm2, 384(%rsp)                # 4-byte Spill
	vmovaps	15296(%rsp), %zmm3              # 64-byte Reload
	vextractf64x4	$1, %zmm3, %ymm2
	vaddps	%zmm2, %zmm3, %zmm2
	vextractf128	$1, %ymm2, %xmm3
	vaddps	%xmm3, %xmm2, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm3         # xmm3 = xmm2[1,0]
	vaddps	%xmm3, %xmm2, %xmm2
	vmovshdup	%xmm2, %xmm3            # xmm3 = xmm2[1,1,3,3]
	vaddss	%xmm3, %xmm2, %xmm2
	vmovss	%xmm2, 15296(%rsp)              # 4-byte Spill
	vmovaps	15808(%rsp), %zmm3              # 64-byte Reload
	vextractf64x4	$1, %zmm3, %ymm2
	vaddps	%zmm2, %zmm3, %zmm2
	vextractf128	$1, %ymm2, %xmm3
	vaddps	%xmm3, %xmm2, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm3         # xmm3 = xmm2[1,0]
	vaddps	%xmm3, %xmm2, %xmm2
	vmovshdup	%xmm2, %xmm3            # xmm3 = xmm2[1,1,3,3]
	vaddss	%xmm3, %xmm2, %xmm2
	vmovss	%xmm2, 368(%rsp)                # 4-byte Spill
	vmovaps	15552(%rsp), %zmm3              # 64-byte Reload
	vextractf64x4	$1, %zmm3, %ymm2
	vaddps	%zmm2, %zmm3, %zmm2
	vextractf128	$1, %ymm2, %xmm3
	vaddps	%xmm3, %xmm2, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm3         # xmm3 = xmm2[1,0]
	vaddps	%xmm3, %xmm2, %xmm2
	vmovshdup	%xmm2, %xmm3            # xmm3 = xmm2[1,1,3,3]
	vaddss	%xmm3, %xmm2, %xmm2
	vmovss	%xmm2, 2672(%rsp)               # 4-byte Spill
	vmovaps	15040(%rsp), %zmm3              # 64-byte Reload
	vextractf64x4	$1, %zmm3, %ymm2
	vaddps	%zmm2, %zmm3, %zmm2
	vextractf128	$1, %ymm2, %xmm3
	vaddps	%xmm3, %xmm2, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm3         # xmm3 = xmm2[1,0]
	vaddps	%xmm3, %xmm2, %xmm2
	vmovshdup	%xmm2, %xmm3            # xmm3 = xmm2[1,1,3,3]
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm27, %xmm2, %xmm23
	vmulps	%zmm4, %zmm29, %zmm2
	vfmadd231ps	%zmm1, %zmm0, %zmm2     # zmm2 = (zmm0 * zmm1) + zmm2
	vmovaps	%zmm1, %zmm20
	vextractf64x4	$1, %zmm2, %ymm3
	vaddps	%zmm3, %zmm2, %zmm2
	vextractf128	$1, %ymm2, %xmm3
	vaddps	%xmm3, %xmm2, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm3         # xmm3 = xmm2[1,0]
	vaddps	%xmm3, %xmm2, %xmm2
	vmovshdup	%xmm2, %xmm3            # xmm3 = xmm2[1,1,3,3]
	vaddss	%xmm3, %xmm2, %xmm0
	vmovss	%xmm0, 15040(%rsp)              # 4-byte Spill
	vmovaps	33152(%rsp), %zmm0              # 64-byte Reload
	vextractf64x4	$1, %zmm0, %ymm2
	vaddps	%zmm2, %zmm0, %zmm2
	vextractf128	$1, %ymm2, %xmm3
	vaddps	%xmm3, %xmm2, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm3         # xmm3 = xmm2[1,0]
	vaddps	%xmm3, %xmm2, %xmm2
	vmovshdup	%xmm2, %xmm3            # xmm3 = xmm2[1,1,3,3]
	vaddss	%xmm3, %xmm2, %xmm0
	vmovss	%xmm0, 352(%rsp)                # 4-byte Spill
	vmovaps	33792(%rsp), %zmm1              # 64-byte Reload
	vextractf64x4	$1, %zmm1, %ymm2
	vaddps	%zmm2, %zmm1, %zmm2
	vextractf128	$1, %ymm2, %xmm3
	vaddps	%xmm3, %xmm2, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm3         # xmm3 = xmm2[1,0]
	vaddps	%xmm3, %xmm2, %xmm2
	vmovshdup	%xmm2, %xmm3            # xmm3 = xmm2[1,1,3,3]
	vaddss	%xmm3, %xmm2, %xmm0
	vmovss	%xmm0, 10048(%rsp)              # 4-byte Spill
	vmovaps	33664(%rsp), %zmm0              # 64-byte Reload
	vextractf64x4	$1, %zmm0, %ymm2
	vaddps	%zmm2, %zmm0, %zmm2
	vextractf32x4	$1, %ymm2, %xmm24
	vaddps	%xmm24, %xmm2, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm24        # xmm24 = xmm2[1,0]
	vaddps	%xmm24, %xmm2, %xmm2
	vmovshdup	%xmm2, %xmm24           # xmm24 = xmm2[1,1,3,3]
	vaddss	%xmm24, %xmm2, %xmm0
	vmovss	%xmm0, 336(%rsp)                # 4-byte Spill
	vmovaps	10880(%rsp), %zmm0              # 64-byte Reload
	vextractf64x4	$1, %zmm0, %ymm24
	vaddps	%zmm24, %zmm0, %zmm24
	vextractf32x4	$1, %ymm24, %xmm25
	vaddps	%xmm25, %xmm24, %xmm24
	vshufpd	$1, %xmm24, %xmm24, %xmm25      # xmm25 = xmm24[1,0]
	vaddps	%xmm25, %xmm24, %xmm24
	vmovshdup	%xmm24, %xmm25          # xmm25 = xmm24[1,1,3,3]
	vaddss	%xmm25, %xmm24, %xmm24
	vaddss	%xmm27, %xmm24, %xmm24
	vinsertps	$16, %xmm24, %xmm22, %xmm22 # xmm22 = xmm22[0],xmm24[0],xmm22[2,3]
	vinsertps	$32, 9088(%rsp), %xmm22, %xmm8 # 16-byte Folded Reload
                                        # xmm8 = xmm22[0,1],mem[0],xmm22[3]
	vmovaps	15168(%rsp), %zmm0              # 64-byte Reload
	vextractf64x4	$1, %zmm0, %ymm22
	vaddps	%zmm22, %zmm0, %zmm22
	vextractf32x4	$1, %ymm22, %xmm24
	vaddps	%xmm24, %xmm22, %xmm22
	vshufpd	$1, %xmm22, %xmm22, %xmm24      # xmm24 = xmm22[1,0]
	vaddps	%xmm24, %xmm22, %xmm22
	vmovshdup	%xmm22, %xmm24          # xmm24 = xmm22[1,1,3,3]
	vaddss	%xmm24, %xmm22, %xmm22
	vaddss	%xmm27, %xmm22, %xmm22
	vinsertps	$48, %xmm22, %xmm8, %xmm0 # xmm0 = xmm8[0,1,2],xmm22[0]
	vmovaps	%xmm0, 2656(%rsp)               # 16-byte Spill
	vmovaps	832(%rsp), %zmm16               # 64-byte Reload
	vmovaps	6720(%rsp), %zmm19              # 64-byte Reload
	vmulps	%zmm16, %zmm19, %zmm8
	vmovaps	960(%rsp), %zmm13               # 64-byte Reload
	vmovaps	6464(%rsp), %zmm15              # 64-byte Reload
	vfmadd231ps	%zmm13, %zmm15, %zmm8   # zmm8 = (zmm15 * zmm13) + zmm8
	vextractf64x4	$1, %zmm8, %ymm22
	vaddps	%zmm22, %zmm8, %zmm8
	vextractf32x4	$1, %ymm8, %xmm22
	vaddps	%xmm22, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm22        # xmm22 = xmm8[1,0]
	vaddps	%xmm22, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm22           # xmm22 = xmm8[1,1,3,3]
	vaddss	%xmm22, %xmm8, %xmm8
	vaddss	%xmm27, %xmm8, %xmm0
	vmovaps	%xmm0, 10880(%rsp)              # 16-byte Spill
	vmovaps	640(%rsp), %zmm2                # 64-byte Reload
	vmulps	%zmm2, %zmm18, %zmm22
	vmovaps	64(%rsp), %zmm0                 # 64-byte Reload
	vfmadd231ps	%zmm0, %zmm5, %zmm22    # zmm22 = (zmm5 * zmm0) + zmm22
	vextractf64x4	$1, %zmm22, %ymm24
	vaddps	%zmm24, %zmm22, %zmm22
	vextractf32x4	$1, %ymm22, %xmm24
	vaddps	%xmm24, %xmm22, %xmm22
	vshufpd	$1, %xmm22, %xmm22, %xmm24      # xmm24 = xmm22[1,0]
	vaddps	%xmm24, %xmm22, %xmm22
	vmovshdup	%xmm22, %xmm24          # xmm24 = xmm22[1,1,3,3]
	vaddss	%xmm24, %xmm22, %xmm22
	vaddss	%xmm27, %xmm22, %xmm22
	vinsertps	$16, %xmm23, %xmm22, %xmm22 # xmm22 = xmm22[0],xmm23[0],xmm22[2,3]
	vinsertps	$32, 9280(%rsp), %xmm22, %xmm18 # 16-byte Folded Reload
                                        # xmm18 = xmm22[0,1],mem[0],xmm22[3]
	vinsertps	$48, 12416(%rsp), %xmm18, %xmm18 # 16-byte Folded Reload
                                        # xmm18 = xmm18[0,1,2],mem[0]
	vextractf64x4	$1, %zmm28, %ymm22
	vaddps	%zmm22, %zmm28, %zmm12
	vextractf32x4	$1, %ymm12, %xmm22
	vaddps	%xmm22, %xmm12, %xmm12
	vshufpd	$1, %xmm12, %xmm12, %xmm22      # xmm22 = xmm12[1,0]
	vaddps	%xmm22, %xmm12, %xmm12
	vmovshdup	%xmm12, %xmm22          # xmm22 = xmm12[1,1,3,3]
	vaddss	%xmm22, %xmm12, %xmm12
	vaddss	%xmm27, %xmm12, %xmm12
	vmovaps	9472(%rsp), %xmm1               # 16-byte Reload
	vinsertps	$16, %xmm12, %xmm1, %xmm12 # xmm12 = xmm1[0],xmm12[0],xmm1[2,3]
	vinsertps	$32, 9408(%rsp), %xmm12, %xmm12 # 16-byte Folded Reload
                                        # xmm12 = xmm12[0,1],mem[0],xmm12[3]
	vinsertps	$48, 9344(%rsp), %xmm12, %xmm12 # 16-byte Folded Reload
                                        # xmm12 = xmm12[0,1,2],mem[0]
	vinsertf32x4	$1, %xmm12, %ymm18, %ymm1
	vmovaps	%zmm1, 33792(%rsp)              # 64-byte Spill
	vmovaps	4864(%rsp), %zmm1               # 64-byte Reload
	vmulps	1600(%rsp), %zmm1, %zmm12       # 64-byte Folded Reload
	vmovaps	%zmm6, %zmm5
	vfmadd231ps	1664(%rsp), %zmm6, %zmm12 # 64-byte Folded Reload
                                        # zmm12 = (zmm6 * mem) + zmm12
	vextractf64x4	$1, %zmm12, %ymm14
	vaddps	%zmm14, %zmm12, %zmm12
	vextractf128	$1, %ymm12, %xmm14
	vaddps	%xmm14, %xmm12, %xmm12
	vshufpd	$1, %xmm12, %xmm12, %xmm14      # xmm14 = xmm12[1,0]
	vaddps	%xmm14, %xmm12, %xmm12
	vmovshdup	%xmm12, %xmm14          # xmm14 = xmm12[1,1,3,3]
	vaddss	%xmm14, %xmm12, %xmm12
	vaddss	%xmm27, %xmm12, %xmm12
	vinsertps	$16, 9792(%rsp), %xmm12, %xmm7 # 16-byte Folded Reload
                                        # xmm7 = xmm12[0],mem[0],xmm12[2,3]
	vinsertps	$32, 12672(%rsp), %xmm7, %xmm7 # 16-byte Folded Reload
                                        # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$48, 9536(%rsp), %xmm7, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm7[0,1,2],mem[0]
	vmovaps	%ymm3, 15808(%rsp)              # 32-byte Spill
	vmovaps	15744(%rsp), %zmm3              # 64-byte Reload
	vextractf64x4	$1, %zmm3, %ymm10
	vaddps	%zmm10, %zmm3, %zmm10
	vextractf128	$1, %ymm10, %xmm11
	vaddps	%xmm11, %xmm10, %xmm10
	vshufpd	$1, %xmm10, %xmm10, %xmm11      # xmm11 = xmm10[1,0]
	vaddps	%xmm11, %xmm10, %xmm10
	vmovshdup	%xmm10, %xmm11          # xmm11 = xmm10[1,1,3,3]
	vaddss	%xmm11, %xmm10, %xmm10
	vaddss	%xmm27, %xmm10, %xmm10
	vmovaps	13056(%rsp), %xmm3              # 16-byte Reload
	vinsertps	$16, %xmm10, %xmm3, %xmm6 # xmm6 = xmm3[0],xmm10[0],xmm3[2,3]
	vinsertps	$32, 12992(%rsp), %xmm6, %xmm6 # 16-byte Folded Reload
                                        # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vmovaps	33216(%rsp), %zmm3              # 64-byte Reload
	vextractf64x4	$1, %zmm3, %ymm9
	vaddps	%zmm9, %zmm3, %zmm9
	vextractf128	$1, %ymm9, %xmm10
	vaddps	%xmm10, %xmm9, %xmm9
	vshufpd	$1, %xmm9, %xmm9, %xmm10        # xmm10 = xmm9[1,0]
	vaddps	%xmm10, %xmm9, %xmm9
	vmovshdup	%xmm9, %xmm10           # xmm10 = xmm9[1,1,3,3]
	vaddss	%xmm10, %xmm9, %xmm9
	vaddss	%xmm27, %xmm9, %xmm9
	vinsertps	$48, %xmm9, %xmm6, %xmm3 # xmm3 = xmm6[0,1,2],xmm9[0]
	vmovaps	%xmm3, 15744(%rsp)              # 16-byte Spill
	vmulps	448(%rsp), %zmm1, %zmm6         # 64-byte Folded Reload
	vfmadd231ps	512(%rsp), %zmm5, %zmm6 # 64-byte Folded Reload
                                        # zmm6 = (zmm5 * mem) + zmm6
	vextractf64x4	$1, %zmm6, %ymm9
	vaddps	%zmm9, %zmm6, %zmm6
	vextractf128	$1, %ymm6, %xmm9
	vaddps	%xmm6, %xmm9, %xmm6
	vmovaps	15488(%rsp), %zmm3              # 64-byte Reload
	vextractf64x4	$1, %zmm3, %ymm9
	vaddps	%zmm9, %zmm3, %zmm9
	vextractf128	$1, %ymm9, %xmm11
	vaddps	%xmm11, %xmm9, %xmm9
	vshufpd	$1, %xmm9, %xmm9, %xmm11        # xmm11 = xmm9[1,0]
	vaddps	%xmm11, %xmm9, %xmm9
	vmovshdup	%xmm9, %xmm11           # xmm11 = xmm9[1,1,3,3]
	vaddss	%xmm11, %xmm9, %xmm9
	vaddss	%xmm27, %xmm9, %xmm3
	vmovaps	%xmm3, 15552(%rsp)              # 16-byte Spill
	vshufpd	$1, %xmm6, %xmm6, %xmm9         # xmm9 = xmm6[1,0]
	vaddps	%xmm6, %xmm9, %xmm6
	vmulps	%zmm4, %zmm19, %zmm9
	vmovaps	%zmm19, %zmm11
	vfmadd231ps	%zmm20, %zmm15, %zmm9   # zmm9 = (zmm15 * zmm20) + zmm9
	vextractf64x4	$1, %zmm9, %ymm12
	vaddps	%zmm12, %zmm9, %zmm9
	vextractf128	$1, %ymm9, %xmm12
	vaddps	%xmm12, %xmm9, %xmm9
	vshufpd	$1, %xmm9, %xmm9, %xmm12        # xmm12 = xmm9[1,0]
	vaddps	%xmm12, %xmm9, %xmm9
	vmovshdup	%xmm9, %xmm12           # xmm12 = xmm9[1,1,3,3]
	vaddss	%xmm12, %xmm9, %xmm9
	vaddss	%xmm27, %xmm9, %xmm3
	vmovaps	%xmm3, 15488(%rsp)              # 16-byte Spill
	vmovshdup	%xmm6, %xmm9            # xmm9 = xmm6[1,1,3,3]
	vaddss	%xmm6, %xmm9, %xmm3
	vmovss	%xmm3, 15168(%rsp)              # 4-byte Spill
	vmovaps	2048(%rsp), %zmm21              # 64-byte Reload
	vmulps	%zmm21, %zmm1, %zmm6
	vfmadd231ps	704(%rsp), %zmm5, %zmm6 # 64-byte Folded Reload
                                        # zmm6 = (zmm5 * mem) + zmm6
	vextractf64x4	$1, %zmm6, %ymm9
	vaddps	%zmm9, %zmm6, %zmm6
	vextractf128	$1, %ymm6, %xmm9
	vaddps	%xmm6, %xmm9, %xmm6
	vmovaps	14976(%rsp), %zmm3              # 64-byte Reload
	vextractf64x4	$1, %zmm3, %ymm9
	vaddps	%zmm9, %zmm3, %zmm9
	vextractf128	$1, %ymm9, %xmm12
	vaddps	%xmm12, %xmm9, %xmm9
	vshufpd	$1, %xmm9, %xmm9, %xmm12        # xmm12 = xmm9[1,0]
	vaddps	%xmm12, %xmm9, %xmm9
	vmovshdup	%xmm9, %xmm12           # xmm12 = xmm9[1,1,3,3]
	vaddss	%xmm12, %xmm9, %xmm9
	vaddss	%xmm27, %xmm9, %xmm12
	vshufpd	$1, %xmm6, %xmm6, %xmm9         # xmm9 = xmm6[1,0]
	vaddps	%xmm6, %xmm9, %xmm6
	vmulps	%zmm16, %zmm1, %zmm9
	vfmadd231ps	%zmm13, %zmm5, %zmm9    # zmm9 = (zmm5 * zmm13) + zmm9
	vextractf64x4	$1, %zmm9, %ymm14
	vaddps	%zmm14, %zmm9, %zmm9
	vextractf128	$1, %ymm9, %xmm14
	vaddps	%xmm14, %xmm9, %xmm9
	vshufpd	$1, %xmm9, %xmm9, %xmm14        # xmm14 = xmm9[1,0]
	vaddps	%xmm14, %xmm9, %xmm9
	vmovshdup	%xmm9, %xmm14           # xmm14 = xmm9[1,1,3,3]
	vaddss	%xmm14, %xmm9, %xmm9
	vaddss	%xmm27, %xmm9, %xmm14
	vmovaps	1984(%rsp), %zmm7               # 64-byte Reload
	vmulps	%zmm7, %zmm1, %zmm9
	vmovaps	1024(%rsp), %zmm31              # 64-byte Reload
	vfmadd231ps	%zmm31, %zmm5, %zmm9    # zmm9 = (zmm5 * zmm31) + zmm9
	vextractf64x4	$1, %zmm9, %ymm15
	vaddps	%zmm15, %zmm9, %zmm9
	vextractf128	$1, %ymm9, %xmm15
	vaddps	%xmm15, %xmm9, %xmm9
	vshufpd	$1, %xmm9, %xmm9, %xmm15        # xmm15 = xmm9[1,0]
	vaddps	%xmm15, %xmm9, %xmm9
	vmovshdup	%xmm9, %xmm15           # xmm15 = xmm9[1,1,3,3]
	vaddss	%xmm15, %xmm9, %xmm9
	vaddss	%xmm27, %xmm9, %xmm15
	vmovshdup	%xmm6, %xmm9            # xmm9 = xmm6[1,1,3,3]
	vaddss	%xmm6, %xmm9, %xmm3
	vmovss	%xmm3, 14976(%rsp)              # 4-byte Spill
	vmovaps	128(%rsp), %zmm16               # 64-byte Reload
	vmulps	%zmm16, %zmm1, %zmm6
	vmovaps	896(%rsp), %zmm19               # 64-byte Reload
	vfmadd231ps	%zmm19, %zmm5, %zmm6    # zmm6 = (zmm5 * zmm19) + zmm6
	vextractf64x4	$1, %zmm6, %ymm18
	vaddps	%zmm18, %zmm6, %zmm6
	vextractf32x4	$1, %ymm6, %xmm18
	vaddps	%xmm18, %xmm6, %xmm6
	vshufpd	$1, %xmm6, %xmm6, %xmm18        # xmm18 = xmm6[1,0]
	vaddps	%xmm18, %xmm6, %xmm6
	vmovshdup	%xmm6, %xmm18           # xmm18 = xmm6[1,1,3,3]
	vaddss	%xmm18, %xmm6, %xmm6
	vaddss	%xmm27, %xmm6, %xmm18
	vmulps	256(%rsp), %zmm1, %zmm6         # 64-byte Folded Reload
	vmovaps	1408(%rsp), %zmm29              # 64-byte Reload
	vfmadd231ps	%zmm29, %zmm5, %zmm6    # zmm6 = (zmm5 * zmm29) + zmm6
	vextractf64x4	$1, %zmm6, %ymm22
	vaddps	%zmm22, %zmm6, %zmm6
	vextractf32x4	$1, %ymm6, %xmm22
	vaddps	%xmm22, %xmm6, %xmm6
	vshufpd	$1, %xmm6, %xmm6, %xmm22        # xmm22 = xmm6[1,0]
	vaddps	%xmm22, %xmm6, %xmm6
	vmovaps	15104(%rsp), %zmm3              # 64-byte Reload
	vextractf64x4	$1, %zmm3, %ymm22
	vaddps	%zmm22, %zmm3, %zmm22
	vextractf32x4	$1, %ymm22, %xmm23
	vaddps	%xmm23, %xmm22, %xmm22
	vshufpd	$1, %xmm22, %xmm22, %xmm23      # xmm23 = xmm22[1,0]
	vaddps	%xmm23, %xmm22, %xmm22
	vmovshdup	%xmm22, %xmm23          # xmm23 = xmm22[1,1,3,3]
	vaddss	%xmm23, %xmm22, %xmm22
	vaddss	%xmm27, %xmm22, %xmm22
	vmovshdup	%xmm6, %xmm23           # xmm23 = xmm6[1,1,3,3]
	vaddss	%xmm23, %xmm6, %xmm3
	vmovss	%xmm3, 15104(%rsp)              # 4-byte Spill
	vmovaps	2368(%rsp), %zmm9               # 64-byte Reload
	vmulps	%zmm9, %zmm1, %zmm23
	vmovaps	2560(%rsp), %zmm8               # 64-byte Reload
	vfmadd231ps	%zmm8, %zmm5, %zmm23    # zmm23 = (zmm5 * zmm8) + zmm23
	vextractf64x4	$1, %zmm23, %ymm24
	vaddps	%zmm24, %zmm23, %zmm23
	vextractf32x4	$1, %ymm23, %xmm24
	vaddps	%xmm24, %xmm23, %xmm23
	vshufpd	$1, %xmm23, %xmm23, %xmm24      # xmm24 = xmm23[1,0]
	vaddps	%xmm24, %xmm23, %xmm23
	vmovshdup	%xmm23, %xmm24          # xmm24 = xmm23[1,1,3,3]
	vaddss	%xmm24, %xmm23, %xmm23
	vaddss	%xmm27, %xmm23, %xmm23
	vinsertps	$16, %xmm23, %xmm18, %xmm18 # xmm18 = xmm18[0],xmm23[0],xmm18[2,3]
	vinsertps	$32, %xmm15, %xmm18, %xmm15 # xmm15 = xmm18[0,1],xmm15[0],xmm18[3]
	vinsertps	$48, %xmm14, %xmm15, %xmm13 # xmm13 = xmm15[0,1,2],xmm14[0]
	vmovaps	%xmm13, 33664(%rsp)             # 16-byte Spill
	vmovaps	33408(%rsp), %zmm3              # 64-byte Reload
	vextractf64x4	$1, %zmm3, %ymm14
	vaddps	%zmm14, %zmm3, %zmm14
	vextractf128	$1, %ymm14, %xmm15
	vaddps	%xmm15, %xmm14, %xmm14
	vshufpd	$1, %xmm14, %xmm14, %xmm15      # xmm15 = xmm14[1,0]
	vaddps	%xmm15, %xmm14, %xmm14
	vmovshdup	%xmm14, %xmm15          # xmm15 = xmm14[1,1,3,3]
	vaddss	%xmm15, %xmm14, %xmm14
	vaddss	%xmm27, %xmm14, %xmm15
	vmulps	%zmm2, %zmm1, %zmm14
	vfmadd231ps	%zmm0, %zmm5, %zmm14    # zmm14 = (zmm5 * zmm0) + zmm14
	vextractf64x4	$1, %zmm14, %ymm18
	vaddps	%zmm18, %zmm14, %zmm14
	vextractf32x4	$1, %ymm14, %xmm18
	vaddps	%xmm18, %xmm14, %xmm14
	vshufpd	$1, %xmm14, %xmm14, %xmm18      # xmm18 = xmm14[1,0]
	vaddps	%xmm18, %xmm14, %xmm14
	vmovshdup	%xmm14, %xmm18          # xmm18 = xmm14[1,1,3,3]
	vaddss	%xmm18, %xmm14, %xmm14
	vaddss	%xmm27, %xmm14, %xmm14
	vinsertps	$16, %xmm22, %xmm14, %xmm14 # xmm14 = xmm14[0],xmm22[0],xmm14[2,3]
	vinsertps	$32, %xmm12, %xmm14, %xmm12 # xmm12 = xmm14[0,1],xmm12[0],xmm14[3]
	vmovaps	14912(%rsp), %zmm0              # 64-byte Reload
	vextractf64x4	$1, %zmm0, %ymm14
	vaddps	%zmm14, %zmm0, %zmm14
	vextractf32x4	$1, %ymm14, %xmm18
	vaddps	%xmm18, %xmm14, %xmm14
	vshufpd	$1, %xmm14, %xmm14, %xmm18      # xmm18 = xmm14[1,0]
	vaddps	%xmm18, %xmm14, %xmm14
	vmovshdup	%xmm14, %xmm18          # xmm18 = xmm14[1,1,3,3]
	vaddss	%xmm18, %xmm14, %xmm14
	vaddss	%xmm27, %xmm14, %xmm14
	vinsertps	$48, %xmm14, %xmm12, %xmm10 # xmm10 = xmm12[0,1,2],xmm14[0]
	vmulps	1280(%rsp), %zmm1, %zmm12       # 64-byte Folded Reload
	vfmadd231ps	1216(%rsp), %zmm5, %zmm12 # 64-byte Folded Reload
                                        # zmm12 = (zmm5 * mem) + zmm12
	vextractf64x4	$1, %zmm12, %ymm14
	vaddps	%zmm14, %zmm12, %zmm12
	vextractf128	$1, %ymm12, %xmm14
	vaddps	%xmm14, %xmm12, %xmm12
	vshufpd	$1, %xmm12, %xmm12, %xmm14      # xmm14 = xmm12[1,0]
	vaddps	%xmm14, %xmm12, %xmm12
	vmovshdup	%xmm12, %xmm14          # xmm14 = xmm12[1,1,3,3]
	vaddss	%xmm14, %xmm12, %xmm12
	vextractf64x4	$1, %zmm26, %ymm14
	vaddps	%zmm14, %zmm26, %zmm14
	vextractf32x4	$1, %ymm14, %xmm17
	vaddps	%xmm17, %xmm14, %xmm14
	vshufpd	$1, %xmm14, %xmm14, %xmm17      # xmm17 = xmm14[1,0]
	vaddps	%xmm17, %xmm14, %xmm14
	vmovshdup	%xmm14, %xmm17          # xmm17 = xmm14[1,1,3,3]
	vaddss	%xmm17, %xmm14, %xmm14
	vaddss	%xmm27, %xmm12, %xmm12
	vaddss	%xmm27, %xmm14, %xmm14
	vinsertps	$16, %xmm14, %xmm12, %xmm12 # xmm12 = xmm12[0],xmm14[0],xmm12[2,3]
	vmulps	8320(%rsp), %zmm1, %zmm14       # 64-byte Folded Reload
	vfmadd231ps	8448(%rsp), %zmm5, %zmm14 # 64-byte Folded Reload
                                        # zmm14 = (zmm5 * mem) + zmm14
	vextractf64x4	$1, %zmm14, %ymm17
	vaddps	%zmm17, %zmm14, %zmm14
	vextractf32x4	$1, %ymm14, %xmm17
	vaddps	%xmm17, %xmm14, %xmm14
	vshufpd	$1, %xmm14, %xmm14, %xmm17      # xmm17 = xmm14[1,0]
	vaddps	%xmm17, %xmm14, %xmm14
	vmovshdup	%xmm14, %xmm17          # xmm17 = xmm14[1,1,3,3]
	vaddss	%xmm17, %xmm14, %xmm14
	vaddss	%xmm27, %xmm14, %xmm14
	vinsertps	$32, %xmm14, %xmm12, %xmm12 # xmm12 = xmm12[0,1],xmm14[0],xmm12[3]
	vmulps	8384(%rsp), %zmm1, %zmm14       # 64-byte Folded Reload
	vfmadd231ps	1536(%rsp), %zmm5, %zmm14 # 64-byte Folded Reload
                                        # zmm14 = (zmm5 * mem) + zmm14
	vextractf64x4	$1, %zmm14, %ymm17
	vaddps	%zmm17, %zmm14, %zmm14
	vextractf32x4	$1, %ymm14, %xmm17
	vaddps	%xmm17, %xmm14, %xmm14
	vshufpd	$1, %xmm14, %xmm14, %xmm17      # xmm17 = xmm14[1,0]
	vaddps	%xmm17, %xmm14, %xmm14
	vmovshdup	%xmm14, %xmm17          # xmm17 = xmm14[1,1,3,3]
	vaddss	%xmm17, %xmm14, %xmm14
	vaddss	%xmm27, %xmm14, %xmm14
	vinsertps	$48, %xmm14, %xmm12, %xmm22 # xmm22 = xmm12[0,1,2],xmm14[0]
	vmovaps	3072(%rsp), %zmm6               # 64-byte Reload
	vmulps	%zmm6, %zmm1, %zmm12
	vmovaps	1472(%rsp), %zmm20              # 64-byte Reload
	vfmadd231ps	%zmm20, %zmm5, %zmm12   # zmm12 = (zmm5 * zmm20) + zmm12
	vextractf64x4	$1, %zmm12, %ymm14
	vaddps	%zmm14, %zmm12, %zmm12
	vextractf128	$1, %ymm12, %xmm14
	vaddps	%xmm14, %xmm12, %xmm12
	vmovaps	34432(%rsp), %zmm0              # 64-byte Reload
	vextractf64x4	$1, %zmm0, %ymm14
	vaddps	%zmm14, %zmm0, %zmm14
	vextractf32x4	$1, %ymm14, %xmm17
	vaddps	%xmm17, %xmm14, %xmm14
	vshufpd	$1, %xmm14, %xmm14, %xmm17      # xmm17 = xmm14[1,0]
	vaddps	%xmm17, %xmm14, %xmm14
	vmovshdup	%xmm14, %xmm17          # xmm17 = xmm14[1,1,3,3]
	vaddss	%xmm17, %xmm14, %xmm14
	vaddss	%xmm27, %xmm14, %xmm23
	vshufpd	$1, %xmm12, %xmm12, %xmm14      # xmm14 = xmm12[1,0]
	vaddps	%xmm14, %xmm12, %xmm12
	vmovshdup	%xmm12, %xmm14          # xmm14 = xmm12[1,1,3,3]
	vaddss	%xmm14, %xmm12, %xmm12
	vmovaps	2112(%rsp), %zmm4               # 64-byte Reload
	vmulps	%zmm4, %zmm1, %zmm14
	vmovaps	2496(%rsp), %zmm3               # 64-byte Reload
	vfmadd231ps	%zmm3, %zmm5, %zmm14    # zmm14 = (zmm5 * zmm3) + zmm14
	vextractf64x4	$1, %zmm14, %ymm17
	vaddps	%zmm17, %zmm14, %zmm14
	vextractf32x4	$1, %ymm14, %xmm17
	vaddps	%xmm17, %xmm14, %xmm14
	vmovaps	35264(%rsp), %zmm13             # 64-byte Reload
	vextractf64x4	$1, %zmm13, %ymm17
	vaddps	%zmm17, %zmm13, %zmm17
	vextractf32x4	$1, %ymm17, %xmm24
	vaddps	%xmm24, %xmm17, %xmm17
	vshufpd	$1, %xmm17, %xmm17, %xmm24      # xmm24 = xmm17[1,0]
	vaddps	%xmm24, %xmm17, %xmm17
	vmovshdup	%xmm17, %xmm24          # xmm24 = xmm17[1,1,3,3]
	vaddss	%xmm24, %xmm17, %xmm17
	vaddss	%xmm27, %xmm17, %xmm25
	vshufpd	$1, %xmm14, %xmm14, %xmm17      # xmm17 = xmm14[1,0]
	vaddps	%xmm17, %xmm14, %xmm14
	vmovshdup	%xmm14, %xmm17          # xmm17 = xmm14[1,1,3,3]
	vaddss	%xmm17, %xmm14, %xmm14
	vmulps	2432(%rsp), %zmm1, %zmm17       # 64-byte Folded Reload
	vfmadd231ps	1792(%rsp), %zmm5, %zmm17 # 64-byte Folded Reload
                                        # zmm17 = (zmm5 * mem) + zmm17
	vextractf64x4	$1, %zmm17, %ymm24
	vaddps	%zmm24, %zmm17, %zmm17
	vextractf32x4	$1, %ymm17, %xmm24
	vaddps	%xmm24, %xmm17, %xmm17
	vshufpd	$1, %xmm17, %xmm17, %xmm24      # xmm24 = xmm17[1,0]
	vaddps	%xmm24, %xmm17, %xmm17
	vmovshdup	%xmm17, %xmm24          # xmm24 = xmm17[1,1,3,3]
	vaddss	%xmm24, %xmm17, %xmm17
	vmovaps	3136(%rsp), %zmm2               # 64-byte Reload
	vmulps	%zmm2, %zmm1, %zmm24
	vmovaps	1856(%rsp), %zmm0               # 64-byte Reload
	vfmadd231ps	%zmm0, %zmm5, %zmm24    # zmm24 = (zmm5 * zmm0) + zmm24
	vaddss	%xmm27, %xmm17, %xmm26
	vextractf64x4	$1, %zmm24, %ymm17
	vaddps	%zmm17, %zmm24, %zmm17
	vmulps	2176(%rsp), %zmm1, %zmm24       # 64-byte Folded Reload
	vfmadd231ps	1728(%rsp), %zmm5, %zmm24 # 64-byte Folded Reload
                                        # zmm24 = (zmm5 * mem) + zmm24
	vextractf64x4	$1, %zmm24, %ymm28
	vaddps	%zmm28, %zmm24, %zmm24
	vextractf32x4	$1, %ymm24, %xmm28
	vaddps	%xmm28, %xmm24, %xmm24
	vshufpd	$1, %xmm24, %xmm24, %xmm28      # xmm28 = xmm24[1,0]
	vaddps	%xmm28, %xmm24, %xmm24
	vmovshdup	%xmm24, %xmm28          # xmm28 = xmm24[1,1,3,3]
	vaddss	%xmm28, %xmm24, %xmm24
	vaddss	%xmm27, %xmm24, %xmm28
	vmulps	192(%rsp), %zmm1, %zmm24        # 64-byte Folded Reload
	vfmadd231ps	1344(%rsp), %zmm5, %zmm24 # 64-byte Folded Reload
                                        # zmm24 = (zmm5 * mem) + zmm24
	vextractf64x4	$1, %zmm24, %ymm30
	vaddps	%zmm30, %zmm24, %zmm24
	vextractf32x4	$1, %ymm24, %xmm30
	vaddps	%xmm30, %xmm24, %xmm24
	vshufpd	$1, %xmm24, %xmm24, %xmm30      # xmm30 = xmm24[1,0]
	vaddps	%xmm30, %xmm24, %xmm24
	vmovshdup	%xmm24, %xmm30          # xmm30 = xmm24[1,1,3,3]
	vaddss	%xmm30, %xmm24, %xmm24
	vaddss	%xmm27, %xmm24, %xmm30
	vextractf32x4	$1, %ymm17, %xmm24
	vaddps	%xmm24, %xmm17, %xmm17
	vmulps	%zmm7, %zmm11, %zmm24
	vmovaps	6464(%rsp), %zmm18              # 64-byte Reload
	vfmadd231ps	%zmm31, %zmm18, %zmm24  # zmm24 = (zmm18 * zmm31) + zmm24
	vextractf64x4	$1, %zmm24, %ymm31
	vaddps	%zmm31, %zmm24, %zmm24
	vextractf32x4	$1, %ymm24, %xmm31
	vaddps	%xmm31, %xmm24, %xmm24
	vshufpd	$1, %xmm24, %xmm24, %xmm31      # xmm31 = xmm24[1,0]
	vaddps	%xmm31, %xmm24, %xmm24
	vmovshdup	%xmm24, %xmm31          # xmm31 = xmm24[1,1,3,3]
	vaddss	%xmm31, %xmm24, %xmm24
	vaddss	%xmm27, %xmm24, %xmm24
	vshufpd	$1, %xmm17, %xmm17, %xmm31      # xmm31 = xmm17[1,0]
	vaddps	%xmm31, %xmm17, %xmm17
	vmovshdup	%xmm17, %xmm31          # xmm31 = xmm17[1,1,3,3]
	vaddss	%xmm31, %xmm17, %xmm17
	vmulps	2240(%rsp), %zmm1, %zmm31       # 64-byte Folded Reload
	vfmadd231ps	1088(%rsp), %zmm5, %zmm31 # 64-byte Folded Reload
                                        # zmm31 = (zmm5 * mem) + zmm31
	vextractf64x4	$1, %zmm31, %ymm13
	vaddps	%zmm13, %zmm31, %zmm13
	vextractf32x4	$1, %ymm13, %xmm31
	vaddps	%xmm31, %xmm13, %xmm13
	vshufpd	$1, %xmm13, %xmm13, %xmm31      # xmm31 = xmm13[1,0]
	vaddps	%xmm31, %xmm13, %xmm13
	vmovshdup	%xmm13, %xmm31          # xmm31 = xmm13[1,1,3,3]
	vaddss	%xmm31, %xmm13, %xmm13
	vaddss	%xmm27, %xmm13, %xmm13
	vinsertps	$16, %xmm13, %xmm30, %xmm13 # xmm13 = xmm30[0],xmm13[0],xmm30[2,3]
	vinsertps	$32, %xmm28, %xmm13, %xmm13 # xmm13 = xmm13[0,1],xmm28[0],xmm13[3]
	vinsertps	$48, %xmm26, %xmm13, %xmm1 # xmm1 = xmm13[0,1,2],xmm26[0]
	vmovaps	%xmm1, 33408(%rsp)              # 16-byte Spill
	vmovaps	15808(%rsp), %ymm1              # 32-byte Reload
	vinsertf128	$1, 15744(%rsp), %ymm1, %ymm1 # 16-byte Folded Reload
	vmovaps	%zmm1, 15744(%rsp)              # 64-byte Spill
	vinsertf32x4	$1, %xmm22, %ymm10, %ymm1
	vmovaps	%zmm1, 15808(%rsp)              # 64-byte Spill
	vmulps	1600(%rsp), %zmm11, %zmm7       # 64-byte Folded Reload
	vmovaps	%zmm11, %zmm5
	vfmadd231ps	1664(%rsp), %zmm18, %zmm7 # 64-byte Folded Reload
                                        # zmm7 = (zmm18 * mem) + zmm7
	vextractf64x4	$1, %zmm7, %ymm10
	vaddps	%zmm10, %zmm7, %zmm7
	vextractf128	$1, %ymm7, %xmm10
	vaddps	%xmm7, %xmm10, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm10        # xmm10 = xmm7[1,0]
	vaddps	%xmm7, %xmm10, %xmm7
	vmovshdup	%xmm7, %xmm10           # xmm10 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm10, %xmm7
	vaddss	%xmm27, %xmm7, %xmm7
	vinsertps	$16, %xmm25, %xmm7, %xmm7 # xmm7 = xmm7[0],xmm25[0],xmm7[2,3]
	vinsertps	$32, %xmm23, %xmm7, %xmm7 # xmm7 = xmm7[0,1],xmm23[0],xmm7[3]
	vinsertps	$48, %xmm15, %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],xmm15[0]
	vmovaps	33920(%rsp), %zmm1              # 64-byte Reload
	vextractf64x4	$1, %zmm1, %ymm10
	vaddps	%zmm10, %zmm1, %zmm10
	vextractf128	$1, %ymm10, %xmm13
	vaddps	%xmm13, %xmm10, %xmm10
	vshufpd	$1, %xmm10, %xmm10, %xmm13      # xmm13 = xmm10[1,0]
	vaddps	%xmm13, %xmm10, %xmm10
	vmovshdup	%xmm10, %xmm13          # xmm13 = xmm10[1,1,3,3]
	vaddss	%xmm13, %xmm10, %xmm10
	vaddss	%xmm27, %xmm10, %xmm10
	vmovaps	15488(%rsp), %xmm1              # 16-byte Reload
	vinsertps	$16, %xmm10, %xmm1, %xmm10 # xmm10 = xmm1[0],xmm10[0],xmm1[2,3]
	vinsertps	$32, 15552(%rsp), %xmm10, %xmm10 # 16-byte Folded Reload
                                        # xmm10 = xmm10[0,1],mem[0],xmm10[3]
	vmovaps	33344(%rsp), %zmm1              # 64-byte Reload
	vextractf64x4	$1, %zmm1, %ymm11
	vaddps	%zmm11, %zmm1, %zmm11
	vextractf128	$1, %ymm11, %xmm13
	vaddps	%xmm13, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm13      # xmm13 = xmm11[1,0]
	vaddps	%xmm13, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm13          # xmm13 = xmm11[1,1,3,3]
	vaddss	%xmm13, %xmm11, %xmm11
	vaddss	%xmm27, %xmm11, %xmm11
	vinsertps	$48, %xmm11, %xmm10, %xmm10 # xmm10 = xmm10[0,1,2],xmm11[0]
	vmulps	%zmm16, %zmm5, %zmm11
	vfmadd231ps	%zmm19, %zmm18, %zmm11  # zmm11 = (zmm18 * zmm19) + zmm11
	vextractf64x4	$1, %zmm11, %ymm13
	vaddps	%zmm13, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm13
	vaddps	%xmm13, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm13      # xmm13 = xmm11[1,0]
	vaddps	%xmm13, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm13          # xmm13 = xmm11[1,1,3,3]
	vaddss	%xmm13, %xmm11, %xmm11
	vaddss	%xmm27, %xmm11, %xmm11
	vmulps	448(%rsp), %zmm5, %zmm13        # 64-byte Folded Reload
	vfmadd231ps	512(%rsp), %zmm18, %zmm13 # 64-byte Folded Reload
                                        # zmm13 = (zmm18 * mem) + zmm13
	vextractf64x4	$1, %zmm13, %ymm15
	vaddps	%zmm15, %zmm13, %zmm13
	vextractf128	$1, %ymm13, %xmm15
	vaddps	%xmm15, %xmm13, %xmm13
	vshufpd	$1, %xmm13, %xmm13, %xmm15      # xmm15 = xmm13[1,0]
	vaddps	%xmm15, %xmm13, %xmm13
	vmovshdup	%xmm13, %xmm15          # xmm15 = xmm13[1,1,3,3]
	vaddss	%xmm15, %xmm13, %xmm15
	vinsertf128	$1, %xmm10, %ymm7, %ymm1
	vmovaps	%zmm1, 33344(%rsp)              # 64-byte Spill
	vmulps	%zmm21, %zmm5, %zmm7
	vfmadd231ps	704(%rsp), %zmm18, %zmm7 # 64-byte Folded Reload
                                        # zmm7 = (zmm18 * mem) + zmm7
	vextractf64x4	$1, %zmm7, %ymm10
	vaddps	%zmm10, %zmm7, %zmm7
	vmulps	%zmm9, %zmm5, %zmm10
	vfmadd231ps	%zmm8, %zmm18, %zmm10   # zmm10 = (zmm18 * zmm8) + zmm10
	vmovaps	%zmm18, %zmm21
	vextractf64x4	$1, %zmm10, %ymm13
	vaddps	%zmm13, %zmm10, %zmm10
	vextractf128	$1, %ymm10, %xmm13
	vaddps	%xmm13, %xmm10, %xmm10
	vshufpd	$1, %xmm10, %xmm10, %xmm13      # xmm13 = xmm10[1,0]
	vaddps	%xmm13, %xmm10, %xmm10
	vmovshdup	%xmm10, %xmm13          # xmm13 = xmm10[1,1,3,3]
	vaddss	%xmm13, %xmm10, %xmm10
	vaddss	%xmm27, %xmm10, %xmm10
	vinsertps	$16, %xmm10, %xmm11, %xmm10 # xmm10 = xmm11[0],xmm10[0],xmm11[2,3]
	vinsertps	$32, %xmm24, %xmm10, %xmm10 # xmm10 = xmm10[0,1],xmm24[0],xmm10[3]
	vinsertps	$48, 10880(%rsp), %xmm10, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm10[0,1,2],mem[0]
	vmovaps	%xmm1, 15552(%rsp)              # 16-byte Spill
	vextractf128	$1, %ymm7, %xmm8
	vaddps	%xmm7, %xmm8, %xmm7
	vmulps	%zmm2, %zmm5, %zmm8
	vfmadd231ps	%zmm0, %zmm18, %zmm8    # zmm8 = (zmm18 * zmm0) + zmm8
	vextractf64x4	$1, %zmm8, %ymm10
	vaddps	%zmm10, %zmm8, %zmm8
	vextractf128	$1, %ymm8, %xmm10
	vaddps	%xmm10, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm10        # xmm10 = xmm8[1,0]
	vaddps	%xmm10, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm10           # xmm10 = xmm8[1,1,3,3]
	vaddss	%xmm10, %xmm8, %xmm8
	vaddss	%xmm27, %xmm8, %xmm1
	vmovaps	%xmm1, 15488(%rsp)              # 16-byte Spill
	vmulps	%zmm4, %zmm5, %zmm8
	vfmadd231ps	%zmm3, %zmm18, %zmm8    # zmm8 = (zmm18 * zmm3) + zmm8
	vextractf64x4	$1, %zmm8, %ymm10
	vaddps	%zmm10, %zmm8, %zmm8
	vextractf128	$1, %ymm8, %xmm10
	vaddps	%xmm10, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm10        # xmm10 = xmm8[1,0]
	vaddps	%xmm10, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm10           # xmm10 = xmm8[1,1,3,3]
	vaddss	%xmm10, %xmm8, %xmm8
	vaddss	%xmm27, %xmm8, %xmm0
	vmovaps	%xmm0, 33216(%rsp)              # 16-byte Spill
	vshufpd	$1, %xmm7, %xmm7, %xmm8         # xmm8 = xmm7[1,0]
	vaddps	%xmm7, %xmm8, %xmm7
	vmulps	%zmm6, %zmm5, %zmm8
	vfmadd231ps	%zmm20, %zmm18, %zmm8   # zmm8 = (zmm18 * zmm20) + zmm8
	vextractf64x4	$1, %zmm8, %ymm10
	vaddps	%zmm10, %zmm8, %zmm8
	vextractf128	$1, %ymm8, %xmm10
	vaddps	%xmm10, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm10        # xmm10 = xmm8[1,0]
	vaddps	%xmm10, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm10           # xmm10 = xmm8[1,1,3,3]
	vaddss	%xmm10, %xmm8, %xmm8
	vaddss	%xmm27, %xmm8, %xmm0
	vmovaps	%xmm0, 33152(%rsp)              # 16-byte Spill
	vmovshdup	%xmm7, %xmm8            # xmm8 = xmm7[1,1,3,3]
	vaddss	%xmm8, %xmm7, %xmm18
	vmulps	256(%rsp), %zmm5, %zmm7         # 64-byte Folded Reload
	vfmadd231ps	%zmm29, %zmm21, %zmm7   # zmm7 = (zmm21 * zmm29) + zmm7
	vextractf64x4	$1, %zmm7, %ymm8
	vaddps	%zmm8, %zmm7, %zmm7
	vextractf128	$1, %ymm7, %xmm8
	vaddps	%xmm7, %xmm8, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm8         # xmm8 = xmm7[1,0]
	vaddps	%xmm7, %xmm8, %xmm7
	vmovshdup	%xmm7, %xmm8            # xmm8 = xmm7[1,1,3,3]
	vaddss	%xmm8, %xmm7, %xmm22
	vxorps	%xmm1, %xmm1, %xmm1
	vaddss	384(%rsp), %xmm1, %xmm23        # 4-byte Folded Reload
	vaddss	15296(%rsp), %xmm1, %xmm13      # 4-byte Folded Reload
	vaddss	368(%rsp), %xmm1, %xmm0         # 4-byte Folded Reload
	vmovaps	%xmm0, 12096(%rsp)              # 16-byte Spill
	vaddss	2672(%rsp), %xmm1, %xmm0        # 4-byte Folded Reload
	vmovaps	%xmm0, 12032(%rsp)              # 16-byte Spill
	vaddss	15040(%rsp), %xmm1, %xmm0       # 4-byte Folded Reload
	vmovaps	%xmm0, 12224(%rsp)              # 16-byte Spill
	vaddss	352(%rsp), %xmm1, %xmm0         # 4-byte Folded Reload
	vmovaps	%xmm0, 12160(%rsp)              # 16-byte Spill
	vaddss	10048(%rsp), %xmm1, %xmm0       # 4-byte Folded Reload
	vmovaps	%xmm0, 9280(%rsp)               # 16-byte Spill
	vaddss	336(%rsp), %xmm1, %xmm0         # 4-byte Folded Reload
	vmovaps	%xmm0, 9088(%rsp)               # 16-byte Spill
	vaddss	15168(%rsp), %xmm1, %xmm0       # 4-byte Folded Reload
	vmovaps	%xmm0, 384(%rsp)                # 16-byte Spill
	vaddss	14976(%rsp), %xmm1, %xmm0       # 4-byte Folded Reload
	vmovaps	%xmm0, 368(%rsp)                # 16-byte Spill
	vaddss	15104(%rsp), %xmm1, %xmm0       # 4-byte Folded Reload
	vmovaps	%xmm0, 2672(%rsp)               # 16-byte Spill
	vaddss	%xmm1, %xmm22, %xmm0
	vmovaps	%xmm0, 15296(%rsp)              # 16-byte Spill
	vaddss	%xmm1, %xmm12, %xmm0
	vmovaps	%xmm0, 15040(%rsp)              # 16-byte Spill
	vaddss	%xmm1, %xmm14, %xmm0
	vmovaps	%xmm0, 14976(%rsp)              # 16-byte Spill
	vaddss	%xmm1, %xmm17, %xmm0
	vmovaps	%xmm0, 14912(%rsp)              # 16-byte Spill
	vaddss	%xmm1, %xmm15, %xmm0
	vmovaps	%xmm0, 15168(%rsp)              # 16-byte Spill
	vaddss	%xmm1, %xmm18, %xmm0
	vmovaps	%xmm0, 15104(%rsp)              # 16-byte Spill
	vmovaps	8320(%rsp), %zmm1               # 64-byte Reload
	vmovaps	5888(%rsp), %zmm24              # 64-byte Reload
	vmulps	%zmm1, %zmm24, %zmm22
	vmovaps	3712(%rsp), %zmm29              # 64-byte Reload
	vmulps	%zmm1, %zmm29, %zmm7
	vmulps	3840(%rsp), %zmm1, %zmm21       # 64-byte Folded Reload
	vmulps	3968(%rsp), %zmm1, %zmm20       # 64-byte Folded Reload
	vmulps	4096(%rsp), %zmm1, %zmm19       # 64-byte Folded Reload
	vmulps	4224(%rsp), %zmm1, %zmm2        # 64-byte Folded Reload
	vmulps	6016(%rsp), %zmm1, %zmm31       # 64-byte Folded Reload
	vmulps	4352(%rsp), %zmm1, %zmm30       # 64-byte Folded Reload
	vmulps	4480(%rsp), %zmm1, %zmm28       # 64-byte Folded Reload
	vmulps	4608(%rsp), %zmm1, %zmm25       # 64-byte Folded Reload
	vmulps	3200(%rsp), %zmm1, %zmm18       # 64-byte Folded Reload
	vmulps	3328(%rsp), %zmm1, %zmm17       # 64-byte Folded Reload
	vmulps	3456(%rsp), %zmm1, %zmm16       # 64-byte Folded Reload
	vmulps	3584(%rsp), %zmm1, %zmm15       # 64-byte Folded Reload
	vmulps	4736(%rsp), %zmm1, %zmm4        # 64-byte Folded Reload
	vmovaps	6144(%rsp), %zmm11              # 64-byte Reload
	vmulps	%zmm1, %zmm11, %zmm10
	vmovaps	7744(%rsp), %zmm26              # 64-byte Reload
	vmulps	%zmm1, %zmm26, %zmm8
	vmulps	6528(%rsp), %zmm1, %zmm14       # 64-byte Folded Reload
	vmulps	6592(%rsp), %zmm1, %zmm12       # 64-byte Folded Reload
	vmulps	5056(%rsp), %zmm1, %zmm6        # 64-byte Folded Reload
	vmulps	5184(%rsp), %zmm1, %zmm0        # 64-byte Folded Reload
	vmulps	5312(%rsp), %zmm1, %zmm5        # 64-byte Folded Reload
	vmulps	7040(%rsp), %zmm1, %zmm3        # 64-byte Folded Reload
	vmovaps	8448(%rsp), %zmm1               # 64-byte Reload
	vmovaps	5952(%rsp), %zmm9               # 64-byte Reload
	vfmadd231ps	%zmm1, %zmm9, %zmm22    # zmm22 = (zmm9 * zmm1) + zmm22
	vmovaps	%zmm22, 28544(%rsp)             # 64-byte Spill
	vmovaps	3776(%rsp), %zmm22              # 64-byte Reload
	vfmadd231ps	%zmm1, %zmm22, %zmm7    # zmm7 = (zmm22 * zmm1) + zmm7
	vmovaps	%zmm7, 28672(%rsp)              # 64-byte Spill
	vfmadd231ps	3904(%rsp), %zmm1, %zmm21 # 64-byte Folded Reload
                                        # zmm21 = (zmm1 * mem) + zmm21
	vmovaps	%zmm21, 28800(%rsp)             # 64-byte Spill
	vfmadd231ps	4032(%rsp), %zmm1, %zmm20 # 64-byte Folded Reload
                                        # zmm20 = (zmm1 * mem) + zmm20
	vmovaps	%zmm20, 28992(%rsp)             # 64-byte Spill
	vfmadd231ps	4160(%rsp), %zmm1, %zmm19 # 64-byte Folded Reload
                                        # zmm19 = (zmm1 * mem) + zmm19
	vmovaps	%zmm19, 29184(%rsp)             # 64-byte Spill
	vfmadd231ps	4288(%rsp), %zmm1, %zmm2 # 64-byte Folded Reload
                                        # zmm2 = (zmm1 * mem) + zmm2
	vmovaps	%zmm2, 29376(%rsp)              # 64-byte Spill
	vfmadd231ps	6080(%rsp), %zmm1, %zmm31 # 64-byte Folded Reload
                                        # zmm31 = (zmm1 * mem) + zmm31
	vmovaps	%zmm31, 29632(%rsp)             # 64-byte Spill
	vfmadd231ps	4416(%rsp), %zmm1, %zmm30 # 64-byte Folded Reload
                                        # zmm30 = (zmm1 * mem) + zmm30
	vmovaps	%zmm30, 29888(%rsp)             # 64-byte Spill
	vfmadd231ps	4544(%rsp), %zmm1, %zmm28 # 64-byte Folded Reload
                                        # zmm28 = (zmm1 * mem) + zmm28
	vmovaps	%zmm28, 30080(%rsp)             # 64-byte Spill
	vfmadd231ps	4672(%rsp), %zmm1, %zmm25 # 64-byte Folded Reload
                                        # zmm25 = (zmm1 * mem) + zmm25
	vmovaps	%zmm25, 30336(%rsp)             # 64-byte Spill
	vfmadd231ps	3264(%rsp), %zmm1, %zmm18 # 64-byte Folded Reload
                                        # zmm18 = (zmm1 * mem) + zmm18
	vmovaps	%zmm18, 30592(%rsp)             # 64-byte Spill
	vfmadd231ps	3392(%rsp), %zmm1, %zmm17 # 64-byte Folded Reload
                                        # zmm17 = (zmm1 * mem) + zmm17
	vmovaps	%zmm17, 30912(%rsp)             # 64-byte Spill
	vfmadd231ps	3520(%rsp), %zmm1, %zmm16 # 64-byte Folded Reload
                                        # zmm16 = (zmm1 * mem) + zmm16
	vmovaps	%zmm16, 12416(%rsp)             # 64-byte Spill
	vfmadd231ps	3648(%rsp), %zmm1, %zmm15 # 64-byte Folded Reload
                                        # zmm15 = (zmm1 * mem) + zmm15
	vmovaps	%zmm15, 12672(%rsp)             # 64-byte Spill
	vfmadd231ps	4800(%rsp), %zmm1, %zmm4 # 64-byte Folded Reload
                                        # zmm4 = (zmm1 * mem) + zmm4
	vmovaps	%zmm4, 13056(%rsp)              # 64-byte Spill
	vmovaps	6208(%rsp), %zmm15              # 64-byte Reload
	vfmadd231ps	%zmm1, %zmm15, %zmm10   # zmm10 = (zmm15 * zmm1) + zmm10
	vmovaps	6272(%rsp), %zmm7               # 64-byte Reload
	vfmadd231ps	%zmm1, %zmm7, %zmm8     # zmm8 = (zmm7 * zmm1) + zmm8
	vmovaps	%zmm8, 10048(%rsp)              # 64-byte Spill
	vfmadd231ps	4928(%rsp), %zmm1, %zmm14 # 64-byte Folded Reload
                                        # zmm14 = (zmm1 * mem) + zmm14
	vmovaps	%zmm14, 33920(%rsp)             # 64-byte Spill
	vfmadd231ps	4992(%rsp), %zmm1, %zmm12 # 64-byte Folded Reload
                                        # zmm12 = (zmm1 * mem) + zmm12
	vmovaps	%zmm12, 34432(%rsp)             # 64-byte Spill
	vfmadd231ps	5120(%rsp), %zmm1, %zmm6 # 64-byte Folded Reload
                                        # zmm6 = (zmm1 * mem) + zmm6
	vmovaps	%zmm6, 35264(%rsp)              # 64-byte Spill
	vfmadd231ps	5248(%rsp), %zmm1, %zmm0 # 64-byte Folded Reload
                                        # zmm0 = (zmm1 * mem) + zmm0
	vmovaps	%zmm0, 10880(%rsp)              # 64-byte Spill
	vfmadd231ps	5376(%rsp), %zmm1, %zmm5 # 64-byte Folded Reload
                                        # zmm5 = (zmm1 * mem) + zmm5
	vmovaps	%zmm5, 8320(%rsp)               # 64-byte Spill
	vfmadd231ps	7104(%rsp), %zmm1, %zmm3 # 64-byte Folded Reload
                                        # zmm3 = (zmm1 * mem) + zmm3
	vmovaps	%zmm3, 8448(%rsp)               # 64-byte Spill
	vmovaps	14848(%rsp), %zmm0              # 64-byte Reload
	vextractf64x4	$1, %zmm0, %ymm3
	vaddps	%zmm3, %zmm0, %zmm3
	vextractf128	$1, %ymm3, %xmm6
	vaddps	%xmm6, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm6         # xmm6 = xmm3[1,0]
	vaddps	%xmm6, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm6            # xmm6 = xmm3[1,1,3,3]
	vaddss	%xmm6, %xmm3, %xmm0
	vmovss	%xmm0, 14848(%rsp)              # 4-byte Spill
	vmovaps	32320(%rsp), %zmm0              # 64-byte Reload
	vextractf64x4	$1, %zmm0, %ymm3
	vaddps	%zmm3, %zmm0, %zmm3
	vextractf128	$1, %ymm3, %xmm6
	vaddps	%xmm6, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm6         # xmm6 = xmm3[1,0]
	vaddps	%xmm6, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm6            # xmm6 = xmm3[1,1,3,3]
	vaddss	%xmm6, %xmm3, %xmm3
	vaddss	%xmm27, %xmm3, %xmm3
	vinsertps	$16, %xmm3, %xmm23, %xmm3 # xmm3 = xmm23[0],xmm3[0],xmm23[2,3]
	vinsertps	$32, %xmm13, %xmm3, %xmm3 # xmm3 = xmm3[0,1],xmm13[0],xmm3[3]
	vmovaps	32192(%rsp), %zmm0              # 64-byte Reload
	vextractf64x4	$1, %zmm0, %ymm6
	vaddps	%zmm6, %zmm0, %zmm6
	vextractf128	$1, %ymm6, %xmm8
	vaddps	%xmm6, %xmm8, %xmm6
	vshufpd	$1, %xmm6, %xmm6, %xmm8         # xmm8 = xmm6[1,0]
	vaddps	%xmm6, %xmm8, %xmm6
	vmovshdup	%xmm6, %xmm8            # xmm8 = xmm6[1,1,3,3]
	vaddss	%xmm6, %xmm8, %xmm6
	vaddss	%xmm27, %xmm6, %xmm6
	vinsertps	$48, %xmm6, %xmm3, %xmm0 # xmm0 = xmm3[0,1,2],xmm6[0]
	vmovaps	%xmm0, 5504(%rsp)               # 16-byte Spill
	vmovaps	2432(%rsp), %zmm0               # 64-byte Reload
	vmulps	%zmm0, %zmm24, %zmm3
	vmovaps	1792(%rsp), %zmm2               # 64-byte Reload
	vfmadd231ps	%zmm9, %zmm2, %zmm3     # zmm3 = (zmm2 * zmm9) + zmm3
	vextractf64x4	$1, %zmm3, %ymm6
	vaddps	%zmm6, %zmm3, %zmm3
	vextractf128	$1, %ymm3, %xmm6
	vaddps	%xmm6, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm6         # xmm6 = xmm3[1,0]
	vaddps	%xmm6, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm6            # xmm6 = xmm3[1,1,3,3]
	vaddss	%xmm6, %xmm3, %xmm1
	vmovss	%xmm1, 352(%rsp)                # 4-byte Spill
	vmovaps	11072(%rsp), %zmm1              # 64-byte Reload
	vextractf64x4	$1, %zmm1, %ymm3
	vaddps	%zmm3, %zmm1, %zmm3
	vextractf128	$1, %ymm3, %xmm6
	vaddps	%xmm6, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm6         # xmm6 = xmm3[1,0]
	vaddps	%xmm6, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm6            # xmm6 = xmm3[1,1,3,3]
	vaddss	%xmm6, %xmm3, %xmm1
	vmovss	%xmm1, 11072(%rsp)              # 4-byte Spill
	vmovaps	15680(%rsp), %zmm1              # 64-byte Reload
	vextractf64x4	$1, %zmm1, %ymm3
	vaddps	%zmm3, %zmm1, %zmm3
	vextractf128	$1, %ymm3, %xmm6
	vaddps	%xmm6, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm6         # xmm6 = xmm3[1,0]
	vaddps	%xmm6, %xmm3, %xmm3
	vmovaps	16320(%rsp), %zmm1              # 64-byte Reload
	vextractf64x4	$1, %zmm1, %ymm6
	vaddps	%zmm6, %zmm1, %zmm6
	vextractf128	$1, %ymm6, %xmm8
	vaddps	%xmm6, %xmm8, %xmm6
	vshufpd	$1, %xmm6, %xmm6, %xmm8         # xmm8 = xmm6[1,0]
	vaddps	%xmm6, %xmm8, %xmm6
	vmovshdup	%xmm6, %xmm8            # xmm8 = xmm6[1,1,3,3]
	vaddss	%xmm6, %xmm8, %xmm6
	vaddss	%xmm27, %xmm6, %xmm12
	vmovshdup	%xmm3, %xmm6            # xmm6 = xmm3[1,1,3,3]
	vaddss	%xmm6, %xmm3, %xmm1
	vmovss	%xmm1, 16320(%rsp)              # 4-byte Spill
	vmovaps	6336(%rsp), %zmm30              # 64-byte Reload
	vmulps	%zmm0, %zmm30, %zmm3
	vmovaps	6400(%rsp), %zmm1               # 64-byte Reload
	vfmadd231ps	%zmm1, %zmm2, %zmm3     # zmm3 = (zmm2 * zmm1) + zmm3
	vextractf64x4	$1, %zmm3, %ymm6
	vaddps	%zmm6, %zmm3, %zmm3
	vextractf128	$1, %ymm3, %xmm6
	vaddps	%xmm6, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm6         # xmm6 = xmm3[1,0]
	vaddps	%xmm6, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm6            # xmm6 = xmm3[1,1,3,3]
	vaddss	%xmm6, %xmm3, %xmm3
	vaddss	%xmm27, %xmm3, %xmm3
	vmulps	2176(%rsp), %zmm30, %zmm6       # 64-byte Folded Reload
	vmovaps	%zmm30, %zmm4
	vfmadd231ps	1728(%rsp), %zmm1, %zmm6 # 64-byte Folded Reload
                                        # zmm6 = (zmm1 * mem) + zmm6
	vmovaps	%zmm1, %zmm5
	vextractf64x4	$1, %zmm6, %ymm8
	vaddps	%zmm8, %zmm6, %zmm6
	vextractf128	$1, %ymm6, %xmm8
	vaddps	%xmm6, %xmm8, %xmm6
	vshufpd	$1, %xmm6, %xmm6, %xmm8         # xmm8 = xmm6[1,0]
	vaddps	%xmm6, %xmm8, %xmm6
	vmovshdup	%xmm6, %xmm8            # xmm8 = xmm6[1,1,3,3]
	vaddss	%xmm6, %xmm8, %xmm6
	vaddss	%xmm27, %xmm6, %xmm6
	vmovaps	32896(%rsp), %zmm0              # 64-byte Reload
	vextractf64x4	$1, %zmm0, %ymm8
	vaddps	%zmm8, %zmm0, %zmm8
	vmulps	192(%rsp), %zmm30, %zmm13       # 64-byte Folded Reload
	vfmadd231ps	1344(%rsp), %zmm1, %zmm13 # 64-byte Folded Reload
                                        # zmm13 = (zmm1 * mem) + zmm13
	vextractf64x4	$1, %zmm13, %ymm14
	vaddps	%zmm14, %zmm13, %zmm13
	vextractf128	$1, %ymm13, %xmm14
	vaddps	%xmm14, %xmm13, %xmm13
	vshufpd	$1, %xmm13, %xmm13, %xmm14      # xmm14 = xmm13[1,0]
	vaddps	%xmm14, %xmm13, %xmm13
	vmovshdup	%xmm13, %xmm14          # xmm14 = xmm13[1,1,3,3]
	vaddss	%xmm14, %xmm13, %xmm13
	vaddss	%xmm27, %xmm13, %xmm13
	vmovaps	1280(%rsp), %zmm30              # 64-byte Reload
	vmulps	%zmm30, %zmm11, %zmm14
	vmovaps	%zmm15, %zmm1
	vmovaps	1216(%rsp), %zmm24              # 64-byte Reload
	vfmadd231ps	%zmm24, %zmm15, %zmm14  # zmm14 = (zmm15 * zmm24) + zmm14
	vextractf64x4	$1, %zmm14, %ymm15
	vaddps	%zmm15, %zmm14, %zmm14
	vextractf128	$1, %ymm14, %xmm15
	vaddps	%xmm15, %xmm14, %xmm14
	vshufpd	$1, %xmm14, %xmm14, %xmm15      # xmm15 = xmm14[1,0]
	vaddps	%xmm15, %xmm14, %xmm14
	vmovshdup	%xmm14, %xmm15          # xmm15 = xmm14[1,1,3,3]
	vaddss	%xmm15, %xmm14, %xmm14
	vaddss	%xmm27, %xmm14, %xmm14
	vextractf128	$1, %ymm8, %xmm15
	vaddps	%xmm15, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm15        # xmm15 = xmm8[1,0]
	vaddps	%xmm15, %xmm8, %xmm8
	vmovaps	2240(%rsp), %zmm31              # 64-byte Reload
	vmulps	%zmm31, %zmm4, %zmm15
	vfmadd231ps	1088(%rsp), %zmm5, %zmm15 # 64-byte Folded Reload
                                        # zmm15 = (zmm5 * mem) + zmm15
	vextractf64x4	$1, %zmm15, %ymm16
	vaddps	%zmm16, %zmm15, %zmm15
	vextractf32x4	$1, %ymm15, %xmm16
	vaddps	%xmm16, %xmm15, %xmm15
	vshufpd	$1, %xmm15, %xmm15, %xmm16      # xmm16 = xmm15[1,0]
	vaddps	%xmm16, %xmm15, %xmm15
	vmovshdup	%xmm15, %xmm16          # xmm16 = xmm15[1,1,3,3]
	vaddss	%xmm16, %xmm15, %xmm15
	vaddss	%xmm27, %xmm15, %xmm15
	vinsertps	$16, %xmm15, %xmm13, %xmm13 # xmm13 = xmm13[0],xmm15[0],xmm13[2,3]
	vinsertps	$32, %xmm6, %xmm13, %xmm6 # xmm6 = xmm13[0,1],xmm6[0],xmm13[3]
	vinsertps	$48, %xmm3, %xmm6, %xmm0 # xmm0 = xmm6[0,1,2],xmm3[0]
	vmovaps	%xmm0, 32896(%rsp)              # 16-byte Spill
	vmovshdup	%xmm8, %xmm3            # xmm3 = xmm8[1,1,3,3]
	vaddss	%xmm3, %xmm8, %xmm0
	vmovss	%xmm0, 15680(%rsp)              # 4-byte Spill
	vmulps	1152(%rsp), %zmm29, %zmm3       # 64-byte Folded Reload
	vfmadd231ps	768(%rsp), %zmm22, %zmm3 # 64-byte Folded Reload
                                        # zmm3 = (zmm22 * mem) + zmm3
	vextractf64x4	$1, %zmm3, %ymm6
	vaddps	%zmm6, %zmm3, %zmm3
	vextractf128	$1, %ymm3, %xmm6
	vaddps	%xmm6, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm6         # xmm6 = xmm3[1,0]
	vaddps	%xmm6, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm6            # xmm6 = xmm3[1,1,3,3]
	vaddss	%xmm6, %xmm3, %xmm0
	vmovss	%xmm0, 336(%rsp)                # 4-byte Spill
	vmovaps	32576(%rsp), %zmm0              # 64-byte Reload
	vextractf64x4	$1, %zmm0, %ymm3
	vaddps	%zmm3, %zmm0, %zmm3
	vextractf128	$1, %ymm3, %xmm6
	vaddps	%xmm6, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm6         # xmm6 = xmm3[1,0]
	vaddps	%xmm6, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm6            # xmm6 = xmm3[1,1,3,3]
	vaddss	%xmm6, %xmm3, %xmm0
	vmovss	%xmm0, 9792(%rsp)               # 4-byte Spill
	vmovaps	32512(%rsp), %zmm0              # 64-byte Reload
	vextractf64x4	$1, %zmm0, %ymm3
	vaddps	%zmm3, %zmm0, %zmm3
	vextractf128	$1, %ymm3, %xmm6
	vaddps	%xmm6, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm6         # xmm6 = xmm3[1,0]
	vaddps	%xmm6, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm6            # xmm6 = xmm3[1,1,3,3]
	vaddss	%xmm6, %xmm3, %xmm0
	vmovss	%xmm0, 9536(%rsp)               # 4-byte Spill
	vmovaps	32256(%rsp), %zmm0              # 64-byte Reload
	vextractf64x4	$1, %zmm0, %ymm3
	vaddps	%zmm3, %zmm0, %zmm3
	vextractf128	$1, %ymm3, %xmm6
	vaddps	%xmm6, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm6         # xmm6 = xmm3[1,0]
	vaddps	%xmm6, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm6            # xmm6 = xmm3[1,1,3,3]
	vaddss	%xmm6, %xmm3, %xmm0
	vmovss	%xmm0, 9472(%rsp)               # 4-byte Spill
	vmovaps	31872(%rsp), %zmm0              # 64-byte Reload
	vextractf64x4	$1, %zmm0, %ymm3
	vaddps	%zmm3, %zmm0, %zmm3
	vextractf128	$1, %ymm3, %xmm15
	vaddps	%xmm3, %xmm15, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm15        # xmm15 = xmm3[1,0]
	vaddps	%xmm3, %xmm15, %xmm3
	vmovshdup	%xmm3, %xmm15           # xmm15 = xmm3[1,1,3,3]
	vaddss	%xmm3, %xmm15, %xmm0
	vmovss	%xmm0, 9408(%rsp)               # 4-byte Spill
	vmovaps	14272(%rsp), %zmm0              # 64-byte Reload
	vextractf64x4	$1, %zmm0, %ymm3
	vaddps	%zmm3, %zmm0, %zmm3
	vextractf128	$1, %ymm3, %xmm15
	vaddps	%xmm3, %xmm15, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm15        # xmm15 = xmm3[1,0]
	vaddps	%xmm3, %xmm15, %xmm3
	vmovshdup	%xmm3, %xmm15           # xmm15 = xmm3[1,1,3,3]
	vaddss	%xmm3, %xmm15, %xmm0
	vmovss	%xmm0, 9344(%rsp)               # 4-byte Spill
	vmovaps	10112(%rsp), %zmm0              # 64-byte Reload
	vextractf64x4	$1, %zmm0, %ymm15
	vaddps	%zmm15, %zmm0, %zmm0
	vextractf128	$1, %ymm0, %xmm15
	vaddps	%xmm0, %xmm15, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm15        # xmm15 = xmm0[1,0]
	vaddps	%xmm0, %xmm15, %xmm0
	vmovshdup	%xmm0, %xmm15           # xmm15 = xmm0[1,1,3,3]
	vaddss	%xmm0, %xmm15, %xmm0
	vaddss	%xmm27, %xmm0, %xmm0
	vinsertps	$16, %xmm0, %xmm14, %xmm0 # xmm0 = xmm14[0],xmm0[0],xmm14[2,3]
	vextractf64x4	$1, %zmm10, %ymm14
	vaddps	%zmm14, %zmm10, %zmm9
	vextractf128	$1, %ymm9, %xmm14
	vaddps	%xmm14, %xmm9, %xmm9
	vshufpd	$1, %xmm9, %xmm9, %xmm14        # xmm14 = xmm9[1,0]
	vaddps	%xmm14, %xmm9, %xmm9
	vmovshdup	%xmm9, %xmm14           # xmm14 = xmm9[1,1,3,3]
	vaddss	%xmm14, %xmm9, %xmm9
	vaddss	%xmm27, %xmm9, %xmm9
	vinsertps	$32, %xmm9, %xmm0, %xmm0 # xmm0 = xmm0[0,1],xmm9[0],xmm0[3]
	vmulps	8384(%rsp), %zmm11, %zmm9       # 64-byte Folded Reload
	vfmadd231ps	1536(%rsp), %zmm1, %zmm9 # 64-byte Folded Reload
                                        # zmm9 = (zmm1 * mem) + zmm9
	vextractf64x4	$1, %zmm9, %ymm14
	vaddps	%zmm14, %zmm9, %zmm9
	vextractf128	$1, %ymm9, %xmm14
	vaddps	%xmm14, %xmm9, %xmm9
	vshufpd	$1, %xmm9, %xmm9, %xmm14        # xmm14 = xmm9[1,0]
	vaddps	%xmm14, %xmm9, %xmm9
	vmovshdup	%xmm9, %xmm14           # xmm14 = xmm9[1,1,3,3]
	vaddss	%xmm14, %xmm9, %xmm9
	vaddss	%xmm27, %xmm9, %xmm9
	vinsertps	$48, %xmm9, %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],xmm9[0]
	vmovaps	%xmm0, 12992(%rsp)              # 16-byte Spill
	vmovaps	1600(%rsp), %zmm2               # 64-byte Reload
	vmovaps	%zmm26, %zmm29
	vmulps	%zmm2, %zmm26, %zmm0
	vmovaps	1664(%rsp), %zmm1               # 64-byte Reload
	vmovaps	%zmm7, %zmm3
	vfmadd231ps	%zmm1, %zmm7, %zmm0     # zmm0 = (zmm7 * zmm1) + zmm0
	vextractf64x4	$1, %zmm0, %ymm9
	vaddps	%zmm9, %zmm0, %zmm0
	vextractf128	$1, %ymm0, %xmm9
	vaddps	%xmm0, %xmm9, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm9         # xmm9 = xmm0[1,0]
	vaddps	%xmm0, %xmm9, %xmm0
	vmovshdup	%xmm0, %xmm9            # xmm9 = xmm0[1,1,3,3]
	vaddss	%xmm0, %xmm9, %xmm0
	vaddss	%xmm27, %xmm0, %xmm0
	vinsertps	$16, %xmm12, %xmm0, %xmm0 # xmm0 = xmm0[0],xmm12[0],xmm0[2,3]
	vinsertps	$32, 12096(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, 12032(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%ymm0, 14272(%rsp)              # 32-byte Spill
	vmovaps	32704(%rsp), %zmm0              # 64-byte Reload
	vextractf64x4	$1, %zmm0, %ymm9
	vaddps	%zmm9, %zmm0, %zmm9
	vextractf128	$1, %ymm9, %xmm12
	vaddps	%xmm12, %xmm9, %xmm9
	vshufpd	$1, %xmm9, %xmm9, %xmm12        # xmm12 = xmm9[1,0]
	vaddps	%xmm12, %xmm9, %xmm9
	vmovshdup	%xmm9, %xmm12           # xmm12 = xmm9[1,1,3,3]
	vaddss	%xmm12, %xmm9, %xmm9
	vaddss	%xmm27, %xmm9, %xmm9
	vmovaps	12224(%rsp), %xmm0              # 16-byte Reload
	vinsertps	$16, %xmm9, %xmm0, %xmm9 # xmm9 = xmm0[0],xmm9[0],xmm0[2,3]
	vinsertps	$32, 12160(%rsp), %xmm9, %xmm9 # 16-byte Folded Reload
                                        # xmm9 = xmm9[0,1],mem[0],xmm9[3]
	vmovaps	32448(%rsp), %zmm0              # 64-byte Reload
	vextractf64x4	$1, %zmm0, %ymm10
	vaddps	%zmm10, %zmm0, %zmm10
	vextractf128	$1, %ymm10, %xmm11
	vaddps	%xmm11, %xmm10, %xmm10
	vshufpd	$1, %xmm10, %xmm10, %xmm11      # xmm11 = xmm10[1,0]
	vaddps	%xmm11, %xmm10, %xmm10
	vmovshdup	%xmm10, %xmm11          # xmm11 = xmm10[1,1,3,3]
	vaddss	%xmm11, %xmm10, %xmm10
	vaddss	%xmm27, %xmm10, %xmm10
	vinsertps	$48, %xmm10, %xmm9, %xmm0 # xmm0 = xmm9[0,1,2],xmm10[0]
	vmovaps	%xmm0, 10112(%rsp)              # 16-byte Spill
	vmulps	%zmm2, %zmm4, %zmm9
	vfmadd231ps	%zmm1, %zmm5, %zmm9     # zmm9 = (zmm5 * zmm1) + zmm9
	vextractf64x4	$1, %zmm9, %ymm10
	vaddps	%zmm10, %zmm9, %zmm9
	vextractf128	$1, %ymm9, %xmm10
	vaddps	%xmm10, %xmm9, %xmm9
	vshufpd	$1, %xmm9, %xmm9, %xmm10        # xmm10 = xmm9[1,0]
	vaddps	%xmm10, %xmm9, %xmm9
	vmovshdup	%xmm9, %xmm10           # xmm10 = xmm9[1,1,3,3]
	vaddss	%xmm10, %xmm9, %xmm9
	vaddss	%xmm27, %xmm9, %xmm9
	vinsertps	$16, 9280(%rsp), %xmm9, %xmm7 # 16-byte Folded Reload
                                        # xmm7 = xmm9[0],mem[0],xmm9[2,3]
	vinsertps	$32, 9088(%rsp), %xmm7, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm7[0,1],mem[0],xmm7[3]
	vmovaps	33856(%rsp), %zmm0              # 64-byte Reload
	vextractf64x4	$1, %zmm0, %ymm7
	vaddps	%zmm7, %zmm0, %zmm7
	vextractf128	$1, %ymm7, %xmm9
	vaddps	%xmm7, %xmm9, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm9         # xmm9 = xmm7[1,0]
	vaddps	%xmm7, %xmm9, %xmm7
	vmovshdup	%xmm7, %xmm9            # xmm9 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm9, %xmm7
	vaddss	%xmm27, %xmm7, %xmm7
	vinsertps	$48, %xmm7, %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],xmm7[0]
	vmovaps	128(%rsp), %zmm15               # 64-byte Reload
	vmovaps	%zmm4, %zmm1
	vmulps	%zmm15, %zmm4, %zmm7
	vmovaps	896(%rsp), %zmm4                # 64-byte Reload
	vfmadd231ps	%zmm4, %zmm5, %zmm7     # zmm7 = (zmm5 * zmm4) + zmm7
	vextractf64x4	$1, %zmm7, %ymm9
	vaddps	%zmm9, %zmm7, %zmm7
	vextractf128	$1, %ymm7, %xmm9
	vaddps	%xmm7, %xmm9, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm9         # xmm9 = xmm7[1,0]
	vaddps	%xmm7, %xmm9, %xmm7
	vinsertf128	$1, 2656(%rsp), %ymm2, %ymm0 # 16-byte Folded Reload
	vmovaps	%zmm0, 32704(%rsp)              # 64-byte Spill
	vmovshdup	%xmm7, %xmm2            # xmm2 = xmm7[1,1,3,3]
	vaddss	%xmm2, %xmm7, %xmm2
	vmovaps	2368(%rsp), %zmm16              # 64-byte Reload
	vmulps	%zmm16, %zmm1, %zmm7
	vmovaps	2560(%rsp), %zmm17              # 64-byte Reload
	vfmadd231ps	%zmm17, %zmm5, %zmm7    # zmm7 = (zmm5 * zmm17) + zmm7
	vmovaps	%zmm5, %zmm0
	vextractf64x4	$1, %zmm7, %ymm9
	vaddps	%zmm9, %zmm7, %zmm7
	vextractf128	$1, %ymm7, %xmm9
	vaddps	%xmm7, %xmm9, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm9         # xmm9 = xmm7[1,0]
	vaddps	%xmm7, %xmm9, %xmm7
	vmovshdup	%xmm7, %xmm9            # xmm9 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm9, %xmm7
	vaddss	%xmm27, %xmm2, %xmm2
	vaddss	%xmm27, %xmm7, %xmm7
	vinsertps	$16, %xmm7, %xmm2, %xmm2 # xmm2 = xmm2[0],xmm7[0],xmm2[2,3]
	vmovaps	1984(%rsp), %zmm13              # 64-byte Reload
	vmulps	%zmm13, %zmm1, %zmm7
	vmovaps	1024(%rsp), %zmm26              # 64-byte Reload
	vfmadd231ps	%zmm26, %zmm5, %zmm7    # zmm7 = (zmm5 * zmm26) + zmm7
	vextractf64x4	$1, %zmm7, %ymm9
	vaddps	%zmm9, %zmm7, %zmm7
	vextractf128	$1, %ymm7, %xmm9
	vaddps	%xmm7, %xmm9, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm9         # xmm9 = xmm7[1,0]
	vaddps	%xmm7, %xmm9, %xmm7
	vmovshdup	%xmm7, %xmm9            # xmm9 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm9, %xmm7
	vaddss	%xmm27, %xmm7, %xmm7
	vinsertps	$32, %xmm7, %xmm2, %xmm2 # xmm2 = xmm2[0,1],xmm7[0],xmm2[3]
	vmovaps	832(%rsp), %zmm6                # 64-byte Reload
	vmulps	%zmm6, %zmm1, %zmm7
	vmovaps	960(%rsp), %zmm5                # 64-byte Reload
	vfmadd231ps	%zmm5, %zmm0, %zmm7     # zmm7 = (zmm0 * zmm5) + zmm7
	vextractf64x4	$1, %zmm7, %ymm9
	vaddps	%zmm9, %zmm7, %zmm7
	vextractf128	$1, %ymm7, %xmm9
	vaddps	%xmm7, %xmm9, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm9         # xmm9 = xmm7[1,0]
	vaddps	%xmm7, %xmm9, %xmm7
	vmovshdup	%xmm7, %xmm9            # xmm9 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm9, %xmm7
	vaddss	%xmm27, %xmm7, %xmm7
	vinsertps	$48, %xmm7, %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],xmm7[0]
	vmovaps	%xmm2, 32576(%rsp)              # 16-byte Spill
	vmovaps	448(%rsp), %zmm19               # 64-byte Reload
	vmulps	%zmm19, %zmm29, %zmm2
	vmovaps	512(%rsp), %zmm22               # 64-byte Reload
	vfmadd231ps	%zmm22, %zmm3, %zmm2    # zmm2 = (zmm3 * zmm22) + zmm2
	vextractf64x4	$1, %zmm2, %ymm7
	vaddps	%zmm7, %zmm2, %zmm2
	vmovaps	3136(%rsp), %zmm25              # 64-byte Reload
	vmulps	%zmm25, %zmm1, %zmm7
	vmovaps	1856(%rsp), %zmm28              # 64-byte Reload
	vfmadd231ps	%zmm28, %zmm0, %zmm7    # zmm7 = (zmm0 * zmm28) + zmm7
	vextractf64x4	$1, %zmm7, %ymm9
	vaddps	%zmm9, %zmm7, %zmm7
	vextractf128	$1, %ymm7, %xmm9
	vaddps	%xmm7, %xmm9, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm9         # xmm9 = xmm7[1,0]
	vaddps	%xmm7, %xmm9, %xmm7
	vmovshdup	%xmm7, %xmm9            # xmm9 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm9, %xmm7
	vaddss	%xmm27, %xmm7, %xmm7
	vmovaps	%xmm7, 32512(%rsp)              # 16-byte Spill
	vmovaps	2112(%rsp), %zmm21              # 64-byte Reload
	vmulps	%zmm21, %zmm1, %zmm7
	vmovaps	2496(%rsp), %zmm20              # 64-byte Reload
	vfmadd231ps	%zmm20, %zmm0, %zmm7    # zmm7 = (zmm0 * zmm20) + zmm7
	vextractf64x4	$1, %zmm7, %ymm9
	vaddps	%zmm9, %zmm7, %zmm7
	vextractf128	$1, %ymm7, %xmm9
	vaddps	%xmm7, %xmm9, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm9         # xmm9 = xmm7[1,0]
	vaddps	%xmm7, %xmm9, %xmm7
	vmovshdup	%xmm7, %xmm9            # xmm9 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm9, %xmm7
	vaddss	%xmm27, %xmm7, %xmm7
	vmovaps	%xmm7, 32448(%rsp)              # 16-byte Spill
	vextractf128	$1, %ymm2, %xmm7
	vaddps	%xmm7, %xmm2, %xmm2
	vmovaps	3072(%rsp), %zmm23              # 64-byte Reload
	vmulps	%zmm23, %zmm1, %zmm7
	vmovaps	1472(%rsp), %zmm8               # 64-byte Reload
	vfmadd231ps	%zmm8, %zmm0, %zmm7     # zmm7 = (zmm0 * zmm8) + zmm7
	vextractf64x4	$1, %zmm7, %ymm9
	vaddps	%zmm9, %zmm7, %zmm7
	vextractf128	$1, %ymm7, %xmm9
	vaddps	%xmm7, %xmm9, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm9         # xmm9 = xmm7[1,0]
	vaddps	%xmm7, %xmm9, %xmm7
	vmovshdup	%xmm7, %xmm9            # xmm9 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm9, %xmm7
	vaddss	%xmm27, %xmm7, %xmm7
	vmovaps	%xmm7, 32320(%rsp)              # 16-byte Spill
	vmovaps	256(%rsp), %zmm14               # 64-byte Reload
	vmulps	%zmm14, %zmm1, %zmm7
	vmovaps	1408(%rsp), %zmm12              # 64-byte Reload
	vfmadd231ps	%zmm12, %zmm0, %zmm7    # zmm7 = (zmm0 * zmm12) + zmm7
	vextractf64x4	$1, %zmm7, %ymm9
	vaddps	%zmm9, %zmm7, %zmm7
	vextractf128	$1, %ymm7, %xmm9
	vaddps	%xmm7, %xmm9, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm9         # xmm9 = xmm7[1,0]
	vaddps	%xmm7, %xmm9, %xmm7
	vmovshdup	%xmm7, %xmm9            # xmm9 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm9, %xmm7
	vaddss	%xmm27, %xmm7, %xmm7
	vmovaps	%xmm7, 32256(%rsp)              # 16-byte Spill
	vshufpd	$1, %xmm2, %xmm2, %xmm7         # xmm7 = xmm2[1,0]
	vaddps	%xmm7, %xmm2, %xmm7
	vmovaps	2048(%rsp), %zmm11              # 64-byte Reload
	vmulps	%zmm11, %zmm1, %zmm2
	vmovaps	704(%rsp), %zmm10               # 64-byte Reload
	vfmadd231ps	%zmm10, %zmm0, %zmm2    # zmm2 = (zmm0 * zmm10) + zmm2
	vextractf64x4	$1, %zmm2, %ymm9
	vaddps	%zmm9, %zmm2, %zmm2
	vextractf128	$1, %ymm2, %xmm9
	vaddps	%xmm2, %xmm9, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm9         # xmm9 = xmm2[1,0]
	vaddps	%xmm2, %xmm9, %xmm2
	vmovshdup	%xmm2, %xmm9            # xmm9 = xmm2[1,1,3,3]
	vaddss	%xmm2, %xmm9, %xmm2
	vaddss	%xmm27, %xmm2, %xmm2
	vmovaps	%xmm2, 32192(%rsp)              # 16-byte Spill
	vmulps	%zmm30, %zmm29, %zmm2
	vfmadd231ps	%zmm24, %zmm3, %zmm2    # zmm2 = (zmm3 * zmm24) + zmm2
	vextractf64x4	$1, %zmm2, %ymm9
	vaddps	%zmm9, %zmm2, %zmm2
	vextractf128	$1, %ymm2, %xmm9
	vaddps	%xmm2, %xmm9, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm9         # xmm9 = xmm2[1,0]
	vaddps	%xmm2, %xmm9, %xmm2
	vmovshdup	%xmm2, %xmm9            # xmm9 = xmm2[1,1,3,3]
	vaddss	%xmm2, %xmm9, %xmm2
	vaddss	%xmm27, %xmm2, %xmm2
	vmovshdup	%xmm7, %xmm9            # xmm9 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm9, %xmm7
	vmulps	%zmm11, %zmm29, %zmm9
	vfmadd231ps	%zmm10, %zmm3, %zmm9    # zmm9 = (zmm3 * zmm10) + zmm9
	vextractf64x4	$1, %zmm9, %ymm10
	vaddps	%zmm10, %zmm9, %zmm9
	vextractf128	$1, %ymm9, %xmm10
	vaddps	%xmm10, %xmm9, %xmm9
	vshufpd	$1, %xmm9, %xmm9, %xmm10        # xmm10 = xmm9[1,0]
	vaddps	%xmm10, %xmm9, %xmm9
	vmovaps	31744(%rsp), %zmm11             # 64-byte Reload
	vextractf64x4	$1, %zmm11, %ymm10
	vaddps	%zmm10, %zmm11, %zmm10
	vextractf128	$1, %ymm10, %xmm11
	vaddps	%xmm11, %xmm10, %xmm10
	vshufpd	$1, %xmm10, %xmm10, %xmm11      # xmm11 = xmm10[1,0]
	vaddps	%xmm11, %xmm10, %xmm10
	vmovshdup	%xmm10, %xmm11          # xmm11 = xmm10[1,1,3,3]
	vaddss	%xmm11, %xmm10, %xmm10
	vaddss	%xmm27, %xmm10, %xmm11
	vmovshdup	%xmm9, %xmm10           # xmm10 = xmm9[1,1,3,3]
	vaddss	%xmm10, %xmm9, %xmm9
	vmulps	%zmm14, %zmm29, %zmm10
	vfmadd231ps	%zmm12, %zmm3, %zmm10   # zmm10 = (zmm3 * zmm12) + zmm10
	vextractf64x4	$1, %zmm10, %ymm12
	vaddps	%zmm12, %zmm10, %zmm10
	vextractf128	$1, %ymm10, %xmm12
	vaddps	%xmm12, %xmm10, %xmm10
	vmovaps	32000(%rsp), %zmm14             # 64-byte Reload
	vextractf64x4	$1, %zmm14, %ymm12
	vaddps	%zmm12, %zmm14, %zmm12
	vextractf128	$1, %ymm12, %xmm14
	vaddps	%xmm14, %xmm12, %xmm12
	vshufpd	$1, %xmm12, %xmm12, %xmm14      # xmm14 = xmm12[1,0]
	vaddps	%xmm14, %xmm12, %xmm12
	vmovshdup	%xmm12, %xmm14          # xmm14 = xmm12[1,1,3,3]
	vaddss	%xmm14, %xmm12, %xmm12
	vaddss	%xmm27, %xmm12, %xmm12
	vshufpd	$1, %xmm10, %xmm10, %xmm14      # xmm14 = xmm10[1,0]
	vaddps	%xmm14, %xmm10, %xmm10
	vmovshdup	%xmm10, %xmm14          # xmm14 = xmm10[1,1,3,3]
	vaddss	%xmm14, %xmm10, %xmm10
	vmulps	%zmm15, %zmm29, %zmm14
	vmovaps	%zmm15, %zmm30
	vfmadd231ps	%zmm4, %zmm3, %zmm14    # zmm14 = (zmm3 * zmm4) + zmm14
	vextractf64x4	$1, %zmm14, %ymm15
	vaddps	%zmm15, %zmm14, %zmm14
	vextractf128	$1, %ymm14, %xmm15
	vaddps	%xmm15, %xmm14, %xmm14
	vmovaps	32128(%rsp), %zmm4              # 64-byte Reload
	vextractf64x4	$1, %zmm4, %ymm15
	vaddps	%zmm15, %zmm4, %zmm15
	vextractf32x4	$1, %ymm15, %xmm18
	vaddps	%xmm18, %xmm15, %xmm15
	vshufpd	$1, %xmm15, %xmm15, %xmm18      # xmm18 = xmm15[1,0]
	vaddps	%xmm18, %xmm15, %xmm15
	vmovshdup	%xmm15, %xmm18          # xmm18 = xmm15[1,1,3,3]
	vaddss	%xmm18, %xmm15, %xmm15
	vaddss	%xmm27, %xmm15, %xmm15
	vmulps	%zmm19, %zmm1, %zmm18
	vfmadd231ps	%zmm22, %zmm0, %zmm18   # zmm18 = (zmm0 * zmm22) + zmm18
	vextractf64x4	$1, %zmm18, %ymm22
	vaddps	%zmm22, %zmm18, %zmm18
	vextractf32x4	$1, %ymm18, %xmm22
	vaddps	%xmm22, %xmm18, %xmm18
	vshufpd	$1, %xmm18, %xmm18, %xmm22      # xmm22 = xmm18[1,0]
	vaddps	%xmm22, %xmm18, %xmm18
	vmovshdup	%xmm18, %xmm22          # xmm22 = xmm18[1,1,3,3]
	vaddss	%xmm22, %xmm18, %xmm18
	vaddss	%xmm27, %xmm18, %xmm1
	vmovaps	%xmm1, 32128(%rsp)              # 16-byte Spill
	vshufpd	$1, %xmm14, %xmm14, %xmm18      # xmm18 = xmm14[1,0]
	vaddps	%xmm18, %xmm14, %xmm14
	vmovshdup	%xmm14, %xmm18          # xmm18 = xmm14[1,1,3,3]
	vaddss	%xmm18, %xmm14, %xmm14
	vmulps	%zmm16, %zmm29, %zmm18
	vfmadd231ps	%zmm17, %zmm3, %zmm18   # zmm18 = (zmm3 * zmm17) + zmm18
	vextractf64x4	$1, %zmm18, %ymm22
	vaddps	%zmm22, %zmm18, %zmm18
	vextractf32x4	$1, %ymm18, %xmm22
	vaddps	%xmm22, %xmm18, %xmm18
	vshufpd	$1, %xmm18, %xmm18, %xmm22      # xmm22 = xmm18[1,0]
	vaddps	%xmm22, %xmm18, %xmm18
	vmovshdup	%xmm18, %xmm22          # xmm22 = xmm18[1,1,3,3]
	vaddss	%xmm22, %xmm18, %xmm18
	vaddss	%xmm27, %xmm14, %xmm14
	vaddss	%xmm27, %xmm18, %xmm18
	vinsertps	$16, %xmm18, %xmm14, %xmm14 # xmm14 = xmm14[0],xmm18[0],xmm14[2,3]
	vmulps	%zmm13, %zmm29, %zmm18
	vfmadd231ps	%zmm26, %zmm3, %zmm18   # zmm18 = (zmm3 * zmm26) + zmm18
	vextractf64x4	$1, %zmm18, %ymm22
	vaddps	%zmm22, %zmm18, %zmm18
	vextractf32x4	$1, %ymm18, %xmm22
	vaddps	%xmm22, %xmm18, %xmm18
	vshufpd	$1, %xmm18, %xmm18, %xmm22      # xmm22 = xmm18[1,0]
	vaddps	%xmm22, %xmm18, %xmm18
	vmovshdup	%xmm18, %xmm22          # xmm22 = xmm18[1,1,3,3]
	vaddss	%xmm22, %xmm18, %xmm18
	vaddss	%xmm27, %xmm18, %xmm18
	vinsertps	$32, %xmm18, %xmm14, %xmm14 # xmm14 = xmm14[0,1],xmm18[0],xmm14[3]
	vmulps	%zmm6, %zmm29, %zmm18
	vfmadd231ps	%zmm5, %zmm3, %zmm18    # zmm18 = (zmm3 * zmm5) + zmm18
	vextractf64x4	$1, %zmm18, %ymm22
	vaddps	%zmm22, %zmm18, %zmm18
	vextractf32x4	$1, %ymm18, %xmm22
	vaddps	%xmm22, %xmm18, %xmm18
	vshufpd	$1, %xmm18, %xmm18, %xmm22      # xmm22 = xmm18[1,0]
	vaddps	%xmm22, %xmm18, %xmm18
	vmovshdup	%xmm18, %xmm22          # xmm22 = xmm18[1,1,3,3]
	vaddss	%xmm22, %xmm18, %xmm18
	vaddss	%xmm27, %xmm18, %xmm18
	vinsertps	$48, %xmm18, %xmm14, %xmm0 # xmm0 = xmm14[0,1,2],xmm18[0]
	vmovaps	%xmm0, 32000(%rsp)              # 16-byte Spill
	vmulps	640(%rsp), %zmm29, %zmm14       # 64-byte Folded Reload
	vfmadd231ps	64(%rsp), %zmm3, %zmm14 # 64-byte Folded Reload
                                        # zmm14 = (zmm3 * mem) + zmm14
	vextractf64x4	$1, %zmm14, %ymm18
	vaddps	%zmm18, %zmm14, %zmm14
	vextractf32x4	$1, %ymm14, %xmm18
	vaddps	%xmm18, %xmm14, %xmm14
	vshufpd	$1, %xmm14, %xmm14, %xmm18      # xmm18 = xmm14[1,0]
	vaddps	%xmm18, %xmm14, %xmm14
	vmovshdup	%xmm14, %xmm18          # xmm18 = xmm14[1,1,3,3]
	vaddss	%xmm18, %xmm14, %xmm14
	vaddss	%xmm27, %xmm14, %xmm14
	vinsertps	$16, %xmm15, %xmm14, %xmm14 # xmm14 = xmm14[0],xmm15[0],xmm14[2,3]
	vinsertps	$32, %xmm12, %xmm14, %xmm12 # xmm12 = xmm14[0,1],xmm12[0],xmm14[3]
	vinsertps	$48, %xmm11, %xmm12, %xmm11 # xmm11 = xmm12[0,1,2],xmm11[0]
	vmovaps	14208(%rsp), %zmm0              # 64-byte Reload
	vextractf64x4	$1, %zmm0, %ymm12
	vaddps	%zmm12, %zmm0, %zmm12
	vextractf128	$1, %ymm12, %xmm14
	vaddps	%xmm14, %xmm12, %xmm12
	vshufpd	$1, %xmm12, %xmm12, %xmm14      # xmm14 = xmm12[1,0]
	vaddps	%xmm14, %xmm12, %xmm12
	vmovshdup	%xmm12, %xmm14          # xmm14 = xmm12[1,1,3,3]
	vaddss	%xmm14, %xmm12, %xmm12
	vaddss	%xmm27, %xmm12, %xmm12
	vinsertps	$16, %xmm12, %xmm2, %xmm2 # xmm2 = xmm2[0],xmm12[0],xmm2[2,3]
	vmovaps	10048(%rsp), %zmm0              # 64-byte Reload
	vextractf64x4	$1, %zmm0, %ymm12
	vaddps	%zmm12, %zmm0, %zmm12
	vextractf128	$1, %ymm12, %xmm14
	vaddps	%xmm14, %xmm12, %xmm12
	vshufpd	$1, %xmm12, %xmm12, %xmm14      # xmm14 = xmm12[1,0]
	vaddps	%xmm14, %xmm12, %xmm12
	vmovshdup	%xmm12, %xmm14          # xmm14 = xmm12[1,1,3,3]
	vaddss	%xmm14, %xmm12, %xmm12
	vaddss	%xmm27, %xmm12, %xmm12
	vinsertps	$32, %xmm12, %xmm2, %xmm2 # xmm2 = xmm2[0,1],xmm12[0],xmm2[3]
	vmovaps	8384(%rsp), %zmm4               # 64-byte Reload
	vmulps	%zmm4, %zmm29, %zmm12
	vmovaps	1536(%rsp), %zmm13              # 64-byte Reload
	vfmadd231ps	%zmm13, %zmm3, %zmm12   # zmm12 = (zmm3 * zmm13) + zmm12
	vextractf64x4	$1, %zmm12, %ymm14
	vaddps	%zmm14, %zmm12, %zmm12
	vextractf128	$1, %ymm12, %xmm14
	vaddps	%xmm14, %xmm12, %xmm12
	vshufpd	$1, %xmm12, %xmm12, %xmm14      # xmm14 = xmm12[1,0]
	vaddps	%xmm14, %xmm12, %xmm12
	vmovshdup	%xmm12, %xmm14          # xmm14 = xmm12[1,1,3,3]
	vaddss	%xmm14, %xmm12, %xmm12
	vaddss	%xmm27, %xmm12, %xmm12
	vinsertps	$48, %xmm12, %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],xmm12[0]
	vmulps	%zmm23, %zmm29, %zmm12
	vfmadd231ps	%zmm8, %zmm3, %zmm12    # zmm12 = (zmm3 * zmm8) + zmm12
	vextractf64x4	$1, %zmm12, %ymm14
	vaddps	%zmm14, %zmm12, %zmm12
	vmulps	2432(%rsp), %zmm29, %zmm14      # 64-byte Folded Reload
	vfmadd231ps	1792(%rsp), %zmm3, %zmm14 # 64-byte Folded Reload
                                        # zmm14 = (zmm3 * mem) + zmm14
	vextractf64x4	$1, %zmm14, %ymm15
	vaddps	%zmm15, %zmm14, %zmm14
	vextractf128	$1, %ymm14, %xmm15
	vaddps	%xmm15, %xmm14, %xmm14
	vshufpd	$1, %xmm14, %xmm14, %xmm15      # xmm15 = xmm14[1,0]
	vaddps	%xmm15, %xmm14, %xmm14
	vmovshdup	%xmm14, %xmm15          # xmm15 = xmm14[1,1,3,3]
	vaddss	%xmm15, %xmm14, %xmm14
	vaddss	%xmm27, %xmm14, %xmm14
	vextractf128	$1, %ymm12, %xmm15
	vaddps	%xmm15, %xmm12, %xmm12
	vmovaps	2176(%rsp), %zmm0               # 64-byte Reload
	vmulps	%zmm0, %zmm29, %zmm15
	vmovaps	1728(%rsp), %zmm23              # 64-byte Reload
	vfmadd231ps	%zmm23, %zmm3, %zmm15   # zmm15 = (zmm3 * zmm23) + zmm15
	vextractf64x4	$1, %zmm15, %ymm18
	vaddps	%zmm18, %zmm15, %zmm15
	vextractf32x4	$1, %ymm15, %xmm18
	vaddps	%xmm18, %xmm15, %xmm15
	vshufpd	$1, %xmm15, %xmm15, %xmm18      # xmm18 = xmm15[1,0]
	vaddps	%xmm18, %xmm15, %xmm15
	vmovshdup	%xmm15, %xmm18          # xmm18 = xmm15[1,1,3,3]
	vaddss	%xmm18, %xmm15, %xmm15
	vaddss	%xmm27, %xmm15, %xmm15
	vshufpd	$1, %xmm12, %xmm12, %xmm18      # xmm18 = xmm12[1,0]
	vaddps	%xmm18, %xmm12, %xmm12
	vmovshdup	%xmm12, %xmm18          # xmm18 = xmm12[1,1,3,3]
	vaddss	%xmm18, %xmm12, %xmm12
	vmulps	%zmm21, %zmm29, %zmm18
	vfmadd231ps	%zmm20, %zmm3, %zmm18   # zmm18 = (zmm3 * zmm20) + zmm18
	vextractf64x4	$1, %zmm18, %ymm19
	vaddps	%zmm19, %zmm18, %zmm18
	vextractf32x4	$1, %ymm18, %xmm19
	vaddps	%xmm19, %xmm18, %xmm18
	vshufpd	$1, %xmm18, %xmm18, %xmm19      # xmm19 = xmm18[1,0]
	vaddps	%xmm19, %xmm18, %xmm18
	vmovaps	192(%rsp), %zmm26               # 64-byte Reload
	vmulps	%zmm26, %zmm29, %zmm19
	vmovaps	1344(%rsp), %zmm24              # 64-byte Reload
	vfmadd231ps	%zmm24, %zmm3, %zmm19   # zmm19 = (zmm3 * zmm24) + zmm19
	vextractf64x4	$1, %zmm19, %ymm21
	vaddps	%zmm21, %zmm19, %zmm19
	vextractf32x4	$1, %ymm19, %xmm21
	vaddps	%xmm21, %xmm19, %xmm19
	vshufpd	$1, %xmm19, %xmm19, %xmm21      # xmm21 = xmm19[1,0]
	vaddps	%xmm21, %xmm19, %xmm19
	vmovshdup	%xmm19, %xmm21          # xmm21 = xmm19[1,1,3,3]
	vaddss	%xmm21, %xmm19, %xmm19
	vaddss	%xmm27, %xmm19, %xmm19
	vmovshdup	%xmm18, %xmm21          # xmm21 = xmm18[1,1,3,3]
	vaddss	%xmm21, %xmm18, %xmm18
	vmulps	%zmm25, %zmm29, %zmm21
	vfmadd231ps	%zmm28, %zmm3, %zmm21   # zmm21 = (zmm3 * zmm28) + zmm21
	vextractf64x4	$1, %zmm21, %ymm22
	vaddps	%zmm22, %zmm21, %zmm21
	vextractf32x4	$1, %ymm21, %xmm22
	vaddps	%xmm22, %xmm21, %xmm21
	vshufpd	$1, %xmm21, %xmm21, %xmm22      # xmm22 = xmm21[1,0]
	vaddps	%xmm22, %xmm21, %xmm21
	vmovshdup	%xmm21, %xmm22          # xmm22 = xmm21[1,1,3,3]
	vaddss	%xmm22, %xmm21, %xmm21
	vinsertf128	$1, %xmm2, %ymm11, %ymm1
	vmovaps	%zmm1, 31872(%rsp)              # 64-byte Spill
	vmulps	%zmm31, %zmm29, %zmm2
	vfmadd231ps	1088(%rsp), %zmm3, %zmm2 # 64-byte Folded Reload
                                        # zmm2 = (zmm3 * mem) + zmm2
	vextractf64x4	$1, %zmm2, %ymm11
	vaddps	%zmm11, %zmm2, %zmm2
	vextractf128	$1, %ymm2, %xmm11
	vaddps	%xmm2, %xmm11, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm11        # xmm11 = xmm2[1,0]
	vaddps	%xmm2, %xmm11, %xmm2
	vmovshdup	%xmm2, %xmm11           # xmm11 = xmm2[1,1,3,3]
	vaddss	%xmm2, %xmm11, %xmm2
	vaddss	%xmm27, %xmm2, %xmm2
	vinsertps	$16, %xmm2, %xmm19, %xmm2 # xmm2 = xmm19[0],xmm2[0],xmm19[2,3]
	vinsertps	$32, %xmm15, %xmm2, %xmm2 # xmm2 = xmm2[0,1],xmm15[0],xmm2[3]
	vinsertps	$48, %xmm14, %xmm2, %xmm1 # xmm1 = xmm2[0,1,2],xmm14[0]
	vmovaps	%xmm1, 31744(%rsp)              # 16-byte Spill
	vmovaps	14272(%rsp), %ymm1              # 32-byte Reload
	vinsertf128	$1, 10112(%rsp), %ymm1, %ymm1 # 16-byte Folded Reload
	vmovaps	%zmm1, 14272(%rsp)              # 64-byte Spill
	vxorps	%xmm2, %xmm2, %xmm2
	vaddss	14848(%rsp), %xmm2, %xmm11      # 4-byte Folded Reload
	vaddss	352(%rsp), %xmm2, %xmm1         # 4-byte Folded Reload
	vmovaps	%xmm1, 28160(%rsp)              # 16-byte Spill
	vaddss	11072(%rsp), %xmm2, %xmm1       # 4-byte Folded Reload
	vmovaps	%xmm1, 5488(%rsp)               # 16-byte Spill
	vaddss	16320(%rsp), %xmm2, %xmm1       # 4-byte Folded Reload
	vmovaps	%xmm1, 28288(%rsp)              # 16-byte Spill
	vaddss	15680(%rsp), %xmm2, %xmm1       # 4-byte Folded Reload
	vmovaps	%xmm1, 28224(%rsp)              # 16-byte Spill
	vaddss	336(%rsp), %xmm2, %xmm1         # 4-byte Folded Reload
	vmovaps	%xmm1, 28416(%rsp)              # 16-byte Spill
	vaddss	9792(%rsp), %xmm2, %xmm1        # 4-byte Folded Reload
	vmovaps	%xmm1, 28352(%rsp)              # 16-byte Spill
	vaddss	9536(%rsp), %xmm2, %xmm1        # 4-byte Folded Reload
	vmovaps	%xmm1, 9536(%rsp)               # 16-byte Spill
	vaddss	9472(%rsp), %xmm2, %xmm1        # 4-byte Folded Reload
	vmovaps	%xmm1, 9472(%rsp)               # 16-byte Spill
	vaddss	9408(%rsp), %xmm2, %xmm1        # 4-byte Folded Reload
	vmovaps	%xmm1, 9408(%rsp)               # 16-byte Spill
	vaddss	9344(%rsp), %xmm2, %xmm1        # 4-byte Folded Reload
	vmovaps	%xmm1, 9344(%rsp)               # 16-byte Spill
	vaddss	%xmm2, %xmm7, %xmm1
	vmovaps	%xmm1, 352(%rsp)                # 16-byte Spill
	vaddss	%xmm2, %xmm9, %xmm1
	vmovaps	%xmm1, 10048(%rsp)              # 16-byte Spill
	vaddss	%xmm2, %xmm10, %xmm1
	vmovaps	%xmm1, 336(%rsp)                # 16-byte Spill
	vaddss	%xmm2, %xmm21, %xmm1
	vmovaps	%xmm1, 14208(%rsp)              # 16-byte Spill
	vaddss	%xmm2, %xmm12, %xmm1
	vmovaps	%xmm1, 10112(%rsp)              # 16-byte Spill
	vaddss	%xmm2, %xmm18, %xmm1
	vmovaps	%xmm1, 2656(%rsp)               # 16-byte Spill
	vmovaps	%zmm4, %zmm1
	vmovaps	5888(%rsp), %zmm29              # 64-byte Reload
	vmulps	%zmm4, %zmm29, %zmm7
	vmulps	3712(%rsp), %zmm4, %zmm31       # 64-byte Folded Reload
	vmulps	3840(%rsp), %zmm4, %zmm28       # 64-byte Folded Reload
	vmulps	3968(%rsp), %zmm4, %zmm25       # 64-byte Folded Reload
	vmulps	4096(%rsp), %zmm4, %zmm6        # 64-byte Folded Reload
	vmulps	4224(%rsp), %zmm4, %zmm16       # 64-byte Folded Reload
	vmulps	6016(%rsp), %zmm4, %zmm3        # 64-byte Folded Reload
	vmulps	4352(%rsp), %zmm4, %zmm2        # 64-byte Folded Reload
	vmulps	4480(%rsp), %zmm4, %zmm22       # 64-byte Folded Reload
	vmulps	4608(%rsp), %zmm4, %zmm21       # 64-byte Folded Reload
	vmulps	3200(%rsp), %zmm4, %zmm20       # 64-byte Folded Reload
	vmulps	3328(%rsp), %zmm4, %zmm19       # 64-byte Folded Reload
	vmulps	3456(%rsp), %zmm4, %zmm18       # 64-byte Folded Reload
	vmulps	3584(%rsp), %zmm4, %zmm17       # 64-byte Folded Reload
	vmulps	4736(%rsp), %zmm4, %zmm15       # 64-byte Folded Reload
	vmulps	8064(%rsp), %zmm4, %zmm5        # 64-byte Folded Reload
	vmulps	6528(%rsp), %zmm4, %zmm4        # 64-byte Folded Reload
	vmulps	6592(%rsp), %zmm1, %zmm14       # 64-byte Folded Reload
	vmulps	5056(%rsp), %zmm1, %zmm12       # 64-byte Folded Reload
	vmulps	5184(%rsp), %zmm1, %zmm10       # 64-byte Folded Reload
	vmulps	5312(%rsp), %zmm1, %zmm9        # 64-byte Folded Reload
	vmulps	7040(%rsp), %zmm1, %zmm8        # 64-byte Folded Reload
	vmovaps	%zmm13, %zmm1
	vmovaps	5952(%rsp), %zmm13              # 64-byte Reload
	vfmadd231ps	%zmm1, %zmm13, %zmm7    # zmm7 = (zmm13 * zmm1) + zmm7
	vfmadd231ps	3776(%rsp), %zmm1, %zmm31 # 64-byte Folded Reload
                                        # zmm31 = (zmm1 * mem) + zmm31
	vmovaps	%zmm31, 28480(%rsp)             # 64-byte Spill
	vfmadd231ps	3904(%rsp), %zmm1, %zmm28 # 64-byte Folded Reload
                                        # zmm28 = (zmm1 * mem) + zmm28
	vmovaps	%zmm28, 28608(%rsp)             # 64-byte Spill
	vfmadd231ps	4032(%rsp), %zmm1, %zmm25 # 64-byte Folded Reload
                                        # zmm25 = (zmm1 * mem) + zmm25
	vmovaps	%zmm25, 28736(%rsp)             # 64-byte Spill
	vfmadd231ps	4160(%rsp), %zmm1, %zmm6 # 64-byte Folded Reload
                                        # zmm6 = (zmm1 * mem) + zmm6
	vmovaps	%zmm6, 28864(%rsp)              # 64-byte Spill
	vfmadd231ps	4288(%rsp), %zmm1, %zmm16 # 64-byte Folded Reload
                                        # zmm16 = (zmm1 * mem) + zmm16
	vmovaps	%zmm16, 29056(%rsp)             # 64-byte Spill
	vfmadd231ps	6080(%rsp), %zmm1, %zmm3 # 64-byte Folded Reload
                                        # zmm3 = (zmm1 * mem) + zmm3
	vmovaps	%zmm3, 29248(%rsp)              # 64-byte Spill
	vfmadd231ps	4416(%rsp), %zmm1, %zmm2 # 64-byte Folded Reload
                                        # zmm2 = (zmm1 * mem) + zmm2
	vmovaps	%zmm2, 29440(%rsp)              # 64-byte Spill
	vfmadd231ps	4544(%rsp), %zmm1, %zmm22 # 64-byte Folded Reload
                                        # zmm22 = (zmm1 * mem) + zmm22
	vmovaps	%zmm22, 29760(%rsp)             # 64-byte Spill
	vfmadd231ps	4672(%rsp), %zmm1, %zmm21 # 64-byte Folded Reload
                                        # zmm21 = (zmm1 * mem) + zmm21
	vmovaps	%zmm21, 12032(%rsp)             # 64-byte Spill
	vfmadd231ps	3264(%rsp), %zmm1, %zmm20 # 64-byte Folded Reload
                                        # zmm20 = (zmm1 * mem) + zmm20
	vmovaps	%zmm20, 12096(%rsp)             # 64-byte Spill
	vfmadd231ps	3392(%rsp), %zmm1, %zmm19 # 64-byte Folded Reload
                                        # zmm19 = (zmm1 * mem) + zmm19
	vmovaps	%zmm19, 12160(%rsp)             # 64-byte Spill
	vfmadd231ps	3520(%rsp), %zmm1, %zmm18 # 64-byte Folded Reload
                                        # zmm18 = (zmm1 * mem) + zmm18
	vmovaps	%zmm18, 12224(%rsp)             # 64-byte Spill
	vfmadd231ps	3648(%rsp), %zmm1, %zmm17 # 64-byte Folded Reload
                                        # zmm17 = (zmm1 * mem) + zmm17
	vmovaps	%zmm17, 9088(%rsp)              # 64-byte Spill
	vfmadd231ps	4800(%rsp), %zmm1, %zmm15 # 64-byte Folded Reload
                                        # zmm15 = (zmm1 * mem) + zmm15
	vmovaps	%zmm15, 9280(%rsp)              # 64-byte Spill
	vfmadd231ps	8128(%rsp), %zmm1, %zmm5 # 64-byte Folded Reload
                                        # zmm5 = (zmm1 * mem) + zmm5
	vmovaps	%zmm5, 9792(%rsp)               # 64-byte Spill
	vfmadd231ps	4928(%rsp), %zmm1, %zmm4 # 64-byte Folded Reload
                                        # zmm4 = (zmm1 * mem) + zmm4
	vmovaps	%zmm4, 14848(%rsp)              # 64-byte Spill
	vfmadd231ps	4992(%rsp), %zmm1, %zmm14 # 64-byte Folded Reload
                                        # zmm14 = (zmm1 * mem) + zmm14
	vmovaps	%zmm14, 15680(%rsp)             # 64-byte Spill
	vfmadd231ps	5120(%rsp), %zmm1, %zmm12 # 64-byte Folded Reload
                                        # zmm12 = (zmm1 * mem) + zmm12
	vmovaps	%zmm12, 33856(%rsp)             # 64-byte Spill
	vfmadd231ps	5248(%rsp), %zmm1, %zmm10 # 64-byte Folded Reload
                                        # zmm10 = (zmm1 * mem) + zmm10
	vmovaps	%zmm10, 16320(%rsp)             # 64-byte Spill
	vfmadd231ps	5376(%rsp), %zmm1, %zmm9 # 64-byte Folded Reload
                                        # zmm9 = (zmm1 * mem) + zmm9
	vmovaps	%zmm9, 11072(%rsp)              # 64-byte Spill
	vfmadd231ps	7104(%rsp), %zmm1, %zmm8 # 64-byte Folded Reload
                                        # zmm8 = (zmm1 * mem) + zmm8
	vmovaps	%zmm8, 8384(%rsp)               # 64-byte Spill
	vmovapd	37760(%rsp), %zmm1              # 64-byte Reload
	vshuff64x2	$228, 8192(%rsp), %zmm1, %zmm3 # 64-byte Folded Reload
                                        # zmm3 = zmm1[0,1,2,3],mem[4,5,6,7]
	vmovapd	18688(%rsp), %zmm1              # 64-byte Reload
	vshuff64x2	$228, 11008(%rsp), %zmm1, %zmm2 # 64-byte Folded Reload
                                        # zmm2 = zmm1[0,1,2,3],mem[4,5,6,7]
	vmovaps	.LCPI0_104(%rip), %zmm9         # zmm9 = [0,1,2,3,4,5,6,7,8,24,u,u,u,u,u,u]
	vpermt2ps	5760(%rsp), %zmm9, %zmm3 # 64-byte Folded Reload
	vpermt2ps	8832(%rsp), %zmm9, %zmm2 # 64-byte Folded Reload
	vmovapd	.LCPI0_113(%rip), %zmm8         # zmm8 = [0,1,2,3,4,12,u,u]
	vpermt2pd	7168(%rsp), %zmm8, %zmm3 # 64-byte Folded Reload
	vpermt2pd	10944(%rsp), %zmm8, %zmm2 # 64-byte Folded Reload
	vmovaps	.LCPI0_123(%rip), %zmm1         # zmm1 = [0,1,2,3,4,5,6,7,8,9,10,24,u,u,u,u]
	vpermt2ps	8256(%rsp), %zmm1, %zmm3 # 64-byte Folded Reload
	vpermt2ps	8768(%rsp), %zmm1, %zmm2 # 64-byte Folded Reload
	vmovapd	.LCPI0_133(%rip), %zmm10        # zmm10 = [0,1,2,3,4,5,12,u]
	vpermt2pd	7232(%rsp), %zmm10, %zmm3 # 64-byte Folded Reload
	vpermt2pd	8704(%rsp), %zmm10, %zmm2 # 64-byte Folded Reload
	vmovaps	.LCPI0_145(%rip), %zmm12        # zmm12 = [0,1,2,3,4,5,6,7,8,9,10,11,12,24,u,u]
	vpermt2ps	7296(%rsp), %zmm12, %zmm3 # 64-byte Folded Reload
	vpermt2ps	19200(%rsp), %zmm12, %zmm2 # 64-byte Folded Reload
	vmovapd	.LCPI0_158(%rip), %zmm14        # zmm14 = [0,1,2,3,4,5,6,12]
	vpermt2pd	1920(%rsp), %zmm14, %zmm3 # 64-byte Folded Reload
	vpermt2pd	19136(%rsp), %zmm14, %zmm2 # 64-byte Folded Reload
	vmovaps	.LCPI0_172(%rip), %zmm15        # zmm15 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,24]
	vpermt2ps	576(%rsp), %zmm15, %zmm3 # 64-byte Folded Reload
	vmovaps	%zmm3, 1536(%rsp)               # 64-byte Spill
	vpermt2ps	19072(%rsp), %zmm15, %zmm2 # 64-byte Folded Reload
	vmovaps	%zmm2, 1920(%rsp)               # 64-byte Spill
	vmovapd	37696(%rsp), %zmm18             # 64-byte Reload
	vshuff64x2	$228, 5824(%rsp), %zmm18, %zmm5 # 64-byte Folded Reload
                                        # zmm5 = zmm18[0,1,2,3],mem[4,5,6,7]
	vmovapd	37632(%rsp), %zmm20             # 64-byte Reload
	vshuff64x2	$228, 19008(%rsp), %zmm20, %zmm2 # 64-byte Folded Reload
                                        # zmm2 = zmm20[0,1,2,3],mem[4,5,6,7]
	vpermt2ps	7360(%rsp), %zmm9, %zmm5 # 64-byte Folded Reload
	vpermt2ps	10432(%rsp), %zmm9, %zmm2 # 64-byte Folded Reload
	vpermt2pd	2304(%rsp), %zmm8, %zmm5 # 64-byte Folded Reload
	vpermt2pd	10496(%rsp), %zmm8, %zmm2 # 64-byte Folded Reload
	vpermt2ps	7680(%rsp), %zmm1, %zmm5 # 64-byte Folded Reload
	vpermt2ps	10624(%rsp), %zmm1, %zmm2 # 64-byte Folded Reload
	vpermt2pd	7616(%rsp), %zmm10, %zmm5 # 64-byte Folded Reload
	vpermt2pd	10688(%rsp), %zmm10, %zmm2 # 64-byte Folded Reload
	vpermt2ps	7552(%rsp), %zmm12, %zmm5 # 64-byte Folded Reload
	vpermt2ps	10560(%rsp), %zmm12, %zmm2 # 64-byte Folded Reload
	vpermt2pd	7488(%rsp), %zmm14, %zmm5 # 64-byte Folded Reload
	vmovaps	19584(%rsp), %zmm1              # 64-byte Reload
	vextractf64x4	$1, %zmm1, %ymm8
	vaddps	%zmm8, %zmm1, %zmm8
	vextractf128	$1, %ymm8, %xmm9
	vaddps	%xmm9, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm9         # xmm9 = xmm8[1,0]
	vaddps	%xmm9, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm9            # xmm9 = xmm8[1,1,3,3]
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm27, %xmm8, %xmm9
	vpermt2pd	10752(%rsp), %zmm14, %zmm2 # 64-byte Folded Reload
	vmovaps	%zmm29, %zmm1
	vmulps	%zmm0, %zmm29, %zmm8
	vmovaps	%zmm13, %zmm18
	vfmadd231ps	%zmm23, %zmm13, %zmm8   # zmm8 = (zmm13 * zmm23) + zmm8
	vextractf64x4	$1, %zmm8, %ymm10
	vaddps	%zmm10, %zmm8, %zmm8
	vextractf128	$1, %ymm8, %xmm10
	vaddps	%xmm10, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm10        # xmm10 = xmm8[1,0]
	vaddps	%xmm10, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm10           # xmm10 = xmm8[1,1,3,3]
	vaddss	%xmm10, %xmm8, %xmm8
	vaddss	%xmm27, %xmm8, %xmm0
	vmovaps	%xmm0, 5824(%rsp)               # 16-byte Spill
	vpermt2ps	7424(%rsp), %zmm15, %zmm5 # 64-byte Folded Reload
	vmovaps	%zmm5, %zmm8
	vmovaps	%zmm5, 2304(%rsp)               # 64-byte Spill
	vpermt2ps	10816(%rsp), %zmm15, %zmm2 # 64-byte Folded Reload
	vmovaps	%zmm2, 576(%rsp)                # 64-byte Spill
	vmovaps	1600(%rsp), %zmm28              # 64-byte Reload
	vmulps	%zmm28, %zmm29, %zmm10
	vmovaps	1664(%rsp), %zmm5               # 64-byte Reload
	vfmadd231ps	%zmm5, %zmm13, %zmm10   # zmm10 = (zmm13 * zmm5) + zmm10
	vextractf64x4	$1, %zmm10, %ymm12
	vaddps	%zmm12, %zmm10, %zmm10
	vextractf128	$1, %ymm10, %xmm12
	vaddps	%xmm12, %xmm10, %xmm10
	vshufpd	$1, %xmm10, %xmm10, %xmm12      # xmm12 = xmm10[1,0]
	vaddps	%xmm12, %xmm10, %xmm10
	vmovshdup	%xmm10, %xmm12          # xmm12 = xmm10[1,1,3,3]
	vaddss	%xmm12, %xmm10, %xmm10
	vaddss	%xmm27, %xmm10, %xmm10
	vinsertps	$16, %xmm9, %xmm10, %xmm9 # xmm9 = xmm10[0],xmm9[0],xmm10[2,3]
	vinsertps	$32, %xmm11, %xmm9, %xmm9 # xmm9 = xmm9[0,1],xmm11[0],xmm9[3]
	vmovaps	12544(%rsp), %zmm0              # 64-byte Reload
	vextractf64x4	$1, %zmm0, %ymm10
	vaddps	%zmm10, %zmm0, %zmm10
	vextractf128	$1, %ymm10, %xmm11
	vaddps	%xmm11, %xmm10, %xmm10
	vshufpd	$1, %xmm10, %xmm10, %xmm11      # xmm11 = xmm10[1,0]
	vaddps	%xmm11, %xmm10, %xmm10
	vmovshdup	%xmm10, %xmm11          # xmm11 = xmm10[1,1,3,3]
	vaddss	%xmm11, %xmm10, %xmm10
	vaddss	%xmm27, %xmm10, %xmm10
	vinsertps	$48, %xmm10, %xmm9, %xmm0 # xmm0 = xmm9[0,1,2],xmm10[0]
	vmovaps	%ymm0, 5760(%rsp)               # 32-byte Spill
	vmovaps	448(%rsp), %zmm25               # 64-byte Reload
	vmulps	%zmm25, %zmm29, %zmm10
	vmovaps	512(%rsp), %zmm3                # 64-byte Reload
	vfmadd231ps	%zmm3, %zmm13, %zmm10   # zmm10 = (zmm13 * zmm3) + zmm10
	vextractf64x4	$1, %zmm10, %ymm11
	vaddps	%zmm11, %zmm10, %zmm10
	vextractf128	$1, %ymm10, %xmm11
	vaddps	%xmm11, %xmm10, %xmm10
	vshufpd	$1, %xmm10, %xmm10, %xmm11      # xmm11 = xmm10[1,0]
	vaddps	%xmm11, %xmm10, %xmm11
	vmovaps	28544(%rsp), %zmm0              # 64-byte Reload
	vextractf64x4	$1, %zmm0, %ymm10
	vaddps	%zmm10, %zmm0, %zmm4
	vextractf128	$1, %ymm4, %xmm10
	vaddps	%xmm4, %xmm10, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm10        # xmm10 = xmm4[1,0]
	vaddps	%xmm4, %xmm10, %xmm4
	vmovshdup	%xmm4, %xmm10           # xmm10 = xmm4[1,1,3,3]
	vaddss	%xmm4, %xmm10, %xmm4
	vaddss	%xmm27, %xmm4, %xmm4
	vmovaps	1280(%rsp), %zmm9               # 64-byte Reload
	vmulps	%zmm9, %zmm29, %zmm10
	vmovaps	1216(%rsp), %zmm31              # 64-byte Reload
	vfmadd231ps	%zmm31, %zmm13, %zmm10  # zmm10 = (zmm13 * zmm31) + zmm10
	vextractf64x4	$1, %zmm10, %ymm12
	vaddps	%zmm12, %zmm10, %zmm10
	vextractf128	$1, %ymm10, %xmm12
	vaddps	%xmm12, %xmm10, %xmm10
	vshufpd	$1, %xmm10, %xmm10, %xmm12      # xmm12 = xmm10[1,0]
	vaddps	%xmm12, %xmm10, %xmm10
	vmovshdup	%xmm10, %xmm12          # xmm12 = xmm10[1,1,3,3]
	vaddss	%xmm12, %xmm10, %xmm10
	vaddss	%xmm27, %xmm10, %xmm10
	vmovshdup	%xmm11, %xmm12          # xmm12 = xmm11[1,1,3,3]
	vaddss	%xmm12, %xmm11, %xmm0
	vmovss	%xmm0, 7680(%rsp)               # 4-byte Spill
	vmovaps	2048(%rsp), %zmm16              # 64-byte Reload
	vmulps	%zmm16, %zmm29, %zmm11
	vmovaps	704(%rsp), %zmm17               # 64-byte Reload
	vfmadd231ps	%zmm17, %zmm13, %zmm11  # zmm11 = (zmm13 * zmm17) + zmm11
	vextractf64x4	$1, %zmm11, %ymm12
	vaddps	%zmm12, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm12
	vaddps	%xmm12, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm12      # xmm12 = xmm11[1,0]
	vaddps	%xmm12, %xmm11, %xmm11
	vmovaps	29504(%rsp), %zmm0              # 64-byte Reload
	vextractf64x4	$1, %zmm0, %ymm12
	vaddps	%zmm12, %zmm0, %zmm12
	vextractf128	$1, %ymm12, %xmm14
	vaddps	%xmm14, %xmm12, %xmm12
	vshufpd	$1, %xmm12, %xmm12, %xmm14      # xmm14 = xmm12[1,0]
	vaddps	%xmm14, %xmm12, %xmm12
	vmovshdup	%xmm12, %xmm14          # xmm14 = xmm12[1,1,3,3]
	vaddss	%xmm14, %xmm12, %xmm12
	vaddss	%xmm27, %xmm12, %xmm12
	vmovshdup	%xmm11, %xmm14          # xmm14 = xmm11[1,1,3,3]
	vaddss	%xmm14, %xmm11, %xmm0
	vmovss	%xmm0, 7552(%rsp)               # 4-byte Spill
	vmovaps	256(%rsp), %zmm13               # 64-byte Reload
	vmulps	%zmm13, %zmm29, %zmm11
	vfmadd231ps	1408(%rsp), %zmm18, %zmm11 # 64-byte Folded Reload
                                        # zmm11 = (zmm18 * mem) + zmm11
	vextractf64x4	$1, %zmm11, %ymm14
	vaddps	%zmm14, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm14
	vaddps	%xmm14, %xmm11, %xmm11
	vmovaps	30208(%rsp), %zmm0              # 64-byte Reload
	vextractf64x4	$1, %zmm0, %ymm14
	vaddps	%zmm14, %zmm0, %zmm14
	vextractf128	$1, %ymm14, %xmm15
	vaddps	%xmm15, %xmm14, %xmm14
	vshufpd	$1, %xmm14, %xmm14, %xmm15      # xmm15 = xmm14[1,0]
	vaddps	%xmm15, %xmm14, %xmm14
	vmovshdup	%xmm14, %xmm15          # xmm15 = xmm14[1,1,3,3]
	vaddss	%xmm15, %xmm14, %xmm14
	vaddss	%xmm27, %xmm14, %xmm14
	vshufpd	$1, %xmm11, %xmm11, %xmm15      # xmm15 = xmm11[1,0]
	vaddps	%xmm15, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm15          # xmm15 = xmm11[1,1,3,3]
	vaddss	%xmm15, %xmm11, %xmm0
	vmovss	%xmm0, 7488(%rsp)               # 4-byte Spill
	vmulps	%zmm30, %zmm29, %zmm11
	vfmadd231ps	896(%rsp), %zmm18, %zmm11 # 64-byte Folded Reload
                                        # zmm11 = (zmm18 * mem) + zmm11
	vextractf64x4	$1, %zmm11, %ymm15
	vaddps	%zmm15, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm15
	vaddps	%xmm15, %xmm11, %xmm15
	vmovaps	30720(%rsp), %zmm0              # 64-byte Reload
	vextractf64x4	$1, %zmm0, %ymm11
	vaddps	%zmm11, %zmm0, %zmm11
	vextractf32x4	$1, %ymm11, %xmm22
	vaddps	%xmm22, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm22      # xmm22 = xmm11[1,0]
	vaddps	%xmm22, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm22          # xmm22 = xmm11[1,1,3,3]
	vaddss	%xmm22, %xmm11, %xmm11
	vaddss	%xmm27, %xmm11, %xmm22
	vmulps	%zmm26, %zmm29, %zmm11
	vfmadd231ps	%zmm24, %zmm18, %zmm11  # zmm11 = (zmm18 * zmm24) + zmm11
	vextractf64x4	$1, %zmm11, %ymm23
	vaddps	%zmm23, %zmm11, %zmm11
	vextractf32x4	$1, %ymm11, %xmm23
	vaddps	%xmm23, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm23      # xmm23 = xmm11[1,0]
	vaddps	%xmm23, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm23          # xmm23 = xmm11[1,1,3,3]
	vaddss	%xmm23, %xmm11, %xmm11
	vaddss	%xmm27, %xmm11, %xmm11
	vshufpd	$1, %xmm15, %xmm15, %xmm23      # xmm23 = xmm15[1,0]
	vaddps	%xmm23, %xmm15, %xmm15
	vmovshdup	%xmm15, %xmm23          # xmm23 = xmm15[1,1,3,3]
	vaddss	%xmm23, %xmm15, %xmm15
	vmovaps	2368(%rsp), %zmm21              # 64-byte Reload
	vmulps	%zmm21, %zmm29, %zmm23
	vmovaps	2560(%rsp), %zmm29              # 64-byte Reload
	vfmadd231ps	%zmm29, %zmm18, %zmm23  # zmm23 = (zmm18 * zmm29) + zmm23
	vextractf64x4	$1, %zmm23, %ymm24
	vaddps	%zmm24, %zmm23, %zmm23
	vextractf32x4	$1, %ymm23, %xmm24
	vaddps	%xmm24, %xmm23, %xmm23
	vshufpd	$1, %xmm23, %xmm23, %xmm24      # xmm24 = xmm23[1,0]
	vaddps	%xmm24, %xmm23, %xmm23
	vmovshdup	%xmm23, %xmm24          # xmm24 = xmm23[1,1,3,3]
	vaddss	%xmm24, %xmm23, %xmm23
	vaddss	%xmm27, %xmm15, %xmm15
	vaddss	%xmm27, %xmm23, %xmm23
	vinsertps	$16, %xmm23, %xmm15, %xmm15 # xmm15 = xmm15[0],xmm23[0],xmm15[2,3]
	vmulps	1984(%rsp), %zmm1, %zmm23       # 64-byte Folded Reload
	vmovaps	1024(%rsp), %zmm19              # 64-byte Reload
	vfmadd231ps	%zmm19, %zmm18, %zmm23  # zmm23 = (zmm18 * zmm19) + zmm23
	vextractf64x4	$1, %zmm23, %ymm24
	vaddps	%zmm24, %zmm23, %zmm23
	vextractf32x4	$1, %ymm23, %xmm24
	vaddps	%xmm24, %xmm23, %xmm23
	vshufpd	$1, %xmm23, %xmm23, %xmm24      # xmm24 = xmm23[1,0]
	vaddps	%xmm24, %xmm23, %xmm23
	vmovshdup	%xmm23, %xmm24          # xmm24 = xmm23[1,1,3,3]
	vaddss	%xmm24, %xmm23, %xmm23
	vaddss	%xmm27, %xmm23, %xmm23
	vinsertps	$32, %xmm23, %xmm15, %xmm15 # xmm15 = xmm15[0,1],xmm23[0],xmm15[3]
	vmovaps	832(%rsp), %zmm26               # 64-byte Reload
	vmulps	%zmm26, %zmm1, %zmm23
	vmovaps	960(%rsp), %zmm30               # 64-byte Reload
	vfmadd231ps	%zmm30, %zmm18, %zmm23  # zmm23 = (zmm18 * zmm30) + zmm23
	vextractf64x4	$1, %zmm23, %ymm24
	vaddps	%zmm24, %zmm23, %zmm23
	vextractf32x4	$1, %ymm23, %xmm24
	vaddps	%xmm24, %xmm23, %xmm23
	vshufpd	$1, %xmm23, %xmm23, %xmm24      # xmm24 = xmm23[1,0]
	vaddps	%xmm24, %xmm23, %xmm23
	vmovshdup	%xmm23, %xmm24          # xmm24 = xmm23[1,1,3,3]
	vaddss	%xmm24, %xmm23, %xmm23
	vaddss	%xmm27, %xmm23, %xmm23
	vinsertps	$48, %xmm23, %xmm15, %xmm0 # xmm0 = xmm15[0,1,2],xmm23[0]
	vmovaps	%xmm0, 7616(%rsp)               # 16-byte Spill
	vmulps	640(%rsp), %zmm1, %zmm15        # 64-byte Folded Reload
	vmovaps	64(%rsp), %zmm24                # 64-byte Reload
	vfmadd231ps	%zmm24, %zmm18, %zmm15  # zmm15 = (zmm18 * zmm24) + zmm15
	vextractf64x4	$1, %zmm15, %ymm23
	vaddps	%zmm23, %zmm15, %zmm15
	vextractf32x4	$1, %ymm15, %xmm23
	vaddps	%xmm23, %xmm15, %xmm15
	vshufpd	$1, %xmm15, %xmm15, %xmm23      # xmm23 = xmm15[1,0]
	vaddps	%xmm23, %xmm15, %xmm15
	vmovshdup	%xmm15, %xmm23          # xmm23 = xmm15[1,1,3,3]
	vaddss	%xmm23, %xmm15, %xmm15
	vaddss	%xmm27, %xmm15, %xmm15
	vinsertps	$16, %xmm22, %xmm15, %xmm15 # xmm15 = xmm15[0],xmm22[0],xmm15[2,3]
	vinsertps	$32, %xmm14, %xmm15, %xmm14 # xmm14 = xmm15[0,1],xmm14[0],xmm15[3]
	vinsertps	$48, %xmm12, %xmm14, %xmm12 # xmm12 = xmm14[0,1,2],xmm12[0]
	vmovaps	28928(%rsp), %zmm0              # 64-byte Reload
	vextractf64x4	$1, %zmm0, %ymm14
	vaddps	%zmm14, %zmm0, %zmm14
	vextractf128	$1, %ymm14, %xmm15
	vaddps	%xmm15, %xmm14, %xmm14
	vshufpd	$1, %xmm14, %xmm14, %xmm15      # xmm15 = xmm14[1,0]
	vaddps	%xmm15, %xmm14, %xmm14
	vmovshdup	%xmm14, %xmm15          # xmm15 = xmm14[1,1,3,3]
	vaddss	%xmm15, %xmm14, %xmm14
	vaddss	%xmm27, %xmm14, %xmm14
	vinsertps	$16, %xmm14, %xmm10, %xmm10 # xmm10 = xmm10[0],xmm14[0],xmm10[2,3]
	vinsertps	$32, %xmm4, %xmm10, %xmm4 # xmm4 = xmm10[0,1],xmm4[0],xmm10[3]
	vextractf64x4	$1, %zmm7, %ymm10
	vaddps	%zmm10, %zmm7, %zmm7
	vextractf128	$1, %ymm7, %xmm10
	vaddps	%xmm7, %xmm10, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm10        # xmm10 = xmm7[1,0]
	vaddps	%xmm7, %xmm10, %xmm7
	vmovshdup	%xmm7, %xmm10           # xmm10 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm10, %xmm7
	vaddss	%xmm27, %xmm7, %xmm7
	vinsertps	$48, %xmm7, %xmm4, %xmm7 # xmm7 = xmm4[0,1,2],xmm7[0]
	vmovaps	3072(%rsp), %zmm20              # 64-byte Reload
	vmulps	%zmm20, %zmm1, %zmm4
	vmovaps	%zmm18, %zmm15
	vmovaps	1472(%rsp), %zmm6               # 64-byte Reload
	vfmadd231ps	%zmm6, %zmm18, %zmm4    # zmm4 = (zmm18 * zmm6) + zmm4
	vextractf64x4	$1, %zmm4, %ymm10
	vaddps	%zmm10, %zmm4, %zmm4
	vextractf128	$1, %ymm4, %xmm10
	vaddps	%xmm4, %xmm10, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm10        # xmm10 = xmm4[1,0]
	vaddps	%xmm4, %xmm10, %xmm4
	vmovshdup	%xmm4, %xmm10           # xmm10 = xmm4[1,1,3,3]
	vaddss	%xmm4, %xmm10, %xmm0
	vmovss	%xmm0, 7296(%rsp)               # 4-byte Spill
	vmovaps	2112(%rsp), %zmm23              # 64-byte Reload
	vmulps	%zmm23, %zmm1, %zmm4
	vmovaps	2496(%rsp), %zmm22              # 64-byte Reload
	vfmadd231ps	%zmm22, %zmm18, %zmm4   # zmm4 = (zmm18 * zmm22) + zmm4
	vextractf64x4	$1, %zmm4, %ymm10
	vaddps	%zmm10, %zmm4, %zmm4
	vextractf128	$1, %ymm4, %xmm10
	vaddps	%xmm4, %xmm10, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm10        # xmm10 = xmm4[1,0]
	vaddps	%xmm4, %xmm10, %xmm10
	vmovaps	12480(%rsp), %zmm0              # 64-byte Reload
	vextractf64x4	$1, %zmm0, %ymm4
	vaddps	%zmm4, %zmm0, %zmm4
	vextractf128	$1, %ymm4, %xmm14
	vaddps	%xmm4, %xmm14, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm14        # xmm14 = xmm4[1,0]
	vaddps	%xmm4, %xmm14, %xmm4
	vmovshdup	%xmm4, %xmm14           # xmm14 = xmm4[1,1,3,3]
	vaddss	%xmm4, %xmm14, %xmm4
	vaddss	%xmm27, %xmm4, %xmm0
	vmovaps	%xmm0, 8704(%rsp)               # 16-byte Spill
	vmovshdup	%xmm10, %xmm14          # xmm14 = xmm10[1,1,3,3]
	vaddss	%xmm14, %xmm10, %xmm0
	vmovss	%xmm0, 7232(%rsp)               # 4-byte Spill
	vmovaps	3136(%rsp), %zmm18              # 64-byte Reload
	vmulps	%zmm18, %zmm1, %zmm10
	vfmadd231ps	1856(%rsp), %zmm15, %zmm10 # 64-byte Folded Reload
                                        # zmm10 = (zmm15 * mem) + zmm10
	vextractf64x4	$1, %zmm10, %ymm14
	vaddps	%zmm14, %zmm10, %zmm10
	vextractf128	$1, %ymm10, %xmm14
	vaddps	%xmm14, %xmm10, %xmm10
	vshufpd	$1, %xmm10, %xmm10, %xmm14      # xmm14 = xmm10[1,0]
	vaddps	%xmm14, %xmm10, %xmm10
	vmovshdup	%xmm10, %xmm14          # xmm14 = xmm10[1,1,3,3]
	vaddss	%xmm14, %xmm10, %xmm0
	vmovss	%xmm0, 7168(%rsp)               # 4-byte Spill
	vmulps	%zmm8, %zmm1, %zmm0
	vfmadd231ps	1536(%rsp), %zmm15, %zmm0 # 64-byte Folded Reload
                                        # zmm0 = (zmm15 * mem) + zmm0
	vmovaps	%zmm0, 7360(%rsp)               # 64-byte Spill
	vmulps	%zmm2, %zmm1, %zmm0
	vfmadd231ps	1920(%rsp), %zmm15, %zmm0 # 64-byte Folded Reload
                                        # zmm0 = (zmm15 * mem) + zmm0
	vmovaps	%zmm0, 7424(%rsp)               # 64-byte Spill
	vmulps	2240(%rsp), %zmm1, %zmm10       # 64-byte Folded Reload
	vfmadd231ps	1088(%rsp), %zmm15, %zmm10 # 64-byte Folded Reload
                                        # zmm10 = (zmm15 * mem) + zmm10
	vextractf64x4	$1, %zmm10, %ymm14
	vaddps	%zmm14, %zmm10, %zmm10
	vextractf128	$1, %ymm10, %xmm14
	vaddps	%xmm14, %xmm10, %xmm10
	vshufpd	$1, %xmm10, %xmm10, %xmm14      # xmm14 = xmm10[1,0]
	vaddps	%xmm14, %xmm10, %xmm10
	vmovshdup	%xmm10, %xmm14          # xmm14 = xmm10[1,1,3,3]
	vaddss	%xmm14, %xmm10, %xmm10
	vaddss	%xmm27, %xmm10, %xmm10
	vinsertps	$16, %xmm10, %xmm11, %xmm10 # xmm10 = xmm11[0],xmm10[0],xmm11[2,3]
	vinsertps	$32, 5824(%rsp), %xmm10, %xmm8 # 16-byte Folded Reload
                                        # xmm8 = xmm10[0,1],mem[0],xmm10[3]
	vinsertps	$48, 28160(%rsp), %xmm8, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm8[0,1,2],mem[0]
	vmovaps	%xmm0, 5952(%rsp)               # 16-byte Spill
	vmovaps	5760(%rsp), %ymm0               # 32-byte Reload
	vinsertf128	$1, 5504(%rsp), %ymm0, %ymm0 # 16-byte Folded Reload
	vmovaps	%zmm0, 5824(%rsp)               # 64-byte Spill
	vinsertf128	$1, %xmm7, %ymm12, %ymm0
	vmovaps	%zmm0, 5888(%rsp)               # 64-byte Spill
	vmovaps	3712(%rsp), %zmm1               # 64-byte Reload
	vmulps	%zmm28, %zmm1, %zmm0
	vmovaps	3776(%rsp), %zmm2               # 64-byte Reload
	vfmadd231ps	%zmm5, %zmm2, %zmm0     # zmm0 = (zmm2 * zmm5) + zmm0
	vextractf64x4	$1, %zmm0, %ymm7
	vaddps	%zmm7, %zmm0, %zmm0
	vextractf128	$1, %ymm0, %xmm7
	vaddps	%xmm7, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm7         # xmm7 = xmm0[1,0]
	vaddps	%xmm7, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm7            # xmm7 = xmm0[1,1,3,3]
	vaddss	%xmm7, %xmm0, %xmm0
	vaddss	%xmm27, %xmm0, %xmm0
	vinsertps	$16, 5488(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, 28288(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, 28224(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%ymm0, 8832(%rsp)               # 32-byte Spill
	vmovaps	9600(%rsp), %zmm5               # 64-byte Reload
	vextractf64x4	$1, %zmm5, %ymm0
	vaddps	%zmm0, %zmm5, %zmm0
	vextractf128	$1, %ymm0, %xmm7
	vaddps	%xmm7, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm7         # xmm7 = xmm0[1,0]
	vaddps	%xmm7, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm7            # xmm7 = xmm0[1,1,3,3]
	vaddss	%xmm7, %xmm0, %xmm0
	vaddss	%xmm27, %xmm0, %xmm0
	vmovaps	28416(%rsp), %xmm4              # 16-byte Reload
	vinsertps	$16, %xmm0, %xmm4, %xmm0 # xmm0 = xmm4[0],xmm0[0],xmm4[2,3]
	vinsertps	$32, 28352(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vmovaps	31168(%rsp), %zmm5              # 64-byte Reload
	vextractf64x4	$1, %zmm5, %ymm7
	vaddps	%zmm7, %zmm5, %zmm7
	vextractf128	$1, %ymm7, %xmm8
	vaddps	%xmm7, %xmm8, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm8         # xmm8 = xmm7[1,0]
	vaddps	%xmm7, %xmm8, %xmm7
	vmovshdup	%xmm7, %xmm8            # xmm8 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm27, %xmm7, %xmm7
	vinsertps	$48, %xmm7, %xmm0, %xmm7 # xmm7 = xmm0[0,1,2],xmm7[0]
	vmulps	%zmm25, %zmm1, %zmm0
	vfmadd231ps	%zmm3, %zmm2, %zmm0     # zmm0 = (zmm2 * zmm3) + zmm0
	vextractf64x4	$1, %zmm0, %ymm8
	vaddps	%zmm8, %zmm0, %zmm0
	vextractf128	$1, %ymm0, %xmm8
	vaddps	%xmm0, %xmm8, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm8         # xmm8 = xmm0[1,0]
	vaddps	%xmm0, %xmm8, %xmm8
	vmovaps	28672(%rsp), %zmm3              # 64-byte Reload
	vextractf64x4	$1, %zmm3, %ymm0
	vaddps	%zmm0, %zmm3, %zmm0
	vextractf128	$1, %ymm0, %xmm5
	vaddps	%xmm5, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm5         # xmm5 = xmm0[1,0]
	vaddps	%xmm5, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm5            # xmm5 = xmm0[1,1,3,3]
	vaddss	%xmm5, %xmm0, %xmm0
	vaddss	%xmm27, %xmm0, %xmm0
	vmulps	%zmm9, %zmm1, %zmm5
	vfmadd231ps	%zmm31, %zmm2, %zmm5    # zmm5 = (zmm2 * zmm31) + zmm5
	vextractf64x4	$1, %zmm5, %ymm9
	vaddps	%zmm9, %zmm5, %zmm5
	vextractf128	$1, %ymm5, %xmm9
	vaddps	%xmm5, %xmm9, %xmm5
	vshufpd	$1, %xmm5, %xmm5, %xmm9         # xmm9 = xmm5[1,0]
	vaddps	%xmm5, %xmm9, %xmm5
	vmovshdup	%xmm5, %xmm9            # xmm9 = xmm5[1,1,3,3]
	vaddss	%xmm5, %xmm9, %xmm5
	vaddss	%xmm27, %xmm5, %xmm9
	vmovshdup	%xmm8, %xmm5            # xmm5 = xmm8[1,1,3,3]
	vaddss	%xmm5, %xmm8, %xmm5
	vmovss	%xmm5, 8256(%rsp)               # 4-byte Spill
	vmulps	%zmm16, %zmm1, %zmm5
	vfmadd231ps	%zmm17, %zmm2, %zmm5    # zmm5 = (zmm2 * zmm17) + zmm5
	vmovaps	%zmm17, %zmm31
	vmovaps	%zmm2, %zmm17
	vextractf64x4	$1, %zmm5, %ymm8
	vaddps	%zmm8, %zmm5, %zmm5
	vextractf128	$1, %ymm5, %xmm8
	vaddps	%xmm5, %xmm8, %xmm5
	vshufpd	$1, %xmm5, %xmm5, %xmm8         # xmm8 = xmm5[1,0]
	vaddps	%xmm5, %xmm8, %xmm5
	vmovaps	29696(%rsp), %zmm2              # 64-byte Reload
	vextractf64x4	$1, %zmm2, %ymm8
	vaddps	%zmm8, %zmm2, %zmm8
	vextractf128	$1, %ymm8, %xmm10
	vaddps	%xmm10, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm10        # xmm10 = xmm8[1,0]
	vaddps	%xmm10, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm10           # xmm10 = xmm8[1,1,3,3]
	vaddss	%xmm10, %xmm8, %xmm8
	vaddss	%xmm27, %xmm8, %xmm8
	vmovshdup	%xmm5, %xmm10           # xmm10 = xmm5[1,1,3,3]
	vaddss	%xmm5, %xmm10, %xmm2
	vmovss	%xmm2, 8192(%rsp)               # 4-byte Spill
	vmulps	%zmm13, %zmm1, %zmm5
	vmovaps	1408(%rsp), %zmm25              # 64-byte Reload
	vfmadd231ps	%zmm25, %zmm17, %zmm5   # zmm5 = (zmm17 * zmm25) + zmm5
	vextractf64x4	$1, %zmm5, %ymm10
	vaddps	%zmm10, %zmm5, %zmm5
	vextractf128	$1, %ymm5, %xmm10
	vaddps	%xmm5, %xmm10, %xmm5
	vmovaps	30464(%rsp), %zmm2              # 64-byte Reload
	vextractf64x4	$1, %zmm2, %ymm10
	vaddps	%zmm10, %zmm2, %zmm10
	vextractf128	$1, %ymm10, %xmm11
	vaddps	%xmm11, %xmm10, %xmm10
	vshufpd	$1, %xmm10, %xmm10, %xmm11      # xmm11 = xmm10[1,0]
	vaddps	%xmm11, %xmm10, %xmm10
	vmovshdup	%xmm10, %xmm11          # xmm11 = xmm10[1,1,3,3]
	vaddss	%xmm11, %xmm10, %xmm10
	vaddss	%xmm27, %xmm10, %xmm10
	vshufpd	$1, %xmm5, %xmm5, %xmm11        # xmm11 = xmm5[1,0]
	vaddps	%xmm5, %xmm11, %xmm5
	vmovshdup	%xmm5, %xmm11           # xmm11 = xmm5[1,1,3,3]
	vaddss	%xmm5, %xmm11, %xmm2
	vmovss	%xmm2, 19584(%rsp)              # 4-byte Spill
	vmovaps	128(%rsp), %zmm13               # 64-byte Reload
	vmulps	%zmm13, %zmm1, %zmm5
	vfmadd231ps	896(%rsp), %zmm17, %zmm5 # 64-byte Folded Reload
                                        # zmm5 = (zmm17 * mem) + zmm5
	vextractf64x4	$1, %zmm5, %ymm11
	vaddps	%zmm11, %zmm5, %zmm5
	vextractf128	$1, %ymm5, %xmm11
	vaddps	%xmm5, %xmm11, %xmm11
	vmovaps	30976(%rsp), %zmm2              # 64-byte Reload
	vextractf64x4	$1, %zmm2, %ymm5
	vaddps	%zmm5, %zmm2, %zmm5
	vextractf128	$1, %ymm5, %xmm12
	vaddps	%xmm5, %xmm12, %xmm5
	vshufpd	$1, %xmm5, %xmm5, %xmm12        # xmm12 = xmm5[1,0]
	vaddps	%xmm5, %xmm12, %xmm5
	vmovshdup	%xmm5, %xmm12           # xmm12 = xmm5[1,1,3,3]
	vaddss	%xmm5, %xmm12, %xmm5
	vaddss	%xmm27, %xmm5, %xmm12
	vmovaps	1152(%rsp), %zmm28              # 64-byte Reload
	vmovaps	3840(%rsp), %zmm2               # 64-byte Reload
	vmulps	%zmm28, %zmm2, %zmm5
	vmovaps	3904(%rsp), %zmm4               # 64-byte Reload
	vfmadd231ps	768(%rsp), %zmm4, %zmm5 # 64-byte Folded Reload
                                        # zmm5 = (zmm4 * mem) + zmm5
	vextractf64x4	$1, %zmm5, %ymm14
	vaddps	%zmm14, %zmm5, %zmm5
	vextractf128	$1, %ymm5, %xmm14
	vaddps	%xmm5, %xmm14, %xmm5
	vshufpd	$1, %xmm5, %xmm5, %xmm14        # xmm14 = xmm5[1,0]
	vaddps	%xmm5, %xmm14, %xmm5
	vmovshdup	%xmm5, %xmm14           # xmm14 = xmm5[1,1,3,3]
	vaddss	%xmm5, %xmm14, %xmm5
	vaddss	%xmm27, %xmm5, %xmm5
	vshufpd	$1, %xmm11, %xmm11, %xmm14      # xmm14 = xmm11[1,0]
	vaddps	%xmm14, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm14          # xmm14 = xmm11[1,1,3,3]
	vaddss	%xmm14, %xmm11, %xmm11
	vmulps	%zmm21, %zmm1, %zmm14
	vfmadd231ps	%zmm29, %zmm17, %zmm14  # zmm14 = (zmm17 * zmm29) + zmm14
	vextractf64x4	$1, %zmm14, %ymm15
	vaddps	%zmm15, %zmm14, %zmm14
	vextractf128	$1, %ymm14, %xmm15
	vaddps	%xmm15, %xmm14, %xmm14
	vshufpd	$1, %xmm14, %xmm14, %xmm15      # xmm15 = xmm14[1,0]
	vaddps	%xmm15, %xmm14, %xmm14
	vmovshdup	%xmm14, %xmm15          # xmm15 = xmm14[1,1,3,3]
	vaddss	%xmm15, %xmm14, %xmm14
	vaddss	%xmm27, %xmm11, %xmm11
	vaddss	%xmm27, %xmm14, %xmm14
	vinsertps	$16, %xmm14, %xmm11, %xmm11 # xmm11 = xmm11[0],xmm14[0],xmm11[2,3]
	vmovaps	1984(%rsp), %zmm21              # 64-byte Reload
	vmulps	%zmm21, %zmm1, %zmm14
	vfmadd231ps	%zmm19, %zmm17, %zmm14  # zmm14 = (zmm17 * zmm19) + zmm14
	vextractf64x4	$1, %zmm14, %ymm15
	vaddps	%zmm15, %zmm14, %zmm14
	vextractf128	$1, %ymm14, %xmm15
	vaddps	%xmm15, %xmm14, %xmm14
	vshufpd	$1, %xmm14, %xmm14, %xmm15      # xmm15 = xmm14[1,0]
	vaddps	%xmm15, %xmm14, %xmm14
	vmovshdup	%xmm14, %xmm15          # xmm15 = xmm14[1,1,3,3]
	vaddss	%xmm15, %xmm14, %xmm14
	vaddss	%xmm27, %xmm14, %xmm14
	vinsertps	$32, %xmm14, %xmm11, %xmm11 # xmm11 = xmm11[0,1],xmm14[0],xmm11[3]
	vmulps	%zmm26, %zmm1, %zmm14
	vfmadd231ps	%zmm30, %zmm17, %zmm14  # zmm14 = (zmm17 * zmm30) + zmm14
	vextractf64x4	$1, %zmm14, %ymm15
	vaddps	%zmm15, %zmm14, %zmm14
	vextractf128	$1, %ymm14, %xmm15
	vaddps	%xmm15, %xmm14, %xmm14
	vshufpd	$1, %xmm14, %xmm14, %xmm15      # xmm15 = xmm14[1,0]
	vaddps	%xmm15, %xmm14, %xmm14
	vmovshdup	%xmm14, %xmm15          # xmm15 = xmm14[1,1,3,3]
	vaddss	%xmm15, %xmm14, %xmm14
	vaddss	%xmm27, %xmm14, %xmm14
	vinsertps	$48, %xmm14, %xmm11, %xmm11 # xmm11 = xmm11[0,1,2],xmm14[0]
	vmovaps	%xmm11, 5760(%rsp)              # 16-byte Spill
	vmulps	640(%rsp), %zmm1, %zmm11        # 64-byte Folded Reload
	vfmadd231ps	%zmm24, %zmm17, %zmm11  # zmm11 = (zmm17 * zmm24) + zmm11
	vextractf64x4	$1, %zmm11, %ymm14
	vaddps	%zmm14, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm14
	vaddps	%xmm14, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm14      # xmm14 = xmm11[1,0]
	vaddps	%xmm14, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm14          # xmm14 = xmm11[1,1,3,3]
	vaddss	%xmm14, %xmm11, %xmm11
	vaddss	%xmm27, %xmm11, %xmm11
	vinsertps	$16, %xmm12, %xmm11, %xmm11 # xmm11 = xmm11[0],xmm12[0],xmm11[2,3]
	vinsertps	$32, %xmm10, %xmm11, %xmm10 # xmm10 = xmm11[0,1],xmm10[0],xmm11[3]
	vinsertps	$48, %xmm8, %xmm10, %xmm8 # xmm8 = xmm10[0,1,2],xmm8[0]
	vmovaps	29120(%rsp), %zmm3              # 64-byte Reload
	vextractf64x4	$1, %zmm3, %ymm10
	vaddps	%zmm10, %zmm3, %zmm10
	vextractf128	$1, %ymm10, %xmm11
	vaddps	%xmm11, %xmm10, %xmm10
	vshufpd	$1, %xmm10, %xmm10, %xmm11      # xmm11 = xmm10[1,0]
	vaddps	%xmm11, %xmm10, %xmm10
	vmovshdup	%xmm10, %xmm11          # xmm11 = xmm10[1,1,3,3]
	vaddss	%xmm11, %xmm10, %xmm10
	vaddss	%xmm27, %xmm10, %xmm10
	vinsertps	$16, %xmm10, %xmm9, %xmm9 # xmm9 = xmm9[0],xmm10[0],xmm9[2,3]
	vinsertps	$32, %xmm0, %xmm9, %xmm0 # xmm0 = xmm9[0,1],xmm0[0],xmm9[3]
	vmovaps	28480(%rsp), %zmm3              # 64-byte Reload
	vextractf64x4	$1, %zmm3, %ymm9
	vaddps	%zmm9, %zmm3, %zmm3
	vextractf128	$1, %ymm3, %xmm9
	vaddps	%xmm3, %xmm9, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm9         # xmm9 = xmm3[1,0]
	vaddps	%xmm3, %xmm9, %xmm3
	vmovshdup	%xmm3, %xmm9            # xmm9 = xmm3[1,1,3,3]
	vaddss	%xmm3, %xmm9, %xmm3
	vaddss	%xmm27, %xmm3, %xmm3
	vinsertps	$48, %xmm3, %xmm0, %xmm3 # xmm3 = xmm0[0,1,2],xmm3[0]
	vmulps	%zmm20, %zmm1, %zmm0
	vfmadd231ps	%zmm6, %zmm17, %zmm0    # zmm0 = (zmm17 * zmm6) + zmm0
	vextractf64x4	$1, %zmm0, %ymm9
	vaddps	%zmm9, %zmm0, %zmm0
	vextractf128	$1, %ymm0, %xmm9
	vaddps	%xmm0, %xmm9, %xmm0
	vmovaps	9856(%rsp), %zmm10              # 64-byte Reload
	vextractf64x4	$1, %zmm10, %ymm9
	vaddps	%zmm9, %zmm10, %zmm9
	vextractf128	$1, %ymm9, %xmm10
	vaddps	%xmm10, %xmm9, %xmm9
	vshufpd	$1, %xmm9, %xmm9, %xmm10        # xmm10 = xmm9[1,0]
	vaddps	%xmm10, %xmm9, %xmm9
	vmovshdup	%xmm9, %xmm10           # xmm10 = xmm9[1,1,3,3]
	vaddss	%xmm10, %xmm9, %xmm9
	vaddss	%xmm27, %xmm9, %xmm9
	vshufpd	$1, %xmm0, %xmm0, %xmm10        # xmm10 = xmm0[1,0]
	vaddps	%xmm0, %xmm10, %xmm0
	vmovshdup	%xmm0, %xmm10           # xmm10 = xmm0[1,1,3,3]
	vaddss	%xmm0, %xmm10, %xmm0
	vmovss	%xmm0, 10944(%rsp)              # 4-byte Spill
	vmulps	%zmm23, %zmm1, %zmm0
	vfmadd231ps	%zmm22, %zmm17, %zmm0   # zmm0 = (zmm17 * zmm22) + zmm0
	vextractf64x4	$1, %zmm0, %ymm10
	vaddps	%zmm10, %zmm0, %zmm0
	vmovaps	9920(%rsp), %zmm11              # 64-byte Reload
	vextractf64x4	$1, %zmm11, %ymm10
	vaddps	%zmm10, %zmm11, %zmm10
	vextractf128	$1, %ymm10, %xmm11
	vaddps	%xmm11, %xmm10, %xmm10
	vshufpd	$1, %xmm10, %xmm10, %xmm11      # xmm11 = xmm10[1,0]
	vaddps	%xmm11, %xmm10, %xmm10
	vmovshdup	%xmm10, %xmm11          # xmm11 = xmm10[1,1,3,3]
	vaddss	%xmm11, %xmm10, %xmm10
	vaddss	%xmm27, %xmm10, %xmm10
	vextractf128	$1, %ymm0, %xmm11
	vaddps	%xmm0, %xmm11, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm11        # xmm11 = xmm0[1,0]
	vaddps	%xmm0, %xmm11, %xmm0
	vmovshdup	%xmm0, %xmm11           # xmm11 = xmm0[1,1,3,3]
	vaddss	%xmm0, %xmm11, %xmm0
	vmovss	%xmm0, 8768(%rsp)               # 4-byte Spill
	vmulps	%zmm18, %zmm1, %zmm0
	vfmadd231ps	1856(%rsp), %zmm17, %zmm0 # 64-byte Folded Reload
                                        # zmm0 = (zmm17 * mem) + zmm0
	vextractf64x4	$1, %zmm0, %ymm11
	vaddps	%zmm11, %zmm0, %zmm0
	vextractf128	$1, %ymm0, %xmm11
	vaddps	%xmm0, %xmm11, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm11        # xmm11 = xmm0[1,0]
	vaddps	%xmm0, %xmm11, %xmm0
	vmovshdup	%xmm0, %xmm11           # xmm11 = xmm0[1,1,3,3]
	vaddss	%xmm0, %xmm11, %xmm0
	vmovss	%xmm0, 19200(%rsp)              # 4-byte Spill
	vmovaps	19264(%rsp), %zmm11             # 64-byte Reload
	vextractf64x4	$1, %zmm11, %ymm0
	vaddps	%zmm0, %zmm11, %zmm0
	vextractf128	$1, %ymm0, %xmm11
	vaddps	%xmm0, %xmm11, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm11        # xmm11 = xmm0[1,0]
	vaddps	%xmm0, %xmm11, %xmm0
	vmovshdup	%xmm0, %xmm11           # xmm11 = xmm0[1,1,3,3]
	vaddss	%xmm0, %xmm11, %xmm0
	vaddss	%xmm27, %xmm0, %xmm11
	vmovaps	192(%rsp), %zmm23               # 64-byte Reload
	vmulps	%zmm23, %zmm1, %zmm0
	vmovaps	1344(%rsp), %zmm22              # 64-byte Reload
	vfmadd231ps	%zmm22, %zmm17, %zmm0   # zmm0 = (zmm17 * zmm22) + zmm0
	vextractf64x4	$1, %zmm0, %ymm12
	vaddps	%zmm12, %zmm0, %zmm0
	vextractf128	$1, %ymm0, %xmm12
	vaddps	%xmm0, %xmm12, %xmm0
	vmovaps	2432(%rsp), %zmm20              # 64-byte Reload
	vmulps	%zmm20, %zmm1, %zmm12
	vmovaps	1792(%rsp), %zmm26              # 64-byte Reload
	vfmadd231ps	%zmm17, %zmm26, %zmm12  # zmm12 = (zmm26 * zmm17) + zmm12
	vextractf64x4	$1, %zmm12, %ymm14
	vaddps	%zmm14, %zmm12, %zmm12
	vextractf128	$1, %ymm12, %xmm14
	vaddps	%xmm14, %xmm12, %xmm12
	vshufpd	$1, %xmm12, %xmm12, %xmm14      # xmm14 = xmm12[1,0]
	vaddps	%xmm14, %xmm12, %xmm12
	vmovshdup	%xmm12, %xmm14          # xmm14 = xmm12[1,1,3,3]
	vaddss	%xmm14, %xmm12, %xmm12
	vaddss	%xmm27, %xmm12, %xmm12
	vshufpd	$1, %xmm0, %xmm0, %xmm14        # xmm14 = xmm0[1,0]
	vaddps	%xmm0, %xmm14, %xmm0
	vmovshdup	%xmm0, %xmm14           # xmm14 = xmm0[1,1,3,3]
	vaddss	%xmm0, %xmm14, %xmm0
	vmulps	2176(%rsp), %zmm1, %zmm14       # 64-byte Folded Reload
	vmovaps	1728(%rsp), %zmm30              # 64-byte Reload
	vfmadd231ps	%zmm30, %zmm17, %zmm14  # zmm14 = (zmm17 * zmm30) + zmm14
	vextractf64x4	$1, %zmm14, %ymm15
	vaddps	%zmm15, %zmm14, %zmm14
	vextractf128	$1, %ymm14, %xmm15
	vaddps	%xmm15, %xmm14, %xmm14
	vshufpd	$1, %xmm14, %xmm14, %xmm15      # xmm15 = xmm14[1,0]
	vaddps	%xmm15, %xmm14, %xmm14
	vmovshdup	%xmm14, %xmm15          # xmm15 = xmm14[1,1,3,3]
	vaddss	%xmm15, %xmm14, %xmm14
	vmovaps	9728(%rsp), %zmm16              # 64-byte Reload
	vextractf64x4	$1, %zmm16, %ymm15
	vaddps	%zmm15, %zmm16, %zmm15
	vextractf32x4	$1, %ymm15, %xmm16
	vaddps	%xmm16, %xmm15, %xmm15
	vshufpd	$1, %xmm15, %xmm15, %xmm16      # xmm16 = xmm15[1,0]
	vaddps	%xmm16, %xmm15, %xmm15
	vmovshdup	%xmm15, %xmm16          # xmm16 = xmm15[1,1,3,3]
	vaddss	%xmm16, %xmm15, %xmm15
	vaddss	%xmm27, %xmm0, %xmm16
	vaddss	%xmm27, %xmm14, %xmm14
	vaddss	%xmm27, %xmm15, %xmm0
	vmovaps	%xmm0, 10624(%rsp)              # 16-byte Spill
	vmovaps	2304(%rsp), %zmm0               # 64-byte Reload
	vmulps	%zmm0, %zmm1, %zmm24
	vmovaps	%zmm17, %zmm15
	vfmadd231ps	1536(%rsp), %zmm17, %zmm24 # 64-byte Folded Reload
                                        # zmm24 = (zmm17 * mem) + zmm24
	vmovaps	%zmm24, 11008(%rsp)             # 64-byte Spill
	vmulps	576(%rsp), %zmm1, %zmm17        # 64-byte Folded Reload
	vmovaps	1920(%rsp), %zmm19              # 64-byte Reload
	vfmadd231ps	%zmm19, %zmm15, %zmm17  # zmm17 = (zmm15 * zmm19) + zmm17
	vmovaps	%zmm17, 19264(%rsp)             # 64-byte Spill
	vmovaps	%zmm15, %zmm17
	vmulps	2240(%rsp), %zmm1, %zmm15       # 64-byte Folded Reload
	vfmadd231ps	1088(%rsp), %zmm17, %zmm15 # 64-byte Folded Reload
                                        # zmm15 = (zmm17 * mem) + zmm15
	vextractf64x4	$1, %zmm15, %ymm17
	vaddps	%zmm17, %zmm15, %zmm15
	vextractf32x4	$1, %ymm15, %xmm17
	vaddps	%xmm17, %xmm15, %xmm15
	vshufpd	$1, %xmm15, %xmm15, %xmm17      # xmm17 = xmm15[1,0]
	vaddps	%xmm17, %xmm15, %xmm15
	vmovshdup	%xmm15, %xmm17          # xmm17 = xmm15[1,1,3,3]
	vaddss	%xmm17, %xmm15, %xmm15
	vaddss	%xmm27, %xmm15, %xmm15
	vinsertps	$16, %xmm15, %xmm16, %xmm15 # xmm15 = xmm16[0],xmm15[0],xmm16[2,3]
	vinsertps	$32, %xmm14, %xmm15, %xmm14 # xmm14 = xmm15[0,1],xmm14[0],xmm15[3]
	vinsertps	$48, %xmm12, %xmm14, %xmm1 # xmm1 = xmm14[0,1,2],xmm12[0]
	vmovaps	%xmm1, 3776(%rsp)               # 16-byte Spill
	vmovaps	8832(%rsp), %ymm1               # 32-byte Reload
	vinsertf128	$1, %xmm7, %ymm1, %ymm1
	vmovaps	%zmm1, 8832(%rsp)               # 64-byte Spill
	vinsertf128	$1, %xmm3, %ymm8, %ymm1
	vmovaps	%zmm1, 3712(%rsp)               # 64-byte Spill
	vmovaps	%zmm2, %zmm1
	vmulps	1600(%rsp), %zmm2, %zmm2        # 64-byte Folded Reload
	vmovaps	%zmm4, %zmm14
	vfmadd231ps	1664(%rsp), %zmm4, %zmm2 # 64-byte Folded Reload
                                        # zmm2 = (zmm4 * mem) + zmm2
	vextractf64x4	$1, %zmm2, %ymm3
	vaddps	%zmm3, %zmm2, %zmm2
	vextractf128	$1, %ymm2, %xmm3
	vaddps	%xmm3, %xmm2, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm3         # xmm3 = xmm2[1,0]
	vaddps	%xmm3, %xmm2, %xmm2
	vmovshdup	%xmm2, %xmm3            # xmm3 = xmm2[1,1,3,3]
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm27, %xmm2, %xmm2
	vinsertps	$16, %xmm11, %xmm2, %xmm2 # xmm2 = xmm2[0],xmm11[0],xmm2[2,3]
	vinsertps	$32, %xmm10, %xmm2, %xmm2 # xmm2 = xmm2[0,1],xmm10[0],xmm2[3]
	vinsertps	$48, %xmm9, %xmm2, %xmm3 # xmm3 = xmm2[0,1,2],xmm9[0]
	vmovaps	12928(%rsp), %zmm4              # 64-byte Reload
	vextractf64x4	$1, %zmm4, %ymm2
	vaddps	%zmm2, %zmm4, %zmm2
	vextractf128	$1, %ymm2, %xmm7
	vaddps	%xmm7, %xmm2, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm7         # xmm7 = xmm2[1,0]
	vaddps	%xmm7, %xmm2, %xmm2
	vmovshdup	%xmm2, %xmm7            # xmm7 = xmm2[1,1,3,3]
	vaddss	%xmm7, %xmm2, %xmm2
	vaddss	%xmm27, %xmm2, %xmm2
	vinsertps	$16, %xmm2, %xmm5, %xmm2 # xmm2 = xmm5[0],xmm2[0],xmm5[2,3]
	vinsertps	$32, 8704(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vmovaps	12736(%rsp), %zmm5              # 64-byte Reload
	vextractf64x4	$1, %zmm5, %ymm4
	vaddps	%zmm4, %zmm5, %zmm4
	vextractf128	$1, %ymm4, %xmm5
	vaddps	%xmm5, %xmm4, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm5         # xmm5 = xmm4[1,0]
	vaddps	%xmm5, %xmm4, %xmm4
	vmovshdup	%xmm4, %xmm5            # xmm5 = xmm4[1,1,3,3]
	vaddss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm27, %xmm4, %xmm4
	vinsertps	$48, %xmm4, %xmm2, %xmm4 # xmm4 = xmm2[0,1,2],xmm4[0]
	vmulps	448(%rsp), %zmm1, %zmm2         # 64-byte Folded Reload
	vfmadd231ps	512(%rsp), %zmm14, %zmm2 # 64-byte Folded Reload
                                        # zmm2 = (zmm14 * mem) + zmm2
	vextractf64x4	$1, %zmm2, %ymm5
	vaddps	%zmm5, %zmm2, %zmm2
	vextractf128	$1, %ymm2, %xmm5
	vaddps	%xmm5, %xmm2, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm5         # xmm5 = xmm2[1,0]
	vaddps	%xmm5, %xmm2, %xmm5
	vmovaps	28800(%rsp), %zmm7              # 64-byte Reload
	vextractf64x4	$1, %zmm7, %ymm2
	vaddps	%zmm2, %zmm7, %zmm2
	vextractf128	$1, %ymm2, %xmm7
	vaddps	%xmm7, %xmm2, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm7         # xmm7 = xmm2[1,0]
	vaddps	%xmm7, %xmm2, %xmm2
	vmovshdup	%xmm2, %xmm7            # xmm7 = xmm2[1,1,3,3]
	vaddss	%xmm7, %xmm2, %xmm2
	vaddss	%xmm27, %xmm2, %xmm2
	vmulps	1280(%rsp), %zmm1, %zmm7        # 64-byte Folded Reload
	vfmadd231ps	1216(%rsp), %zmm14, %zmm7 # 64-byte Folded Reload
                                        # zmm7 = (zmm14 * mem) + zmm7
	vextractf64x4	$1, %zmm7, %ymm8
	vaddps	%zmm8, %zmm7, %zmm7
	vextractf128	$1, %ymm7, %xmm8
	vaddps	%xmm7, %xmm8, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm8         # xmm8 = xmm7[1,0]
	vaddps	%xmm7, %xmm8, %xmm7
	vmovshdup	%xmm7, %xmm8            # xmm8 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm27, %xmm7, %xmm8
	vmovshdup	%xmm5, %xmm7            # xmm7 = xmm5[1,1,3,3]
	vaddss	%xmm7, %xmm5, %xmm5
	vmovss	%xmm5, 10816(%rsp)              # 4-byte Spill
	vmulps	2048(%rsp), %zmm1, %zmm5        # 64-byte Folded Reload
	vfmadd231ps	%zmm31, %zmm14, %zmm5   # zmm5 = (zmm14 * zmm31) + zmm5
	vextractf64x4	$1, %zmm5, %ymm7
	vaddps	%zmm7, %zmm5, %zmm5
	vextractf128	$1, %ymm5, %xmm7
	vaddps	%xmm7, %xmm5, %xmm5
	vshufpd	$1, %xmm5, %xmm5, %xmm7         # xmm7 = xmm5[1,0]
	vaddps	%xmm7, %xmm5, %xmm5
	vmovaps	29952(%rsp), %zmm9              # 64-byte Reload
	vextractf64x4	$1, %zmm9, %ymm7
	vaddps	%zmm7, %zmm9, %zmm7
	vextractf128	$1, %ymm7, %xmm9
	vaddps	%xmm7, %xmm9, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm9         # xmm9 = xmm7[1,0]
	vaddps	%xmm7, %xmm9, %xmm7
	vmovshdup	%xmm7, %xmm9            # xmm9 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm9, %xmm7
	vaddss	%xmm27, %xmm7, %xmm7
	vmovshdup	%xmm5, %xmm9            # xmm9 = xmm5[1,1,3,3]
	vaddss	%xmm5, %xmm9, %xmm5
	vmovss	%xmm5, 10752(%rsp)              # 4-byte Spill
	vmulps	256(%rsp), %zmm1, %zmm5         # 64-byte Folded Reload
	vfmadd231ps	%zmm25, %zmm14, %zmm5   # zmm5 = (zmm14 * zmm25) + zmm5
	vextractf64x4	$1, %zmm5, %ymm9
	vaddps	%zmm9, %zmm5, %zmm5
	vextractf128	$1, %ymm5, %xmm9
	vaddps	%xmm5, %xmm9, %xmm5
	vmovaps	30784(%rsp), %zmm10             # 64-byte Reload
	vextractf64x4	$1, %zmm10, %ymm9
	vaddps	%zmm9, %zmm10, %zmm9
	vextractf128	$1, %ymm9, %xmm10
	vaddps	%xmm10, %xmm9, %xmm9
	vshufpd	$1, %xmm9, %xmm9, %xmm10        # xmm10 = xmm9[1,0]
	vaddps	%xmm10, %xmm9, %xmm9
	vmovshdup	%xmm9, %xmm10           # xmm10 = xmm9[1,1,3,3]
	vaddss	%xmm10, %xmm9, %xmm9
	vaddss	%xmm27, %xmm9, %xmm9
	vshufpd	$1, %xmm5, %xmm5, %xmm10        # xmm10 = xmm5[1,0]
	vaddps	%xmm5, %xmm10, %xmm5
	vmovshdup	%xmm5, %xmm10           # xmm10 = xmm5[1,1,3,3]
	vaddss	%xmm5, %xmm10, %xmm5
	vmovss	%xmm5, 10688(%rsp)              # 4-byte Spill
	vmulps	%zmm13, %zmm1, %zmm5
	vfmadd231ps	896(%rsp), %zmm14, %zmm5 # 64-byte Folded Reload
                                        # zmm5 = (zmm14 * mem) + zmm5
	vextractf64x4	$1, %zmm5, %ymm10
	vaddps	%zmm10, %zmm5, %zmm5
	vextractf128	$1, %ymm5, %xmm10
	vaddps	%xmm5, %xmm10, %xmm10
	vmovaps	31104(%rsp), %zmm11             # 64-byte Reload
	vextractf64x4	$1, %zmm11, %ymm5
	vaddps	%zmm5, %zmm11, %zmm5
	vextractf128	$1, %ymm5, %xmm11
	vaddps	%xmm5, %xmm11, %xmm5
	vshufpd	$1, %xmm5, %xmm5, %xmm11        # xmm11 = xmm5[1,0]
	vaddps	%xmm5, %xmm11, %xmm5
	vmovshdup	%xmm5, %xmm11           # xmm11 = xmm5[1,1,3,3]
	vaddss	%xmm5, %xmm11, %xmm5
	vaddss	%xmm27, %xmm5, %xmm11
	vmovaps	3968(%rsp), %zmm29              # 64-byte Reload
	vmulps	%zmm28, %zmm29, %zmm5
	vmovaps	4032(%rsp), %zmm17              # 64-byte Reload
	vfmadd231ps	768(%rsp), %zmm17, %zmm5 # 64-byte Folded Reload
                                        # zmm5 = (zmm17 * mem) + zmm5
	vextractf64x4	$1, %zmm5, %ymm12
	vaddps	%zmm12, %zmm5, %zmm5
	vextractf128	$1, %ymm5, %xmm12
	vaddps	%xmm5, %xmm12, %xmm5
	vshufpd	$1, %xmm5, %xmm5, %xmm12        # xmm12 = xmm5[1,0]
	vaddps	%xmm5, %xmm12, %xmm5
	vmovshdup	%xmm5, %xmm12           # xmm12 = xmm5[1,1,3,3]
	vaddss	%xmm5, %xmm12, %xmm5
	vaddss	%xmm27, %xmm5, %xmm5
	vshufpd	$1, %xmm10, %xmm10, %xmm12      # xmm12 = xmm10[1,0]
	vaddps	%xmm12, %xmm10, %xmm10
	vmovshdup	%xmm10, %xmm12          # xmm12 = xmm10[1,1,3,3]
	vaddss	%xmm12, %xmm10, %xmm10
	vmulps	2368(%rsp), %zmm1, %zmm12       # 64-byte Folded Reload
	vmovaps	%zmm14, %zmm6
	vmovaps	2560(%rsp), %zmm18              # 64-byte Reload
	vfmadd231ps	%zmm18, %zmm14, %zmm12  # zmm12 = (zmm14 * zmm18) + zmm12
	vextractf64x4	$1, %zmm12, %ymm14
	vaddps	%zmm14, %zmm12, %zmm12
	vextractf128	$1, %ymm12, %xmm14
	vaddps	%xmm14, %xmm12, %xmm12
	vshufpd	$1, %xmm12, %xmm12, %xmm14      # xmm14 = xmm12[1,0]
	vaddps	%xmm14, %xmm12, %xmm12
	vmovshdup	%xmm12, %xmm14          # xmm14 = xmm12[1,1,3,3]
	vaddss	%xmm14, %xmm12, %xmm12
	vaddss	%xmm27, %xmm10, %xmm10
	vaddss	%xmm27, %xmm12, %xmm12
	vinsertps	$16, %xmm12, %xmm10, %xmm10 # xmm10 = xmm10[0],xmm12[0],xmm10[2,3]
	vmulps	%zmm21, %zmm1, %zmm12
	vfmadd231ps	1024(%rsp), %zmm6, %zmm12 # 64-byte Folded Reload
                                        # zmm12 = (zmm6 * mem) + zmm12
	vextractf64x4	$1, %zmm12, %ymm14
	vaddps	%zmm14, %zmm12, %zmm12
	vextractf128	$1, %ymm12, %xmm14
	vaddps	%xmm14, %xmm12, %xmm12
	vshufpd	$1, %xmm12, %xmm12, %xmm14      # xmm14 = xmm12[1,0]
	vaddps	%xmm14, %xmm12, %xmm12
	vmovshdup	%xmm12, %xmm14          # xmm14 = xmm12[1,1,3,3]
	vaddss	%xmm14, %xmm12, %xmm12
	vaddss	%xmm27, %xmm12, %xmm12
	vinsertps	$32, %xmm12, %xmm10, %xmm10 # xmm10 = xmm10[0,1],xmm12[0],xmm10[3]
	vmulps	832(%rsp), %zmm1, %zmm12        # 64-byte Folded Reload
	vfmadd231ps	960(%rsp), %zmm6, %zmm12 # 64-byte Folded Reload
                                        # zmm12 = (zmm6 * mem) + zmm12
	vextractf64x4	$1, %zmm12, %ymm14
	vaddps	%zmm14, %zmm12, %zmm12
	vextractf128	$1, %ymm12, %xmm14
	vaddps	%xmm14, %xmm12, %xmm12
	vshufpd	$1, %xmm12, %xmm12, %xmm14      # xmm14 = xmm12[1,0]
	vaddps	%xmm14, %xmm12, %xmm12
	vmovshdup	%xmm12, %xmm14          # xmm14 = xmm12[1,1,3,3]
	vaddss	%xmm14, %xmm12, %xmm12
	vaddss	%xmm27, %xmm12, %xmm12
	vinsertps	$48, %xmm12, %xmm10, %xmm10 # xmm10 = xmm10[0,1,2],xmm12[0]
	vmovaps	%xmm10, 8704(%rsp)              # 16-byte Spill
	vmovaps	640(%rsp), %zmm24               # 64-byte Reload
	vmulps	%zmm24, %zmm1, %zmm10
	vmovaps	64(%rsp), %zmm21                # 64-byte Reload
	vfmadd231ps	%zmm21, %zmm6, %zmm10   # zmm10 = (zmm6 * zmm21) + zmm10
	vextractf64x4	$1, %zmm10, %ymm12
	vaddps	%zmm12, %zmm10, %zmm10
	vextractf128	$1, %ymm10, %xmm12
	vaddps	%xmm12, %xmm10, %xmm10
	vshufpd	$1, %xmm10, %xmm10, %xmm12      # xmm12 = xmm10[1,0]
	vaddps	%xmm12, %xmm10, %xmm10
	vmovshdup	%xmm10, %xmm12          # xmm12 = xmm10[1,1,3,3]
	vaddss	%xmm12, %xmm10, %xmm10
	vaddss	%xmm27, %xmm10, %xmm10
	vinsertps	$16, %xmm11, %xmm10, %xmm10 # xmm10 = xmm10[0],xmm11[0],xmm10[2,3]
	vinsertps	$32, %xmm9, %xmm10, %xmm9 # xmm9 = xmm10[0,1],xmm9[0],xmm10[3]
	vinsertps	$48, %xmm7, %xmm9, %xmm7 # xmm7 = xmm9[0,1,2],xmm7[0]
	vmovaps	29312(%rsp), %zmm10             # 64-byte Reload
	vextractf64x4	$1, %zmm10, %ymm9
	vaddps	%zmm9, %zmm10, %zmm9
	vextractf128	$1, %ymm9, %xmm10
	vaddps	%xmm10, %xmm9, %xmm9
	vshufpd	$1, %xmm9, %xmm9, %xmm10        # xmm10 = xmm9[1,0]
	vaddps	%xmm10, %xmm9, %xmm9
	vmovshdup	%xmm9, %xmm10           # xmm10 = xmm9[1,1,3,3]
	vaddss	%xmm10, %xmm9, %xmm9
	vaddss	%xmm27, %xmm9, %xmm9
	vinsertps	$16, %xmm9, %xmm8, %xmm8 # xmm8 = xmm8[0],xmm9[0],xmm8[2,3]
	vinsertps	$32, %xmm2, %xmm8, %xmm2 # xmm2 = xmm8[0,1],xmm2[0],xmm8[3]
	vmovaps	28608(%rsp), %zmm9              # 64-byte Reload
	vextractf64x4	$1, %zmm9, %ymm8
	vaddps	%zmm8, %zmm9, %zmm8
	vextractf128	$1, %ymm8, %xmm9
	vaddps	%xmm9, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm9         # xmm9 = xmm8[1,0]
	vaddps	%xmm9, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm9            # xmm9 = xmm8[1,1,3,3]
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm27, %xmm8, %xmm8
	vinsertps	$48, %xmm8, %xmm2, %xmm8 # xmm8 = xmm2[0,1,2],xmm8[0]
	vmulps	3072(%rsp), %zmm1, %zmm2        # 64-byte Folded Reload
	vmovaps	1472(%rsp), %zmm31              # 64-byte Reload
	vfmadd231ps	%zmm31, %zmm6, %zmm2    # zmm2 = (zmm6 * zmm31) + zmm2
	vextractf64x4	$1, %zmm2, %ymm9
	vaddps	%zmm9, %zmm2, %zmm2
	vextractf128	$1, %ymm2, %xmm9
	vaddps	%xmm2, %xmm9, %xmm2
	vmovaps	9984(%rsp), %zmm10              # 64-byte Reload
	vextractf64x4	$1, %zmm10, %ymm9
	vaddps	%zmm9, %zmm10, %zmm9
	vextractf128	$1, %ymm9, %xmm10
	vaddps	%xmm10, %xmm9, %xmm9
	vshufpd	$1, %xmm9, %xmm9, %xmm10        # xmm10 = xmm9[1,0]
	vaddps	%xmm10, %xmm9, %xmm9
	vmovshdup	%xmm9, %xmm10           # xmm10 = xmm9[1,1,3,3]
	vaddss	%xmm10, %xmm9, %xmm9
	vaddss	%xmm27, %xmm9, %xmm9
	vshufpd	$1, %xmm2, %xmm2, %xmm10        # xmm10 = xmm2[1,0]
	vaddps	%xmm2, %xmm10, %xmm2
	vmovshdup	%xmm2, %xmm10           # xmm10 = xmm2[1,1,3,3]
	vaddss	%xmm2, %xmm10, %xmm2
	vmovss	%xmm2, 10560(%rsp)              # 4-byte Spill
	vmulps	2112(%rsp), %zmm1, %zmm2        # 64-byte Folded Reload
	vfmadd231ps	2496(%rsp), %zmm6, %zmm2 # 64-byte Folded Reload
                                        # zmm2 = (zmm6 * mem) + zmm2
	vextractf64x4	$1, %zmm2, %ymm10
	vaddps	%zmm10, %zmm2, %zmm2
	vmovaps	31424(%rsp), %zmm11             # 64-byte Reload
	vextractf64x4	$1, %zmm11, %ymm10
	vaddps	%zmm10, %zmm11, %zmm10
	vextractf128	$1, %ymm10, %xmm11
	vaddps	%xmm11, %xmm10, %xmm10
	vshufpd	$1, %xmm10, %xmm10, %xmm11      # xmm11 = xmm10[1,0]
	vaddps	%xmm11, %xmm10, %xmm10
	vmovshdup	%xmm10, %xmm11          # xmm11 = xmm10[1,1,3,3]
	vaddss	%xmm11, %xmm10, %xmm10
	vaddss	%xmm27, %xmm10, %xmm10
	vextractf128	$1, %ymm2, %xmm11
	vaddps	%xmm2, %xmm11, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm11        # xmm11 = xmm2[1,0]
	vaddps	%xmm2, %xmm11, %xmm2
	vmovshdup	%xmm2, %xmm11           # xmm11 = xmm2[1,1,3,3]
	vaddss	%xmm2, %xmm11, %xmm2
	vmovss	%xmm2, 10496(%rsp)              # 4-byte Spill
	vmovaps	3136(%rsp), %zmm25              # 64-byte Reload
	vmulps	%zmm25, %zmm1, %zmm2
	vmovaps	1856(%rsp), %zmm28              # 64-byte Reload
	vfmadd231ps	%zmm28, %zmm6, %zmm2    # zmm2 = (zmm6 * zmm28) + zmm2
	vextractf64x4	$1, %zmm2, %ymm11
	vaddps	%zmm11, %zmm2, %zmm2
	vextractf128	$1, %ymm2, %xmm11
	vaddps	%xmm2, %xmm11, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm11        # xmm11 = xmm2[1,0]
	vaddps	%xmm2, %xmm11, %xmm2
	vmovshdup	%xmm2, %xmm11           # xmm11 = xmm2[1,1,3,3]
	vaddss	%xmm2, %xmm11, %xmm2
	vmovss	%xmm2, 10432(%rsp)              # 4-byte Spill
	vmovaps	19328(%rsp), %zmm11             # 64-byte Reload
	vextractf64x4	$1, %zmm11, %ymm2
	vaddps	%zmm2, %zmm11, %zmm2
	vextractf128	$1, %ymm2, %xmm11
	vaddps	%xmm2, %xmm11, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm11        # xmm11 = xmm2[1,0]
	vaddps	%xmm2, %xmm11, %xmm2
	vmovshdup	%xmm2, %xmm11           # xmm11 = xmm2[1,1,3,3]
	vaddss	%xmm2, %xmm11, %xmm2
	vaddss	%xmm27, %xmm2, %xmm11
	vmulps	%zmm23, %zmm1, %zmm2
	vfmadd231ps	%zmm22, %zmm6, %zmm2    # zmm2 = (zmm6 * zmm22) + zmm2
	vextractf64x4	$1, %zmm2, %ymm12
	vaddps	%zmm12, %zmm2, %zmm2
	vextractf128	$1, %ymm2, %xmm12
	vaddps	%xmm2, %xmm12, %xmm2
	vmulps	%zmm20, %zmm1, %zmm12
	vfmadd231ps	%zmm6, %zmm26, %zmm12   # zmm12 = (zmm26 * zmm6) + zmm12
	vmovaps	%zmm26, %zmm22
	vextractf64x4	$1, %zmm12, %ymm13
	vaddps	%zmm13, %zmm12, %zmm12
	vextractf128	$1, %ymm12, %xmm13
	vaddps	%xmm13, %xmm12, %xmm12
	vshufpd	$1, %xmm12, %xmm12, %xmm13      # xmm13 = xmm12[1,0]
	vaddps	%xmm13, %xmm12, %xmm12
	vmovshdup	%xmm12, %xmm13          # xmm13 = xmm12[1,1,3,3]
	vaddss	%xmm13, %xmm12, %xmm12
	vaddss	%xmm27, %xmm12, %xmm12
	vshufpd	$1, %xmm2, %xmm2, %xmm13        # xmm13 = xmm2[1,0]
	vaddps	%xmm2, %xmm13, %xmm2
	vmovshdup	%xmm2, %xmm13           # xmm13 = xmm2[1,1,3,3]
	vaddss	%xmm2, %xmm13, %xmm2
	vmulps	2176(%rsp), %zmm1, %zmm13       # 64-byte Folded Reload
	vfmadd231ps	%zmm30, %zmm6, %zmm13   # zmm13 = (zmm6 * zmm30) + zmm13
	vextractf64x4	$1, %zmm13, %ymm14
	vaddps	%zmm14, %zmm13, %zmm13
	vextractf128	$1, %ymm13, %xmm14
	vaddps	%xmm14, %xmm13, %xmm13
	vshufpd	$1, %xmm13, %xmm13, %xmm14      # xmm14 = xmm13[1,0]
	vaddps	%xmm14, %xmm13, %xmm13
	vmovshdup	%xmm13, %xmm14          # xmm14 = xmm13[1,1,3,3]
	vaddss	%xmm14, %xmm13, %xmm13
	vmovaps	13312(%rsp), %zmm15             # 64-byte Reload
	vextractf64x4	$1, %zmm15, %ymm14
	vaddps	%zmm14, %zmm15, %zmm14
	vextractf128	$1, %ymm14, %xmm15
	vaddps	%xmm15, %xmm14, %xmm14
	vshufpd	$1, %xmm14, %xmm14, %xmm15      # xmm15 = xmm14[1,0]
	vaddps	%xmm15, %xmm14, %xmm14
	vmovshdup	%xmm14, %xmm15          # xmm15 = xmm14[1,1,3,3]
	vaddss	%xmm15, %xmm14, %xmm14
	vaddss	%xmm27, %xmm2, %xmm15
	vaddss	%xmm27, %xmm13, %xmm13
	vaddss	%xmm27, %xmm14, %xmm2
	vmovaps	%xmm2, 9984(%rsp)               # 16-byte Spill
	vmulps	%zmm0, %zmm1, %zmm14
	vfmadd231ps	1536(%rsp), %zmm6, %zmm14 # 64-byte Folded Reload
                                        # zmm14 = (zmm6 * mem) + zmm14
	vmovaps	%zmm14, 19072(%rsp)             # 64-byte Spill
	vmulps	576(%rsp), %zmm1, %zmm14        # 64-byte Folded Reload
	vfmadd231ps	%zmm19, %zmm6, %zmm14   # zmm14 = (zmm6 * zmm19) + zmm14
	vmovaps	%zmm14, 19328(%rsp)             # 64-byte Spill
	vmulps	2240(%rsp), %zmm1, %zmm14       # 64-byte Folded Reload
	vfmadd231ps	1088(%rsp), %zmm6, %zmm14 # 64-byte Folded Reload
                                        # zmm14 = (zmm6 * mem) + zmm14
	vextractf64x4	$1, %zmm14, %ymm16
	vaddps	%zmm16, %zmm14, %zmm14
	vextractf32x4	$1, %ymm14, %xmm16
	vaddps	%xmm16, %xmm14, %xmm14
	vshufpd	$1, %xmm14, %xmm14, %xmm16      # xmm16 = xmm14[1,0]
	vaddps	%xmm16, %xmm14, %xmm14
	vmovshdup	%xmm14, %xmm16          # xmm16 = xmm14[1,1,3,3]
	vaddss	%xmm16, %xmm14, %xmm14
	vaddss	%xmm27, %xmm14, %xmm14
	vinsertps	$16, %xmm14, %xmm15, %xmm14 # xmm14 = xmm15[0],xmm14[0],xmm15[2,3]
	vinsertps	$32, %xmm13, %xmm14, %xmm13 # xmm13 = xmm14[0,1],xmm13[0],xmm14[3]
	vinsertps	$48, %xmm12, %xmm13, %xmm1 # xmm1 = xmm13[0,1,2],xmm12[0]
	vmovaps	%xmm1, 3904(%rsp)               # 16-byte Spill
	vinsertf128	$1, %xmm4, %ymm3, %ymm1
	vmovaps	%zmm1, 19136(%rsp)              # 64-byte Spill
	vinsertf128	$1, %xmm8, %ymm7, %ymm1
	vmovaps	%zmm1, 3840(%rsp)               # 64-byte Spill
	vmovaps	%zmm29, %zmm1
	vmovaps	1600(%rsp), %zmm19              # 64-byte Reload
	vmulps	%zmm19, %zmm29, %zmm3
	vmovaps	%zmm17, %zmm15
	vmovaps	1664(%rsp), %zmm2               # 64-byte Reload
	vfmadd231ps	%zmm2, %zmm17, %zmm3    # zmm3 = (zmm17 * zmm2) + zmm3
	vextractf64x4	$1, %zmm3, %ymm4
	vaddps	%zmm4, %zmm3, %zmm3
	vextractf128	$1, %ymm3, %xmm4
	vaddps	%xmm4, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm4         # xmm4 = xmm3[1,0]
	vaddps	%xmm4, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm4            # xmm4 = xmm3[1,1,3,3]
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm27, %xmm3, %xmm3
	vinsertps	$16, %xmm11, %xmm3, %xmm3 # xmm3 = xmm3[0],xmm11[0],xmm3[2,3]
	vinsertps	$32, %xmm10, %xmm3, %xmm3 # xmm3 = xmm3[0,1],xmm10[0],xmm3[3]
	vinsertps	$48, %xmm9, %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],xmm9[0]
	vmovaps	31360(%rsp), %zmm6              # 64-byte Reload
	vextractf64x4	$1, %zmm6, %ymm4
	vaddps	%zmm4, %zmm6, %zmm4
	vextractf128	$1, %ymm4, %xmm7
	vaddps	%xmm7, %xmm4, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm7         # xmm7 = xmm4[1,0]
	vaddps	%xmm7, %xmm4, %xmm4
	vmovshdup	%xmm4, %xmm7            # xmm7 = xmm4[1,1,3,3]
	vaddss	%xmm7, %xmm4, %xmm4
	vaddss	%xmm27, %xmm4, %xmm4
	vinsertps	$16, %xmm4, %xmm5, %xmm4 # xmm4 = xmm5[0],xmm4[0],xmm5[2,3]
	vinsertps	$32, 10624(%rsp), %xmm4, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm4[0,1],mem[0],xmm4[3]
	vmovaps	13184(%rsp), %zmm5              # 64-byte Reload
	vextractf64x4	$1, %zmm5, %ymm4
	vaddps	%zmm4, %zmm5, %zmm4
	vextractf128	$1, %ymm4, %xmm5
	vaddps	%xmm5, %xmm4, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm5         # xmm5 = xmm4[1,0]
	vaddps	%xmm5, %xmm4, %xmm4
	vmovshdup	%xmm4, %xmm5            # xmm5 = xmm4[1,1,3,3]
	vaddss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm27, %xmm4, %xmm4
	vinsertps	$48, %xmm4, %xmm0, %xmm4 # xmm4 = xmm0[0,1,2],xmm4[0]
	vmulps	448(%rsp), %zmm29, %zmm0        # 64-byte Folded Reload
	vfmadd231ps	512(%rsp), %zmm17, %zmm0 # 64-byte Folded Reload
                                        # zmm0 = (zmm17 * mem) + zmm0
	vextractf64x4	$1, %zmm0, %ymm5
	vaddps	%zmm5, %zmm0, %zmm0
	vextractf128	$1, %ymm0, %xmm5
	vaddps	%xmm5, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm5         # xmm5 = xmm0[1,0]
	vaddps	%xmm5, %xmm0, %xmm5
	vmovaps	28992(%rsp), %zmm6              # 64-byte Reload
	vextractf64x4	$1, %zmm6, %ymm0
	vaddps	%zmm0, %zmm6, %zmm0
	vextractf128	$1, %ymm0, %xmm7
	vaddps	%xmm7, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm7         # xmm7 = xmm0[1,0]
	vaddps	%xmm7, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm7            # xmm7 = xmm0[1,1,3,3]
	vaddss	%xmm7, %xmm0, %xmm0
	vaddss	%xmm27, %xmm0, %xmm0
	vmulps	1280(%rsp), %zmm29, %zmm7       # 64-byte Folded Reload
	vfmadd231ps	1216(%rsp), %zmm17, %zmm7 # 64-byte Folded Reload
                                        # zmm7 = (zmm17 * mem) + zmm7
	vextractf64x4	$1, %zmm7, %ymm8
	vaddps	%zmm8, %zmm7, %zmm7
	vextractf128	$1, %ymm7, %xmm8
	vaddps	%xmm7, %xmm8, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm8         # xmm8 = xmm7[1,0]
	vaddps	%xmm7, %xmm8, %xmm7
	vmovshdup	%xmm7, %xmm8            # xmm8 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm27, %xmm7, %xmm8
	vmovshdup	%xmm5, %xmm7            # xmm7 = xmm5[1,1,3,3]
	vaddss	%xmm7, %xmm5, %xmm5
	vmovss	%xmm5, 18688(%rsp)              # 4-byte Spill
	vmulps	2048(%rsp), %zmm29, %zmm5       # 64-byte Folded Reload
	vfmadd231ps	704(%rsp), %zmm17, %zmm5 # 64-byte Folded Reload
                                        # zmm5 = (zmm17 * mem) + zmm5
	vextractf64x4	$1, %zmm5, %ymm7
	vaddps	%zmm7, %zmm5, %zmm5
	vextractf128	$1, %ymm5, %xmm7
	vaddps	%xmm7, %xmm5, %xmm5
	vshufpd	$1, %xmm5, %xmm5, %xmm7         # xmm7 = xmm5[1,0]
	vaddps	%xmm7, %xmm5, %xmm5
	vmovaps	30144(%rsp), %zmm6              # 64-byte Reload
	vextractf64x4	$1, %zmm6, %ymm7
	vaddps	%zmm7, %zmm6, %zmm7
	vextractf128	$1, %ymm7, %xmm9
	vaddps	%xmm7, %xmm9, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm9         # xmm9 = xmm7[1,0]
	vaddps	%xmm7, %xmm9, %xmm7
	vmovshdup	%xmm7, %xmm9            # xmm9 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm9, %xmm7
	vaddss	%xmm27, %xmm7, %xmm7
	vmovshdup	%xmm5, %xmm9            # xmm9 = xmm5[1,1,3,3]
	vaddss	%xmm5, %xmm9, %xmm5
	vmovss	%xmm5, 37696(%rsp)              # 4-byte Spill
	vmovaps	256(%rsp), %zmm26               # 64-byte Reload
	vmulps	%zmm26, %zmm29, %zmm5
	vmovaps	1408(%rsp), %zmm20              # 64-byte Reload
	vfmadd231ps	%zmm20, %zmm17, %zmm5   # zmm5 = (zmm17 * zmm20) + zmm5
	vextractf64x4	$1, %zmm5, %ymm9
	vaddps	%zmm9, %zmm5, %zmm5
	vextractf128	$1, %ymm5, %xmm9
	vaddps	%xmm5, %xmm9, %xmm5
	vmovaps	12288(%rsp), %zmm6              # 64-byte Reload
	vextractf64x4	$1, %zmm6, %ymm9
	vaddps	%zmm9, %zmm6, %zmm9
	vextractf128	$1, %ymm9, %xmm10
	vaddps	%xmm10, %xmm9, %xmm9
	vshufpd	$1, %xmm9, %xmm9, %xmm10        # xmm10 = xmm9[1,0]
	vaddps	%xmm10, %xmm9, %xmm9
	vmovshdup	%xmm9, %xmm10           # xmm10 = xmm9[1,1,3,3]
	vaddss	%xmm10, %xmm9, %xmm9
	vaddss	%xmm27, %xmm9, %xmm9
	vshufpd	$1, %xmm5, %xmm5, %xmm10        # xmm10 = xmm5[1,0]
	vaddps	%xmm5, %xmm10, %xmm5
	vmovshdup	%xmm5, %xmm10           # xmm10 = xmm5[1,1,3,3]
	vaddss	%xmm5, %xmm10, %xmm5
	vmovss	%xmm5, 37632(%rsp)              # 4-byte Spill
	vmovaps	128(%rsp), %zmm29               # 64-byte Reload
	vmulps	%zmm29, %zmm1, %zmm5
	vfmadd231ps	896(%rsp), %zmm17, %zmm5 # 64-byte Folded Reload
                                        # zmm5 = (zmm17 * mem) + zmm5
	vextractf64x4	$1, %zmm5, %ymm10
	vaddps	%zmm10, %zmm5, %zmm5
	vextractf128	$1, %ymm5, %xmm10
	vaddps	%xmm5, %xmm10, %xmm10
	vmovaps	9664(%rsp), %zmm6               # 64-byte Reload
	vextractf64x4	$1, %zmm6, %ymm5
	vaddps	%zmm5, %zmm6, %zmm5
	vextractf128	$1, %ymm5, %xmm11
	vaddps	%xmm5, %xmm11, %xmm5
	vshufpd	$1, %xmm5, %xmm5, %xmm11        # xmm11 = xmm5[1,0]
	vaddps	%xmm5, %xmm11, %xmm5
	vmovshdup	%xmm5, %xmm11           # xmm11 = xmm5[1,1,3,3]
	vaddss	%xmm5, %xmm11, %xmm5
	vaddss	%xmm27, %xmm5, %xmm11
	vmovaps	4096(%rsp), %zmm17              # 64-byte Reload
	vmulps	1152(%rsp), %zmm17, %zmm5       # 64-byte Folded Reload
	vmovaps	4160(%rsp), %zmm16              # 64-byte Reload
	vfmadd231ps	768(%rsp), %zmm16, %zmm5 # 64-byte Folded Reload
                                        # zmm5 = (zmm16 * mem) + zmm5
	vextractf64x4	$1, %zmm5, %ymm12
	vaddps	%zmm12, %zmm5, %zmm5
	vextractf128	$1, %ymm5, %xmm12
	vaddps	%xmm5, %xmm12, %xmm5
	vshufpd	$1, %xmm5, %xmm5, %xmm12        # xmm12 = xmm5[1,0]
	vaddps	%xmm5, %xmm12, %xmm5
	vmovshdup	%xmm5, %xmm12           # xmm12 = xmm5[1,1,3,3]
	vaddss	%xmm5, %xmm12, %xmm5
	vaddss	%xmm27, %xmm5, %xmm5
	vshufpd	$1, %xmm10, %xmm10, %xmm12      # xmm12 = xmm10[1,0]
	vaddps	%xmm12, %xmm10, %xmm10
	vmovshdup	%xmm10, %xmm12          # xmm12 = xmm10[1,1,3,3]
	vaddss	%xmm12, %xmm10, %xmm10
	vmulps	2368(%rsp), %zmm1, %zmm12       # 64-byte Folded Reload
	vfmadd231ps	%zmm18, %zmm15, %zmm12  # zmm12 = (zmm15 * zmm18) + zmm12
	vextractf64x4	$1, %zmm12, %ymm13
	vaddps	%zmm13, %zmm12, %zmm12
	vextractf128	$1, %ymm12, %xmm13
	vaddps	%xmm13, %xmm12, %xmm12
	vshufpd	$1, %xmm12, %xmm12, %xmm13      # xmm13 = xmm12[1,0]
	vaddps	%xmm13, %xmm12, %xmm12
	vmovshdup	%xmm12, %xmm13          # xmm13 = xmm12[1,1,3,3]
	vaddss	%xmm13, %xmm12, %xmm12
	vaddss	%xmm27, %xmm10, %xmm10
	vaddss	%xmm27, %xmm12, %xmm12
	vinsertps	$16, %xmm12, %xmm10, %xmm10 # xmm10 = xmm10[0],xmm12[0],xmm10[2,3]
	vmulps	1984(%rsp), %zmm1, %zmm12       # 64-byte Folded Reload
	vfmadd231ps	1024(%rsp), %zmm15, %zmm12 # 64-byte Folded Reload
                                        # zmm12 = (zmm15 * mem) + zmm12
	vextractf64x4	$1, %zmm12, %ymm13
	vaddps	%zmm13, %zmm12, %zmm12
	vextractf128	$1, %ymm12, %xmm13
	vaddps	%xmm13, %xmm12, %xmm12
	vshufpd	$1, %xmm12, %xmm12, %xmm13      # xmm13 = xmm12[1,0]
	vaddps	%xmm13, %xmm12, %xmm12
	vmovshdup	%xmm12, %xmm13          # xmm13 = xmm12[1,1,3,3]
	vaddss	%xmm13, %xmm12, %xmm12
	vaddss	%xmm27, %xmm12, %xmm12
	vinsertps	$32, %xmm12, %xmm10, %xmm10 # xmm10 = xmm10[0,1],xmm12[0],xmm10[3]
	vmovaps	832(%rsp), %zmm23               # 64-byte Reload
	vmulps	%zmm23, %zmm1, %zmm12
	vfmadd231ps	960(%rsp), %zmm15, %zmm12 # 64-byte Folded Reload
                                        # zmm12 = (zmm15 * mem) + zmm12
	vextractf64x4	$1, %zmm12, %ymm13
	vaddps	%zmm13, %zmm12, %zmm12
	vextractf128	$1, %ymm12, %xmm13
	vaddps	%xmm13, %xmm12, %xmm12
	vshufpd	$1, %xmm12, %xmm12, %xmm13      # xmm13 = xmm12[1,0]
	vaddps	%xmm13, %xmm12, %xmm12
	vmovshdup	%xmm12, %xmm13          # xmm13 = xmm12[1,1,3,3]
	vaddss	%xmm13, %xmm12, %xmm12
	vaddss	%xmm27, %xmm12, %xmm12
	vinsertps	$48, %xmm12, %xmm10, %xmm6 # xmm6 = xmm10[0,1,2],xmm12[0]
	vmovaps	%xmm6, 10624(%rsp)              # 16-byte Spill
	vmulps	%zmm24, %zmm1, %zmm10
	vfmadd231ps	%zmm21, %zmm15, %zmm10  # zmm10 = (zmm15 * zmm21) + zmm10
	vextractf64x4	$1, %zmm10, %ymm12
	vaddps	%zmm12, %zmm10, %zmm10
	vextractf128	$1, %ymm10, %xmm12
	vaddps	%xmm12, %xmm10, %xmm10
	vshufpd	$1, %xmm10, %xmm10, %xmm12      # xmm12 = xmm10[1,0]
	vaddps	%xmm12, %xmm10, %xmm10
	vmovshdup	%xmm10, %xmm12          # xmm12 = xmm10[1,1,3,3]
	vaddss	%xmm12, %xmm10, %xmm10
	vaddss	%xmm27, %xmm10, %xmm10
	vinsertps	$16, %xmm11, %xmm10, %xmm10 # xmm10 = xmm10[0],xmm11[0],xmm10[2,3]
	vinsertps	$32, %xmm9, %xmm10, %xmm9 # xmm9 = xmm10[0,1],xmm9[0],xmm10[3]
	vinsertps	$48, %xmm7, %xmm9, %xmm7 # xmm7 = xmm9[0,1,2],xmm7[0]
	vmovaps	29568(%rsp), %zmm6              # 64-byte Reload
	vextractf64x4	$1, %zmm6, %ymm9
	vaddps	%zmm9, %zmm6, %zmm9
	vextractf128	$1, %ymm9, %xmm10
	vaddps	%xmm10, %xmm9, %xmm9
	vshufpd	$1, %xmm9, %xmm9, %xmm10        # xmm10 = xmm9[1,0]
	vaddps	%xmm10, %xmm9, %xmm9
	vmovshdup	%xmm9, %xmm10           # xmm10 = xmm9[1,1,3,3]
	vaddss	%xmm10, %xmm9, %xmm9
	vaddss	%xmm27, %xmm9, %xmm9
	vinsertps	$16, %xmm9, %xmm8, %xmm8 # xmm8 = xmm8[0],xmm9[0],xmm8[2,3]
	vinsertps	$32, %xmm0, %xmm8, %xmm0 # xmm0 = xmm8[0,1],xmm0[0],xmm8[3]
	vmovaps	28736(%rsp), %zmm6              # 64-byte Reload
	vextractf64x4	$1, %zmm6, %ymm8
	vaddps	%zmm8, %zmm6, %zmm6
	vextractf128	$1, %ymm6, %xmm8
	vaddps	%xmm6, %xmm8, %xmm6
	vshufpd	$1, %xmm6, %xmm6, %xmm8         # xmm8 = xmm6[1,0]
	vaddps	%xmm6, %xmm8, %xmm6
	vmovshdup	%xmm6, %xmm8            # xmm8 = xmm6[1,1,3,3]
	vaddss	%xmm6, %xmm8, %xmm6
	vaddss	%xmm27, %xmm6, %xmm6
	vinsertps	$48, %xmm6, %xmm0, %xmm6 # xmm6 = xmm0[0,1,2],xmm6[0]
	vmulps	3072(%rsp), %zmm1, %zmm0        # 64-byte Folded Reload
	vfmadd231ps	%zmm31, %zmm15, %zmm0   # zmm0 = (zmm15 * zmm31) + zmm0
	vextractf64x4	$1, %zmm0, %ymm8
	vaddps	%zmm8, %zmm0, %zmm0
	vextractf128	$1, %ymm0, %xmm8
	vaddps	%xmm0, %xmm8, %xmm0
	vmovaps	31488(%rsp), %zmm9              # 64-byte Reload
	vextractf64x4	$1, %zmm9, %ymm8
	vaddps	%zmm8, %zmm9, %zmm8
	vextractf128	$1, %ymm8, %xmm9
	vaddps	%xmm9, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm9         # xmm9 = xmm8[1,0]
	vaddps	%xmm9, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm9            # xmm9 = xmm8[1,1,3,3]
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm27, %xmm8, %xmm8
	vshufpd	$1, %xmm0, %xmm0, %xmm9         # xmm9 = xmm0[1,0]
	vaddps	%xmm0, %xmm9, %xmm0
	vmovshdup	%xmm0, %xmm9            # xmm9 = xmm0[1,1,3,3]
	vaddss	%xmm0, %xmm9, %xmm0
	vmovss	%xmm0, 31488(%rsp)              # 4-byte Spill
	vmulps	2112(%rsp), %zmm1, %zmm0        # 64-byte Folded Reload
	vmovaps	2496(%rsp), %zmm30              # 64-byte Reload
	vfmadd231ps	%zmm30, %zmm15, %zmm0   # zmm0 = (zmm15 * zmm30) + zmm0
	vextractf64x4	$1, %zmm0, %ymm9
	vaddps	%zmm9, %zmm0, %zmm0
	vmovaps	31552(%rsp), %zmm10             # 64-byte Reload
	vextractf64x4	$1, %zmm10, %ymm9
	vaddps	%zmm9, %zmm10, %zmm9
	vextractf128	$1, %ymm9, %xmm10
	vaddps	%xmm10, %xmm9, %xmm9
	vshufpd	$1, %xmm9, %xmm9, %xmm10        # xmm10 = xmm9[1,0]
	vaddps	%xmm10, %xmm9, %xmm9
	vmovshdup	%xmm9, %xmm10           # xmm10 = xmm9[1,1,3,3]
	vaddss	%xmm10, %xmm9, %xmm9
	vaddss	%xmm27, %xmm9, %xmm9
	vextractf128	$1, %ymm0, %xmm10
	vaddps	%xmm0, %xmm10, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm10        # xmm10 = xmm0[1,0]
	vaddps	%xmm0, %xmm10, %xmm0
	vmovshdup	%xmm0, %xmm10           # xmm10 = xmm0[1,1,3,3]
	vaddss	%xmm0, %xmm10, %xmm0
	vmovss	%xmm0, 31424(%rsp)              # 4-byte Spill
	vmulps	%zmm25, %zmm1, %zmm0
	vfmadd231ps	%zmm28, %zmm15, %zmm0   # zmm0 = (zmm15 * zmm28) + zmm0
	vextractf64x4	$1, %zmm0, %ymm10
	vaddps	%zmm10, %zmm0, %zmm0
	vextractf128	$1, %ymm0, %xmm10
	vaddps	%xmm0, %xmm10, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm10        # xmm10 = xmm0[1,0]
	vaddps	%xmm0, %xmm10, %xmm0
	vmovshdup	%xmm0, %xmm10           # xmm10 = xmm0[1,1,3,3]
	vaddss	%xmm0, %xmm10, %xmm0
	vmovss	%xmm0, 31360(%rsp)              # 4-byte Spill
	vmovaps	19392(%rsp), %zmm10             # 64-byte Reload
	vextractf64x4	$1, %zmm10, %ymm0
	vaddps	%zmm0, %zmm10, %zmm0
	vextractf128	$1, %ymm0, %xmm10
	vaddps	%xmm0, %xmm10, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm10        # xmm10 = xmm0[1,0]
	vaddps	%xmm0, %xmm10, %xmm0
	vmovshdup	%xmm0, %xmm10           # xmm10 = xmm0[1,1,3,3]
	vaddss	%xmm0, %xmm10, %xmm0
	vaddss	%xmm27, %xmm0, %xmm10
	vmulps	192(%rsp), %zmm1, %zmm0         # 64-byte Folded Reload
	vfmadd231ps	1344(%rsp), %zmm15, %zmm0 # 64-byte Folded Reload
                                        # zmm0 = (zmm15 * mem) + zmm0
	vextractf64x4	$1, %zmm0, %ymm11
	vaddps	%zmm11, %zmm0, %zmm0
	vextractf128	$1, %ymm0, %xmm11
	vaddps	%xmm0, %xmm11, %xmm0
	vmulps	2432(%rsp), %zmm1, %zmm11       # 64-byte Folded Reload
	vfmadd231ps	%zmm15, %zmm22, %zmm11  # zmm11 = (zmm22 * zmm15) + zmm11
	vextractf64x4	$1, %zmm11, %ymm12
	vaddps	%zmm12, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm12
	vaddps	%xmm12, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm12      # xmm12 = xmm11[1,0]
	vaddps	%xmm12, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm12          # xmm12 = xmm11[1,1,3,3]
	vaddss	%xmm12, %xmm11, %xmm11
	vaddss	%xmm27, %xmm11, %xmm11
	vshufpd	$1, %xmm0, %xmm0, %xmm12        # xmm12 = xmm0[1,0]
	vaddps	%xmm0, %xmm12, %xmm0
	vmovshdup	%xmm0, %xmm12           # xmm12 = xmm0[1,1,3,3]
	vaddss	%xmm0, %xmm12, %xmm0
	vmovaps	2176(%rsp), %zmm21              # 64-byte Reload
	vmulps	%zmm21, %zmm1, %zmm12
	vfmadd231ps	1728(%rsp), %zmm15, %zmm12 # 64-byte Folded Reload
                                        # zmm12 = (zmm15 * mem) + zmm12
	vextractf64x4	$1, %zmm12, %ymm13
	vaddps	%zmm13, %zmm12, %zmm12
	vextractf128	$1, %ymm12, %xmm13
	vaddps	%xmm13, %xmm12, %xmm12
	vshufpd	$1, %xmm12, %xmm12, %xmm13      # xmm13 = xmm12[1,0]
	vaddps	%xmm13, %xmm12, %xmm12
	vmovshdup	%xmm12, %xmm13          # xmm13 = xmm12[1,1,3,3]
	vaddss	%xmm13, %xmm12, %xmm12
	vmovaps	13632(%rsp), %zmm14             # 64-byte Reload
	vextractf64x4	$1, %zmm14, %ymm13
	vaddps	%zmm13, %zmm14, %zmm13
	vextractf128	$1, %ymm13, %xmm14
	vaddps	%xmm14, %xmm13, %xmm13
	vshufpd	$1, %xmm13, %xmm13, %xmm14      # xmm14 = xmm13[1,0]
	vaddps	%xmm14, %xmm13, %xmm13
	vmovshdup	%xmm13, %xmm14          # xmm14 = xmm13[1,1,3,3]
	vaddss	%xmm14, %xmm13, %xmm13
	vaddss	%xmm27, %xmm0, %xmm14
	vaddss	%xmm27, %xmm12, %xmm12
	vaddss	%xmm27, %xmm13, %xmm0
	vmovaps	%xmm0, 9920(%rsp)               # 16-byte Spill
	vmovaps	2304(%rsp), %zmm22              # 64-byte Reload
	vmulps	%zmm22, %zmm1, %zmm18
	vmovaps	%zmm15, %zmm13
	vmovaps	1536(%rsp), %zmm0               # 64-byte Reload
	vfmadd231ps	%zmm0, %zmm15, %zmm18   # zmm18 = (zmm15 * zmm0) + zmm18
	vmovaps	%zmm18, 37760(%rsp)             # 64-byte Spill
	vmovaps	576(%rsp), %zmm28               # 64-byte Reload
	vmulps	%zmm28, %zmm1, %zmm15
	vfmadd231ps	1920(%rsp), %zmm13, %zmm15 # 64-byte Folded Reload
                                        # zmm15 = (zmm13 * mem) + zmm15
	vmovaps	%zmm15, 19392(%rsp)             # 64-byte Spill
	vmovaps	%zmm13, %zmm15
	vmovaps	2240(%rsp), %zmm25              # 64-byte Reload
	vmulps	%zmm25, %zmm1, %zmm13
	vfmadd231ps	1088(%rsp), %zmm15, %zmm13 # 64-byte Folded Reload
                                        # zmm13 = (zmm15 * mem) + zmm13
	vextractf64x4	$1, %zmm13, %ymm15
	vaddps	%zmm15, %zmm13, %zmm13
	vextractf128	$1, %ymm13, %xmm15
	vaddps	%xmm15, %xmm13, %xmm13
	vshufpd	$1, %xmm13, %xmm13, %xmm15      # xmm15 = xmm13[1,0]
	vaddps	%xmm15, %xmm13, %xmm13
	vmovshdup	%xmm13, %xmm15          # xmm15 = xmm13[1,1,3,3]
	vaddss	%xmm15, %xmm13, %xmm13
	vaddss	%xmm27, %xmm13, %xmm13
	vinsertps	$16, %xmm13, %xmm14, %xmm13 # xmm13 = xmm14[0],xmm13[0],xmm14[2,3]
	vinsertps	$32, %xmm12, %xmm13, %xmm12 # xmm12 = xmm13[0,1],xmm12[0],xmm13[3]
	vinsertps	$48, %xmm11, %xmm12, %xmm1 # xmm1 = xmm12[0,1,2],xmm11[0]
	vmovaps	%xmm1, 4032(%rsp)               # 16-byte Spill
	vinsertf128	$1, %xmm4, %ymm3, %ymm1
	vmovaps	%zmm1, 19008(%rsp)              # 64-byte Spill
	vinsertf128	$1, %xmm6, %ymm7, %ymm1
	vmovaps	%zmm1, 3968(%rsp)               # 64-byte Spill
	vmovaps	%zmm17, %zmm1
	vmulps	%zmm19, %zmm17, %zmm3
	vmovaps	%zmm16, %zmm15
	vfmadd231ps	%zmm2, %zmm16, %zmm3    # zmm3 = (zmm16 * zmm2) + zmm3
	vextractf64x4	$1, %zmm3, %ymm4
	vaddps	%zmm4, %zmm3, %zmm3
	vextractf128	$1, %ymm3, %xmm4
	vaddps	%xmm4, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm4         # xmm4 = xmm3[1,0]
	vaddps	%xmm4, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm4            # xmm4 = xmm3[1,1,3,3]
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm27, %xmm3, %xmm3
	vinsertps	$16, %xmm10, %xmm3, %xmm3 # xmm3 = xmm3[0],xmm10[0],xmm3[2,3]
	vinsertps	$32, %xmm9, %xmm3, %xmm3 # xmm3 = xmm3[0,1],xmm9[0],xmm3[3]
	vinsertps	$48, %xmm8, %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],xmm8[0]
	vmovaps	13824(%rsp), %zmm6              # 64-byte Reload
	vextractf64x4	$1, %zmm6, %ymm4
	vaddps	%zmm4, %zmm6, %zmm4
	vextractf128	$1, %ymm4, %xmm6
	vaddps	%xmm6, %xmm4, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm6         # xmm6 = xmm4[1,0]
	vaddps	%xmm6, %xmm4, %xmm4
	vmovshdup	%xmm4, %xmm6            # xmm6 = xmm4[1,1,3,3]
	vaddss	%xmm6, %xmm4, %xmm4
	vaddss	%xmm27, %xmm4, %xmm4
	vinsertps	$16, %xmm4, %xmm5, %xmm4 # xmm4 = xmm5[0],xmm4[0],xmm5[2,3]
	vinsertps	$32, 9984(%rsp), %xmm4, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm4[0,1],mem[0],xmm4[3]
	vmovaps	13504(%rsp), %zmm5              # 64-byte Reload
	vextractf64x4	$1, %zmm5, %ymm4
	vaddps	%zmm4, %zmm5, %zmm4
	vextractf128	$1, %ymm4, %xmm5
	vaddps	%xmm5, %xmm4, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm5         # xmm5 = xmm4[1,0]
	vaddps	%xmm5, %xmm4, %xmm4
	vmovshdup	%xmm4, %xmm5            # xmm5 = xmm4[1,1,3,3]
	vaddss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm27, %xmm4, %xmm4
	vinsertps	$48, %xmm4, %xmm2, %xmm4 # xmm4 = xmm2[0,1,2],xmm4[0]
	vmulps	448(%rsp), %zmm17, %zmm2        # 64-byte Folded Reload
	vfmadd231ps	512(%rsp), %zmm16, %zmm2 # 64-byte Folded Reload
                                        # zmm2 = (zmm16 * mem) + zmm2
	vextractf64x4	$1, %zmm2, %ymm5
	vaddps	%zmm5, %zmm2, %zmm2
	vextractf128	$1, %ymm2, %xmm5
	vaddps	%xmm5, %xmm2, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm5         # xmm5 = xmm2[1,0]
	vaddps	%xmm5, %xmm2, %xmm5
	vmovaps	29184(%rsp), %zmm6              # 64-byte Reload
	vextractf64x4	$1, %zmm6, %ymm2
	vaddps	%zmm2, %zmm6, %zmm2
	vextractf128	$1, %ymm2, %xmm6
	vaddps	%xmm6, %xmm2, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm6         # xmm6 = xmm2[1,0]
	vaddps	%xmm6, %xmm2, %xmm2
	vmovshdup	%xmm2, %xmm6            # xmm6 = xmm2[1,1,3,3]
	vaddss	%xmm6, %xmm2, %xmm2
	vaddss	%xmm27, %xmm2, %xmm2
	vmulps	1280(%rsp), %zmm17, %zmm6       # 64-byte Folded Reload
	vfmadd231ps	1216(%rsp), %zmm16, %zmm6 # 64-byte Folded Reload
                                        # zmm6 = (zmm16 * mem) + zmm6
	vextractf64x4	$1, %zmm6, %ymm7
	vaddps	%zmm7, %zmm6, %zmm6
	vextractf128	$1, %ymm6, %xmm7
	vaddps	%xmm7, %xmm6, %xmm6
	vshufpd	$1, %xmm6, %xmm6, %xmm7         # xmm7 = xmm6[1,0]
	vaddps	%xmm7, %xmm6, %xmm6
	vmovshdup	%xmm6, %xmm7            # xmm7 = xmm6[1,1,3,3]
	vaddss	%xmm7, %xmm6, %xmm6
	vaddss	%xmm27, %xmm6, %xmm7
	vmovshdup	%xmm5, %xmm6            # xmm6 = xmm5[1,1,3,3]
	vaddss	%xmm6, %xmm5, %xmm5
	vmovss	%xmm5, 13824(%rsp)              # 4-byte Spill
	vmovaps	2048(%rsp), %zmm31              # 64-byte Reload
	vmulps	%zmm31, %zmm17, %zmm5
	vmovaps	704(%rsp), %zmm24               # 64-byte Reload
	vfmadd231ps	%zmm24, %zmm16, %zmm5   # zmm5 = (zmm16 * zmm24) + zmm5
	vextractf64x4	$1, %zmm5, %ymm6
	vaddps	%zmm6, %zmm5, %zmm5
	vextractf128	$1, %ymm5, %xmm6
	vaddps	%xmm6, %xmm5, %xmm5
	vshufpd	$1, %xmm5, %xmm5, %xmm6         # xmm6 = xmm5[1,0]
	vaddps	%xmm6, %xmm5, %xmm5
	vmovaps	30400(%rsp), %zmm8              # 64-byte Reload
	vextractf64x4	$1, %zmm8, %ymm6
	vaddps	%zmm6, %zmm8, %zmm6
	vextractf128	$1, %ymm6, %xmm8
	vaddps	%xmm6, %xmm8, %xmm6
	vshufpd	$1, %xmm6, %xmm6, %xmm8         # xmm8 = xmm6[1,0]
	vaddps	%xmm6, %xmm8, %xmm6
	vmovshdup	%xmm6, %xmm8            # xmm8 = xmm6[1,1,3,3]
	vaddss	%xmm6, %xmm8, %xmm6
	vaddss	%xmm27, %xmm6, %xmm6
	vmovshdup	%xmm5, %xmm8            # xmm8 = xmm5[1,1,3,3]
	vaddss	%xmm5, %xmm8, %xmm5
	vmovss	%xmm5, 13632(%rsp)              # 4-byte Spill
	vmulps	%zmm26, %zmm17, %zmm5
	vfmadd231ps	%zmm20, %zmm16, %zmm5   # zmm5 = (zmm16 * zmm20) + zmm5
	vextractf64x4	$1, %zmm5, %ymm8
	vaddps	%zmm8, %zmm5, %zmm5
	vextractf128	$1, %ymm5, %xmm8
	vaddps	%xmm5, %xmm8, %xmm5
	vmovaps	9216(%rsp), %zmm9               # 64-byte Reload
	vextractf64x4	$1, %zmm9, %ymm8
	vaddps	%zmm8, %zmm9, %zmm8
	vextractf128	$1, %ymm8, %xmm9
	vaddps	%xmm9, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm9         # xmm9 = xmm8[1,0]
	vaddps	%xmm9, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm9            # xmm9 = xmm8[1,1,3,3]
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm27, %xmm8, %xmm8
	vshufpd	$1, %xmm5, %xmm5, %xmm9         # xmm9 = xmm5[1,0]
	vaddps	%xmm5, %xmm9, %xmm5
	vmovshdup	%xmm5, %xmm9            # xmm9 = xmm5[1,1,3,3]
	vaddss	%xmm5, %xmm9, %xmm5
	vmovss	%xmm5, 13504(%rsp)              # 4-byte Spill
	vmulps	%zmm29, %zmm17, %zmm5
	vmovaps	896(%rsp), %zmm18               # 64-byte Reload
	vfmadd231ps	%zmm18, %zmm16, %zmm5   # zmm5 = (zmm16 * zmm18) + zmm5
	vextractf64x4	$1, %zmm5, %ymm9
	vaddps	%zmm9, %zmm5, %zmm5
	vextractf128	$1, %ymm5, %xmm9
	vaddps	%xmm5, %xmm9, %xmm9
	vmovaps	13120(%rsp), %zmm10             # 64-byte Reload
	vextractf64x4	$1, %zmm10, %ymm5
	vaddps	%zmm5, %zmm10, %zmm5
	vextractf128	$1, %ymm5, %xmm10
	vaddps	%xmm5, %xmm10, %xmm5
	vshufpd	$1, %xmm5, %xmm5, %xmm10        # xmm10 = xmm5[1,0]
	vaddps	%xmm5, %xmm10, %xmm5
	vmovshdup	%xmm5, %xmm10           # xmm10 = xmm5[1,1,3,3]
	vaddss	%xmm5, %xmm10, %xmm5
	vaddss	%xmm27, %xmm5, %xmm10
	vmovaps	4224(%rsp), %zmm17              # 64-byte Reload
	vmulps	1152(%rsp), %zmm17, %zmm5       # 64-byte Folded Reload
	vmovaps	4288(%rsp), %zmm16              # 64-byte Reload
	vfmadd231ps	768(%rsp), %zmm16, %zmm5 # 64-byte Folded Reload
                                        # zmm5 = (zmm16 * mem) + zmm5
	vextractf64x4	$1, %zmm5, %ymm11
	vaddps	%zmm11, %zmm5, %zmm5
	vextractf128	$1, %ymm5, %xmm11
	vaddps	%xmm5, %xmm11, %xmm5
	vshufpd	$1, %xmm5, %xmm5, %xmm11        # xmm11 = xmm5[1,0]
	vaddps	%xmm5, %xmm11, %xmm5
	vmovshdup	%xmm5, %xmm11           # xmm11 = xmm5[1,1,3,3]
	vaddss	%xmm5, %xmm11, %xmm5
	vaddss	%xmm27, %xmm5, %xmm5
	vshufpd	$1, %xmm9, %xmm9, %xmm11        # xmm11 = xmm9[1,0]
	vaddps	%xmm11, %xmm9, %xmm9
	vmovshdup	%xmm9, %xmm11           # xmm11 = xmm9[1,1,3,3]
	vaddss	%xmm11, %xmm9, %xmm9
	vmulps	2368(%rsp), %zmm1, %zmm11       # 64-byte Folded Reload
	vfmadd231ps	2560(%rsp), %zmm15, %zmm11 # 64-byte Folded Reload
                                        # zmm11 = (zmm15 * mem) + zmm11
	vextractf64x4	$1, %zmm11, %ymm12
	vaddps	%zmm12, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm12
	vaddps	%xmm12, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm12      # xmm12 = xmm11[1,0]
	vaddps	%xmm12, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm12          # xmm12 = xmm11[1,1,3,3]
	vaddss	%xmm12, %xmm11, %xmm11
	vaddss	%xmm27, %xmm9, %xmm9
	vaddss	%xmm27, %xmm11, %xmm11
	vinsertps	$16, %xmm11, %xmm9, %xmm9 # xmm9 = xmm9[0],xmm11[0],xmm9[2,3]
	vmovaps	1984(%rsp), %zmm29              # 64-byte Reload
	vmulps	%zmm29, %zmm1, %zmm11
	vfmadd231ps	1024(%rsp), %zmm15, %zmm11 # 64-byte Folded Reload
                                        # zmm11 = (zmm15 * mem) + zmm11
	vextractf64x4	$1, %zmm11, %ymm12
	vaddps	%zmm12, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm12
	vaddps	%xmm12, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm12      # xmm12 = xmm11[1,0]
	vaddps	%xmm12, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm12          # xmm12 = xmm11[1,1,3,3]
	vaddss	%xmm12, %xmm11, %xmm11
	vaddss	%xmm27, %xmm11, %xmm11
	vinsertps	$32, %xmm11, %xmm9, %xmm9 # xmm9 = xmm9[0,1],xmm11[0],xmm9[3]
	vmulps	%zmm23, %zmm1, %zmm11
	vfmadd231ps	960(%rsp), %zmm15, %zmm11 # 64-byte Folded Reload
                                        # zmm11 = (zmm15 * mem) + zmm11
	vextractf64x4	$1, %zmm11, %ymm12
	vaddps	%zmm12, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm12
	vaddps	%xmm12, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm12      # xmm12 = xmm11[1,0]
	vaddps	%xmm12, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm12          # xmm12 = xmm11[1,1,3,3]
	vaddss	%xmm12, %xmm11, %xmm11
	vaddss	%xmm27, %xmm11, %xmm11
	vinsertps	$48, %xmm11, %xmm9, %xmm9 # xmm9 = xmm9[0,1,2],xmm11[0]
	vmovaps	%xmm9, 31552(%rsp)              # 16-byte Spill
	vmulps	640(%rsp), %zmm1, %zmm9         # 64-byte Folded Reload
	vfmadd231ps	64(%rsp), %zmm15, %zmm9 # 64-byte Folded Reload
                                        # zmm9 = (zmm15 * mem) + zmm9
	vextractf64x4	$1, %zmm9, %ymm11
	vaddps	%zmm11, %zmm9, %zmm9
	vextractf128	$1, %ymm9, %xmm11
	vaddps	%xmm11, %xmm9, %xmm9
	vshufpd	$1, %xmm9, %xmm9, %xmm11        # xmm11 = xmm9[1,0]
	vaddps	%xmm11, %xmm9, %xmm9
	vmovshdup	%xmm9, %xmm11           # xmm11 = xmm9[1,1,3,3]
	vaddss	%xmm11, %xmm9, %xmm9
	vaddss	%xmm27, %xmm9, %xmm9
	vinsertps	$16, %xmm10, %xmm9, %xmm9 # xmm9 = xmm9[0],xmm10[0],xmm9[2,3]
	vinsertps	$32, %xmm8, %xmm9, %xmm8 # xmm8 = xmm9[0,1],xmm8[0],xmm9[3]
	vinsertps	$48, %xmm6, %xmm8, %xmm6 # xmm6 = xmm8[0,1,2],xmm6[0]
	vmovaps	29824(%rsp), %zmm9              # 64-byte Reload
	vextractf64x4	$1, %zmm9, %ymm8
	vaddps	%zmm8, %zmm9, %zmm8
	vextractf128	$1, %ymm8, %xmm9
	vaddps	%xmm9, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm9         # xmm9 = xmm8[1,0]
	vaddps	%xmm9, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm9            # xmm9 = xmm8[1,1,3,3]
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm27, %xmm8, %xmm8
	vinsertps	$16, %xmm8, %xmm7, %xmm7 # xmm7 = xmm7[0],xmm8[0],xmm7[2,3]
	vinsertps	$32, %xmm2, %xmm7, %xmm2 # xmm2 = xmm7[0,1],xmm2[0],xmm7[3]
	vmovaps	28864(%rsp), %zmm8              # 64-byte Reload
	vextractf64x4	$1, %zmm8, %ymm7
	vaddps	%zmm7, %zmm8, %zmm7
	vextractf128	$1, %ymm7, %xmm8
	vaddps	%xmm7, %xmm8, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm8         # xmm8 = xmm7[1,0]
	vaddps	%xmm7, %xmm8, %xmm7
	vmovshdup	%xmm7, %xmm8            # xmm8 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm27, %xmm7, %xmm7
	vinsertps	$48, %xmm7, %xmm2, %xmm7 # xmm7 = xmm2[0,1,2],xmm7[0]
	vmulps	3072(%rsp), %zmm1, %zmm2        # 64-byte Folded Reload
	vfmadd231ps	1472(%rsp), %zmm15, %zmm2 # 64-byte Folded Reload
                                        # zmm2 = (zmm15 * mem) + zmm2
	vextractf64x4	$1, %zmm2, %ymm8
	vaddps	%zmm8, %zmm2, %zmm2
	vextractf128	$1, %ymm2, %xmm8
	vaddps	%xmm2, %xmm8, %xmm2
	vmovaps	31680(%rsp), %zmm9              # 64-byte Reload
	vextractf64x4	$1, %zmm9, %ymm8
	vaddps	%zmm8, %zmm9, %zmm8
	vextractf128	$1, %ymm8, %xmm9
	vaddps	%xmm9, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm9         # xmm9 = xmm8[1,0]
	vaddps	%xmm9, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm9            # xmm9 = xmm8[1,1,3,3]
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm27, %xmm8, %xmm8
	vshufpd	$1, %xmm2, %xmm2, %xmm9         # xmm9 = xmm2[1,0]
	vaddps	%xmm2, %xmm9, %xmm2
	vmovshdup	%xmm2, %xmm9            # xmm9 = xmm2[1,1,3,3]
	vaddss	%xmm2, %xmm9, %xmm2
	vmovss	%xmm2, 9984(%rsp)               # 4-byte Spill
	vmulps	2112(%rsp), %zmm1, %zmm2        # 64-byte Folded Reload
	vfmadd231ps	%zmm30, %zmm15, %zmm2   # zmm2 = (zmm15 * zmm30) + zmm2
	vextractf64x4	$1, %zmm2, %ymm9
	vaddps	%zmm9, %zmm2, %zmm2
	vmovaps	31936(%rsp), %zmm10             # 64-byte Reload
	vextractf64x4	$1, %zmm10, %ymm9
	vaddps	%zmm9, %zmm10, %zmm9
	vextractf128	$1, %ymm9, %xmm10
	vaddps	%xmm10, %xmm9, %xmm9
	vshufpd	$1, %xmm9, %xmm9, %xmm10        # xmm10 = xmm9[1,0]
	vaddps	%xmm10, %xmm9, %xmm9
	vmovshdup	%xmm9, %xmm10           # xmm10 = xmm9[1,1,3,3]
	vaddss	%xmm10, %xmm9, %xmm9
	vaddss	%xmm27, %xmm9, %xmm9
	vextractf128	$1, %ymm2, %xmm10
	vaddps	%xmm2, %xmm10, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm10        # xmm10 = xmm2[1,0]
	vaddps	%xmm2, %xmm10, %xmm2
	vmovshdup	%xmm2, %xmm10           # xmm10 = xmm2[1,1,3,3]
	vaddss	%xmm2, %xmm10, %xmm2
	vmovss	%xmm2, 13312(%rsp)              # 4-byte Spill
	vmulps	3136(%rsp), %zmm1, %zmm2        # 64-byte Folded Reload
	vfmadd231ps	1856(%rsp), %zmm15, %zmm2 # 64-byte Folded Reload
                                        # zmm2 = (zmm15 * mem) + zmm2
	vextractf64x4	$1, %zmm2, %ymm10
	vaddps	%zmm10, %zmm2, %zmm2
	vextractf128	$1, %ymm2, %xmm10
	vaddps	%xmm2, %xmm10, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm10        # xmm10 = xmm2[1,0]
	vaddps	%xmm2, %xmm10, %xmm2
	vmovshdup	%xmm2, %xmm10           # xmm10 = xmm2[1,1,3,3]
	vaddss	%xmm2, %xmm10, %xmm2
	vmovss	%xmm2, 13184(%rsp)              # 4-byte Spill
	vmovaps	19456(%rsp), %zmm10             # 64-byte Reload
	vextractf64x4	$1, %zmm10, %ymm2
	vaddps	%zmm2, %zmm10, %zmm2
	vextractf128	$1, %ymm2, %xmm10
	vaddps	%xmm2, %xmm10, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm10        # xmm10 = xmm2[1,0]
	vaddps	%xmm2, %xmm10, %xmm2
	vmovshdup	%xmm2, %xmm10           # xmm10 = xmm2[1,1,3,3]
	vaddss	%xmm2, %xmm10, %xmm2
	vaddss	%xmm27, %xmm2, %xmm10
	vmulps	192(%rsp), %zmm1, %zmm2         # 64-byte Folded Reload
	vfmadd231ps	1344(%rsp), %zmm15, %zmm2 # 64-byte Folded Reload
                                        # zmm2 = (zmm15 * mem) + zmm2
	vextractf64x4	$1, %zmm2, %ymm11
	vaddps	%zmm11, %zmm2, %zmm2
	vextractf128	$1, %ymm2, %xmm11
	vaddps	%xmm2, %xmm11, %xmm2
	vmulps	2432(%rsp), %zmm1, %zmm11       # 64-byte Folded Reload
	vfmadd231ps	1792(%rsp), %zmm15, %zmm11 # 64-byte Folded Reload
                                        # zmm11 = (zmm15 * mem) + zmm11
	vextractf64x4	$1, %zmm11, %ymm12
	vaddps	%zmm12, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm12
	vaddps	%xmm12, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm12      # xmm12 = xmm11[1,0]
	vaddps	%xmm12, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm12          # xmm12 = xmm11[1,1,3,3]
	vaddss	%xmm12, %xmm11, %xmm11
	vaddss	%xmm27, %xmm11, %xmm11
	vshufpd	$1, %xmm2, %xmm2, %xmm12        # xmm12 = xmm2[1,0]
	vaddps	%xmm2, %xmm12, %xmm2
	vmovshdup	%xmm2, %xmm12           # xmm12 = xmm2[1,1,3,3]
	vaddss	%xmm2, %xmm12, %xmm2
	vmulps	%zmm21, %zmm1, %zmm12
	vmovaps	1728(%rsp), %zmm23              # 64-byte Reload
	vfmadd231ps	%zmm23, %zmm15, %zmm12  # zmm12 = (zmm15 * zmm23) + zmm12
	vextractf64x4	$1, %zmm12, %ymm13
	vaddps	%zmm13, %zmm12, %zmm12
	vextractf128	$1, %ymm12, %xmm13
	vaddps	%xmm13, %xmm12, %xmm12
	vshufpd	$1, %xmm12, %xmm12, %xmm13      # xmm13 = xmm12[1,0]
	vaddps	%xmm13, %xmm12, %xmm12
	vmovshdup	%xmm12, %xmm13          # xmm13 = xmm12[1,1,3,3]
	vaddss	%xmm13, %xmm12, %xmm12
	vmovaps	14016(%rsp), %zmm14             # 64-byte Reload
	vextractf64x4	$1, %zmm14, %ymm13
	vaddps	%zmm13, %zmm14, %zmm13
	vextractf128	$1, %ymm13, %xmm14
	vaddps	%xmm14, %xmm13, %xmm13
	vshufpd	$1, %xmm13, %xmm13, %xmm14      # xmm14 = xmm13[1,0]
	vaddps	%xmm14, %xmm13, %xmm13
	vmovshdup	%xmm13, %xmm14          # xmm14 = xmm13[1,1,3,3]
	vaddss	%xmm14, %xmm13, %xmm13
	vaddss	%xmm27, %xmm2, %xmm14
	vaddss	%xmm27, %xmm12, %xmm12
	vaddss	%xmm27, %xmm13, %xmm2
	vmovaps	%xmm2, 9856(%rsp)               # 16-byte Spill
	vmulps	%zmm22, %zmm1, %zmm19
	vmovaps	%zmm15, %zmm13
	vfmadd231ps	%zmm0, %zmm15, %zmm19   # zmm19 = (zmm15 * zmm0) + zmm19
	vmovaps	%zmm19, 31680(%rsp)             # 64-byte Spill
	vmulps	%zmm28, %zmm1, %zmm15
	vfmadd231ps	1920(%rsp), %zmm13, %zmm15 # 64-byte Folded Reload
                                        # zmm15 = (zmm13 * mem) + zmm15
	vmovaps	%zmm15, 19456(%rsp)             # 64-byte Spill
	vmovaps	%zmm13, %zmm15
	vmulps	%zmm25, %zmm1, %zmm13
	vmovaps	1088(%rsp), %zmm30              # 64-byte Reload
	vfmadd231ps	%zmm30, %zmm15, %zmm13  # zmm13 = (zmm15 * zmm30) + zmm13
	vextractf64x4	$1, %zmm13, %ymm15
	vaddps	%zmm15, %zmm13, %zmm13
	vextractf128	$1, %ymm13, %xmm15
	vaddps	%xmm15, %xmm13, %xmm13
	vshufpd	$1, %xmm13, %xmm13, %xmm15      # xmm15 = xmm13[1,0]
	vaddps	%xmm15, %xmm13, %xmm13
	vmovshdup	%xmm13, %xmm15          # xmm15 = xmm13[1,1,3,3]
	vaddss	%xmm15, %xmm13, %xmm13
	vaddss	%xmm27, %xmm13, %xmm13
	vinsertps	$16, %xmm13, %xmm14, %xmm13 # xmm13 = xmm14[0],xmm13[0],xmm14[2,3]
	vinsertps	$32, %xmm12, %xmm13, %xmm12 # xmm12 = xmm13[0,1],xmm12[0],xmm13[3]
	vinsertps	$48, %xmm11, %xmm12, %xmm1 # xmm1 = xmm12[0,1,2],xmm11[0]
	vmovaps	%xmm1, 4160(%rsp)               # 16-byte Spill
	vinsertf128	$1, %xmm4, %ymm3, %ymm1
	vmovaps	%zmm1, 31936(%rsp)              # 64-byte Spill
	vinsertf128	$1, %xmm7, %ymm6, %ymm1
	vmovaps	%zmm1, 4096(%rsp)               # 64-byte Spill
	vmovaps	%zmm17, %zmm1
	vmovaps	1600(%rsp), %zmm2               # 64-byte Reload
	vmulps	%zmm2, %zmm17, %zmm3
	vmovaps	%zmm16, %zmm15
	vfmadd231ps	1664(%rsp), %zmm16, %zmm3 # 64-byte Folded Reload
                                        # zmm3 = (zmm16 * mem) + zmm3
	vextractf64x4	$1, %zmm3, %ymm4
	vaddps	%zmm4, %zmm3, %zmm3
	vextractf128	$1, %ymm3, %xmm4
	vaddps	%xmm4, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm4         # xmm4 = xmm3[1,0]
	vaddps	%xmm4, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm4            # xmm4 = xmm3[1,1,3,3]
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm27, %xmm3, %xmm3
	vinsertps	$16, %xmm10, %xmm3, %xmm3 # xmm3 = xmm3[0],xmm10[0],xmm3[2,3]
	vinsertps	$32, %xmm9, %xmm3, %xmm3 # xmm3 = xmm3[0,1],xmm9[0],xmm3[3]
	vinsertps	$48, %xmm8, %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],xmm8[0]
	vmovaps	31808(%rsp), %zmm6              # 64-byte Reload
	vextractf64x4	$1, %zmm6, %ymm4
	vaddps	%zmm4, %zmm6, %zmm4
	vextractf128	$1, %ymm4, %xmm6
	vaddps	%xmm6, %xmm4, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm6         # xmm6 = xmm4[1,0]
	vaddps	%xmm6, %xmm4, %xmm4
	vmovshdup	%xmm4, %xmm6            # xmm6 = xmm4[1,1,3,3]
	vaddss	%xmm6, %xmm4, %xmm4
	vaddss	%xmm27, %xmm4, %xmm4
	vinsertps	$16, %xmm4, %xmm5, %xmm4 # xmm4 = xmm5[0],xmm4[0],xmm5[2,3]
	vinsertps	$32, 9920(%rsp), %xmm4, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm4[0,1],mem[0],xmm4[3]
	vmovaps	13952(%rsp), %zmm5              # 64-byte Reload
	vextractf64x4	$1, %zmm5, %ymm4
	vaddps	%zmm4, %zmm5, %zmm4
	vextractf128	$1, %ymm4, %xmm5
	vaddps	%xmm5, %xmm4, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm5         # xmm5 = xmm4[1,0]
	vaddps	%xmm5, %xmm4, %xmm4
	vmovshdup	%xmm4, %xmm5            # xmm5 = xmm4[1,1,3,3]
	vaddss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm27, %xmm4, %xmm4
	vinsertps	$48, %xmm4, %xmm0, %xmm4 # xmm4 = xmm0[0,1,2],xmm4[0]
	vmovaps	448(%rsp), %zmm20               # 64-byte Reload
	vmulps	%zmm20, %zmm17, %zmm0
	vmovaps	512(%rsp), %zmm26               # 64-byte Reload
	vfmadd231ps	%zmm26, %zmm16, %zmm0   # zmm0 = (zmm16 * zmm26) + zmm0
	vextractf64x4	$1, %zmm0, %ymm5
	vaddps	%zmm5, %zmm0, %zmm0
	vextractf128	$1, %ymm0, %xmm5
	vaddps	%xmm5, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm5         # xmm5 = xmm0[1,0]
	vaddps	%xmm5, %xmm0, %xmm5
	vmovaps	29376(%rsp), %zmm6              # 64-byte Reload
	vextractf64x4	$1, %zmm6, %ymm0
	vaddps	%zmm0, %zmm6, %zmm0
	vextractf128	$1, %ymm0, %xmm6
	vaddps	%xmm6, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm6         # xmm6 = xmm0[1,0]
	vaddps	%xmm6, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm6            # xmm6 = xmm0[1,1,3,3]
	vaddss	%xmm6, %xmm0, %xmm0
	vaddss	%xmm27, %xmm0, %xmm0
	vmulps	1280(%rsp), %zmm17, %zmm6       # 64-byte Folded Reload
	vfmadd231ps	1216(%rsp), %zmm16, %zmm6 # 64-byte Folded Reload
                                        # zmm6 = (zmm16 * mem) + zmm6
	vextractf64x4	$1, %zmm6, %ymm7
	vaddps	%zmm7, %zmm6, %zmm6
	vextractf128	$1, %ymm6, %xmm7
	vaddps	%xmm7, %xmm6, %xmm6
	vshufpd	$1, %xmm6, %xmm6, %xmm7         # xmm7 = xmm6[1,0]
	vaddps	%xmm7, %xmm6, %xmm6
	vmovshdup	%xmm6, %xmm7            # xmm7 = xmm6[1,1,3,3]
	vaddss	%xmm7, %xmm6, %xmm6
	vaddss	%xmm27, %xmm6, %xmm7
	vmovshdup	%xmm5, %xmm6            # xmm6 = xmm5[1,1,3,3]
	vaddss	%xmm6, %xmm5, %xmm5
	vmovss	%xmm5, 14016(%rsp)              # 4-byte Spill
	vmulps	%zmm31, %zmm17, %zmm5
	vfmadd231ps	%zmm24, %zmm16, %zmm5   # zmm5 = (zmm16 * zmm24) + zmm5
	vextractf64x4	$1, %zmm5, %ymm6
	vaddps	%zmm6, %zmm5, %zmm5
	vextractf128	$1, %ymm5, %xmm6
	vaddps	%xmm6, %xmm5, %xmm5
	vshufpd	$1, %xmm5, %xmm5, %xmm6         # xmm6 = xmm5[1,0]
	vaddps	%xmm6, %xmm5, %xmm5
	vmovaps	30656(%rsp), %zmm8              # 64-byte Reload
	vextractf64x4	$1, %zmm8, %ymm6
	vaddps	%zmm6, %zmm8, %zmm6
	vextractf128	$1, %ymm6, %xmm8
	vaddps	%xmm6, %xmm8, %xmm6
	vshufpd	$1, %xmm6, %xmm6, %xmm8         # xmm8 = xmm6[1,0]
	vaddps	%xmm6, %xmm8, %xmm6
	vmovshdup	%xmm6, %xmm8            # xmm8 = xmm6[1,1,3,3]
	vaddss	%xmm6, %xmm8, %xmm6
	vaddss	%xmm27, %xmm6, %xmm6
	vmovshdup	%xmm5, %xmm8            # xmm8 = xmm5[1,1,3,3]
	vaddss	%xmm5, %xmm8, %xmm5
	vmovss	%xmm5, 13952(%rsp)              # 4-byte Spill
	vmulps	256(%rsp), %zmm17, %zmm5        # 64-byte Folded Reload
	vmovaps	1408(%rsp), %zmm24              # 64-byte Reload
	vfmadd231ps	%zmm24, %zmm16, %zmm5   # zmm5 = (zmm16 * zmm24) + zmm5
	vextractf64x4	$1, %zmm5, %ymm8
	vaddps	%zmm8, %zmm5, %zmm5
	vextractf128	$1, %ymm5, %xmm8
	vaddps	%xmm5, %xmm8, %xmm5
	vmovaps	12864(%rsp), %zmm9              # 64-byte Reload
	vextractf64x4	$1, %zmm9, %ymm8
	vaddps	%zmm8, %zmm9, %zmm8
	vextractf128	$1, %ymm8, %xmm9
	vaddps	%xmm9, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm9         # xmm9 = xmm8[1,0]
	vaddps	%xmm9, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm9            # xmm9 = xmm8[1,1,3,3]
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm27, %xmm8, %xmm8
	vshufpd	$1, %xmm5, %xmm5, %xmm9         # xmm9 = xmm5[1,0]
	vaddps	%xmm5, %xmm9, %xmm5
	vmovshdup	%xmm5, %xmm9            # xmm9 = xmm5[1,1,3,3]
	vaddss	%xmm5, %xmm9, %xmm5
	vmovss	%xmm5, 13120(%rsp)              # 4-byte Spill
	vmulps	128(%rsp), %zmm17, %zmm5        # 64-byte Folded Reload
	vfmadd231ps	%zmm18, %zmm16, %zmm5   # zmm5 = (zmm16 * zmm18) + zmm5
	vextractf64x4	$1, %zmm5, %ymm9
	vaddps	%zmm9, %zmm5, %zmm5
	vextractf128	$1, %ymm5, %xmm9
	vaddps	%xmm5, %xmm9, %xmm9
	vmovaps	13440(%rsp), %zmm10             # 64-byte Reload
	vextractf64x4	$1, %zmm10, %ymm5
	vaddps	%zmm5, %zmm10, %zmm5
	vextractf128	$1, %ymm5, %xmm10
	vaddps	%xmm5, %xmm10, %xmm5
	vshufpd	$1, %xmm5, %xmm5, %xmm10        # xmm10 = xmm5[1,0]
	vaddps	%xmm5, %xmm10, %xmm5
	vmovshdup	%xmm5, %xmm10           # xmm10 = xmm5[1,1,3,3]
	vaddss	%xmm5, %xmm10, %xmm5
	vaddss	%xmm27, %xmm5, %xmm10
	vmovaps	6016(%rsp), %zmm17              # 64-byte Reload
	vmovaps	1152(%rsp), %zmm25              # 64-byte Reload
	vmulps	%zmm25, %zmm17, %zmm5
	vmovaps	6080(%rsp), %zmm16              # 64-byte Reload
	vmovaps	768(%rsp), %zmm28               # 64-byte Reload
	vfmadd231ps	%zmm28, %zmm16, %zmm5   # zmm5 = (zmm16 * zmm28) + zmm5
	vextractf64x4	$1, %zmm5, %ymm11
	vaddps	%zmm11, %zmm5, %zmm5
	vextractf128	$1, %ymm5, %xmm11
	vaddps	%xmm5, %xmm11, %xmm5
	vshufpd	$1, %xmm5, %xmm5, %xmm11        # xmm11 = xmm5[1,0]
	vaddps	%xmm5, %xmm11, %xmm5
	vmovshdup	%xmm5, %xmm11           # xmm11 = xmm5[1,1,3,3]
	vaddss	%xmm5, %xmm11, %xmm5
	vaddss	%xmm27, %xmm5, %xmm5
	vshufpd	$1, %xmm9, %xmm9, %xmm11        # xmm11 = xmm9[1,0]
	vaddps	%xmm11, %xmm9, %xmm9
	vmovshdup	%xmm9, %xmm11           # xmm11 = xmm9[1,1,3,3]
	vaddss	%xmm11, %xmm9, %xmm9
	vmulps	2368(%rsp), %zmm1, %zmm11       # 64-byte Folded Reload
	vfmadd231ps	2560(%rsp), %zmm15, %zmm11 # 64-byte Folded Reload
                                        # zmm11 = (zmm15 * mem) + zmm11
	vextractf64x4	$1, %zmm11, %ymm12
	vaddps	%zmm12, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm12
	vaddps	%xmm12, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm12      # xmm12 = xmm11[1,0]
	vaddps	%xmm12, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm12          # xmm12 = xmm11[1,1,3,3]
	vaddss	%xmm12, %xmm11, %xmm11
	vaddss	%xmm27, %xmm9, %xmm9
	vaddss	%xmm27, %xmm11, %xmm11
	vinsertps	$16, %xmm11, %xmm9, %xmm9 # xmm9 = xmm9[0],xmm11[0],xmm9[2,3]
	vmulps	%zmm29, %zmm1, %zmm11
	vmovaps	1024(%rsp), %zmm18              # 64-byte Reload
	vfmadd231ps	%zmm18, %zmm15, %zmm11  # zmm11 = (zmm15 * zmm18) + zmm11
	vextractf64x4	$1, %zmm11, %ymm12
	vaddps	%zmm12, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm12
	vaddps	%xmm12, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm12      # xmm12 = xmm11[1,0]
	vaddps	%xmm12, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm12          # xmm12 = xmm11[1,1,3,3]
	vaddss	%xmm12, %xmm11, %xmm11
	vaddss	%xmm27, %xmm11, %xmm11
	vinsertps	$32, %xmm11, %xmm9, %xmm9 # xmm9 = xmm9[0,1],xmm11[0],xmm9[3]
	vmulps	832(%rsp), %zmm1, %zmm11        # 64-byte Folded Reload
	vmovaps	960(%rsp), %zmm29               # 64-byte Reload
	vfmadd231ps	%zmm29, %zmm15, %zmm11  # zmm11 = (zmm15 * zmm29) + zmm11
	vextractf64x4	$1, %zmm11, %ymm12
	vaddps	%zmm12, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm12
	vaddps	%xmm12, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm12      # xmm12 = xmm11[1,0]
	vaddps	%xmm12, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm12          # xmm12 = xmm11[1,1,3,3]
	vaddss	%xmm12, %xmm11, %xmm11
	vaddss	%xmm27, %xmm11, %xmm11
	vinsertps	$48, %xmm11, %xmm9, %xmm9 # xmm9 = xmm9[0,1,2],xmm11[0]
	vmovaps	%xmm9, 31808(%rsp)              # 16-byte Spill
	vmulps	640(%rsp), %zmm1, %zmm9         # 64-byte Folded Reload
	vfmadd231ps	64(%rsp), %zmm15, %zmm9 # 64-byte Folded Reload
                                        # zmm9 = (zmm15 * mem) + zmm9
	vextractf64x4	$1, %zmm9, %ymm11
	vaddps	%zmm11, %zmm9, %zmm9
	vextractf128	$1, %ymm9, %xmm11
	vaddps	%xmm11, %xmm9, %xmm9
	vshufpd	$1, %xmm9, %xmm9, %xmm11        # xmm11 = xmm9[1,0]
	vaddps	%xmm11, %xmm9, %xmm9
	vmovshdup	%xmm9, %xmm11           # xmm11 = xmm9[1,1,3,3]
	vaddss	%xmm11, %xmm9, %xmm9
	vaddss	%xmm27, %xmm9, %xmm9
	vinsertps	$16, %xmm10, %xmm9, %xmm9 # xmm9 = xmm9[0],xmm10[0],xmm9[2,3]
	vinsertps	$32, %xmm8, %xmm9, %xmm8 # xmm8 = xmm9[0,1],xmm8[0],xmm9[3]
	vinsertps	$48, %xmm6, %xmm8, %xmm6 # xmm6 = xmm8[0,1,2],xmm6[0]
	vmovaps	30016(%rsp), %zmm9              # 64-byte Reload
	vextractf64x4	$1, %zmm9, %ymm8
	vaddps	%zmm8, %zmm9, %zmm8
	vextractf128	$1, %ymm8, %xmm9
	vaddps	%xmm9, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm9         # xmm9 = xmm8[1,0]
	vaddps	%xmm9, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm9            # xmm9 = xmm8[1,1,3,3]
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm27, %xmm8, %xmm8
	vinsertps	$16, %xmm8, %xmm7, %xmm7 # xmm7 = xmm7[0],xmm8[0],xmm7[2,3]
	vinsertps	$32, %xmm0, %xmm7, %xmm0 # xmm0 = xmm7[0,1],xmm0[0],xmm7[3]
	vmovaps	29056(%rsp), %zmm8              # 64-byte Reload
	vextractf64x4	$1, %zmm8, %ymm7
	vaddps	%zmm7, %zmm8, %zmm7
	vextractf128	$1, %ymm7, %xmm8
	vaddps	%xmm7, %xmm8, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm8         # xmm8 = xmm7[1,0]
	vaddps	%xmm7, %xmm8, %xmm7
	vmovshdup	%xmm7, %xmm8            # xmm8 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm27, %xmm7, %xmm7
	vinsertps	$48, %xmm7, %xmm0, %xmm7 # xmm7 = xmm0[0,1,2],xmm7[0]
	vmulps	3072(%rsp), %zmm1, %zmm0        # 64-byte Folded Reload
	vfmadd231ps	1472(%rsp), %zmm15, %zmm0 # 64-byte Folded Reload
                                        # zmm0 = (zmm15 * mem) + zmm0
	vextractf64x4	$1, %zmm0, %ymm8
	vaddps	%zmm8, %zmm0, %zmm0
	vextractf128	$1, %ymm0, %xmm8
	vaddps	%xmm0, %xmm8, %xmm0
	vmovaps	32384(%rsp), %zmm9              # 64-byte Reload
	vextractf64x4	$1, %zmm9, %ymm8
	vaddps	%zmm8, %zmm9, %zmm8
	vextractf128	$1, %ymm8, %xmm9
	vaddps	%xmm9, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm9         # xmm9 = xmm8[1,0]
	vaddps	%xmm9, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm9            # xmm9 = xmm8[1,1,3,3]
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm27, %xmm8, %xmm8
	vshufpd	$1, %xmm0, %xmm0, %xmm9         # xmm9 = xmm0[1,0]
	vaddps	%xmm0, %xmm9, %xmm0
	vmovshdup	%xmm0, %xmm9            # xmm9 = xmm0[1,1,3,3]
	vaddss	%xmm0, %xmm9, %xmm0
	vmovss	%xmm0, 13440(%rsp)              # 4-byte Spill
	vmovaps	2112(%rsp), %zmm22              # 64-byte Reload
	vmulps	%zmm22, %zmm1, %zmm0
	vmovaps	2496(%rsp), %zmm31              # 64-byte Reload
	vfmadd231ps	%zmm31, %zmm15, %zmm0   # zmm0 = (zmm15 * zmm31) + zmm0
	vextractf64x4	$1, %zmm0, %ymm9
	vaddps	%zmm9, %zmm0, %zmm0
	vmovaps	32768(%rsp), %zmm10             # 64-byte Reload
	vextractf64x4	$1, %zmm10, %ymm9
	vaddps	%zmm9, %zmm10, %zmm9
	vextractf128	$1, %ymm9, %xmm10
	vaddps	%xmm10, %xmm9, %xmm9
	vshufpd	$1, %xmm9, %xmm9, %xmm10        # xmm10 = xmm9[1,0]
	vaddps	%xmm10, %xmm9, %xmm9
	vmovshdup	%xmm9, %xmm10           # xmm10 = xmm9[1,1,3,3]
	vaddss	%xmm10, %xmm9, %xmm9
	vaddss	%xmm27, %xmm9, %xmm9
	vextractf128	$1, %ymm0, %xmm10
	vaddps	%xmm0, %xmm10, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm10        # xmm10 = xmm0[1,0]
	vaddps	%xmm0, %xmm10, %xmm0
	vmovshdup	%xmm0, %xmm10           # xmm10 = xmm0[1,1,3,3]
	vaddss	%xmm0, %xmm10, %xmm0
	vmovss	%xmm0, 9920(%rsp)               # 4-byte Spill
	vmulps	3136(%rsp), %zmm1, %zmm0        # 64-byte Folded Reload
	vfmadd231ps	1856(%rsp), %zmm15, %zmm0 # 64-byte Folded Reload
                                        # zmm0 = (zmm15 * mem) + zmm0
	vextractf64x4	$1, %zmm0, %ymm10
	vaddps	%zmm10, %zmm0, %zmm0
	vextractf128	$1, %ymm0, %xmm10
	vaddps	%xmm0, %xmm10, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm10        # xmm10 = xmm0[1,0]
	vaddps	%xmm0, %xmm10, %xmm0
	vmovshdup	%xmm0, %xmm10           # xmm10 = xmm0[1,1,3,3]
	vaddss	%xmm0, %xmm10, %xmm0
	vmovss	%xmm0, 12928(%rsp)              # 4-byte Spill
	vmovaps	19520(%rsp), %zmm10             # 64-byte Reload
	vextractf64x4	$1, %zmm10, %ymm0
	vaddps	%zmm0, %zmm10, %zmm0
	vextractf128	$1, %ymm0, %xmm10
	vaddps	%xmm0, %xmm10, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm10        # xmm10 = xmm0[1,0]
	vaddps	%xmm0, %xmm10, %xmm0
	vmovshdup	%xmm0, %xmm10           # xmm10 = xmm0[1,1,3,3]
	vaddss	%xmm0, %xmm10, %xmm0
	vaddss	%xmm27, %xmm0, %xmm10
	vmovaps	192(%rsp), %zmm19               # 64-byte Reload
	vmulps	%zmm19, %zmm1, %zmm0
	vfmadd231ps	1344(%rsp), %zmm15, %zmm0 # 64-byte Folded Reload
                                        # zmm0 = (zmm15 * mem) + zmm0
	vextractf64x4	$1, %zmm0, %ymm11
	vaddps	%zmm11, %zmm0, %zmm0
	vextractf128	$1, %ymm0, %xmm11
	vaddps	%xmm0, %xmm11, %xmm0
	vmulps	2432(%rsp), %zmm1, %zmm11       # 64-byte Folded Reload
	vfmadd231ps	1792(%rsp), %zmm15, %zmm11 # 64-byte Folded Reload
                                        # zmm11 = (zmm15 * mem) + zmm11
	vextractf64x4	$1, %zmm11, %ymm12
	vaddps	%zmm12, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm12
	vaddps	%xmm12, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm12      # xmm12 = xmm11[1,0]
	vaddps	%xmm12, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm12          # xmm12 = xmm11[1,1,3,3]
	vaddss	%xmm12, %xmm11, %xmm11
	vaddss	%xmm27, %xmm11, %xmm11
	vshufpd	$1, %xmm0, %xmm0, %xmm12        # xmm12 = xmm0[1,0]
	vaddps	%xmm0, %xmm12, %xmm0
	vmovshdup	%xmm0, %xmm12           # xmm12 = xmm0[1,1,3,3]
	vaddss	%xmm0, %xmm12, %xmm0
	vmulps	%zmm21, %zmm1, %zmm12
	vfmadd231ps	%zmm23, %zmm15, %zmm12  # zmm12 = (zmm15 * zmm23) + zmm12
	vextractf64x4	$1, %zmm12, %ymm13
	vaddps	%zmm13, %zmm12, %zmm12
	vextractf128	$1, %ymm12, %xmm13
	vaddps	%xmm13, %xmm12, %xmm12
	vshufpd	$1, %xmm12, %xmm12, %xmm13      # xmm13 = xmm12[1,0]
	vaddps	%xmm13, %xmm12, %xmm12
	vmovshdup	%xmm12, %xmm13          # xmm13 = xmm12[1,1,3,3]
	vaddss	%xmm13, %xmm12, %xmm12
	vmovaps	14528(%rsp), %zmm14             # 64-byte Reload
	vextractf64x4	$1, %zmm14, %ymm13
	vaddps	%zmm13, %zmm14, %zmm13
	vextractf128	$1, %ymm13, %xmm14
	vaddps	%xmm14, %xmm13, %xmm13
	vshufpd	$1, %xmm13, %xmm13, %xmm14      # xmm14 = xmm13[1,0]
	vaddps	%xmm14, %xmm13, %xmm13
	vmovshdup	%xmm13, %xmm14          # xmm14 = xmm13[1,1,3,3]
	vaddss	%xmm14, %xmm13, %xmm13
	vaddss	%xmm27, %xmm0, %xmm14
	vaddss	%xmm27, %xmm12, %xmm12
	vaddss	%xmm27, %xmm13, %xmm0
	vmovaps	%xmm0, 9728(%rsp)               # 16-byte Spill
	vmulps	2304(%rsp), %zmm1, %zmm21       # 64-byte Folded Reload
	vmovaps	%zmm15, %zmm13
	vfmadd231ps	1536(%rsp), %zmm15, %zmm21 # 64-byte Folded Reload
                                        # zmm21 = (zmm15 * mem) + zmm21
	vmovaps	%zmm21, 32384(%rsp)             # 64-byte Spill
	vmulps	576(%rsp), %zmm1, %zmm15        # 64-byte Folded Reload
	vmovaps	1920(%rsp), %zmm0               # 64-byte Reload
	vfmadd231ps	%zmm0, %zmm13, %zmm15   # zmm15 = (zmm13 * zmm0) + zmm15
	vmovaps	%zmm15, 19520(%rsp)             # 64-byte Spill
	vmovaps	%zmm13, %zmm15
	vmulps	2240(%rsp), %zmm1, %zmm13       # 64-byte Folded Reload
	vfmadd231ps	%zmm30, %zmm15, %zmm13  # zmm13 = (zmm15 * zmm30) + zmm13
	vextractf64x4	$1, %zmm13, %ymm15
	vaddps	%zmm15, %zmm13, %zmm13
	vextractf128	$1, %ymm13, %xmm15
	vaddps	%xmm15, %xmm13, %xmm13
	vshufpd	$1, %xmm13, %xmm13, %xmm15      # xmm15 = xmm13[1,0]
	vaddps	%xmm15, %xmm13, %xmm13
	vmovshdup	%xmm13, %xmm15          # xmm15 = xmm13[1,1,3,3]
	vaddss	%xmm15, %xmm13, %xmm13
	vaddss	%xmm27, %xmm13, %xmm13
	vinsertps	$16, %xmm13, %xmm14, %xmm13 # xmm13 = xmm14[0],xmm13[0],xmm14[2,3]
	vinsertps	$32, %xmm12, %xmm13, %xmm12 # xmm12 = xmm13[0,1],xmm12[0],xmm13[3]
	vinsertps	$48, %xmm11, %xmm12, %xmm1 # xmm1 = xmm12[0,1,2],xmm11[0]
	vmovaps	%xmm1, 4288(%rsp)               # 16-byte Spill
	vinsertf128	$1, %xmm4, %ymm3, %ymm1
	vmovaps	%zmm1, 32768(%rsp)              # 64-byte Spill
	vinsertf128	$1, %xmm7, %ymm6, %ymm1
	vmovaps	%zmm1, 4224(%rsp)               # 64-byte Spill
	vmovaps	%zmm17, %zmm1
	vmulps	%zmm2, %zmm17, %zmm3
	vmovaps	%zmm16, %zmm15
	vfmadd231ps	1664(%rsp), %zmm16, %zmm3 # 64-byte Folded Reload
                                        # zmm3 = (zmm16 * mem) + zmm3
	vextractf64x4	$1, %zmm3, %ymm4
	vaddps	%zmm4, %zmm3, %zmm3
	vextractf128	$1, %ymm3, %xmm4
	vaddps	%xmm4, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm4         # xmm4 = xmm3[1,0]
	vaddps	%xmm4, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm4            # xmm4 = xmm3[1,1,3,3]
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm27, %xmm3, %xmm3
	vinsertps	$16, %xmm10, %xmm3, %xmm3 # xmm3 = xmm3[0],xmm10[0],xmm3[2,3]
	vinsertps	$32, %xmm9, %xmm3, %xmm3 # xmm3 = xmm3[0,1],xmm9[0],xmm3[3]
	vinsertps	$48, %xmm8, %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],xmm8[0]
	vmovaps	32640(%rsp), %zmm6              # 64-byte Reload
	vextractf64x4	$1, %zmm6, %ymm4
	vaddps	%zmm4, %zmm6, %zmm4
	vextractf128	$1, %ymm4, %xmm6
	vaddps	%xmm6, %xmm4, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm6         # xmm6 = xmm4[1,0]
	vaddps	%xmm6, %xmm4, %xmm4
	vmovshdup	%xmm4, %xmm6            # xmm6 = xmm4[1,1,3,3]
	vaddss	%xmm6, %xmm4, %xmm4
	vaddss	%xmm27, %xmm4, %xmm4
	vinsertps	$16, %xmm4, %xmm5, %xmm4 # xmm4 = xmm5[0],xmm4[0],xmm5[2,3]
	vinsertps	$32, 9856(%rsp), %xmm4, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm4[0,1],mem[0],xmm4[3]
	vmovaps	14400(%rsp), %zmm5              # 64-byte Reload
	vextractf64x4	$1, %zmm5, %ymm4
	vaddps	%zmm4, %zmm5, %zmm4
	vextractf128	$1, %ymm4, %xmm5
	vaddps	%xmm5, %xmm4, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm5         # xmm5 = xmm4[1,0]
	vaddps	%xmm5, %xmm4, %xmm4
	vmovshdup	%xmm4, %xmm5            # xmm5 = xmm4[1,1,3,3]
	vaddss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm27, %xmm4, %xmm4
	vinsertps	$48, %xmm4, %xmm2, %xmm4 # xmm4 = xmm2[0,1,2],xmm4[0]
	vmulps	%zmm20, %zmm17, %zmm2
	vfmadd231ps	%zmm26, %zmm16, %zmm2   # zmm2 = (zmm16 * zmm26) + zmm2
	vextractf64x4	$1, %zmm2, %ymm5
	vaddps	%zmm5, %zmm2, %zmm2
	vextractf128	$1, %ymm2, %xmm5
	vaddps	%xmm5, %xmm2, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm5         # xmm5 = xmm2[1,0]
	vaddps	%xmm5, %xmm2, %xmm5
	vmovaps	29632(%rsp), %zmm6              # 64-byte Reload
	vextractf64x4	$1, %zmm6, %ymm2
	vaddps	%zmm2, %zmm6, %zmm2
	vextractf128	$1, %ymm2, %xmm6
	vaddps	%xmm6, %xmm2, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm6         # xmm6 = xmm2[1,0]
	vaddps	%xmm6, %xmm2, %xmm2
	vmovshdup	%xmm2, %xmm6            # xmm6 = xmm2[1,1,3,3]
	vaddss	%xmm6, %xmm2, %xmm2
	vaddss	%xmm27, %xmm2, %xmm2
	vmovaps	1280(%rsp), %zmm20              # 64-byte Reload
	vmulps	%zmm20, %zmm17, %zmm6
	vmovaps	1216(%rsp), %zmm30              # 64-byte Reload
	vfmadd231ps	%zmm30, %zmm16, %zmm6   # zmm6 = (zmm16 * zmm30) + zmm6
	vextractf64x4	$1, %zmm6, %ymm7
	vaddps	%zmm7, %zmm6, %zmm6
	vextractf128	$1, %ymm6, %xmm7
	vaddps	%xmm7, %xmm6, %xmm6
	vshufpd	$1, %xmm6, %xmm6, %xmm7         # xmm7 = xmm6[1,0]
	vaddps	%xmm7, %xmm6, %xmm6
	vmovshdup	%xmm6, %xmm7            # xmm7 = xmm6[1,1,3,3]
	vaddss	%xmm7, %xmm6, %xmm6
	vaddss	%xmm27, %xmm6, %xmm7
	vmovshdup	%xmm5, %xmm6            # xmm6 = xmm5[1,1,3,3]
	vaddss	%xmm6, %xmm5, %xmm5
	vmovss	%xmm5, 14528(%rsp)              # 4-byte Spill
	vmulps	2048(%rsp), %zmm17, %zmm5       # 64-byte Folded Reload
	vfmadd231ps	704(%rsp), %zmm16, %zmm5 # 64-byte Folded Reload
                                        # zmm5 = (zmm16 * mem) + zmm5
	vextractf64x4	$1, %zmm5, %ymm6
	vaddps	%zmm6, %zmm5, %zmm5
	vextractf128	$1, %ymm5, %xmm6
	vaddps	%xmm6, %xmm5, %xmm5
	vshufpd	$1, %xmm5, %xmm5, %xmm6         # xmm6 = xmm5[1,0]
	vaddps	%xmm6, %xmm5, %xmm5
	vmovaps	31040(%rsp), %zmm8              # 64-byte Reload
	vextractf64x4	$1, %zmm8, %ymm6
	vaddps	%zmm6, %zmm8, %zmm6
	vextractf128	$1, %ymm6, %xmm8
	vaddps	%xmm6, %xmm8, %xmm6
	vshufpd	$1, %xmm6, %xmm6, %xmm8         # xmm8 = xmm6[1,0]
	vaddps	%xmm6, %xmm8, %xmm6
	vmovshdup	%xmm6, %xmm8            # xmm8 = xmm6[1,1,3,3]
	vaddss	%xmm6, %xmm8, %xmm6
	vaddss	%xmm27, %xmm6, %xmm6
	vmovshdup	%xmm5, %xmm8            # xmm8 = xmm5[1,1,3,3]
	vaddss	%xmm5, %xmm8, %xmm5
	vmovss	%xmm5, 14400(%rsp)              # 4-byte Spill
	vmulps	256(%rsp), %zmm17, %zmm5        # 64-byte Folded Reload
	vfmadd231ps	%zmm24, %zmm16, %zmm5   # zmm5 = (zmm16 * zmm24) + zmm5
	vextractf64x4	$1, %zmm5, %ymm8
	vaddps	%zmm8, %zmm5, %zmm5
	vextractf128	$1, %ymm5, %xmm8
	vaddps	%xmm5, %xmm8, %xmm5
	vmovaps	13376(%rsp), %zmm9              # 64-byte Reload
	vextractf64x4	$1, %zmm9, %ymm8
	vaddps	%zmm8, %zmm9, %zmm8
	vextractf128	$1, %ymm8, %xmm9
	vaddps	%xmm9, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm9         # xmm9 = xmm8[1,0]
	vaddps	%xmm9, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm9            # xmm9 = xmm8[1,1,3,3]
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm27, %xmm8, %xmm8
	vshufpd	$1, %xmm5, %xmm5, %xmm9         # xmm9 = xmm5[1,0]
	vaddps	%xmm5, %xmm9, %xmm5
	vmovshdup	%xmm5, %xmm9            # xmm9 = xmm5[1,1,3,3]
	vaddss	%xmm5, %xmm9, %xmm5
	vmovss	%xmm5, 13376(%rsp)              # 4-byte Spill
	vmulps	128(%rsp), %zmm17, %zmm5        # 64-byte Folded Reload
	vfmadd231ps	896(%rsp), %zmm16, %zmm5 # 64-byte Folded Reload
                                        # zmm5 = (zmm16 * mem) + zmm5
	vextractf64x4	$1, %zmm5, %ymm9
	vaddps	%zmm9, %zmm5, %zmm5
	vextractf128	$1, %ymm5, %xmm9
	vaddps	%xmm5, %xmm9, %xmm9
	vmovaps	13888(%rsp), %zmm10             # 64-byte Reload
	vextractf64x4	$1, %zmm10, %ymm5
	vaddps	%zmm5, %zmm10, %zmm5
	vextractf128	$1, %ymm5, %xmm10
	vaddps	%xmm5, %xmm10, %xmm5
	vshufpd	$1, %xmm5, %xmm5, %xmm10        # xmm10 = xmm5[1,0]
	vaddps	%xmm5, %xmm10, %xmm5
	vmovshdup	%xmm5, %xmm10           # xmm10 = xmm5[1,1,3,3]
	vaddss	%xmm5, %xmm10, %xmm5
	vaddss	%xmm27, %xmm5, %xmm10
	vmovaps	4352(%rsp), %zmm17              # 64-byte Reload
	vmulps	%zmm25, %zmm17, %zmm5
	vmovaps	4416(%rsp), %zmm16              # 64-byte Reload
	vfmadd231ps	%zmm28, %zmm16, %zmm5   # zmm5 = (zmm16 * zmm28) + zmm5
	vextractf64x4	$1, %zmm5, %ymm11
	vaddps	%zmm11, %zmm5, %zmm5
	vextractf128	$1, %ymm5, %xmm11
	vaddps	%xmm5, %xmm11, %xmm5
	vshufpd	$1, %xmm5, %xmm5, %xmm11        # xmm11 = xmm5[1,0]
	vaddps	%xmm5, %xmm11, %xmm5
	vmovshdup	%xmm5, %xmm11           # xmm11 = xmm5[1,1,3,3]
	vaddss	%xmm5, %xmm11, %xmm5
	vaddss	%xmm27, %xmm5, %xmm5
	vshufpd	$1, %xmm9, %xmm9, %xmm11        # xmm11 = xmm9[1,0]
	vaddps	%xmm11, %xmm9, %xmm9
	vmovshdup	%xmm9, %xmm11           # xmm11 = xmm9[1,1,3,3]
	vaddss	%xmm11, %xmm9, %xmm9
	vmovaps	2368(%rsp), %zmm28              # 64-byte Reload
	vmulps	%zmm28, %zmm1, %zmm11
	vfmadd231ps	2560(%rsp), %zmm15, %zmm11 # 64-byte Folded Reload
                                        # zmm11 = (zmm15 * mem) + zmm11
	vextractf64x4	$1, %zmm11, %ymm12
	vaddps	%zmm12, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm12
	vaddps	%xmm12, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm12      # xmm12 = xmm11[1,0]
	vaddps	%xmm12, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm12          # xmm12 = xmm11[1,1,3,3]
	vaddss	%xmm12, %xmm11, %xmm11
	vaddss	%xmm27, %xmm9, %xmm9
	vaddss	%xmm27, %xmm11, %xmm11
	vinsertps	$16, %xmm11, %xmm9, %xmm9 # xmm9 = xmm9[0],xmm11[0],xmm9[2,3]
	vmulps	1984(%rsp), %zmm1, %zmm11       # 64-byte Folded Reload
	vfmadd231ps	%zmm18, %zmm15, %zmm11  # zmm11 = (zmm15 * zmm18) + zmm11
	vextractf64x4	$1, %zmm11, %ymm12
	vaddps	%zmm12, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm12
	vaddps	%xmm12, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm12      # xmm12 = xmm11[1,0]
	vaddps	%xmm12, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm12          # xmm12 = xmm11[1,1,3,3]
	vaddss	%xmm12, %xmm11, %xmm11
	vaddss	%xmm27, %xmm11, %xmm11
	vinsertps	$32, %xmm11, %xmm9, %xmm9 # xmm9 = xmm9[0,1],xmm11[0],xmm9[3]
	vmulps	832(%rsp), %zmm1, %zmm11        # 64-byte Folded Reload
	vfmadd231ps	%zmm29, %zmm15, %zmm11  # zmm11 = (zmm15 * zmm29) + zmm11
	vextractf64x4	$1, %zmm11, %ymm12
	vaddps	%zmm12, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm12
	vaddps	%xmm12, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm12      # xmm12 = xmm11[1,0]
	vaddps	%xmm12, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm12          # xmm12 = xmm11[1,1,3,3]
	vaddss	%xmm12, %xmm11, %xmm11
	vaddss	%xmm27, %xmm11, %xmm11
	vinsertps	$48, %xmm11, %xmm9, %xmm9 # xmm9 = xmm9[0,1,2],xmm11[0]
	vmovaps	%xmm9, 32640(%rsp)              # 16-byte Spill
	vmulps	640(%rsp), %zmm1, %zmm9         # 64-byte Folded Reload
	vfmadd231ps	64(%rsp), %zmm15, %zmm9 # 64-byte Folded Reload
                                        # zmm9 = (zmm15 * mem) + zmm9
	vextractf64x4	$1, %zmm9, %ymm11
	vaddps	%zmm11, %zmm9, %zmm9
	vextractf128	$1, %ymm9, %xmm11
	vaddps	%xmm11, %xmm9, %xmm9
	vshufpd	$1, %xmm9, %xmm9, %xmm11        # xmm11 = xmm9[1,0]
	vaddps	%xmm11, %xmm9, %xmm9
	vmovshdup	%xmm9, %xmm11           # xmm11 = xmm9[1,1,3,3]
	vaddss	%xmm11, %xmm9, %xmm9
	vaddss	%xmm27, %xmm9, %xmm9
	vinsertps	$16, %xmm10, %xmm9, %xmm9 # xmm9 = xmm9[0],xmm10[0],xmm9[2,3]
	vinsertps	$32, %xmm8, %xmm9, %xmm8 # xmm8 = xmm9[0,1],xmm8[0],xmm9[3]
	vinsertps	$48, %xmm6, %xmm8, %xmm6 # xmm6 = xmm8[0,1,2],xmm6[0]
	vmovaps	30272(%rsp), %zmm9              # 64-byte Reload
	vextractf64x4	$1, %zmm9, %ymm8
	vaddps	%zmm8, %zmm9, %zmm8
	vextractf128	$1, %ymm8, %xmm9
	vaddps	%xmm9, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm9         # xmm9 = xmm8[1,0]
	vaddps	%xmm9, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm9            # xmm9 = xmm8[1,1,3,3]
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm27, %xmm8, %xmm8
	vinsertps	$16, %xmm8, %xmm7, %xmm7 # xmm7 = xmm7[0],xmm8[0],xmm7[2,3]
	vinsertps	$32, %xmm2, %xmm7, %xmm2 # xmm2 = xmm7[0,1],xmm2[0],xmm7[3]
	vmovaps	29248(%rsp), %zmm8              # 64-byte Reload
	vextractf64x4	$1, %zmm8, %ymm7
	vaddps	%zmm7, %zmm8, %zmm7
	vextractf128	$1, %ymm7, %xmm8
	vaddps	%xmm7, %xmm8, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm8         # xmm8 = xmm7[1,0]
	vaddps	%xmm7, %xmm8, %xmm7
	vmovshdup	%xmm7, %xmm8            # xmm8 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm27, %xmm7, %xmm7
	vinsertps	$48, %xmm7, %xmm2, %xmm7 # xmm7 = xmm2[0,1,2],xmm7[0]
	vmovaps	3072(%rsp), %zmm26              # 64-byte Reload
	vmulps	%zmm26, %zmm1, %zmm2
	vmovaps	1472(%rsp), %zmm29              # 64-byte Reload
	vfmadd231ps	%zmm29, %zmm15, %zmm2   # zmm2 = (zmm15 * zmm29) + zmm2
	vextractf64x4	$1, %zmm2, %ymm8
	vaddps	%zmm8, %zmm2, %zmm2
	vextractf128	$1, %ymm2, %xmm8
	vaddps	%xmm2, %xmm8, %xmm2
	vmovaps	32960(%rsp), %zmm9              # 64-byte Reload
	vextractf64x4	$1, %zmm9, %ymm8
	vaddps	%zmm8, %zmm9, %zmm8
	vextractf128	$1, %ymm8, %xmm9
	vaddps	%xmm9, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm9         # xmm9 = xmm8[1,0]
	vaddps	%xmm9, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm9            # xmm9 = xmm8[1,1,3,3]
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm27, %xmm8, %xmm8
	vshufpd	$1, %xmm2, %xmm2, %xmm9         # xmm9 = xmm2[1,0]
	vaddps	%xmm2, %xmm9, %xmm2
	vmovshdup	%xmm2, %xmm9            # xmm9 = xmm2[1,1,3,3]
	vaddss	%xmm2, %xmm9, %xmm2
	vmovss	%xmm2, 13888(%rsp)              # 4-byte Spill
	vmulps	%zmm22, %zmm1, %zmm2
	vfmadd231ps	%zmm31, %zmm15, %zmm2   # zmm2 = (zmm15 * zmm31) + zmm2
	vextractf64x4	$1, %zmm2, %ymm9
	vaddps	%zmm9, %zmm2, %zmm2
	vmovaps	33088(%rsp), %zmm10             # 64-byte Reload
	vextractf64x4	$1, %zmm10, %ymm9
	vaddps	%zmm9, %zmm10, %zmm9
	vextractf128	$1, %ymm9, %xmm10
	vaddps	%xmm10, %xmm9, %xmm9
	vshufpd	$1, %xmm9, %xmm9, %xmm10        # xmm10 = xmm9[1,0]
	vaddps	%xmm10, %xmm9, %xmm9
	vmovshdup	%xmm9, %xmm10           # xmm10 = xmm9[1,1,3,3]
	vaddss	%xmm10, %xmm9, %xmm9
	vaddss	%xmm27, %xmm9, %xmm9
	vextractf128	$1, %ymm2, %xmm10
	vaddps	%xmm2, %xmm10, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm10        # xmm10 = xmm2[1,0]
	vaddps	%xmm2, %xmm10, %xmm2
	vmovshdup	%xmm2, %xmm10           # xmm10 = xmm2[1,1,3,3]
	vaddss	%xmm2, %xmm10, %xmm2
	vmovss	%xmm2, 9856(%rsp)               # 4-byte Spill
	vmovaps	3136(%rsp), %zmm21              # 64-byte Reload
	vmulps	%zmm21, %zmm1, %zmm2
	vmovaps	1856(%rsp), %zmm23              # 64-byte Reload
	vfmadd231ps	%zmm23, %zmm15, %zmm2   # zmm2 = (zmm15 * zmm23) + zmm2
	vextractf64x4	$1, %zmm2, %ymm10
	vaddps	%zmm10, %zmm2, %zmm2
	vextractf128	$1, %ymm2, %xmm10
	vaddps	%xmm2, %xmm10, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm10        # xmm10 = xmm2[1,0]
	vaddps	%xmm2, %xmm10, %xmm2
	vmovshdup	%xmm2, %xmm10           # xmm10 = xmm2[1,1,3,3]
	vaddss	%xmm2, %xmm10, %xmm2
	vmovss	%xmm2, 12864(%rsp)              # 4-byte Spill
	vmovaps	19648(%rsp), %zmm10             # 64-byte Reload
	vextractf64x4	$1, %zmm10, %ymm2
	vaddps	%zmm2, %zmm10, %zmm2
	vextractf128	$1, %ymm2, %xmm10
	vaddps	%xmm2, %xmm10, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm10        # xmm10 = xmm2[1,0]
	vaddps	%xmm2, %xmm10, %xmm2
	vmovshdup	%xmm2, %xmm10           # xmm10 = xmm2[1,1,3,3]
	vaddss	%xmm2, %xmm10, %xmm2
	vaddss	%xmm27, %xmm2, %xmm10
	vmulps	%zmm19, %zmm1, %zmm2
	vfmadd231ps	1344(%rsp), %zmm15, %zmm2 # 64-byte Folded Reload
                                        # zmm2 = (zmm15 * mem) + zmm2
	vextractf64x4	$1, %zmm2, %ymm11
	vaddps	%zmm11, %zmm2, %zmm2
	vextractf128	$1, %ymm2, %xmm11
	vaddps	%xmm2, %xmm11, %xmm2
	vmovaps	2432(%rsp), %zmm25              # 64-byte Reload
	vmulps	%zmm25, %zmm1, %zmm11
	vfmadd231ps	1792(%rsp), %zmm15, %zmm11 # 64-byte Folded Reload
                                        # zmm11 = (zmm15 * mem) + zmm11
	vextractf64x4	$1, %zmm11, %ymm12
	vaddps	%zmm12, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm12
	vaddps	%xmm12, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm12      # xmm12 = xmm11[1,0]
	vaddps	%xmm12, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm12          # xmm12 = xmm11[1,1,3,3]
	vaddss	%xmm12, %xmm11, %xmm11
	vaddss	%xmm27, %xmm11, %xmm11
	vshufpd	$1, %xmm2, %xmm2, %xmm12        # xmm12 = xmm2[1,0]
	vaddps	%xmm2, %xmm12, %xmm2
	vmovshdup	%xmm2, %xmm12           # xmm12 = xmm2[1,1,3,3]
	vaddss	%xmm2, %xmm12, %xmm2
	vmovaps	2176(%rsp), %zmm24              # 64-byte Reload
	vmulps	%zmm24, %zmm1, %zmm12
	vfmadd231ps	1728(%rsp), %zmm15, %zmm12 # 64-byte Folded Reload
                                        # zmm12 = (zmm15 * mem) + zmm12
	vextractf64x4	$1, %zmm12, %ymm13
	vaddps	%zmm13, %zmm12, %zmm12
	vextractf128	$1, %ymm12, %xmm13
	vaddps	%xmm13, %xmm12, %xmm12
	vshufpd	$1, %xmm12, %xmm12, %xmm13      # xmm13 = xmm12[1,0]
	vaddps	%xmm13, %xmm12, %xmm12
	vmovshdup	%xmm12, %xmm13          # xmm13 = xmm12[1,1,3,3]
	vaddss	%xmm13, %xmm12, %xmm12
	vmovaps	14784(%rsp), %zmm14             # 64-byte Reload
	vextractf64x4	$1, %zmm14, %ymm13
	vaddps	%zmm13, %zmm14, %zmm13
	vextractf128	$1, %ymm13, %xmm14
	vaddps	%xmm14, %xmm13, %xmm13
	vshufpd	$1, %xmm13, %xmm13, %xmm14      # xmm14 = xmm13[1,0]
	vaddps	%xmm14, %xmm13, %xmm13
	vmovshdup	%xmm13, %xmm14          # xmm14 = xmm13[1,1,3,3]
	vaddss	%xmm14, %xmm13, %xmm13
	vaddss	%xmm27, %xmm2, %xmm14
	vaddss	%xmm27, %xmm12, %xmm12
	vaddss	%xmm27, %xmm13, %xmm2
	vmovaps	%xmm2, 9664(%rsp)               # 16-byte Spill
	vmovaps	2304(%rsp), %zmm2               # 64-byte Reload
	vmulps	%zmm2, %zmm1, %zmm18
	vmovaps	%zmm15, %zmm13
	vfmadd231ps	1536(%rsp), %zmm15, %zmm18 # 64-byte Folded Reload
                                        # zmm18 = (zmm15 * mem) + zmm18
	vmovaps	%zmm18, 32960(%rsp)             # 64-byte Spill
	vmulps	576(%rsp), %zmm1, %zmm15        # 64-byte Folded Reload
	vfmadd231ps	%zmm0, %zmm13, %zmm15   # zmm15 = (zmm13 * zmm0) + zmm15
	vmovaps	%zmm15, 19648(%rsp)             # 64-byte Spill
	vmovaps	%zmm13, %zmm15
	vmulps	2240(%rsp), %zmm1, %zmm13       # 64-byte Folded Reload
	vfmadd231ps	1088(%rsp), %zmm15, %zmm13 # 64-byte Folded Reload
                                        # zmm13 = (zmm15 * mem) + zmm13
	vextractf64x4	$1, %zmm13, %ymm15
	vaddps	%zmm15, %zmm13, %zmm13
	vextractf128	$1, %ymm13, %xmm15
	vaddps	%xmm15, %xmm13, %xmm13
	vshufpd	$1, %xmm13, %xmm13, %xmm15      # xmm15 = xmm13[1,0]
	vaddps	%xmm15, %xmm13, %xmm13
	vmovshdup	%xmm13, %xmm15          # xmm15 = xmm13[1,1,3,3]
	vaddss	%xmm15, %xmm13, %xmm13
	vaddss	%xmm27, %xmm13, %xmm13
	vinsertps	$16, %xmm13, %xmm14, %xmm13 # xmm13 = xmm14[0],xmm13[0],xmm14[2,3]
	vinsertps	$32, %xmm12, %xmm13, %xmm12 # xmm12 = xmm13[0,1],xmm12[0],xmm13[3]
	vinsertps	$48, %xmm11, %xmm12, %xmm1 # xmm1 = xmm12[0,1,2],xmm11[0]
	vmovaps	%xmm1, 6080(%rsp)               # 16-byte Spill
	vinsertf128	$1, %xmm4, %ymm3, %ymm1
	vmovaps	%zmm1, 33088(%rsp)              # 64-byte Spill
	vinsertf128	$1, %xmm7, %ymm6, %ymm1
	vmovaps	%zmm1, 6016(%rsp)               # 64-byte Spill
	vmovaps	%zmm17, %zmm1
	vmulps	1600(%rsp), %zmm17, %zmm3       # 64-byte Folded Reload
	vmovaps	%zmm16, %zmm15
	vmovaps	1664(%rsp), %zmm22              # 64-byte Reload
	vfmadd231ps	%zmm22, %zmm16, %zmm3   # zmm3 = (zmm16 * zmm22) + zmm3
	vextractf64x4	$1, %zmm3, %ymm4
	vaddps	%zmm4, %zmm3, %zmm3
	vextractf128	$1, %ymm3, %xmm4
	vaddps	%xmm4, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm4         # xmm4 = xmm3[1,0]
	vaddps	%xmm4, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm4            # xmm4 = xmm3[1,1,3,3]
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm27, %xmm3, %xmm3
	vinsertps	$16, %xmm10, %xmm3, %xmm3 # xmm3 = xmm3[0],xmm10[0],xmm3[2,3]
	vinsertps	$32, %xmm9, %xmm3, %xmm3 # xmm3 = xmm3[0,1],xmm9[0],xmm3[3]
	vinsertps	$48, %xmm8, %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],xmm8[0]
	vmovaps	33024(%rsp), %zmm6              # 64-byte Reload
	vextractf64x4	$1, %zmm6, %ymm4
	vaddps	%zmm4, %zmm6, %zmm4
	vextractf128	$1, %ymm4, %xmm6
	vaddps	%xmm6, %xmm4, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm6         # xmm6 = xmm4[1,0]
	vaddps	%xmm6, %xmm4, %xmm4
	vmovshdup	%xmm4, %xmm6            # xmm6 = xmm4[1,1,3,3]
	vaddss	%xmm6, %xmm4, %xmm4
	vaddss	%xmm27, %xmm4, %xmm4
	vinsertps	$16, %xmm4, %xmm5, %xmm4 # xmm4 = xmm5[0],xmm4[0],xmm5[2,3]
	vinsertps	$32, 9728(%rsp), %xmm4, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm4[0,1],mem[0],xmm4[3]
	vmovaps	14656(%rsp), %zmm5              # 64-byte Reload
	vextractf64x4	$1, %zmm5, %ymm4
	vaddps	%zmm4, %zmm5, %zmm4
	vextractf128	$1, %ymm4, %xmm5
	vaddps	%xmm5, %xmm4, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm5         # xmm5 = xmm4[1,0]
	vaddps	%xmm5, %xmm4, %xmm4
	vmovshdup	%xmm4, %xmm5            # xmm5 = xmm4[1,1,3,3]
	vaddss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm27, %xmm4, %xmm4
	vinsertps	$48, %xmm4, %xmm0, %xmm4 # xmm4 = xmm0[0,1,2],xmm4[0]
	vmulps	448(%rsp), %zmm17, %zmm0        # 64-byte Folded Reload
	vfmadd231ps	512(%rsp), %zmm16, %zmm0 # 64-byte Folded Reload
                                        # zmm0 = (zmm16 * mem) + zmm0
	vextractf64x4	$1, %zmm0, %ymm5
	vaddps	%zmm5, %zmm0, %zmm0
	vextractf128	$1, %ymm0, %xmm5
	vaddps	%xmm5, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm5         # xmm5 = xmm0[1,0]
	vaddps	%xmm5, %xmm0, %xmm5
	vmovaps	29888(%rsp), %zmm6              # 64-byte Reload
	vextractf64x4	$1, %zmm6, %ymm0
	vaddps	%zmm0, %zmm6, %zmm0
	vextractf128	$1, %ymm0, %xmm6
	vaddps	%xmm6, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm6         # xmm6 = xmm0[1,0]
	vaddps	%xmm6, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm6            # xmm6 = xmm0[1,1,3,3]
	vaddss	%xmm6, %xmm0, %xmm0
	vaddss	%xmm27, %xmm0, %xmm0
	vmulps	%zmm20, %zmm17, %zmm6
	vfmadd231ps	%zmm30, %zmm16, %zmm6   # zmm6 = (zmm16 * zmm30) + zmm6
	vextractf64x4	$1, %zmm6, %ymm7
	vaddps	%zmm7, %zmm6, %zmm6
	vextractf128	$1, %ymm6, %xmm7
	vaddps	%xmm7, %xmm6, %xmm6
	vshufpd	$1, %xmm6, %xmm6, %xmm7         # xmm7 = xmm6[1,0]
	vaddps	%xmm7, %xmm6, %xmm6
	vmovshdup	%xmm6, %xmm7            # xmm7 = xmm6[1,1,3,3]
	vaddss	%xmm7, %xmm6, %xmm6
	vaddss	%xmm27, %xmm6, %xmm7
	vmovshdup	%xmm5, %xmm6            # xmm6 = xmm5[1,1,3,3]
	vaddss	%xmm6, %xmm5, %xmm5
	vmovss	%xmm5, 14784(%rsp)              # 4-byte Spill
	vmovaps	2048(%rsp), %zmm30              # 64-byte Reload
	vmulps	%zmm30, %zmm17, %zmm5
	vmovaps	704(%rsp), %zmm20               # 64-byte Reload
	vfmadd231ps	%zmm20, %zmm16, %zmm5   # zmm5 = (zmm16 * zmm20) + zmm5
	vextractf64x4	$1, %zmm5, %ymm6
	vaddps	%zmm6, %zmm5, %zmm5
	vextractf128	$1, %ymm5, %xmm6
	vaddps	%xmm6, %xmm5, %xmm5
	vshufpd	$1, %xmm5, %xmm5, %xmm6         # xmm6 = xmm5[1,0]
	vaddps	%xmm6, %xmm5, %xmm5
	vmovaps	9152(%rsp), %zmm8               # 64-byte Reload
	vextractf64x4	$1, %zmm8, %ymm6
	vaddps	%zmm6, %zmm8, %zmm6
	vextractf128	$1, %ymm6, %xmm8
	vaddps	%xmm6, %xmm8, %xmm6
	vshufpd	$1, %xmm6, %xmm6, %xmm8         # xmm8 = xmm6[1,0]
	vaddps	%xmm6, %xmm8, %xmm6
	vmovshdup	%xmm6, %xmm8            # xmm8 = xmm6[1,1,3,3]
	vaddss	%xmm6, %xmm8, %xmm6
	vaddss	%xmm27, %xmm6, %xmm6
	vmovshdup	%xmm5, %xmm8            # xmm8 = xmm5[1,1,3,3]
	vaddss	%xmm5, %xmm8, %xmm5
	vmovss	%xmm5, 14656(%rsp)              # 4-byte Spill
	vmulps	256(%rsp), %zmm17, %zmm5        # 64-byte Folded Reload
	vfmadd231ps	1408(%rsp), %zmm16, %zmm5 # 64-byte Folded Reload
                                        # zmm5 = (zmm16 * mem) + zmm5
	vextractf64x4	$1, %zmm5, %ymm8
	vaddps	%zmm8, %zmm5, %zmm5
	vextractf128	$1, %ymm5, %xmm8
	vaddps	%xmm5, %xmm8, %xmm5
	vmovaps	13696(%rsp), %zmm9              # 64-byte Reload
	vextractf64x4	$1, %zmm9, %ymm8
	vaddps	%zmm8, %zmm9, %zmm8
	vextractf128	$1, %ymm8, %xmm9
	vaddps	%xmm9, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm9         # xmm9 = xmm8[1,0]
	vaddps	%xmm9, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm9            # xmm9 = xmm8[1,1,3,3]
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm27, %xmm8, %xmm8
	vshufpd	$1, %xmm5, %xmm5, %xmm9         # xmm9 = xmm5[1,0]
	vaddps	%xmm5, %xmm9, %xmm5
	vmovshdup	%xmm5, %xmm9            # xmm9 = xmm5[1,1,3,3]
	vaddss	%xmm5, %xmm9, %xmm5
	vmovss	%xmm5, 13696(%rsp)              # 4-byte Spill
	vmulps	128(%rsp), %zmm17, %zmm5        # 64-byte Folded Reload
	vfmadd231ps	896(%rsp), %zmm16, %zmm5 # 64-byte Folded Reload
                                        # zmm5 = (zmm16 * mem) + zmm5
	vextractf64x4	$1, %zmm5, %ymm9
	vaddps	%zmm9, %zmm5, %zmm5
	vextractf128	$1, %ymm5, %xmm9
	vaddps	%xmm5, %xmm9, %xmm9
	vmovaps	14336(%rsp), %zmm10             # 64-byte Reload
	vextractf64x4	$1, %zmm10, %ymm5
	vaddps	%zmm5, %zmm10, %zmm5
	vextractf128	$1, %ymm5, %xmm10
	vaddps	%xmm5, %xmm10, %xmm5
	vshufpd	$1, %xmm5, %xmm5, %xmm10        # xmm10 = xmm5[1,0]
	vaddps	%xmm5, %xmm10, %xmm5
	vmovshdup	%xmm5, %xmm10           # xmm10 = xmm5[1,1,3,3]
	vaddss	%xmm5, %xmm10, %xmm5
	vaddss	%xmm27, %xmm5, %xmm10
	vmovaps	4480(%rsp), %zmm16              # 64-byte Reload
	vmulps	1152(%rsp), %zmm16, %zmm5       # 64-byte Folded Reload
	vmovaps	4544(%rsp), %zmm17              # 64-byte Reload
	vfmadd231ps	768(%rsp), %zmm17, %zmm5 # 64-byte Folded Reload
                                        # zmm5 = (zmm17 * mem) + zmm5
	vextractf64x4	$1, %zmm5, %ymm11
	vaddps	%zmm11, %zmm5, %zmm5
	vextractf128	$1, %ymm5, %xmm11
	vaddps	%xmm5, %xmm11, %xmm5
	vshufpd	$1, %xmm5, %xmm5, %xmm11        # xmm11 = xmm5[1,0]
	vaddps	%xmm5, %xmm11, %xmm5
	vmovshdup	%xmm5, %xmm11           # xmm11 = xmm5[1,1,3,3]
	vaddss	%xmm5, %xmm11, %xmm5
	vaddss	%xmm27, %xmm5, %xmm5
	vshufpd	$1, %xmm9, %xmm9, %xmm11        # xmm11 = xmm9[1,0]
	vaddps	%xmm11, %xmm9, %xmm9
	vmovshdup	%xmm9, %xmm11           # xmm11 = xmm9[1,1,3,3]
	vaddss	%xmm11, %xmm9, %xmm9
	vmulps	%zmm28, %zmm1, %zmm11
	vmovaps	2560(%rsp), %zmm19              # 64-byte Reload
	vfmadd231ps	%zmm19, %zmm15, %zmm11  # zmm11 = (zmm15 * zmm19) + zmm11
	vextractf64x4	$1, %zmm11, %ymm12
	vaddps	%zmm12, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm12
	vaddps	%xmm12, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm12      # xmm12 = xmm11[1,0]
	vaddps	%xmm12, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm12          # xmm12 = xmm11[1,1,3,3]
	vaddss	%xmm12, %xmm11, %xmm11
	vaddss	%xmm27, %xmm9, %xmm9
	vaddss	%xmm27, %xmm11, %xmm11
	vinsertps	$16, %xmm11, %xmm9, %xmm9 # xmm9 = xmm9[0],xmm11[0],xmm9[2,3]
	vmulps	1984(%rsp), %zmm1, %zmm11       # 64-byte Folded Reload
	vfmadd231ps	1024(%rsp), %zmm15, %zmm11 # 64-byte Folded Reload
                                        # zmm11 = (zmm15 * mem) + zmm11
	vextractf64x4	$1, %zmm11, %ymm12
	vaddps	%zmm12, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm12
	vaddps	%xmm12, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm12      # xmm12 = xmm11[1,0]
	vaddps	%xmm12, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm12          # xmm12 = xmm11[1,1,3,3]
	vaddss	%xmm12, %xmm11, %xmm11
	vaddss	%xmm27, %xmm11, %xmm11
	vinsertps	$32, %xmm11, %xmm9, %xmm9 # xmm9 = xmm9[0,1],xmm11[0],xmm9[3]
	vmovaps	832(%rsp), %zmm18               # 64-byte Reload
	vmulps	%zmm18, %zmm1, %zmm11
	vfmadd231ps	960(%rsp), %zmm15, %zmm11 # 64-byte Folded Reload
                                        # zmm11 = (zmm15 * mem) + zmm11
	vextractf64x4	$1, %zmm11, %ymm12
	vaddps	%zmm12, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm12
	vaddps	%xmm12, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm12      # xmm12 = xmm11[1,0]
	vaddps	%xmm12, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm12          # xmm12 = xmm11[1,1,3,3]
	vaddss	%xmm12, %xmm11, %xmm11
	vaddss	%xmm27, %xmm11, %xmm11
	vinsertps	$48, %xmm11, %xmm9, %xmm9 # xmm9 = xmm9[0,1,2],xmm11[0]
	vmovaps	%xmm9, 33024(%rsp)              # 16-byte Spill
	vmulps	640(%rsp), %zmm1, %zmm9         # 64-byte Folded Reload
	vfmadd231ps	64(%rsp), %zmm15, %zmm9 # 64-byte Folded Reload
                                        # zmm9 = (zmm15 * mem) + zmm9
	vextractf64x4	$1, %zmm9, %ymm11
	vaddps	%zmm11, %zmm9, %zmm9
	vextractf128	$1, %ymm9, %xmm11
	vaddps	%xmm11, %xmm9, %xmm9
	vshufpd	$1, %xmm9, %xmm9, %xmm11        # xmm11 = xmm9[1,0]
	vaddps	%xmm11, %xmm9, %xmm9
	vmovshdup	%xmm9, %xmm11           # xmm11 = xmm9[1,1,3,3]
	vaddss	%xmm11, %xmm9, %xmm9
	vaddss	%xmm27, %xmm9, %xmm9
	vinsertps	$16, %xmm10, %xmm9, %xmm9 # xmm9 = xmm9[0],xmm10[0],xmm9[2,3]
	vinsertps	$32, %xmm8, %xmm9, %xmm8 # xmm8 = xmm9[0,1],xmm8[0],xmm9[3]
	vinsertps	$48, %xmm6, %xmm8, %xmm6 # xmm6 = xmm8[0,1,2],xmm6[0]
	vmovaps	30528(%rsp), %zmm9              # 64-byte Reload
	vextractf64x4	$1, %zmm9, %ymm8
	vaddps	%zmm8, %zmm9, %zmm8
	vextractf128	$1, %ymm8, %xmm9
	vaddps	%xmm9, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm9         # xmm9 = xmm8[1,0]
	vaddps	%xmm9, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm9            # xmm9 = xmm8[1,1,3,3]
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm27, %xmm8, %xmm8
	vinsertps	$16, %xmm8, %xmm7, %xmm7 # xmm7 = xmm7[0],xmm8[0],xmm7[2,3]
	vinsertps	$32, %xmm0, %xmm7, %xmm0 # xmm0 = xmm7[0,1],xmm0[0],xmm7[3]
	vmovaps	29440(%rsp), %zmm8              # 64-byte Reload
	vextractf64x4	$1, %zmm8, %ymm7
	vaddps	%zmm7, %zmm8, %zmm7
	vextractf128	$1, %ymm7, %xmm8
	vaddps	%xmm7, %xmm8, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm8         # xmm8 = xmm7[1,0]
	vaddps	%xmm7, %xmm8, %xmm7
	vmovshdup	%xmm7, %xmm8            # xmm8 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm27, %xmm7, %xmm7
	vinsertps	$48, %xmm7, %xmm0, %xmm7 # xmm7 = xmm0[0,1,2],xmm7[0]
	vmulps	%zmm26, %zmm1, %zmm0
	vfmadd231ps	%zmm29, %zmm15, %zmm0   # zmm0 = (zmm15 * zmm29) + zmm0
	vextractf64x4	$1, %zmm0, %ymm8
	vaddps	%zmm8, %zmm0, %zmm0
	vextractf128	$1, %ymm0, %xmm8
	vaddps	%xmm0, %xmm8, %xmm0
	vmovaps	33472(%rsp), %zmm9              # 64-byte Reload
	vextractf64x4	$1, %zmm9, %ymm8
	vaddps	%zmm8, %zmm9, %zmm8
	vextractf128	$1, %ymm8, %xmm9
	vaddps	%xmm9, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm9         # xmm9 = xmm8[1,0]
	vaddps	%xmm9, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm9            # xmm9 = xmm8[1,1,3,3]
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm27, %xmm8, %xmm8
	vshufpd	$1, %xmm0, %xmm0, %xmm9         # xmm9 = xmm0[1,0]
	vaddps	%xmm0, %xmm9, %xmm0
	vmovshdup	%xmm0, %xmm9            # xmm9 = xmm0[1,1,3,3]
	vaddss	%xmm0, %xmm9, %xmm0
	vmovss	%xmm0, 14336(%rsp)              # 4-byte Spill
	vmulps	2112(%rsp), %zmm1, %zmm0        # 64-byte Folded Reload
	vfmadd231ps	%zmm31, %zmm15, %zmm0   # zmm0 = (zmm15 * zmm31) + zmm0
	vextractf64x4	$1, %zmm0, %ymm9
	vaddps	%zmm9, %zmm0, %zmm0
	vmovaps	33600(%rsp), %zmm10             # 64-byte Reload
	vextractf64x4	$1, %zmm10, %ymm9
	vaddps	%zmm9, %zmm10, %zmm9
	vextractf128	$1, %ymm9, %xmm10
	vaddps	%xmm10, %xmm9, %xmm9
	vshufpd	$1, %xmm9, %xmm9, %xmm10        # xmm10 = xmm9[1,0]
	vaddps	%xmm10, %xmm9, %xmm9
	vmovshdup	%xmm9, %xmm10           # xmm10 = xmm9[1,1,3,3]
	vaddss	%xmm10, %xmm9, %xmm9
	vaddss	%xmm27, %xmm9, %xmm9
	vextractf128	$1, %ymm0, %xmm10
	vaddps	%xmm0, %xmm10, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm10        # xmm10 = xmm0[1,0]
	vaddps	%xmm0, %xmm10, %xmm0
	vmovshdup	%xmm0, %xmm10           # xmm10 = xmm0[1,1,3,3]
	vaddss	%xmm0, %xmm10, %xmm0
	vmovss	%xmm0, 9728(%rsp)               # 4-byte Spill
	vmulps	%zmm21, %zmm1, %zmm0
	vfmadd231ps	%zmm23, %zmm15, %zmm0   # zmm0 = (zmm15 * zmm23) + zmm0
	vextractf64x4	$1, %zmm0, %ymm10
	vaddps	%zmm10, %zmm0, %zmm0
	vextractf128	$1, %ymm0, %xmm10
	vaddps	%xmm0, %xmm10, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm10        # xmm10 = xmm0[1,0]
	vaddps	%xmm0, %xmm10, %xmm0
	vmovshdup	%xmm0, %xmm10           # xmm10 = xmm0[1,1,3,3]
	vaddss	%xmm0, %xmm10, %xmm0
	vmovss	%xmm0, 12736(%rsp)              # 4-byte Spill
	vmovaps	19712(%rsp), %zmm10             # 64-byte Reload
	vextractf64x4	$1, %zmm10, %ymm0
	vaddps	%zmm0, %zmm10, %zmm0
	vextractf128	$1, %ymm0, %xmm10
	vaddps	%xmm0, %xmm10, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm10        # xmm10 = xmm0[1,0]
	vaddps	%xmm0, %xmm10, %xmm0
	vmovshdup	%xmm0, %xmm10           # xmm10 = xmm0[1,1,3,3]
	vaddss	%xmm0, %xmm10, %xmm0
	vaddss	%xmm27, %xmm0, %xmm10
	vmulps	192(%rsp), %zmm1, %zmm0         # 64-byte Folded Reload
	vmovaps	1344(%rsp), %zmm23              # 64-byte Reload
	vfmadd231ps	%zmm23, %zmm15, %zmm0   # zmm0 = (zmm15 * zmm23) + zmm0
	vextractf64x4	$1, %zmm0, %ymm11
	vaddps	%zmm11, %zmm0, %zmm0
	vextractf128	$1, %ymm0, %xmm11
	vaddps	%xmm0, %xmm11, %xmm0
	vmulps	%zmm25, %zmm1, %zmm11
	vmovaps	1792(%rsp), %zmm25              # 64-byte Reload
	vfmadd231ps	%zmm15, %zmm25, %zmm11  # zmm11 = (zmm25 * zmm15) + zmm11
	vextractf64x4	$1, %zmm11, %ymm12
	vaddps	%zmm12, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm12
	vaddps	%xmm12, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm12      # xmm12 = xmm11[1,0]
	vaddps	%xmm12, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm12          # xmm12 = xmm11[1,1,3,3]
	vaddss	%xmm12, %xmm11, %xmm11
	vaddss	%xmm27, %xmm11, %xmm11
	vshufpd	$1, %xmm0, %xmm0, %xmm12        # xmm12 = xmm0[1,0]
	vaddps	%xmm0, %xmm12, %xmm0
	vmovshdup	%xmm0, %xmm12           # xmm12 = xmm0[1,1,3,3]
	vaddss	%xmm0, %xmm12, %xmm0
	vmulps	%zmm24, %zmm1, %zmm12
	vmovaps	1728(%rsp), %zmm31              # 64-byte Reload
	vfmadd231ps	%zmm31, %zmm15, %zmm12  # zmm12 = (zmm15 * zmm31) + zmm12
	vextractf64x4	$1, %zmm12, %ymm13
	vaddps	%zmm13, %zmm12, %zmm12
	vextractf128	$1, %ymm12, %xmm13
	vaddps	%xmm13, %xmm12, %xmm12
	vshufpd	$1, %xmm12, %xmm12, %xmm13      # xmm13 = xmm12[1,0]
	vaddps	%xmm13, %xmm12, %xmm12
	vmovshdup	%xmm12, %xmm13          # xmm13 = xmm12[1,1,3,3]
	vaddss	%xmm13, %xmm12, %xmm12
	vmovaps	15616(%rsp), %zmm14             # 64-byte Reload
	vextractf64x4	$1, %zmm14, %ymm13
	vaddps	%zmm13, %zmm14, %zmm13
	vextractf128	$1, %ymm13, %xmm14
	vaddps	%xmm14, %xmm13, %xmm13
	vshufpd	$1, %xmm13, %xmm13, %xmm14      # xmm14 = xmm13[1,0]
	vaddps	%xmm14, %xmm13, %xmm13
	vmovshdup	%xmm13, %xmm14          # xmm14 = xmm13[1,1,3,3]
	vaddss	%xmm14, %xmm13, %xmm13
	vaddss	%xmm27, %xmm0, %xmm14
	vaddss	%xmm27, %xmm12, %xmm12
	vaddss	%xmm27, %xmm13, %xmm0
	vmovaps	%xmm0, 9600(%rsp)               # 16-byte Spill
	vmulps	%zmm2, %zmm1, %zmm21
	vmovaps	%zmm15, %zmm13
	vmovaps	1536(%rsp), %zmm29              # 64-byte Reload
	vfmadd231ps	%zmm29, %zmm15, %zmm21  # zmm21 = (zmm15 * zmm29) + zmm21
	vmovaps	%zmm21, 33472(%rsp)             # 64-byte Spill
	vmulps	576(%rsp), %zmm1, %zmm15        # 64-byte Folded Reload
	vmovaps	1920(%rsp), %zmm26              # 64-byte Reload
	vfmadd231ps	%zmm26, %zmm13, %zmm15  # zmm15 = (zmm13 * zmm26) + zmm15
	vmovaps	%zmm15, 19712(%rsp)             # 64-byte Spill
	vmovaps	%zmm13, %zmm15
	vmovaps	2240(%rsp), %zmm0               # 64-byte Reload
	vmulps	%zmm0, %zmm1, %zmm13
	vfmadd231ps	1088(%rsp), %zmm15, %zmm13 # 64-byte Folded Reload
                                        # zmm13 = (zmm15 * mem) + zmm13
	vextractf64x4	$1, %zmm13, %ymm15
	vaddps	%zmm15, %zmm13, %zmm13
	vextractf128	$1, %ymm13, %xmm15
	vaddps	%xmm15, %xmm13, %xmm13
	vshufpd	$1, %xmm13, %xmm13, %xmm15      # xmm15 = xmm13[1,0]
	vaddps	%xmm15, %xmm13, %xmm13
	vmovshdup	%xmm13, %xmm15          # xmm15 = xmm13[1,1,3,3]
	vaddss	%xmm15, %xmm13, %xmm13
	vaddss	%xmm27, %xmm13, %xmm13
	vinsertps	$16, %xmm13, %xmm14, %xmm13 # xmm13 = xmm14[0],xmm13[0],xmm14[2,3]
	vinsertps	$32, %xmm12, %xmm13, %xmm12 # xmm12 = xmm13[0,1],xmm12[0],xmm13[3]
	vinsertps	$48, %xmm11, %xmm12, %xmm1 # xmm1 = xmm12[0,1,2],xmm11[0]
	vmovaps	%xmm1, 4416(%rsp)               # 16-byte Spill
	vinsertf128	$1, %xmm4, %ymm3, %ymm1
	vmovaps	%zmm1, 33600(%rsp)              # 64-byte Spill
	vinsertf128	$1, %xmm7, %ymm6, %ymm1
	vmovaps	%zmm1, 4352(%rsp)               # 64-byte Spill
	vmovaps	%zmm16, %zmm1
	vmulps	1600(%rsp), %zmm16, %zmm3       # 64-byte Folded Reload
	vmovaps	%zmm17, %zmm15
	vfmadd231ps	%zmm22, %zmm17, %zmm3   # zmm3 = (zmm17 * zmm22) + zmm3
	vextractf64x4	$1, %zmm3, %ymm4
	vaddps	%zmm4, %zmm3, %zmm3
	vextractf128	$1, %ymm3, %xmm4
	vaddps	%xmm4, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm4         # xmm4 = xmm3[1,0]
	vaddps	%xmm4, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm4            # xmm4 = xmm3[1,1,3,3]
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm27, %xmm3, %xmm3
	vinsertps	$16, %xmm10, %xmm3, %xmm3 # xmm3 = xmm3[0],xmm10[0],xmm3[2,3]
	vinsertps	$32, %xmm9, %xmm3, %xmm3 # xmm3 = xmm3[0,1],xmm9[0],xmm3[3]
	vinsertps	$48, %xmm8, %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],xmm8[0]
	vmovaps	33536(%rsp), %zmm6              # 64-byte Reload
	vextractf64x4	$1, %zmm6, %ymm4
	vaddps	%zmm4, %zmm6, %zmm4
	vextractf128	$1, %ymm4, %xmm6
	vaddps	%xmm6, %xmm4, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm6         # xmm6 = xmm4[1,0]
	vaddps	%xmm6, %xmm4, %xmm4
	vmovshdup	%xmm4, %xmm6            # xmm6 = xmm4[1,1,3,3]
	vaddss	%xmm6, %xmm4, %xmm4
	vaddss	%xmm27, %xmm4, %xmm4
	vinsertps	$16, %xmm4, %xmm5, %xmm4 # xmm4 = xmm5[0],xmm4[0],xmm5[2,3]
	vinsertps	$32, 9664(%rsp), %xmm4, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm4[0,1],mem[0],xmm4[3]
	vmovaps	15360(%rsp), %zmm5              # 64-byte Reload
	vextractf64x4	$1, %zmm5, %ymm4
	vaddps	%zmm4, %zmm5, %zmm4
	vextractf128	$1, %ymm4, %xmm5
	vaddps	%xmm5, %xmm4, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm5         # xmm5 = xmm4[1,0]
	vaddps	%xmm5, %xmm4, %xmm4
	vmovshdup	%xmm4, %xmm5            # xmm5 = xmm4[1,1,3,3]
	vaddss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm27, %xmm4, %xmm4
	vinsertps	$48, %xmm4, %xmm2, %xmm4 # xmm4 = xmm2[0,1,2],xmm4[0]
	vmulps	448(%rsp), %zmm16, %zmm2        # 64-byte Folded Reload
	vfmadd231ps	512(%rsp), %zmm17, %zmm2 # 64-byte Folded Reload
                                        # zmm2 = (zmm17 * mem) + zmm2
	vextractf64x4	$1, %zmm2, %ymm5
	vaddps	%zmm5, %zmm2, %zmm2
	vextractf128	$1, %ymm2, %xmm5
	vaddps	%xmm5, %xmm2, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm5         # xmm5 = xmm2[1,0]
	vaddps	%xmm5, %xmm2, %xmm5
	vmovaps	30080(%rsp), %zmm6              # 64-byte Reload
	vextractf64x4	$1, %zmm6, %ymm2
	vaddps	%zmm2, %zmm6, %zmm2
	vextractf128	$1, %ymm2, %xmm6
	vaddps	%xmm6, %xmm2, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm6         # xmm6 = xmm2[1,0]
	vaddps	%xmm6, %xmm2, %xmm2
	vmovshdup	%xmm2, %xmm6            # xmm6 = xmm2[1,1,3,3]
	vaddss	%xmm6, %xmm2, %xmm2
	vaddss	%xmm27, %xmm2, %xmm2
	vmulps	1280(%rsp), %zmm16, %zmm6       # 64-byte Folded Reload
	vfmadd231ps	1216(%rsp), %zmm17, %zmm6 # 64-byte Folded Reload
                                        # zmm6 = (zmm17 * mem) + zmm6
	vextractf64x4	$1, %zmm6, %ymm7
	vaddps	%zmm7, %zmm6, %zmm6
	vextractf128	$1, %ymm6, %xmm7
	vaddps	%xmm7, %xmm6, %xmm6
	vshufpd	$1, %xmm6, %xmm6, %xmm7         # xmm7 = xmm6[1,0]
	vaddps	%xmm7, %xmm6, %xmm6
	vmovshdup	%xmm6, %xmm7            # xmm7 = xmm6[1,1,3,3]
	vaddss	%xmm7, %xmm6, %xmm6
	vaddss	%xmm27, %xmm6, %xmm7
	vmovshdup	%xmm5, %xmm6            # xmm6 = xmm5[1,1,3,3]
	vaddss	%xmm6, %xmm5, %xmm5
	vmovss	%xmm5, 15616(%rsp)              # 4-byte Spill
	vmulps	%zmm30, %zmm16, %zmm5
	vfmadd231ps	%zmm20, %zmm17, %zmm5   # zmm5 = (zmm17 * zmm20) + zmm5
	vextractf64x4	$1, %zmm5, %ymm6
	vaddps	%zmm6, %zmm5, %zmm5
	vextractf128	$1, %ymm5, %xmm6
	vaddps	%xmm6, %xmm5, %xmm5
	vshufpd	$1, %xmm5, %xmm5, %xmm6         # xmm6 = xmm5[1,0]
	vaddps	%xmm6, %xmm5, %xmm5
	vmovaps	12800(%rsp), %zmm8              # 64-byte Reload
	vextractf64x4	$1, %zmm8, %ymm6
	vaddps	%zmm6, %zmm8, %zmm6
	vextractf128	$1, %ymm6, %xmm8
	vaddps	%xmm6, %xmm8, %xmm6
	vshufpd	$1, %xmm6, %xmm6, %xmm8         # xmm8 = xmm6[1,0]
	vaddps	%xmm6, %xmm8, %xmm6
	vmovshdup	%xmm6, %xmm8            # xmm8 = xmm6[1,1,3,3]
	vaddss	%xmm6, %xmm8, %xmm6
	vaddss	%xmm27, %xmm6, %xmm6
	vmovshdup	%xmm5, %xmm8            # xmm8 = xmm5[1,1,3,3]
	vaddss	%xmm5, %xmm8, %xmm5
	vmovss	%xmm5, 15360(%rsp)              # 4-byte Spill
	vmulps	256(%rsp), %zmm16, %zmm5        # 64-byte Folded Reload
	vfmadd231ps	1408(%rsp), %zmm17, %zmm5 # 64-byte Folded Reload
                                        # zmm5 = (zmm17 * mem) + zmm5
	vextractf64x4	$1, %zmm5, %ymm8
	vaddps	%zmm8, %zmm5, %zmm5
	vextractf128	$1, %ymm5, %xmm8
	vaddps	%xmm5, %xmm8, %xmm5
	vmovaps	14080(%rsp), %zmm9              # 64-byte Reload
	vextractf64x4	$1, %zmm9, %ymm8
	vaddps	%zmm8, %zmm9, %zmm8
	vextractf128	$1, %ymm8, %xmm9
	vaddps	%xmm9, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm9         # xmm9 = xmm8[1,0]
	vaddps	%xmm9, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm9            # xmm9 = xmm8[1,1,3,3]
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm27, %xmm8, %xmm8
	vshufpd	$1, %xmm5, %xmm5, %xmm9         # xmm9 = xmm5[1,0]
	vaddps	%xmm5, %xmm9, %xmm5
	vmovshdup	%xmm5, %xmm9            # xmm9 = xmm5[1,1,3,3]
	vaddss	%xmm5, %xmm9, %xmm5
	vmovss	%xmm5, 14080(%rsp)              # 4-byte Spill
	vmovaps	128(%rsp), %zmm30               # 64-byte Reload
	vmulps	%zmm30, %zmm16, %zmm5
	vfmadd231ps	896(%rsp), %zmm17, %zmm5 # 64-byte Folded Reload
                                        # zmm5 = (zmm17 * mem) + zmm5
	vextractf64x4	$1, %zmm5, %ymm9
	vaddps	%zmm9, %zmm5, %zmm5
	vextractf128	$1, %ymm5, %xmm9
	vaddps	%xmm5, %xmm9, %xmm9
	vmovaps	14592(%rsp), %zmm10             # 64-byte Reload
	vextractf64x4	$1, %zmm10, %ymm5
	vaddps	%zmm5, %zmm10, %zmm5
	vextractf128	$1, %ymm5, %xmm10
	vaddps	%xmm5, %xmm10, %xmm5
	vshufpd	$1, %xmm5, %xmm5, %xmm10        # xmm10 = xmm5[1,0]
	vaddps	%xmm5, %xmm10, %xmm5
	vmovshdup	%xmm5, %xmm10           # xmm10 = xmm5[1,1,3,3]
	vaddss	%xmm5, %xmm10, %xmm5
	vaddss	%xmm27, %xmm5, %xmm10
	vmovaps	4608(%rsp), %zmm16              # 64-byte Reload
	vmulps	1152(%rsp), %zmm16, %zmm5       # 64-byte Folded Reload
	vmovaps	4672(%rsp), %zmm17              # 64-byte Reload
	vmovaps	768(%rsp), %zmm21               # 64-byte Reload
	vfmadd231ps	%zmm21, %zmm17, %zmm5   # zmm5 = (zmm17 * zmm21) + zmm5
	vextractf64x4	$1, %zmm5, %ymm11
	vaddps	%zmm11, %zmm5, %zmm5
	vextractf128	$1, %ymm5, %xmm11
	vaddps	%xmm5, %xmm11, %xmm5
	vshufpd	$1, %xmm5, %xmm5, %xmm11        # xmm11 = xmm5[1,0]
	vaddps	%xmm5, %xmm11, %xmm5
	vmovshdup	%xmm5, %xmm11           # xmm11 = xmm5[1,1,3,3]
	vaddss	%xmm5, %xmm11, %xmm5
	vaddss	%xmm27, %xmm5, %xmm5
	vshufpd	$1, %xmm9, %xmm9, %xmm11        # xmm11 = xmm9[1,0]
	vaddps	%xmm11, %xmm9, %xmm9
	vmovshdup	%xmm9, %xmm11           # xmm11 = xmm9[1,1,3,3]
	vaddss	%xmm11, %xmm9, %xmm9
	vmulps	%zmm28, %zmm1, %zmm11
	vfmadd231ps	%zmm19, %zmm15, %zmm11  # zmm11 = (zmm15 * zmm19) + zmm11
	vextractf64x4	$1, %zmm11, %ymm12
	vaddps	%zmm12, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm12
	vaddps	%xmm12, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm12      # xmm12 = xmm11[1,0]
	vaddps	%xmm12, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm12          # xmm12 = xmm11[1,1,3,3]
	vaddss	%xmm12, %xmm11, %xmm11
	vaddss	%xmm27, %xmm9, %xmm9
	vaddss	%xmm27, %xmm11, %xmm11
	vinsertps	$16, %xmm11, %xmm9, %xmm9 # xmm9 = xmm9[0],xmm11[0],xmm9[2,3]
	vmovaps	1984(%rsp), %zmm19              # 64-byte Reload
	vmulps	%zmm19, %zmm1, %zmm11
	vfmadd231ps	1024(%rsp), %zmm15, %zmm11 # 64-byte Folded Reload
                                        # zmm11 = (zmm15 * mem) + zmm11
	vextractf64x4	$1, %zmm11, %ymm12
	vaddps	%zmm12, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm12
	vaddps	%xmm12, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm12      # xmm12 = xmm11[1,0]
	vaddps	%xmm12, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm12          # xmm12 = xmm11[1,1,3,3]
	vaddss	%xmm12, %xmm11, %xmm11
	vaddss	%xmm27, %xmm11, %xmm11
	vinsertps	$32, %xmm11, %xmm9, %xmm9 # xmm9 = xmm9[0,1],xmm11[0],xmm9[3]
	vmulps	%zmm18, %zmm1, %zmm11
	vfmadd231ps	960(%rsp), %zmm15, %zmm11 # 64-byte Folded Reload
                                        # zmm11 = (zmm15 * mem) + zmm11
	vextractf64x4	$1, %zmm11, %ymm12
	vaddps	%zmm12, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm12
	vaddps	%xmm12, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm12      # xmm12 = xmm11[1,0]
	vaddps	%xmm12, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm12          # xmm12 = xmm11[1,1,3,3]
	vaddss	%xmm12, %xmm11, %xmm11
	vaddss	%xmm27, %xmm11, %xmm11
	vinsertps	$48, %xmm11, %xmm9, %xmm9 # xmm9 = xmm9[0,1,2],xmm11[0]
	vmovaps	%xmm9, 33536(%rsp)              # 16-byte Spill
	vmovaps	640(%rsp), %zmm24               # 64-byte Reload
	vmulps	%zmm24, %zmm1, %zmm9
	vmovaps	64(%rsp), %zmm28                # 64-byte Reload
	vfmadd231ps	%zmm28, %zmm15, %zmm9   # zmm9 = (zmm15 * zmm28) + zmm9
	vextractf64x4	$1, %zmm9, %ymm11
	vaddps	%zmm11, %zmm9, %zmm9
	vextractf128	$1, %ymm9, %xmm11
	vaddps	%xmm11, %xmm9, %xmm9
	vshufpd	$1, %xmm9, %xmm9, %xmm11        # xmm11 = xmm9[1,0]
	vaddps	%xmm11, %xmm9, %xmm9
	vmovshdup	%xmm9, %xmm11           # xmm11 = xmm9[1,1,3,3]
	vaddss	%xmm11, %xmm9, %xmm9
	vaddss	%xmm27, %xmm9, %xmm9
	vinsertps	$16, %xmm10, %xmm9, %xmm9 # xmm9 = xmm9[0],xmm10[0],xmm9[2,3]
	vinsertps	$32, %xmm8, %xmm9, %xmm8 # xmm8 = xmm9[0,1],xmm8[0],xmm9[3]
	vinsertps	$48, %xmm6, %xmm8, %xmm6 # xmm6 = xmm8[0,1,2],xmm6[0]
	vmovaps	30848(%rsp), %zmm9              # 64-byte Reload
	vextractf64x4	$1, %zmm9, %ymm8
	vaddps	%zmm8, %zmm9, %zmm8
	vextractf128	$1, %ymm8, %xmm9
	vaddps	%xmm9, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm9         # xmm9 = xmm8[1,0]
	vaddps	%xmm9, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm9            # xmm9 = xmm8[1,1,3,3]
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm27, %xmm8, %xmm8
	vinsertps	$16, %xmm8, %xmm7, %xmm7 # xmm7 = xmm7[0],xmm8[0],xmm7[2,3]
	vinsertps	$32, %xmm2, %xmm7, %xmm2 # xmm2 = xmm7[0,1],xmm2[0],xmm7[3]
	vmovaps	29760(%rsp), %zmm8              # 64-byte Reload
	vextractf64x4	$1, %zmm8, %ymm7
	vaddps	%zmm7, %zmm8, %zmm7
	vextractf128	$1, %ymm7, %xmm8
	vaddps	%xmm7, %xmm8, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm8         # xmm8 = xmm7[1,0]
	vaddps	%xmm7, %xmm8, %xmm7
	vmovshdup	%xmm7, %xmm8            # xmm8 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm27, %xmm7, %xmm7
	vinsertps	$48, %xmm7, %xmm2, %xmm7 # xmm7 = xmm2[0,1,2],xmm7[0]
	vmulps	3072(%rsp), %zmm1, %zmm2        # 64-byte Folded Reload
	vfmadd231ps	1472(%rsp), %zmm15, %zmm2 # 64-byte Folded Reload
                                        # zmm2 = (zmm15 * mem) + zmm2
	vextractf64x4	$1, %zmm2, %ymm8
	vaddps	%zmm8, %zmm2, %zmm2
	vextractf128	$1, %ymm2, %xmm8
	vaddps	%xmm2, %xmm8, %xmm2
	vmovaps	33984(%rsp), %zmm9              # 64-byte Reload
	vextractf64x4	$1, %zmm9, %ymm8
	vaddps	%zmm8, %zmm9, %zmm8
	vextractf128	$1, %ymm8, %xmm9
	vaddps	%xmm9, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm9         # xmm9 = xmm8[1,0]
	vaddps	%xmm9, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm9            # xmm9 = xmm8[1,1,3,3]
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm27, %xmm8, %xmm8
	vshufpd	$1, %xmm2, %xmm2, %xmm9         # xmm9 = xmm2[1,0]
	vaddps	%xmm2, %xmm9, %xmm2
	vmovshdup	%xmm2, %xmm9            # xmm9 = xmm2[1,1,3,3]
	vaddss	%xmm2, %xmm9, %xmm2
	vmovss	%xmm2, 14592(%rsp)              # 4-byte Spill
	vmulps	2112(%rsp), %zmm1, %zmm2        # 64-byte Folded Reload
	vfmadd231ps	2496(%rsp), %zmm15, %zmm2 # 64-byte Folded Reload
                                        # zmm2 = (zmm15 * mem) + zmm2
	vextractf64x4	$1, %zmm2, %ymm9
	vaddps	%zmm9, %zmm2, %zmm2
	vmovaps	34112(%rsp), %zmm10             # 64-byte Reload
	vextractf64x4	$1, %zmm10, %ymm9
	vaddps	%zmm9, %zmm10, %zmm9
	vextractf128	$1, %ymm9, %xmm10
	vaddps	%xmm10, %xmm9, %xmm9
	vshufpd	$1, %xmm9, %xmm9, %xmm10        # xmm10 = xmm9[1,0]
	vaddps	%xmm10, %xmm9, %xmm9
	vmovshdup	%xmm9, %xmm10           # xmm10 = xmm9[1,1,3,3]
	vaddss	%xmm10, %xmm9, %xmm9
	vaddss	%xmm27, %xmm9, %xmm9
	vextractf128	$1, %ymm2, %xmm10
	vaddps	%xmm2, %xmm10, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm10        # xmm10 = xmm2[1,0]
	vaddps	%xmm2, %xmm10, %xmm2
	vmovshdup	%xmm2, %xmm10           # xmm10 = xmm2[1,1,3,3]
	vaddss	%xmm2, %xmm10, %xmm2
	vmovss	%xmm2, 12800(%rsp)              # 4-byte Spill
	vmulps	3136(%rsp), %zmm1, %zmm2        # 64-byte Folded Reload
	vfmadd231ps	1856(%rsp), %zmm15, %zmm2 # 64-byte Folded Reload
                                        # zmm2 = (zmm15 * mem) + zmm2
	vextractf64x4	$1, %zmm2, %ymm10
	vaddps	%zmm10, %zmm2, %zmm2
	vextractf128	$1, %ymm2, %xmm10
	vaddps	%xmm2, %xmm10, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm10        # xmm10 = xmm2[1,0]
	vaddps	%xmm2, %xmm10, %xmm2
	vmovshdup	%xmm2, %xmm10           # xmm10 = xmm2[1,1,3,3]
	vaddss	%xmm2, %xmm10, %xmm2
	vmovss	%xmm2, 9664(%rsp)               # 4-byte Spill
	vmovaps	19776(%rsp), %zmm10             # 64-byte Reload
	vextractf64x4	$1, %zmm10, %ymm2
	vaddps	%zmm2, %zmm10, %zmm2
	vextractf128	$1, %ymm2, %xmm10
	vaddps	%xmm2, %xmm10, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm10        # xmm10 = xmm2[1,0]
	vaddps	%xmm2, %xmm10, %xmm2
	vmovshdup	%xmm2, %xmm10           # xmm10 = xmm2[1,1,3,3]
	vaddss	%xmm2, %xmm10, %xmm2
	vaddss	%xmm27, %xmm2, %xmm10
	vmulps	192(%rsp), %zmm1, %zmm2         # 64-byte Folded Reload
	vfmadd231ps	%zmm23, %zmm15, %zmm2   # zmm2 = (zmm15 * zmm23) + zmm2
	vextractf64x4	$1, %zmm2, %ymm11
	vaddps	%zmm11, %zmm2, %zmm2
	vextractf128	$1, %ymm2, %xmm11
	vaddps	%xmm2, %xmm11, %xmm2
	vmulps	2432(%rsp), %zmm1, %zmm11       # 64-byte Folded Reload
	vfmadd231ps	%zmm15, %zmm25, %zmm11  # zmm11 = (zmm25 * zmm15) + zmm11
	vextractf64x4	$1, %zmm11, %ymm12
	vaddps	%zmm12, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm12
	vaddps	%xmm12, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm12      # xmm12 = xmm11[1,0]
	vaddps	%xmm12, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm12          # xmm12 = xmm11[1,1,3,3]
	vaddss	%xmm12, %xmm11, %xmm11
	vaddss	%xmm27, %xmm11, %xmm11
	vshufpd	$1, %xmm2, %xmm2, %xmm12        # xmm12 = xmm2[1,0]
	vaddps	%xmm2, %xmm12, %xmm2
	vmovshdup	%xmm2, %xmm12           # xmm12 = xmm2[1,1,3,3]
	vaddss	%xmm2, %xmm12, %xmm2
	vmovaps	2176(%rsp), %zmm23              # 64-byte Reload
	vmulps	%zmm23, %zmm1, %zmm12
	vfmadd231ps	%zmm31, %zmm15, %zmm12  # zmm12 = (zmm15 * zmm31) + zmm12
	vextractf64x4	$1, %zmm12, %ymm13
	vaddps	%zmm13, %zmm12, %zmm12
	vextractf128	$1, %ymm12, %xmm13
	vaddps	%xmm13, %xmm12, %xmm12
	vshufpd	$1, %xmm12, %xmm12, %xmm13      # xmm13 = xmm12[1,0]
	vaddps	%xmm13, %xmm12, %xmm12
	vmovshdup	%xmm12, %xmm13          # xmm13 = xmm12[1,1,3,3]
	vaddss	%xmm13, %xmm12, %xmm12
	vmovaps	16000(%rsp), %zmm14             # 64-byte Reload
	vextractf64x4	$1, %zmm14, %ymm13
	vaddps	%zmm13, %zmm14, %zmm13
	vextractf128	$1, %ymm13, %xmm14
	vaddps	%xmm14, %xmm13, %xmm13
	vshufpd	$1, %xmm13, %xmm13, %xmm14      # xmm14 = xmm13[1,0]
	vaddps	%xmm14, %xmm13, %xmm13
	vmovshdup	%xmm13, %xmm14          # xmm14 = xmm13[1,1,3,3]
	vaddss	%xmm14, %xmm13, %xmm13
	vaddss	%xmm27, %xmm2, %xmm14
	vaddss	%xmm27, %xmm12, %xmm12
	vaddss	%xmm27, %xmm13, %xmm2
	vmovaps	%xmm2, 9216(%rsp)               # 16-byte Spill
	vmulps	2304(%rsp), %zmm1, %zmm18       # 64-byte Folded Reload
	vmovaps	%zmm15, %zmm13
	vfmadd231ps	%zmm29, %zmm15, %zmm18  # zmm18 = (zmm15 * zmm29) + zmm18
	vmovaps	%zmm18, 33984(%rsp)             # 64-byte Spill
	vmovaps	576(%rsp), %zmm18               # 64-byte Reload
	vmulps	%zmm18, %zmm1, %zmm15
	vfmadd231ps	%zmm26, %zmm13, %zmm15  # zmm15 = (zmm13 * zmm26) + zmm15
	vmovaps	%zmm15, 19776(%rsp)             # 64-byte Spill
	vmovaps	%zmm13, %zmm15
	vmulps	%zmm0, %zmm1, %zmm13
	vmovaps	1088(%rsp), %zmm2               # 64-byte Reload
	vfmadd231ps	%zmm2, %zmm15, %zmm13   # zmm13 = (zmm15 * zmm2) + zmm13
	vextractf64x4	$1, %zmm13, %ymm15
	vaddps	%zmm15, %zmm13, %zmm13
	vextractf128	$1, %ymm13, %xmm15
	vaddps	%xmm15, %xmm13, %xmm13
	vshufpd	$1, %xmm13, %xmm13, %xmm15      # xmm15 = xmm13[1,0]
	vaddps	%xmm15, %xmm13, %xmm13
	vmovshdup	%xmm13, %xmm15          # xmm15 = xmm13[1,1,3,3]
	vaddss	%xmm15, %xmm13, %xmm13
	vaddss	%xmm27, %xmm13, %xmm13
	vinsertps	$16, %xmm13, %xmm14, %xmm13 # xmm13 = xmm14[0],xmm13[0],xmm14[2,3]
	vinsertps	$32, %xmm12, %xmm13, %xmm12 # xmm12 = xmm13[0,1],xmm12[0],xmm13[3]
	vinsertps	$48, %xmm11, %xmm12, %xmm1 # xmm1 = xmm12[0,1,2],xmm11[0]
	vmovaps	%xmm1, 4544(%rsp)               # 16-byte Spill
	vinsertf128	$1, %xmm4, %ymm3, %ymm1
	vmovaps	%zmm1, 34112(%rsp)              # 64-byte Spill
	vinsertf128	$1, %xmm7, %ymm6, %ymm1
	vmovaps	%zmm1, 4480(%rsp)               # 64-byte Spill
	vmovaps	%zmm16, %zmm1
	vmovaps	1600(%rsp), %zmm22              # 64-byte Reload
	vmulps	%zmm22, %zmm16, %zmm3
	vmovaps	%zmm17, %zmm15
	vmovaps	1664(%rsp), %zmm20              # 64-byte Reload
	vfmadd231ps	%zmm20, %zmm17, %zmm3   # zmm3 = (zmm17 * zmm20) + zmm3
	vextractf64x4	$1, %zmm3, %ymm4
	vaddps	%zmm4, %zmm3, %zmm3
	vextractf128	$1, %ymm3, %xmm4
	vaddps	%xmm4, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm4         # xmm4 = xmm3[1,0]
	vaddps	%xmm4, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm4            # xmm4 = xmm3[1,1,3,3]
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm27, %xmm3, %xmm3
	vinsertps	$16, %xmm10, %xmm3, %xmm3 # xmm3 = xmm3[0],xmm10[0],xmm3[2,3]
	vinsertps	$32, %xmm9, %xmm3, %xmm3 # xmm3 = xmm3[0,1],xmm9[0],xmm3[3]
	vinsertps	$48, %xmm8, %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],xmm8[0]
	vmovaps	34048(%rsp), %zmm6              # 64-byte Reload
	vextractf64x4	$1, %zmm6, %ymm4
	vaddps	%zmm4, %zmm6, %zmm4
	vextractf128	$1, %ymm4, %xmm6
	vaddps	%xmm6, %xmm4, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm6         # xmm6 = xmm4[1,0]
	vaddps	%xmm6, %xmm4, %xmm4
	vmovshdup	%xmm4, %xmm6            # xmm6 = xmm4[1,1,3,3]
	vaddss	%xmm6, %xmm4, %xmm4
	vaddss	%xmm27, %xmm4, %xmm4
	vinsertps	$16, %xmm4, %xmm5, %xmm4 # xmm4 = xmm5[0],xmm4[0],xmm5[2,3]
	vinsertps	$32, 9600(%rsp), %xmm4, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm4[0,1],mem[0],xmm4[3]
	vmovaps	15936(%rsp), %zmm5              # 64-byte Reload
	vextractf64x4	$1, %zmm5, %ymm4
	vaddps	%zmm4, %zmm5, %zmm4
	vextractf128	$1, %ymm4, %xmm5
	vaddps	%xmm5, %xmm4, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm5         # xmm5 = xmm4[1,0]
	vaddps	%xmm5, %xmm4, %xmm4
	vmovshdup	%xmm4, %xmm5            # xmm5 = xmm4[1,1,3,3]
	vaddss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm27, %xmm4, %xmm4
	vinsertps	$48, %xmm4, %xmm0, %xmm4 # xmm4 = xmm0[0,1,2],xmm4[0]
	vmovaps	448(%rsp), %zmm31               # 64-byte Reload
	vmulps	%zmm31, %zmm16, %zmm0
	vfmadd231ps	512(%rsp), %zmm17, %zmm0 # 64-byte Folded Reload
                                        # zmm0 = (zmm17 * mem) + zmm0
	vextractf64x4	$1, %zmm0, %ymm5
	vaddps	%zmm5, %zmm0, %zmm0
	vextractf128	$1, %ymm0, %xmm5
	vaddps	%xmm5, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm5         # xmm5 = xmm0[1,0]
	vaddps	%xmm5, %xmm0, %xmm5
	vmovaps	30336(%rsp), %zmm6              # 64-byte Reload
	vextractf64x4	$1, %zmm6, %ymm0
	vaddps	%zmm0, %zmm6, %zmm0
	vextractf128	$1, %ymm0, %xmm6
	vaddps	%xmm6, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm6         # xmm6 = xmm0[1,0]
	vaddps	%xmm6, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm6            # xmm6 = xmm0[1,1,3,3]
	vaddss	%xmm6, %xmm0, %xmm0
	vaddss	%xmm27, %xmm0, %xmm0
	vmovaps	1280(%rsp), %zmm26              # 64-byte Reload
	vmulps	%zmm26, %zmm16, %zmm6
	vmovaps	1216(%rsp), %zmm29              # 64-byte Reload
	vfmadd231ps	%zmm29, %zmm17, %zmm6   # zmm6 = (zmm17 * zmm29) + zmm6
	vextractf64x4	$1, %zmm6, %ymm7
	vaddps	%zmm7, %zmm6, %zmm6
	vextractf128	$1, %ymm6, %xmm7
	vaddps	%xmm7, %xmm6, %xmm6
	vshufpd	$1, %xmm6, %xmm6, %xmm7         # xmm7 = xmm6[1,0]
	vaddps	%xmm7, %xmm6, %xmm6
	vmovshdup	%xmm6, %xmm7            # xmm7 = xmm6[1,1,3,3]
	vaddss	%xmm7, %xmm6, %xmm6
	vaddss	%xmm27, %xmm6, %xmm7
	vmovshdup	%xmm5, %xmm6            # xmm6 = xmm5[1,1,3,3]
	vaddss	%xmm6, %xmm5, %xmm5
	vmovss	%xmm5, 16000(%rsp)              # 4-byte Spill
	vmulps	2048(%rsp), %zmm16, %zmm5       # 64-byte Folded Reload
	vfmadd231ps	704(%rsp), %zmm17, %zmm5 # 64-byte Folded Reload
                                        # zmm5 = (zmm17 * mem) + zmm5
	vextractf64x4	$1, %zmm5, %ymm6
	vaddps	%zmm6, %zmm5, %zmm5
	vextractf128	$1, %ymm5, %xmm6
	vaddps	%xmm6, %xmm5, %xmm5
	vshufpd	$1, %xmm5, %xmm5, %xmm6         # xmm6 = xmm5[1,0]
	vaddps	%xmm6, %xmm5, %xmm5
	vmovaps	13248(%rsp), %zmm8              # 64-byte Reload
	vextractf64x4	$1, %zmm8, %ymm6
	vaddps	%zmm6, %zmm8, %zmm6
	vextractf128	$1, %ymm6, %xmm8
	vaddps	%xmm6, %xmm8, %xmm6
	vshufpd	$1, %xmm6, %xmm6, %xmm8         # xmm8 = xmm6[1,0]
	vaddps	%xmm6, %xmm8, %xmm6
	vmovshdup	%xmm6, %xmm8            # xmm8 = xmm6[1,1,3,3]
	vaddss	%xmm6, %xmm8, %xmm6
	vaddss	%xmm27, %xmm6, %xmm6
	vmovshdup	%xmm5, %xmm8            # xmm8 = xmm5[1,1,3,3]
	vaddss	%xmm5, %xmm8, %xmm5
	vmovss	%xmm5, 15936(%rsp)              # 4-byte Spill
	vmovaps	256(%rsp), %zmm25               # 64-byte Reload
	vmulps	%zmm25, %zmm16, %zmm5
	vfmadd231ps	1408(%rsp), %zmm17, %zmm5 # 64-byte Folded Reload
                                        # zmm5 = (zmm17 * mem) + zmm5
	vextractf64x4	$1, %zmm5, %ymm8
	vaddps	%zmm8, %zmm5, %zmm5
	vextractf128	$1, %ymm5, %xmm8
	vaddps	%xmm5, %xmm8, %xmm5
	vmovaps	14464(%rsp), %zmm9              # 64-byte Reload
	vextractf64x4	$1, %zmm9, %ymm8
	vaddps	%zmm8, %zmm9, %zmm8
	vextractf128	$1, %ymm8, %xmm9
	vaddps	%xmm9, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm9         # xmm9 = xmm8[1,0]
	vaddps	%xmm9, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm9            # xmm9 = xmm8[1,1,3,3]
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm27, %xmm8, %xmm8
	vshufpd	$1, %xmm5, %xmm5, %xmm9         # xmm9 = xmm5[1,0]
	vaddps	%xmm5, %xmm9, %xmm5
	vmovshdup	%xmm5, %xmm9            # xmm9 = xmm5[1,1,3,3]
	vaddss	%xmm5, %xmm9, %xmm5
	vmovss	%xmm5, 14464(%rsp)              # 4-byte Spill
	vmulps	%zmm30, %zmm16, %zmm5
	vmovaps	896(%rsp), %zmm30               # 64-byte Reload
	vfmadd231ps	%zmm30, %zmm17, %zmm5   # zmm5 = (zmm17 * zmm30) + zmm5
	vextractf64x4	$1, %zmm5, %ymm9
	vaddps	%zmm9, %zmm5, %zmm5
	vextractf128	$1, %ymm5, %xmm9
	vaddps	%xmm5, %xmm9, %xmm9
	vmovaps	15232(%rsp), %zmm10             # 64-byte Reload
	vextractf64x4	$1, %zmm10, %ymm5
	vaddps	%zmm5, %zmm10, %zmm5
	vextractf128	$1, %ymm5, %xmm10
	vaddps	%xmm5, %xmm10, %xmm5
	vshufpd	$1, %xmm5, %xmm5, %xmm10        # xmm10 = xmm5[1,0]
	vaddps	%xmm5, %xmm10, %xmm5
	vmovshdup	%xmm5, %xmm10           # xmm10 = xmm5[1,1,3,3]
	vaddss	%xmm5, %xmm10, %xmm5
	vaddss	%xmm27, %xmm5, %xmm10
	vmovaps	3200(%rsp), %zmm16              # 64-byte Reload
	vmulps	1152(%rsp), %zmm16, %zmm5       # 64-byte Folded Reload
	vmovaps	3264(%rsp), %zmm17              # 64-byte Reload
	vfmadd231ps	%zmm21, %zmm17, %zmm5   # zmm5 = (zmm17 * zmm21) + zmm5
	vextractf64x4	$1, %zmm5, %ymm11
	vaddps	%zmm11, %zmm5, %zmm5
	vextractf128	$1, %ymm5, %xmm11
	vaddps	%xmm5, %xmm11, %xmm5
	vshufpd	$1, %xmm5, %xmm5, %xmm11        # xmm11 = xmm5[1,0]
	vaddps	%xmm5, %xmm11, %xmm5
	vmovshdup	%xmm5, %xmm11           # xmm11 = xmm5[1,1,3,3]
	vaddss	%xmm5, %xmm11, %xmm5
	vaddss	%xmm27, %xmm5, %xmm5
	vshufpd	$1, %xmm9, %xmm9, %xmm11        # xmm11 = xmm9[1,0]
	vaddps	%xmm11, %xmm9, %xmm9
	vmovshdup	%xmm9, %xmm11           # xmm11 = xmm9[1,1,3,3]
	vaddss	%xmm11, %xmm9, %xmm9
	vmulps	2368(%rsp), %zmm1, %zmm11       # 64-byte Folded Reload
	vfmadd231ps	2560(%rsp), %zmm15, %zmm11 # 64-byte Folded Reload
                                        # zmm11 = (zmm15 * mem) + zmm11
	vextractf64x4	$1, %zmm11, %ymm12
	vaddps	%zmm12, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm12
	vaddps	%xmm12, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm12      # xmm12 = xmm11[1,0]
	vaddps	%xmm12, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm12          # xmm12 = xmm11[1,1,3,3]
	vaddss	%xmm12, %xmm11, %xmm11
	vaddss	%xmm27, %xmm9, %xmm9
	vaddss	%xmm27, %xmm11, %xmm11
	vinsertps	$16, %xmm11, %xmm9, %xmm9 # xmm9 = xmm9[0],xmm11[0],xmm9[2,3]
	vmulps	%zmm19, %zmm1, %zmm11
	vfmadd231ps	1024(%rsp), %zmm15, %zmm11 # 64-byte Folded Reload
                                        # zmm11 = (zmm15 * mem) + zmm11
	vextractf64x4	$1, %zmm11, %ymm12
	vaddps	%zmm12, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm12
	vaddps	%xmm12, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm12      # xmm12 = xmm11[1,0]
	vaddps	%xmm12, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm12          # xmm12 = xmm11[1,1,3,3]
	vaddss	%xmm12, %xmm11, %xmm11
	vaddss	%xmm27, %xmm11, %xmm11
	vinsertps	$32, %xmm11, %xmm9, %xmm9 # xmm9 = xmm9[0,1],xmm11[0],xmm9[3]
	vmulps	832(%rsp), %zmm1, %zmm11        # 64-byte Folded Reload
	vfmadd231ps	960(%rsp), %zmm15, %zmm11 # 64-byte Folded Reload
                                        # zmm11 = (zmm15 * mem) + zmm11
	vextractf64x4	$1, %zmm11, %ymm12
	vaddps	%zmm12, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm12
	vaddps	%xmm12, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm12      # xmm12 = xmm11[1,0]
	vaddps	%xmm12, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm12          # xmm12 = xmm11[1,1,3,3]
	vaddss	%xmm12, %xmm11, %xmm11
	vaddss	%xmm27, %xmm11, %xmm11
	vinsertps	$48, %xmm11, %xmm9, %xmm9 # xmm9 = xmm9[0,1,2],xmm11[0]
	vmovaps	%xmm9, 34048(%rsp)              # 16-byte Spill
	vmulps	%zmm24, %zmm1, %zmm9
	vfmadd231ps	%zmm28, %zmm15, %zmm9   # zmm9 = (zmm15 * zmm28) + zmm9
	vextractf64x4	$1, %zmm9, %ymm11
	vaddps	%zmm11, %zmm9, %zmm9
	vextractf128	$1, %ymm9, %xmm11
	vaddps	%xmm11, %xmm9, %xmm9
	vshufpd	$1, %xmm9, %xmm9, %xmm11        # xmm11 = xmm9[1,0]
	vaddps	%xmm11, %xmm9, %xmm9
	vmovshdup	%xmm9, %xmm11           # xmm11 = xmm9[1,1,3,3]
	vaddss	%xmm11, %xmm9, %xmm9
	vaddss	%xmm27, %xmm9, %xmm9
	vinsertps	$16, %xmm10, %xmm9, %xmm9 # xmm9 = xmm9[0],xmm10[0],xmm9[2,3]
	vinsertps	$32, %xmm8, %xmm9, %xmm8 # xmm8 = xmm9[0,1],xmm8[0],xmm9[3]
	vinsertps	$48, %xmm6, %xmm8, %xmm6 # xmm6 = xmm8[0,1,2],xmm6[0]
	vmovaps	12352(%rsp), %zmm9              # 64-byte Reload
	vextractf64x4	$1, %zmm9, %ymm8
	vaddps	%zmm8, %zmm9, %zmm8
	vextractf128	$1, %ymm8, %xmm9
	vaddps	%xmm9, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm9         # xmm9 = xmm8[1,0]
	vaddps	%xmm9, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm9            # xmm9 = xmm8[1,1,3,3]
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm27, %xmm8, %xmm8
	vinsertps	$16, %xmm8, %xmm7, %xmm7 # xmm7 = xmm7[0],xmm8[0],xmm7[2,3]
	vinsertps	$32, %xmm0, %xmm7, %xmm0 # xmm0 = xmm7[0,1],xmm0[0],xmm7[3]
	vmovaps	12032(%rsp), %zmm8              # 64-byte Reload
	vextractf64x4	$1, %zmm8, %ymm7
	vaddps	%zmm7, %zmm8, %zmm7
	vextractf128	$1, %ymm7, %xmm8
	vaddps	%xmm7, %xmm8, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm8         # xmm8 = xmm7[1,0]
	vaddps	%xmm7, %xmm8, %xmm7
	vmovshdup	%xmm7, %xmm8            # xmm8 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm27, %xmm7, %xmm7
	vinsertps	$48, %xmm7, %xmm0, %xmm7 # xmm7 = xmm0[0,1,2],xmm7[0]
	vmulps	3072(%rsp), %zmm1, %zmm0        # 64-byte Folded Reload
	vfmadd231ps	1472(%rsp), %zmm15, %zmm0 # 64-byte Folded Reload
                                        # zmm0 = (zmm15 * mem) + zmm0
	vextractf64x4	$1, %zmm0, %ymm8
	vaddps	%zmm8, %zmm0, %zmm0
	vextractf128	$1, %ymm0, %xmm8
	vaddps	%xmm0, %xmm8, %xmm0
	vmovaps	34624(%rsp), %zmm9              # 64-byte Reload
	vextractf64x4	$1, %zmm9, %ymm8
	vaddps	%zmm8, %zmm9, %zmm8
	vextractf128	$1, %ymm8, %xmm9
	vaddps	%xmm9, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm9         # xmm9 = xmm8[1,0]
	vaddps	%xmm9, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm9            # xmm9 = xmm8[1,1,3,3]
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm27, %xmm8, %xmm8
	vshufpd	$1, %xmm0, %xmm0, %xmm9         # xmm9 = xmm0[1,0]
	vaddps	%xmm0, %xmm9, %xmm0
	vmovshdup	%xmm0, %xmm9            # xmm9 = xmm0[1,1,3,3]
	vaddss	%xmm0, %xmm9, %xmm0
	vmovss	%xmm0, 15232(%rsp)              # 4-byte Spill
	vmovaps	2112(%rsp), %zmm21              # 64-byte Reload
	vmulps	%zmm21, %zmm1, %zmm0
	vfmadd231ps	2496(%rsp), %zmm15, %zmm0 # 64-byte Folded Reload
                                        # zmm0 = (zmm15 * mem) + zmm0
	vextractf64x4	$1, %zmm0, %ymm9
	vaddps	%zmm9, %zmm0, %zmm0
	vmovaps	34944(%rsp), %zmm10             # 64-byte Reload
	vextractf64x4	$1, %zmm10, %ymm9
	vaddps	%zmm9, %zmm10, %zmm9
	vextractf128	$1, %ymm9, %xmm10
	vaddps	%xmm10, %xmm9, %xmm9
	vshufpd	$1, %xmm9, %xmm9, %xmm10        # xmm10 = xmm9[1,0]
	vaddps	%xmm10, %xmm9, %xmm9
	vmovshdup	%xmm9, %xmm10           # xmm10 = xmm9[1,1,3,3]
	vaddss	%xmm10, %xmm9, %xmm9
	vaddss	%xmm27, %xmm9, %xmm9
	vextractf128	$1, %ymm0, %xmm10
	vaddps	%xmm0, %xmm10, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm10        # xmm10 = xmm0[1,0]
	vaddps	%xmm0, %xmm10, %xmm0
	vmovshdup	%xmm0, %xmm10           # xmm10 = xmm0[1,1,3,3]
	vaddss	%xmm0, %xmm10, %xmm0
	vmovss	%xmm0, 13248(%rsp)              # 4-byte Spill
	vmulps	3136(%rsp), %zmm1, %zmm0        # 64-byte Folded Reload
	vfmadd231ps	1856(%rsp), %zmm15, %zmm0 # 64-byte Folded Reload
                                        # zmm0 = (zmm15 * mem) + zmm0
	vextractf64x4	$1, %zmm0, %ymm10
	vaddps	%zmm10, %zmm0, %zmm0
	vextractf128	$1, %ymm0, %xmm10
	vaddps	%xmm0, %xmm10, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm10        # xmm10 = xmm0[1,0]
	vaddps	%xmm0, %xmm10, %xmm0
	vmovshdup	%xmm0, %xmm10           # xmm10 = xmm0[1,1,3,3]
	vaddss	%xmm0, %xmm10, %xmm0
	vmovss	%xmm0, 9600(%rsp)               # 4-byte Spill
	vmovaps	35200(%rsp), %zmm10             # 64-byte Reload
	vextractf64x4	$1, %zmm10, %ymm0
	vaddps	%zmm0, %zmm10, %zmm0
	vextractf128	$1, %ymm0, %xmm10
	vaddps	%xmm0, %xmm10, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm10        # xmm10 = xmm0[1,0]
	vaddps	%xmm0, %xmm10, %xmm0
	vmovshdup	%xmm0, %xmm10           # xmm10 = xmm0[1,1,3,3]
	vaddss	%xmm0, %xmm10, %xmm0
	vaddss	%xmm27, %xmm0, %xmm10
	vmulps	192(%rsp), %zmm1, %zmm0         # 64-byte Folded Reload
	vfmadd231ps	1344(%rsp), %zmm15, %zmm0 # 64-byte Folded Reload
                                        # zmm0 = (zmm15 * mem) + zmm0
	vextractf64x4	$1, %zmm0, %ymm11
	vaddps	%zmm11, %zmm0, %zmm0
	vextractf128	$1, %ymm0, %xmm11
	vaddps	%xmm0, %xmm11, %xmm0
	vmulps	2432(%rsp), %zmm1, %zmm11       # 64-byte Folded Reload
	vfmadd231ps	1792(%rsp), %zmm15, %zmm11 # 64-byte Folded Reload
                                        # zmm11 = (zmm15 * mem) + zmm11
	vextractf64x4	$1, %zmm11, %ymm12
	vaddps	%zmm12, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm12
	vaddps	%xmm12, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm12      # xmm12 = xmm11[1,0]
	vaddps	%xmm12, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm12          # xmm12 = xmm11[1,1,3,3]
	vaddss	%xmm12, %xmm11, %xmm11
	vaddss	%xmm27, %xmm11, %xmm11
	vshufpd	$1, %xmm0, %xmm0, %xmm12        # xmm12 = xmm0[1,0]
	vaddps	%xmm0, %xmm12, %xmm0
	vmovshdup	%xmm0, %xmm12           # xmm12 = xmm0[1,1,3,3]
	vaddss	%xmm0, %xmm12, %xmm0
	vmulps	%zmm23, %zmm1, %zmm12
	vfmadd231ps	1728(%rsp), %zmm15, %zmm12 # 64-byte Folded Reload
                                        # zmm12 = (zmm15 * mem) + zmm12
	vextractf64x4	$1, %zmm12, %ymm13
	vaddps	%zmm13, %zmm12, %zmm12
	vextractf128	$1, %ymm12, %xmm13
	vaddps	%xmm13, %xmm12, %xmm12
	vshufpd	$1, %xmm12, %xmm12, %xmm13      # xmm13 = xmm12[1,0]
	vaddps	%xmm13, %xmm12, %xmm12
	vmovshdup	%xmm12, %xmm13          # xmm13 = xmm12[1,1,3,3]
	vaddss	%xmm13, %xmm12, %xmm12
	vmovaps	34240(%rsp), %zmm14             # 64-byte Reload
	vextractf64x4	$1, %zmm14, %ymm13
	vaddps	%zmm13, %zmm14, %zmm13
	vextractf128	$1, %ymm13, %xmm14
	vaddps	%xmm14, %xmm13, %xmm13
	vshufpd	$1, %xmm13, %xmm13, %xmm14      # xmm14 = xmm13[1,0]
	vaddps	%xmm14, %xmm13, %xmm13
	vmovshdup	%xmm13, %xmm14          # xmm14 = xmm13[1,1,3,3]
	vaddss	%xmm14, %xmm13, %xmm13
	vaddss	%xmm27, %xmm0, %xmm14
	vaddss	%xmm27, %xmm12, %xmm12
	vaddss	%xmm27, %xmm13, %xmm0
	vmovaps	%xmm0, 9152(%rsp)               # 16-byte Spill
	vmovaps	2304(%rsp), %zmm19              # 64-byte Reload
	vmulps	%zmm19, %zmm1, %zmm23
	vmovaps	%zmm15, %zmm13
	vfmadd231ps	1536(%rsp), %zmm15, %zmm23 # 64-byte Folded Reload
                                        # zmm23 = (zmm15 * mem) + zmm23
	vmovaps	%zmm23, 34624(%rsp)             # 64-byte Spill
	vmulps	%zmm18, %zmm1, %zmm15
	vmovaps	1920(%rsp), %zmm0               # 64-byte Reload
	vfmadd231ps	%zmm0, %zmm13, %zmm15   # zmm15 = (zmm13 * zmm0) + zmm15
	vmovaps	%zmm15, 35200(%rsp)             # 64-byte Spill
	vmovaps	%zmm13, %zmm15
	vmulps	2240(%rsp), %zmm1, %zmm13       # 64-byte Folded Reload
	vfmadd231ps	%zmm2, %zmm15, %zmm13   # zmm13 = (zmm15 * zmm2) + zmm13
	vextractf64x4	$1, %zmm13, %ymm15
	vaddps	%zmm15, %zmm13, %zmm13
	vextractf128	$1, %ymm13, %xmm15
	vaddps	%xmm15, %xmm13, %xmm13
	vshufpd	$1, %xmm13, %xmm13, %xmm15      # xmm15 = xmm13[1,0]
	vaddps	%xmm15, %xmm13, %xmm13
	vmovshdup	%xmm13, %xmm15          # xmm15 = xmm13[1,1,3,3]
	vaddss	%xmm15, %xmm13, %xmm13
	vaddss	%xmm27, %xmm13, %xmm13
	vinsertps	$16, %xmm13, %xmm14, %xmm13 # xmm13 = xmm14[0],xmm13[0],xmm14[2,3]
	vinsertps	$32, %xmm12, %xmm13, %xmm12 # xmm12 = xmm13[0,1],xmm12[0],xmm13[3]
	vinsertps	$48, %xmm11, %xmm12, %xmm1 # xmm1 = xmm12[0,1,2],xmm11[0]
	vmovaps	%xmm1, 4672(%rsp)               # 16-byte Spill
	vinsertf128	$1, %xmm4, %ymm3, %ymm1
	vmovaps	%zmm1, 34944(%rsp)              # 64-byte Spill
	vinsertf128	$1, %xmm7, %ymm6, %ymm1
	vmovaps	%zmm1, 4608(%rsp)               # 64-byte Spill
	vmovaps	%zmm16, %zmm1
	vmulps	%zmm22, %zmm16, %zmm3
	vmovaps	%zmm22, %zmm18
	vmovaps	%zmm17, %zmm15
	vfmadd231ps	%zmm20, %zmm17, %zmm3   # zmm3 = (zmm17 * zmm20) + zmm3
	vextractf64x4	$1, %zmm3, %ymm4
	vaddps	%zmm4, %zmm3, %zmm3
	vextractf128	$1, %ymm3, %xmm4
	vaddps	%xmm4, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm4         # xmm4 = xmm3[1,0]
	vaddps	%xmm4, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm4            # xmm4 = xmm3[1,1,3,3]
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm27, %xmm3, %xmm3
	vinsertps	$16, %xmm10, %xmm3, %xmm3 # xmm3 = xmm3[0],xmm10[0],xmm3[2,3]
	vinsertps	$32, %xmm9, %xmm3, %xmm3 # xmm3 = xmm3[0,1],xmm9[0],xmm3[3]
	vinsertps	$48, %xmm8, %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],xmm8[0]
	vmovaps	34752(%rsp), %zmm6              # 64-byte Reload
	vextractf64x4	$1, %zmm6, %ymm4
	vaddps	%zmm4, %zmm6, %zmm4
	vextractf128	$1, %ymm4, %xmm6
	vaddps	%xmm6, %xmm4, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm6         # xmm6 = xmm4[1,0]
	vaddps	%xmm6, %xmm4, %xmm4
	vmovshdup	%xmm4, %xmm6            # xmm6 = xmm4[1,1,3,3]
	vaddss	%xmm6, %xmm4, %xmm4
	vaddss	%xmm27, %xmm4, %xmm4
	vinsertps	$16, %xmm4, %xmm5, %xmm4 # xmm4 = xmm5[0],xmm4[0],xmm5[2,3]
	vinsertps	$32, 9216(%rsp), %xmm4, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm4[0,1],mem[0],xmm4[3]
	vmovaps	16128(%rsp), %zmm5              # 64-byte Reload
	vextractf64x4	$1, %zmm5, %ymm4
	vaddps	%zmm4, %zmm5, %zmm4
	vextractf128	$1, %ymm4, %xmm5
	vaddps	%xmm5, %xmm4, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm5         # xmm5 = xmm4[1,0]
	vaddps	%xmm5, %xmm4, %xmm4
	vmovshdup	%xmm4, %xmm5            # xmm5 = xmm4[1,1,3,3]
	vaddss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm27, %xmm4, %xmm4
	vinsertps	$48, %xmm4, %xmm2, %xmm4 # xmm4 = xmm2[0,1,2],xmm4[0]
	vmulps	%zmm31, %zmm16, %zmm2
	vmovaps	512(%rsp), %zmm24               # 64-byte Reload
	vfmadd231ps	%zmm24, %zmm17, %zmm2   # zmm2 = (zmm17 * zmm24) + zmm2
	vextractf64x4	$1, %zmm2, %ymm5
	vaddps	%zmm5, %zmm2, %zmm2
	vextractf128	$1, %ymm2, %xmm5
	vaddps	%xmm5, %xmm2, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm5         # xmm5 = xmm2[1,0]
	vaddps	%xmm5, %xmm2, %xmm5
	vmovaps	30592(%rsp), %zmm6              # 64-byte Reload
	vextractf64x4	$1, %zmm6, %ymm2
	vaddps	%zmm2, %zmm6, %zmm2
	vextractf128	$1, %ymm2, %xmm6
	vaddps	%xmm6, %xmm2, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm6         # xmm6 = xmm2[1,0]
	vaddps	%xmm6, %xmm2, %xmm2
	vmovshdup	%xmm2, %xmm6            # xmm6 = xmm2[1,1,3,3]
	vaddss	%xmm6, %xmm2, %xmm2
	vaddss	%xmm27, %xmm2, %xmm2
	vmulps	%zmm26, %zmm16, %zmm6
	vfmadd231ps	%zmm29, %zmm17, %zmm6   # zmm6 = (zmm17 * zmm29) + zmm6
	vextractf64x4	$1, %zmm6, %ymm7
	vaddps	%zmm7, %zmm6, %zmm6
	vextractf128	$1, %ymm6, %xmm7
	vaddps	%xmm7, %xmm6, %xmm6
	vshufpd	$1, %xmm6, %xmm6, %xmm7         # xmm7 = xmm6[1,0]
	vaddps	%xmm7, %xmm6, %xmm6
	vmovshdup	%xmm6, %xmm7            # xmm7 = xmm6[1,1,3,3]
	vaddss	%xmm7, %xmm6, %xmm6
	vaddss	%xmm27, %xmm6, %xmm7
	vmovshdup	%xmm5, %xmm6            # xmm6 = xmm5[1,1,3,3]
	vaddss	%xmm6, %xmm5, %xmm5
	vmovss	%xmm5, 16128(%rsp)              # 4-byte Spill
	vmulps	2048(%rsp), %zmm16, %zmm5       # 64-byte Folded Reload
	vfmadd231ps	704(%rsp), %zmm17, %zmm5 # 64-byte Folded Reload
                                        # zmm5 = (zmm17 * mem) + zmm5
	vextractf64x4	$1, %zmm5, %ymm6
	vaddps	%zmm6, %zmm5, %zmm5
	vextractf128	$1, %ymm5, %xmm6
	vaddps	%xmm6, %xmm5, %xmm5
	vshufpd	$1, %xmm5, %xmm5, %xmm6         # xmm6 = xmm5[1,0]
	vaddps	%xmm6, %xmm5, %xmm5
	vmovaps	13568(%rsp), %zmm8              # 64-byte Reload
	vextractf64x4	$1, %zmm8, %ymm6
	vaddps	%zmm6, %zmm8, %zmm6
	vextractf128	$1, %ymm6, %xmm8
	vaddps	%xmm6, %xmm8, %xmm6
	vshufpd	$1, %xmm6, %xmm6, %xmm8         # xmm8 = xmm6[1,0]
	vaddps	%xmm6, %xmm8, %xmm6
	vmovshdup	%xmm6, %xmm8            # xmm8 = xmm6[1,1,3,3]
	vaddss	%xmm6, %xmm8, %xmm6
	vaddss	%xmm27, %xmm6, %xmm6
	vmovshdup	%xmm5, %xmm8            # xmm8 = xmm5[1,1,3,3]
	vaddss	%xmm5, %xmm8, %xmm5
	vmovss	%xmm5, 13568(%rsp)              # 4-byte Spill
	vmulps	%zmm25, %zmm16, %zmm5
	vmovaps	1408(%rsp), %zmm22              # 64-byte Reload
	vfmadd231ps	%zmm22, %zmm17, %zmm5   # zmm5 = (zmm17 * zmm22) + zmm5
	vextractf64x4	$1, %zmm5, %ymm8
	vaddps	%zmm8, %zmm5, %zmm5
	vextractf128	$1, %ymm5, %xmm8
	vaddps	%xmm5, %xmm8, %xmm5
	vmovaps	14720(%rsp), %zmm9              # 64-byte Reload
	vextractf64x4	$1, %zmm9, %ymm8
	vaddps	%zmm8, %zmm9, %zmm8
	vextractf128	$1, %ymm8, %xmm9
	vaddps	%xmm9, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm9         # xmm9 = xmm8[1,0]
	vaddps	%xmm9, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm9            # xmm9 = xmm8[1,1,3,3]
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm27, %xmm8, %xmm8
	vshufpd	$1, %xmm5, %xmm5, %xmm9         # xmm9 = xmm5[1,0]
	vaddps	%xmm5, %xmm9, %xmm5
	vmovshdup	%xmm5, %xmm9            # xmm9 = xmm5[1,1,3,3]
	vaddss	%xmm5, %xmm9, %xmm5
	vmovss	%xmm5, 14720(%rsp)              # 4-byte Spill
	vmulps	128(%rsp), %zmm16, %zmm5        # 64-byte Folded Reload
	vfmadd231ps	%zmm30, %zmm17, %zmm5   # zmm5 = (zmm17 * zmm30) + zmm5
	vextractf64x4	$1, %zmm5, %ymm9
	vaddps	%zmm9, %zmm5, %zmm5
	vextractf128	$1, %ymm5, %xmm9
	vaddps	%xmm5, %xmm9, %xmm9
	vmovaps	15872(%rsp), %zmm10             # 64-byte Reload
	vextractf64x4	$1, %zmm10, %ymm5
	vaddps	%zmm5, %zmm10, %zmm5
	vextractf128	$1, %ymm5, %xmm10
	vaddps	%xmm5, %xmm10, %xmm5
	vshufpd	$1, %xmm5, %xmm5, %xmm10        # xmm10 = xmm5[1,0]
	vaddps	%xmm5, %xmm10, %xmm5
	vmovshdup	%xmm5, %xmm10           # xmm10 = xmm5[1,1,3,3]
	vaddss	%xmm5, %xmm10, %xmm5
	vaddss	%xmm27, %xmm5, %xmm10
	vmovaps	3328(%rsp), %zmm16              # 64-byte Reload
	vmovaps	1152(%rsp), %zmm28              # 64-byte Reload
	vmulps	%zmm28, %zmm16, %zmm5
	vmovaps	3392(%rsp), %zmm17              # 64-byte Reload
	vfmadd231ps	768(%rsp), %zmm17, %zmm5 # 64-byte Folded Reload
                                        # zmm5 = (zmm17 * mem) + zmm5
	vextractf64x4	$1, %zmm5, %ymm11
	vaddps	%zmm11, %zmm5, %zmm5
	vextractf128	$1, %ymm5, %xmm11
	vaddps	%xmm5, %xmm11, %xmm5
	vshufpd	$1, %xmm5, %xmm5, %xmm11        # xmm11 = xmm5[1,0]
	vaddps	%xmm5, %xmm11, %xmm5
	vmovshdup	%xmm5, %xmm11           # xmm11 = xmm5[1,1,3,3]
	vaddss	%xmm5, %xmm11, %xmm5
	vaddss	%xmm27, %xmm5, %xmm5
	vshufpd	$1, %xmm9, %xmm9, %xmm11        # xmm11 = xmm9[1,0]
	vaddps	%xmm11, %xmm9, %xmm9
	vmovshdup	%xmm9, %xmm11           # xmm11 = xmm9[1,1,3,3]
	vaddss	%xmm11, %xmm9, %xmm9
	vmulps	2368(%rsp), %zmm1, %zmm11       # 64-byte Folded Reload
	vfmadd231ps	2560(%rsp), %zmm15, %zmm11 # 64-byte Folded Reload
                                        # zmm11 = (zmm15 * mem) + zmm11
	vextractf64x4	$1, %zmm11, %ymm12
	vaddps	%zmm12, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm12
	vaddps	%xmm12, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm12      # xmm12 = xmm11[1,0]
	vaddps	%xmm12, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm12          # xmm12 = xmm11[1,1,3,3]
	vaddss	%xmm12, %xmm11, %xmm11
	vaddss	%xmm27, %xmm9, %xmm9
	vaddss	%xmm27, %xmm11, %xmm11
	vinsertps	$16, %xmm11, %xmm9, %xmm9 # xmm9 = xmm9[0],xmm11[0],xmm9[2,3]
	vmulps	1984(%rsp), %zmm1, %zmm11       # 64-byte Folded Reload
	vmovaps	1024(%rsp), %zmm30              # 64-byte Reload
	vfmadd231ps	%zmm30, %zmm15, %zmm11  # zmm11 = (zmm15 * zmm30) + zmm11
	vextractf64x4	$1, %zmm11, %ymm12
	vaddps	%zmm12, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm12
	vaddps	%xmm12, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm12      # xmm12 = xmm11[1,0]
	vaddps	%xmm12, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm12          # xmm12 = xmm11[1,1,3,3]
	vaddss	%xmm12, %xmm11, %xmm11
	vaddss	%xmm27, %xmm11, %xmm11
	vinsertps	$32, %xmm11, %xmm9, %xmm9 # xmm9 = xmm9[0,1],xmm11[0],xmm9[3]
	vmulps	832(%rsp), %zmm1, %zmm11        # 64-byte Folded Reload
	vmovaps	960(%rsp), %zmm31               # 64-byte Reload
	vfmadd231ps	%zmm31, %zmm15, %zmm11  # zmm11 = (zmm15 * zmm31) + zmm11
	vextractf64x4	$1, %zmm11, %ymm12
	vaddps	%zmm12, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm12
	vaddps	%xmm12, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm12      # xmm12 = xmm11[1,0]
	vaddps	%xmm12, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm12          # xmm12 = xmm11[1,1,3,3]
	vaddss	%xmm12, %xmm11, %xmm11
	vaddss	%xmm27, %xmm11, %xmm11
	vinsertps	$48, %xmm11, %xmm9, %xmm9 # xmm9 = xmm9[0,1,2],xmm11[0]
	vmovaps	%xmm9, 34752(%rsp)              # 16-byte Spill
	vmulps	640(%rsp), %zmm1, %zmm9         # 64-byte Folded Reload
	vfmadd231ps	64(%rsp), %zmm15, %zmm9 # 64-byte Folded Reload
                                        # zmm9 = (zmm15 * mem) + zmm9
	vextractf64x4	$1, %zmm9, %ymm11
	vaddps	%zmm11, %zmm9, %zmm9
	vextractf128	$1, %ymm9, %xmm11
	vaddps	%xmm11, %xmm9, %xmm9
	vshufpd	$1, %xmm9, %xmm9, %xmm11        # xmm11 = xmm9[1,0]
	vaddps	%xmm11, %xmm9, %xmm9
	vmovshdup	%xmm9, %xmm11           # xmm11 = xmm9[1,1,3,3]
	vaddss	%xmm11, %xmm9, %xmm9
	vaddss	%xmm27, %xmm9, %xmm9
	vinsertps	$16, %xmm10, %xmm9, %xmm9 # xmm9 = xmm9[0],xmm10[0],xmm9[2,3]
	vinsertps	$32, %xmm8, %xmm9, %xmm8 # xmm8 = xmm9[0,1],xmm8[0],xmm9[3]
	vinsertps	$48, %xmm6, %xmm8, %xmm6 # xmm6 = xmm8[0,1,2],xmm6[0]
	vmovaps	12608(%rsp), %zmm9              # 64-byte Reload
	vextractf64x4	$1, %zmm9, %ymm8
	vaddps	%zmm8, %zmm9, %zmm8
	vextractf128	$1, %ymm8, %xmm9
	vaddps	%xmm9, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm9         # xmm9 = xmm8[1,0]
	vaddps	%xmm9, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm9            # xmm9 = xmm8[1,1,3,3]
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm27, %xmm8, %xmm8
	vinsertps	$16, %xmm8, %xmm7, %xmm7 # xmm7 = xmm7[0],xmm8[0],xmm7[2,3]
	vinsertps	$32, %xmm2, %xmm7, %xmm2 # xmm2 = xmm7[0,1],xmm2[0],xmm7[3]
	vmovaps	12096(%rsp), %zmm8              # 64-byte Reload
	vextractf64x4	$1, %zmm8, %ymm7
	vaddps	%zmm7, %zmm8, %zmm7
	vextractf128	$1, %ymm7, %xmm8
	vaddps	%xmm7, %xmm8, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm8         # xmm8 = xmm7[1,0]
	vaddps	%xmm7, %xmm8, %xmm7
	vmovshdup	%xmm7, %xmm8            # xmm8 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm27, %xmm7, %xmm7
	vinsertps	$48, %xmm7, %xmm2, %xmm7 # xmm7 = xmm2[0,1,2],xmm7[0]
	vmulps	3072(%rsp), %zmm1, %zmm2        # 64-byte Folded Reload
	vfmadd231ps	1472(%rsp), %zmm15, %zmm2 # 64-byte Folded Reload
                                        # zmm2 = (zmm15 * mem) + zmm2
	vextractf64x4	$1, %zmm2, %ymm8
	vaddps	%zmm8, %zmm2, %zmm2
	vextractf128	$1, %ymm2, %xmm8
	vaddps	%xmm2, %xmm8, %xmm2
	vmovaps	35392(%rsp), %zmm9              # 64-byte Reload
	vextractf64x4	$1, %zmm9, %ymm8
	vaddps	%zmm8, %zmm9, %zmm8
	vextractf128	$1, %ymm8, %xmm9
	vaddps	%xmm9, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm9         # xmm9 = xmm8[1,0]
	vaddps	%xmm9, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm9            # xmm9 = xmm8[1,1,3,3]
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm27, %xmm8, %xmm8
	vshufpd	$1, %xmm2, %xmm2, %xmm9         # xmm9 = xmm2[1,0]
	vaddps	%xmm2, %xmm9, %xmm2
	vmovshdup	%xmm2, %xmm9            # xmm9 = xmm2[1,1,3,3]
	vaddss	%xmm2, %xmm9, %xmm2
	vmovss	%xmm2, 15872(%rsp)              # 4-byte Spill
	vmulps	%zmm21, %zmm1, %zmm2
	vfmadd231ps	2496(%rsp), %zmm15, %zmm2 # 64-byte Folded Reload
                                        # zmm2 = (zmm15 * mem) + zmm2
	vextractf64x4	$1, %zmm2, %ymm9
	vaddps	%zmm9, %zmm2, %zmm2
	vmovaps	35584(%rsp), %zmm10             # 64-byte Reload
	vextractf64x4	$1, %zmm10, %ymm9
	vaddps	%zmm9, %zmm10, %zmm9
	vextractf128	$1, %ymm9, %xmm10
	vaddps	%xmm10, %xmm9, %xmm9
	vshufpd	$1, %xmm9, %xmm9, %xmm10        # xmm10 = xmm9[1,0]
	vaddps	%xmm10, %xmm9, %xmm9
	vmovshdup	%xmm9, %xmm10           # xmm10 = xmm9[1,1,3,3]
	vaddss	%xmm10, %xmm9, %xmm9
	vaddss	%xmm27, %xmm9, %xmm9
	vextractf128	$1, %ymm2, %xmm10
	vaddps	%xmm2, %xmm10, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm10        # xmm10 = xmm2[1,0]
	vaddps	%xmm2, %xmm10, %xmm2
	vmovshdup	%xmm2, %xmm10           # xmm10 = xmm2[1,1,3,3]
	vaddss	%xmm2, %xmm10, %xmm2
	vmovss	%xmm2, 12608(%rsp)              # 4-byte Spill
	vmulps	3136(%rsp), %zmm1, %zmm2        # 64-byte Folded Reload
	vfmadd231ps	1856(%rsp), %zmm15, %zmm2 # 64-byte Folded Reload
                                        # zmm2 = (zmm15 * mem) + zmm2
	vextractf64x4	$1, %zmm2, %ymm10
	vaddps	%zmm10, %zmm2, %zmm2
	vextractf128	$1, %ymm2, %xmm10
	vaddps	%xmm2, %xmm10, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm10        # xmm10 = xmm2[1,0]
	vaddps	%xmm2, %xmm10, %xmm2
	vmovshdup	%xmm2, %xmm10           # xmm10 = xmm2[1,1,3,3]
	vaddss	%xmm2, %xmm10, %xmm2
	vmovss	%xmm2, 9216(%rsp)               # 4-byte Spill
	vmovaps	35968(%rsp), %zmm10             # 64-byte Reload
	vextractf64x4	$1, %zmm10, %ymm2
	vaddps	%zmm2, %zmm10, %zmm2
	vextractf128	$1, %ymm2, %xmm10
	vaddps	%xmm2, %xmm10, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm10        # xmm10 = xmm2[1,0]
	vaddps	%xmm2, %xmm10, %xmm2
	vmovshdup	%xmm2, %xmm10           # xmm10 = xmm2[1,1,3,3]
	vaddss	%xmm2, %xmm10, %xmm2
	vaddss	%xmm27, %xmm2, %xmm10
	vmovaps	192(%rsp), %zmm21               # 64-byte Reload
	vmulps	%zmm21, %zmm1, %zmm2
	vmovaps	1344(%rsp), %zmm20              # 64-byte Reload
	vfmadd231ps	%zmm20, %zmm15, %zmm2   # zmm2 = (zmm15 * zmm20) + zmm2
	vextractf64x4	$1, %zmm2, %ymm11
	vaddps	%zmm11, %zmm2, %zmm2
	vextractf128	$1, %ymm2, %xmm11
	vaddps	%xmm2, %xmm11, %xmm2
	vmovaps	2432(%rsp), %zmm26              # 64-byte Reload
	vmulps	%zmm26, %zmm1, %zmm11
	vmovaps	1792(%rsp), %zmm23              # 64-byte Reload
	vfmadd231ps	%zmm15, %zmm23, %zmm11  # zmm11 = (zmm23 * zmm15) + zmm11
	vextractf64x4	$1, %zmm11, %ymm12
	vaddps	%zmm12, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm12
	vaddps	%xmm12, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm12      # xmm12 = xmm11[1,0]
	vaddps	%xmm12, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm12          # xmm12 = xmm11[1,1,3,3]
	vaddss	%xmm12, %xmm11, %xmm11
	vaddss	%xmm27, %xmm11, %xmm11
	vshufpd	$1, %xmm2, %xmm2, %xmm12        # xmm12 = xmm2[1,0]
	vaddps	%xmm2, %xmm12, %xmm2
	vmovshdup	%xmm2, %xmm12           # xmm12 = xmm2[1,1,3,3]
	vaddss	%xmm2, %xmm12, %xmm2
	vmulps	2176(%rsp), %zmm1, %zmm12       # 64-byte Folded Reload
	vmovaps	1728(%rsp), %zmm29              # 64-byte Reload
	vfmadd231ps	%zmm29, %zmm15, %zmm12  # zmm12 = (zmm15 * zmm29) + zmm12
	vextractf64x4	$1, %zmm12, %ymm13
	vaddps	%zmm13, %zmm12, %zmm12
	vextractf128	$1, %ymm12, %xmm13
	vaddps	%xmm13, %xmm12, %xmm12
	vshufpd	$1, %xmm12, %xmm12, %xmm13      # xmm13 = xmm12[1,0]
	vaddps	%xmm13, %xmm12, %xmm12
	vmovshdup	%xmm12, %xmm13          # xmm13 = xmm12[1,1,3,3]
	vaddss	%xmm13, %xmm12, %xmm12
	vmovaps	35072(%rsp), %zmm14             # 64-byte Reload
	vextractf64x4	$1, %zmm14, %ymm13
	vaddps	%zmm13, %zmm14, %zmm13
	vextractf128	$1, %ymm13, %xmm14
	vaddps	%xmm14, %xmm13, %xmm13
	vshufpd	$1, %xmm13, %xmm13, %xmm14      # xmm14 = xmm13[1,0]
	vaddps	%xmm14, %xmm13, %xmm13
	vmovshdup	%xmm13, %xmm14          # xmm14 = xmm13[1,1,3,3]
	vaddss	%xmm14, %xmm13, %xmm13
	vaddss	%xmm27, %xmm2, %xmm14
	vaddss	%xmm27, %xmm12, %xmm12
	vaddss	%xmm27, %xmm13, %xmm2
	vmovaps	%xmm2, 12544(%rsp)              # 16-byte Spill
	vmulps	%zmm19, %zmm1, %zmm25
	vmovaps	%zmm15, %zmm13
	vmovaps	1536(%rsp), %zmm2               # 64-byte Reload
	vfmadd231ps	%zmm2, %zmm15, %zmm25   # zmm25 = (zmm15 * zmm2) + zmm25
	vmovaps	%zmm25, 34240(%rsp)             # 64-byte Spill
	vmulps	576(%rsp), %zmm1, %zmm15        # 64-byte Folded Reload
	vfmadd231ps	%zmm0, %zmm13, %zmm15   # zmm15 = (zmm13 * zmm0) + zmm15
	vmovaps	%zmm15, 35392(%rsp)             # 64-byte Spill
	vmovaps	%zmm13, %zmm15
	vmovaps	2240(%rsp), %zmm25              # 64-byte Reload
	vmulps	%zmm25, %zmm1, %zmm13
	vfmadd231ps	1088(%rsp), %zmm15, %zmm13 # 64-byte Folded Reload
                                        # zmm13 = (zmm15 * mem) + zmm13
	vextractf64x4	$1, %zmm13, %ymm15
	vaddps	%zmm15, %zmm13, %zmm13
	vextractf128	$1, %ymm13, %xmm15
	vaddps	%xmm15, %xmm13, %xmm13
	vshufpd	$1, %xmm13, %xmm13, %xmm15      # xmm15 = xmm13[1,0]
	vaddps	%xmm15, %xmm13, %xmm13
	vmovshdup	%xmm13, %xmm15          # xmm15 = xmm13[1,1,3,3]
	vaddss	%xmm15, %xmm13, %xmm13
	vaddss	%xmm27, %xmm13, %xmm13
	vinsertps	$16, %xmm13, %xmm14, %xmm13 # xmm13 = xmm14[0],xmm13[0],xmm14[2,3]
	vinsertps	$32, %xmm12, %xmm13, %xmm12 # xmm12 = xmm13[0,1],xmm12[0],xmm13[3]
	vinsertps	$48, %xmm11, %xmm12, %xmm1 # xmm1 = xmm12[0,1,2],xmm11[0]
	vmovaps	%xmm1, 35968(%rsp)              # 16-byte Spill
	vinsertf128	$1, %xmm4, %ymm3, %ymm1
	vmovaps	%zmm1, 35072(%rsp)              # 64-byte Spill
	vinsertf128	$1, %xmm7, %ymm6, %ymm1
	vmovaps	%zmm1, 35584(%rsp)              # 64-byte Spill
	vmovaps	%zmm16, %zmm1
	vmulps	%zmm18, %zmm16, %zmm3
	vmovaps	%zmm17, %zmm15
	vmovaps	1664(%rsp), %zmm18              # 64-byte Reload
	vfmadd231ps	%zmm18, %zmm17, %zmm3   # zmm3 = (zmm17 * zmm18) + zmm3
	vextractf64x4	$1, %zmm3, %ymm4
	vaddps	%zmm4, %zmm3, %zmm3
	vextractf128	$1, %ymm3, %xmm4
	vaddps	%xmm4, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm4         # xmm4 = xmm3[1,0]
	vaddps	%xmm4, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm4            # xmm4 = xmm3[1,1,3,3]
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm27, %xmm3, %xmm3
	vinsertps	$16, %xmm10, %xmm3, %xmm3 # xmm3 = xmm3[0],xmm10[0],xmm3[2,3]
	vinsertps	$32, %xmm9, %xmm3, %xmm3 # xmm3 = xmm3[0,1],xmm9[0],xmm3[3]
	vinsertps	$48, %xmm8, %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],xmm8[0]
	vmovaps	35520(%rsp), %zmm6              # 64-byte Reload
	vextractf64x4	$1, %zmm6, %ymm4
	vaddps	%zmm4, %zmm6, %zmm4
	vextractf128	$1, %ymm4, %xmm6
	vaddps	%xmm6, %xmm4, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm6         # xmm6 = xmm4[1,0]
	vaddps	%xmm6, %xmm4, %xmm4
	vmovshdup	%xmm4, %xmm6            # xmm6 = xmm4[1,1,3,3]
	vaddss	%xmm6, %xmm4, %xmm4
	vaddss	%xmm27, %xmm4, %xmm4
	vinsertps	$16, %xmm4, %xmm5, %xmm4 # xmm4 = xmm5[0],xmm4[0],xmm5[2,3]
	vinsertps	$32, 9152(%rsp), %xmm4, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm4[0,1],mem[0],xmm4[3]
	vmovaps	35008(%rsp), %zmm5              # 64-byte Reload
	vextractf64x4	$1, %zmm5, %ymm4
	vaddps	%zmm4, %zmm5, %zmm4
	vextractf128	$1, %ymm4, %xmm5
	vaddps	%xmm5, %xmm4, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm5         # xmm5 = xmm4[1,0]
	vaddps	%xmm5, %xmm4, %xmm4
	vmovshdup	%xmm4, %xmm5            # xmm5 = xmm4[1,1,3,3]
	vaddss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm27, %xmm4, %xmm4
	vinsertps	$48, %xmm4, %xmm0, %xmm4 # xmm4 = xmm0[0,1,2],xmm4[0]
	vmulps	448(%rsp), %zmm16, %zmm0        # 64-byte Folded Reload
	vfmadd231ps	%zmm24, %zmm17, %zmm0   # zmm0 = (zmm17 * zmm24) + zmm0
	vextractf64x4	$1, %zmm0, %ymm5
	vaddps	%zmm5, %zmm0, %zmm0
	vextractf128	$1, %ymm0, %xmm5
	vaddps	%xmm5, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm5         # xmm5 = xmm0[1,0]
	vaddps	%xmm5, %xmm0, %xmm5
	vmovaps	30912(%rsp), %zmm6              # 64-byte Reload
	vextractf64x4	$1, %zmm6, %ymm0
	vaddps	%zmm0, %zmm6, %zmm0
	vextractf128	$1, %ymm0, %xmm6
	vaddps	%xmm6, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm6         # xmm6 = xmm0[1,0]
	vaddps	%xmm6, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm6            # xmm6 = xmm0[1,1,3,3]
	vaddss	%xmm6, %xmm0, %xmm0
	vaddss	%xmm27, %xmm0, %xmm0
	vmulps	1280(%rsp), %zmm16, %zmm6       # 64-byte Folded Reload
	vfmadd231ps	1216(%rsp), %zmm17, %zmm6 # 64-byte Folded Reload
                                        # zmm6 = (zmm17 * mem) + zmm6
	vextractf64x4	$1, %zmm6, %ymm7
	vaddps	%zmm7, %zmm6, %zmm6
	vextractf128	$1, %ymm6, %xmm7
	vaddps	%xmm7, %xmm6, %xmm6
	vshufpd	$1, %xmm6, %xmm6, %xmm7         # xmm7 = xmm6[1,0]
	vaddps	%xmm7, %xmm6, %xmm6
	vmovshdup	%xmm6, %xmm7            # xmm7 = xmm6[1,1,3,3]
	vaddss	%xmm7, %xmm6, %xmm6
	vaddss	%xmm27, %xmm6, %xmm7
	vmovshdup	%xmm5, %xmm6            # xmm6 = xmm5[1,1,3,3]
	vaddss	%xmm6, %xmm5, %xmm5
	vmovss	%xmm5, 3264(%rsp)               # 4-byte Spill
	vmulps	2048(%rsp), %zmm16, %zmm5       # 64-byte Folded Reload
	vfmadd231ps	704(%rsp), %zmm17, %zmm5 # 64-byte Folded Reload
                                        # zmm5 = (zmm17 * mem) + zmm5
	vextractf64x4	$1, %zmm5, %ymm6
	vaddps	%zmm6, %zmm5, %zmm5
	vextractf128	$1, %ymm5, %xmm6
	vaddps	%xmm6, %xmm5, %xmm5
	vshufpd	$1, %xmm5, %xmm5, %xmm6         # xmm6 = xmm5[1,0]
	vaddps	%xmm6, %xmm5, %xmm5
	vmovaps	31616(%rsp), %zmm8              # 64-byte Reload
	vextractf64x4	$1, %zmm8, %ymm6
	vaddps	%zmm6, %zmm8, %zmm6
	vextractf128	$1, %ymm6, %xmm8
	vaddps	%xmm6, %xmm8, %xmm6
	vshufpd	$1, %xmm6, %xmm6, %xmm8         # xmm8 = xmm6[1,0]
	vaddps	%xmm6, %xmm8, %xmm6
	vmovshdup	%xmm6, %xmm8            # xmm8 = xmm6[1,1,3,3]
	vaddss	%xmm6, %xmm8, %xmm6
	vaddss	%xmm27, %xmm6, %xmm6
	vmovshdup	%xmm5, %xmm8            # xmm8 = xmm5[1,1,3,3]
	vaddss	%xmm5, %xmm8, %xmm5
	vmovss	%xmm5, 3200(%rsp)               # 4-byte Spill
	vmulps	256(%rsp), %zmm16, %zmm5        # 64-byte Folded Reload
	vfmadd231ps	%zmm22, %zmm17, %zmm5   # zmm5 = (zmm17 * zmm22) + zmm5
	vextractf64x4	$1, %zmm5, %ymm8
	vaddps	%zmm8, %zmm5, %zmm5
	vextractf128	$1, %ymm5, %xmm8
	vaddps	%xmm5, %xmm8, %xmm5
	vmovaps	33280(%rsp), %zmm9              # 64-byte Reload
	vextractf64x4	$1, %zmm9, %ymm8
	vaddps	%zmm8, %zmm9, %zmm8
	vextractf128	$1, %ymm8, %xmm9
	vaddps	%xmm9, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm9         # xmm9 = xmm8[1,0]
	vaddps	%xmm9, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm9            # xmm9 = xmm8[1,1,3,3]
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm27, %xmm8, %xmm8
	vshufpd	$1, %xmm5, %xmm5, %xmm9         # xmm9 = xmm5[1,0]
	vaddps	%xmm5, %xmm9, %xmm5
	vmovshdup	%xmm5, %xmm9            # xmm9 = xmm5[1,1,3,3]
	vaddss	%xmm5, %xmm9, %xmm5
	vmovss	%xmm5, 9152(%rsp)               # 4-byte Spill
	vmulps	128(%rsp), %zmm16, %zmm5        # 64-byte Folded Reload
	vfmadd231ps	896(%rsp), %zmm17, %zmm5 # 64-byte Folded Reload
                                        # zmm5 = (zmm17 * mem) + zmm5
	vextractf64x4	$1, %zmm5, %ymm9
	vaddps	%zmm9, %zmm5, %zmm5
	vextractf128	$1, %ymm5, %xmm9
	vaddps	%xmm5, %xmm9, %xmm9
	vmovaps	16064(%rsp), %zmm10             # 64-byte Reload
	vextractf64x4	$1, %zmm10, %ymm5
	vaddps	%zmm5, %zmm10, %zmm5
	vextractf128	$1, %ymm5, %xmm10
	vaddps	%xmm5, %xmm10, %xmm5
	vshufpd	$1, %xmm5, %xmm5, %xmm10        # xmm10 = xmm5[1,0]
	vaddps	%xmm5, %xmm10, %xmm5
	vmovshdup	%xmm5, %xmm10           # xmm10 = xmm5[1,1,3,3]
	vaddss	%xmm5, %xmm10, %xmm5
	vaddss	%xmm27, %xmm5, %xmm10
	vmovaps	3456(%rsp), %zmm16              # 64-byte Reload
	vmulps	%zmm28, %zmm16, %zmm5
	vmovaps	3520(%rsp), %zmm17              # 64-byte Reload
	vfmadd231ps	768(%rsp), %zmm17, %zmm5 # 64-byte Folded Reload
                                        # zmm5 = (zmm17 * mem) + zmm5
	vextractf64x4	$1, %zmm5, %ymm11
	vaddps	%zmm11, %zmm5, %zmm5
	vextractf128	$1, %ymm5, %xmm11
	vaddps	%xmm5, %xmm11, %xmm5
	vshufpd	$1, %xmm5, %xmm5, %xmm11        # xmm11 = xmm5[1,0]
	vaddps	%xmm5, %xmm11, %xmm5
	vmovshdup	%xmm5, %xmm11           # xmm11 = xmm5[1,1,3,3]
	vaddss	%xmm5, %xmm11, %xmm5
	vaddss	%xmm27, %xmm5, %xmm5
	vshufpd	$1, %xmm9, %xmm9, %xmm11        # xmm11 = xmm9[1,0]
	vaddps	%xmm11, %xmm9, %xmm9
	vmovshdup	%xmm9, %xmm11           # xmm11 = xmm9[1,1,3,3]
	vaddss	%xmm11, %xmm9, %xmm9
	vmovaps	2368(%rsp), %zmm22              # 64-byte Reload
	vmulps	%zmm22, %zmm1, %zmm11
	vfmadd231ps	2560(%rsp), %zmm15, %zmm11 # 64-byte Folded Reload
                                        # zmm11 = (zmm15 * mem) + zmm11
	vextractf64x4	$1, %zmm11, %ymm12
	vaddps	%zmm12, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm12
	vaddps	%xmm12, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm12      # xmm12 = xmm11[1,0]
	vaddps	%xmm12, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm12          # xmm12 = xmm11[1,1,3,3]
	vaddss	%xmm12, %xmm11, %xmm11
	vaddss	%xmm27, %xmm9, %xmm9
	vaddss	%xmm27, %xmm11, %xmm11
	vinsertps	$16, %xmm11, %xmm9, %xmm9 # xmm9 = xmm9[0],xmm11[0],xmm9[2,3]
	vmulps	1984(%rsp), %zmm1, %zmm11       # 64-byte Folded Reload
	vfmadd231ps	%zmm30, %zmm15, %zmm11  # zmm11 = (zmm15 * zmm30) + zmm11
	vextractf64x4	$1, %zmm11, %ymm12
	vaddps	%zmm12, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm12
	vaddps	%xmm12, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm12      # xmm12 = xmm11[1,0]
	vaddps	%xmm12, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm12          # xmm12 = xmm11[1,1,3,3]
	vaddss	%xmm12, %xmm11, %xmm11
	vaddss	%xmm27, %xmm11, %xmm11
	vinsertps	$32, %xmm11, %xmm9, %xmm9 # xmm9 = xmm9[0,1],xmm11[0],xmm9[3]
	vmovaps	832(%rsp), %zmm28               # 64-byte Reload
	vmulps	%zmm28, %zmm1, %zmm11
	vfmadd231ps	%zmm31, %zmm15, %zmm11  # zmm11 = (zmm15 * zmm31) + zmm11
	vextractf64x4	$1, %zmm11, %ymm12
	vaddps	%zmm12, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm12
	vaddps	%xmm12, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm12      # xmm12 = xmm11[1,0]
	vaddps	%xmm12, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm12          # xmm12 = xmm11[1,1,3,3]
	vaddss	%xmm12, %xmm11, %xmm11
	vaddss	%xmm27, %xmm11, %xmm11
	vinsertps	$48, %xmm11, %xmm9, %xmm9 # xmm9 = xmm9[0,1,2],xmm11[0]
	vmovaps	%xmm9, 35520(%rsp)              # 16-byte Spill
	vmulps	640(%rsp), %zmm1, %zmm9         # 64-byte Folded Reload
	vfmadd231ps	64(%rsp), %zmm15, %zmm9 # 64-byte Folded Reload
                                        # zmm9 = (zmm15 * mem) + zmm9
	vextractf64x4	$1, %zmm9, %ymm11
	vaddps	%zmm11, %zmm9, %zmm9
	vextractf128	$1, %ymm9, %xmm11
	vaddps	%xmm11, %xmm9, %xmm9
	vshufpd	$1, %xmm9, %xmm9, %xmm11        # xmm11 = xmm9[1,0]
	vaddps	%xmm11, %xmm9, %xmm9
	vmovshdup	%xmm9, %xmm11           # xmm11 = xmm9[1,1,3,3]
	vaddss	%xmm11, %xmm9, %xmm9
	vaddss	%xmm27, %xmm9, %xmm9
	vinsertps	$16, %xmm10, %xmm9, %xmm9 # xmm9 = xmm9[0],xmm10[0],xmm9[2,3]
	vinsertps	$32, %xmm8, %xmm9, %xmm8 # xmm8 = xmm9[0,1],xmm8[0],xmm9[3]
	vinsertps	$48, %xmm6, %xmm8, %xmm6 # xmm6 = xmm8[0,1,2],xmm6[0]
	vmovaps	31232(%rsp), %zmm9              # 64-byte Reload
	vextractf64x4	$1, %zmm9, %ymm8
	vaddps	%zmm8, %zmm9, %zmm8
	vextractf128	$1, %ymm8, %xmm9
	vaddps	%xmm9, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm9         # xmm9 = xmm8[1,0]
	vaddps	%xmm9, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm9            # xmm9 = xmm8[1,1,3,3]
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm27, %xmm8, %xmm8
	vinsertps	$16, %xmm8, %xmm7, %xmm7 # xmm7 = xmm7[0],xmm8[0],xmm7[2,3]
	vinsertps	$32, %xmm0, %xmm7, %xmm0 # xmm0 = xmm7[0,1],xmm0[0],xmm7[3]
	vmovaps	12160(%rsp), %zmm8              # 64-byte Reload
	vextractf64x4	$1, %zmm8, %ymm7
	vaddps	%zmm7, %zmm8, %zmm7
	vextractf128	$1, %ymm7, %xmm8
	vaddps	%xmm7, %xmm8, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm8         # xmm8 = xmm7[1,0]
	vaddps	%xmm7, %xmm8, %xmm7
	vmovshdup	%xmm7, %xmm8            # xmm8 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm27, %xmm7, %xmm7
	vinsertps	$48, %xmm7, %xmm0, %xmm7 # xmm7 = xmm0[0,1,2],xmm7[0]
	vmulps	3072(%rsp), %zmm1, %zmm0        # 64-byte Folded Reload
	vmovaps	1472(%rsp), %zmm24              # 64-byte Reload
	vfmadd231ps	%zmm24, %zmm15, %zmm0   # zmm0 = (zmm15 * zmm24) + zmm0
	vextractf64x4	$1, %zmm0, %ymm8
	vaddps	%zmm8, %zmm0, %zmm0
	vextractf128	$1, %ymm0, %xmm8
	vaddps	%xmm0, %xmm8, %xmm0
	vmovaps	36096(%rsp), %zmm9              # 64-byte Reload
	vextractf64x4	$1, %zmm9, %ymm8
	vaddps	%zmm8, %zmm9, %zmm8
	vextractf128	$1, %ymm8, %xmm9
	vaddps	%xmm9, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm9         # xmm9 = xmm8[1,0]
	vaddps	%xmm9, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm9            # xmm9 = xmm8[1,1,3,3]
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm27, %xmm8, %xmm8
	vshufpd	$1, %xmm0, %xmm0, %xmm9         # xmm9 = xmm0[1,0]
	vaddps	%xmm0, %xmm9, %xmm0
	vmovshdup	%xmm0, %xmm9            # xmm9 = xmm0[1,1,3,3]
	vaddss	%xmm0, %xmm9, %xmm0
	vmovss	%xmm0, 16064(%rsp)              # 4-byte Spill
	vmulps	2112(%rsp), %zmm1, %zmm0        # 64-byte Folded Reload
	vfmadd231ps	2496(%rsp), %zmm15, %zmm0 # 64-byte Folded Reload
                                        # zmm0 = (zmm15 * mem) + zmm0
	vextractf64x4	$1, %zmm0, %ymm9
	vaddps	%zmm9, %zmm0, %zmm0
	vmovaps	36160(%rsp), %zmm10             # 64-byte Reload
	vextractf64x4	$1, %zmm10, %ymm9
	vaddps	%zmm9, %zmm10, %zmm9
	vextractf128	$1, %ymm9, %xmm10
	vaddps	%xmm10, %xmm9, %xmm9
	vshufpd	$1, %xmm9, %xmm9, %xmm10        # xmm10 = xmm9[1,0]
	vaddps	%xmm10, %xmm9, %xmm9
	vmovshdup	%xmm9, %xmm10           # xmm10 = xmm9[1,1,3,3]
	vaddss	%xmm10, %xmm9, %xmm9
	vaddss	%xmm27, %xmm9, %xmm9
	vextractf128	$1, %ymm0, %xmm10
	vaddps	%xmm0, %xmm10, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm10        # xmm10 = xmm0[1,0]
	vaddps	%xmm0, %xmm10, %xmm0
	vmovshdup	%xmm0, %xmm10           # xmm10 = xmm0[1,1,3,3]
	vaddss	%xmm0, %xmm10, %xmm0
	vmovss	%xmm0, 12352(%rsp)              # 4-byte Spill
	vmulps	3136(%rsp), %zmm1, %zmm0        # 64-byte Folded Reload
	vfmadd231ps	1856(%rsp), %zmm15, %zmm0 # 64-byte Folded Reload
                                        # zmm0 = (zmm15 * mem) + zmm0
	vextractf64x4	$1, %zmm0, %ymm10
	vaddps	%zmm10, %zmm0, %zmm0
	vextractf128	$1, %ymm0, %xmm10
	vaddps	%xmm0, %xmm10, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm10        # xmm10 = xmm0[1,0]
	vaddps	%xmm0, %xmm10, %xmm0
	vmovshdup	%xmm0, %xmm10           # xmm10 = xmm0[1,1,3,3]
	vaddss	%xmm0, %xmm10, %xmm0
	vmovss	%xmm0, 12288(%rsp)              # 4-byte Spill
	vmovaps	36352(%rsp), %zmm10             # 64-byte Reload
	vextractf64x4	$1, %zmm10, %ymm0
	vaddps	%zmm0, %zmm10, %zmm0
	vextractf128	$1, %ymm0, %xmm10
	vaddps	%xmm0, %xmm10, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm10        # xmm10 = xmm0[1,0]
	vaddps	%xmm0, %xmm10, %xmm0
	vmovshdup	%xmm0, %xmm10           # xmm10 = xmm0[1,1,3,3]
	vaddss	%xmm0, %xmm10, %xmm0
	vaddss	%xmm27, %xmm0, %xmm10
	vmulps	%zmm21, %zmm1, %zmm0
	vfmadd231ps	%zmm20, %zmm15, %zmm0   # zmm0 = (zmm15 * zmm20) + zmm0
	vextractf64x4	$1, %zmm0, %ymm11
	vaddps	%zmm11, %zmm0, %zmm0
	vextractf128	$1, %ymm0, %xmm11
	vaddps	%xmm0, %xmm11, %xmm0
	vmulps	%zmm26, %zmm1, %zmm11
	vfmadd231ps	%zmm15, %zmm23, %zmm11  # zmm11 = (zmm23 * zmm15) + zmm11
	vextractf64x4	$1, %zmm11, %ymm12
	vaddps	%zmm12, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm12
	vaddps	%xmm12, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm12      # xmm12 = xmm11[1,0]
	vaddps	%xmm12, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm12          # xmm12 = xmm11[1,1,3,3]
	vaddss	%xmm12, %xmm11, %xmm11
	vaddss	%xmm27, %xmm11, %xmm11
	vshufpd	$1, %xmm0, %xmm0, %xmm12        # xmm12 = xmm0[1,0]
	vaddps	%xmm0, %xmm12, %xmm0
	vmovshdup	%xmm0, %xmm12           # xmm12 = xmm0[1,1,3,3]
	vaddss	%xmm0, %xmm12, %xmm0
	vmovaps	2176(%rsp), %zmm26              # 64-byte Reload
	vmulps	%zmm26, %zmm1, %zmm12
	vfmadd231ps	%zmm29, %zmm15, %zmm12  # zmm12 = (zmm15 * zmm29) + zmm12
	vextractf64x4	$1, %zmm12, %ymm13
	vaddps	%zmm13, %zmm12, %zmm12
	vextractf128	$1, %ymm12, %xmm13
	vaddps	%xmm13, %xmm12, %xmm12
	vshufpd	$1, %xmm12, %xmm12, %xmm13      # xmm13 = xmm12[1,0]
	vaddps	%xmm13, %xmm12, %xmm12
	vmovshdup	%xmm12, %xmm13          # xmm13 = xmm12[1,1,3,3]
	vaddss	%xmm13, %xmm12, %xmm12
	vmovaps	35904(%rsp), %zmm14             # 64-byte Reload
	vextractf64x4	$1, %zmm14, %ymm13
	vaddps	%zmm13, %zmm14, %zmm13
	vextractf128	$1, %ymm13, %xmm14
	vaddps	%xmm14, %xmm13, %xmm13
	vshufpd	$1, %xmm13, %xmm13, %xmm14      # xmm14 = xmm13[1,0]
	vaddps	%xmm14, %xmm13, %xmm13
	vmovshdup	%xmm13, %xmm14          # xmm14 = xmm13[1,1,3,3]
	vaddss	%xmm14, %xmm13, %xmm13
	vaddss	%xmm27, %xmm0, %xmm14
	vaddss	%xmm27, %xmm12, %xmm12
	vaddss	%xmm27, %xmm13, %xmm0
	vmovaps	%xmm0, 12480(%rsp)              # 16-byte Spill
	vmulps	%zmm19, %zmm1, %zmm0
	vmovaps	%zmm15, %zmm13
	vfmadd231ps	%zmm2, %zmm15, %zmm0    # zmm0 = (zmm15 * zmm2) + zmm0
	vmovaps	%zmm0, 35008(%rsp)              # 64-byte Spill
	vmovaps	576(%rsp), %zmm19               # 64-byte Reload
	vmulps	%zmm19, %zmm1, %zmm15
	vmovaps	1920(%rsp), %zmm20              # 64-byte Reload
	vfmadd231ps	%zmm20, %zmm13, %zmm15  # zmm15 = (zmm13 * zmm20) + zmm15
	vmovaps	%zmm15, 36096(%rsp)             # 64-byte Spill
	vmovaps	%zmm13, %zmm15
	vmulps	%zmm25, %zmm1, %zmm13
	vmovaps	1088(%rsp), %zmm25              # 64-byte Reload
	vfmadd231ps	%zmm25, %zmm15, %zmm13  # zmm13 = (zmm15 * zmm25) + zmm13
	vextractf64x4	$1, %zmm13, %ymm15
	vaddps	%zmm15, %zmm13, %zmm13
	vextractf128	$1, %ymm13, %xmm15
	vaddps	%xmm15, %xmm13, %xmm13
	vshufpd	$1, %xmm13, %xmm13, %xmm15      # xmm15 = xmm13[1,0]
	vaddps	%xmm15, %xmm13, %xmm13
	vmovshdup	%xmm13, %xmm15          # xmm15 = xmm13[1,1,3,3]
	vaddss	%xmm15, %xmm13, %xmm13
	vaddss	%xmm27, %xmm13, %xmm13
	vinsertps	$16, %xmm13, %xmm14, %xmm13 # xmm13 = xmm14[0],xmm13[0],xmm14[2,3]
	vinsertps	$32, %xmm12, %xmm13, %xmm12 # xmm12 = xmm13[0,1],xmm12[0],xmm13[3]
	vinsertps	$48, %xmm11, %xmm12, %xmm1 # xmm1 = xmm12[0,1,2],xmm11[0]
	vmovaps	%xmm1, 36352(%rsp)              # 16-byte Spill
	vinsertf128	$1, %xmm4, %ymm3, %ymm1
	vmovaps	%zmm1, 35904(%rsp)              # 64-byte Spill
	vinsertf128	$1, %xmm7, %ymm6, %ymm1
	vmovaps	%zmm1, 36160(%rsp)              # 64-byte Spill
	vmovaps	%zmm16, %zmm1
	vmulps	1600(%rsp), %zmm16, %zmm3       # 64-byte Folded Reload
	vmovaps	%zmm17, %zmm15
	vfmadd231ps	%zmm18, %zmm17, %zmm3   # zmm3 = (zmm17 * zmm18) + zmm3
	vextractf64x4	$1, %zmm3, %ymm4
	vaddps	%zmm4, %zmm3, %zmm3
	vextractf128	$1, %ymm3, %xmm4
	vaddps	%xmm4, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm4         # xmm4 = xmm3[1,0]
	vaddps	%xmm4, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm4            # xmm4 = xmm3[1,1,3,3]
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm27, %xmm3, %xmm3
	vinsertps	$16, %xmm10, %xmm3, %xmm3 # xmm3 = xmm3[0],xmm10[0],xmm3[2,3]
	vinsertps	$32, %xmm9, %xmm3, %xmm3 # xmm3 = xmm3[0,1],xmm9[0],xmm3[3]
	vinsertps	$48, %xmm8, %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],xmm8[0]
	vmovaps	16960(%rsp), %zmm6              # 64-byte Reload
	vextractf64x4	$1, %zmm6, %ymm4
	vaddps	%zmm4, %zmm6, %zmm4
	vextractf128	$1, %ymm4, %xmm6
	vaddps	%xmm6, %xmm4, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm6         # xmm6 = xmm4[1,0]
	vaddps	%xmm6, %xmm4, %xmm4
	vmovshdup	%xmm4, %xmm6            # xmm6 = xmm4[1,1,3,3]
	vaddss	%xmm6, %xmm4, %xmm4
	vaddss	%xmm27, %xmm4, %xmm4
	vinsertps	$16, %xmm4, %xmm5, %xmm4 # xmm4 = xmm5[0],xmm4[0],xmm5[2,3]
	vinsertps	$32, 12544(%rsp), %xmm4, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm4[0,1],mem[0],xmm4[3]
	vmovaps	35840(%rsp), %zmm5              # 64-byte Reload
	vextractf64x4	$1, %zmm5, %ymm4
	vaddps	%zmm4, %zmm5, %zmm4
	vextractf128	$1, %ymm4, %xmm5
	vaddps	%xmm5, %xmm4, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm5         # xmm5 = xmm4[1,0]
	vaddps	%xmm5, %xmm4, %xmm4
	vmovshdup	%xmm4, %xmm5            # xmm5 = xmm4[1,1,3,3]
	vaddss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm27, %xmm4, %xmm4
	vinsertps	$48, %xmm4, %xmm2, %xmm4 # xmm4 = xmm2[0,1,2],xmm4[0]
	vmulps	448(%rsp), %zmm16, %zmm2        # 64-byte Folded Reload
	vfmadd231ps	512(%rsp), %zmm17, %zmm2 # 64-byte Folded Reload
                                        # zmm2 = (zmm17 * mem) + zmm2
	vextractf64x4	$1, %zmm2, %ymm5
	vaddps	%zmm5, %zmm2, %zmm2
	vextractf128	$1, %ymm2, %xmm5
	vaddps	%xmm5, %xmm2, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm5         # xmm5 = xmm2[1,0]
	vaddps	%xmm5, %xmm2, %xmm5
	vmovaps	12416(%rsp), %zmm6              # 64-byte Reload
	vextractf64x4	$1, %zmm6, %ymm2
	vaddps	%zmm2, %zmm6, %zmm2
	vextractf128	$1, %ymm2, %xmm6
	vaddps	%xmm6, %xmm2, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm6         # xmm6 = xmm2[1,0]
	vaddps	%xmm6, %xmm2, %xmm2
	vmovshdup	%xmm2, %xmm6            # xmm6 = xmm2[1,1,3,3]
	vaddss	%xmm6, %xmm2, %xmm2
	vaddss	%xmm27, %xmm2, %xmm2
	vmulps	1280(%rsp), %zmm16, %zmm6       # 64-byte Folded Reload
	vmovaps	1216(%rsp), %zmm21              # 64-byte Reload
	vfmadd231ps	%zmm21, %zmm17, %zmm6   # zmm6 = (zmm17 * zmm21) + zmm6
	vextractf64x4	$1, %zmm6, %ymm7
	vaddps	%zmm7, %zmm6, %zmm6
	vextractf128	$1, %ymm6, %xmm7
	vaddps	%xmm7, %xmm6, %xmm6
	vshufpd	$1, %xmm6, %xmm6, %xmm7         # xmm7 = xmm6[1,0]
	vaddps	%xmm7, %xmm6, %xmm6
	vmovshdup	%xmm6, %xmm7            # xmm7 = xmm6[1,1,3,3]
	vaddss	%xmm7, %xmm6, %xmm6
	vaddss	%xmm27, %xmm6, %xmm7
	vmovshdup	%xmm5, %xmm6            # xmm6 = xmm5[1,1,3,3]
	vaddss	%xmm6, %xmm5, %xmm0
	vmovss	%xmm0, 3392(%rsp)               # 4-byte Spill
	vmovaps	2048(%rsp), %zmm23              # 64-byte Reload
	vmulps	%zmm23, %zmm16, %zmm5
	vmovaps	704(%rsp), %zmm18               # 64-byte Reload
	vfmadd231ps	%zmm18, %zmm17, %zmm5   # zmm5 = (zmm17 * zmm18) + zmm5
	vextractf64x4	$1, %zmm5, %ymm6
	vaddps	%zmm6, %zmm5, %zmm5
	vextractf128	$1, %ymm5, %xmm6
	vaddps	%xmm6, %xmm5, %xmm5
	vshufpd	$1, %xmm5, %xmm5, %xmm6         # xmm6 = xmm5[1,0]
	vaddps	%xmm6, %xmm5, %xmm5
	vmovaps	32064(%rsp), %zmm8              # 64-byte Reload
	vextractf64x4	$1, %zmm8, %ymm6
	vaddps	%zmm6, %zmm8, %zmm6
	vextractf128	$1, %ymm6, %xmm8
	vaddps	%xmm6, %xmm8, %xmm6
	vshufpd	$1, %xmm6, %xmm6, %xmm8         # xmm8 = xmm6[1,0]
	vaddps	%xmm6, %xmm8, %xmm6
	vmovshdup	%xmm6, %xmm8            # xmm8 = xmm6[1,1,3,3]
	vaddss	%xmm6, %xmm8, %xmm6
	vaddss	%xmm27, %xmm6, %xmm6
	vmovshdup	%xmm5, %xmm8            # xmm8 = xmm5[1,1,3,3]
	vaddss	%xmm5, %xmm8, %xmm0
	vmovss	%xmm0, 3328(%rsp)               # 4-byte Spill
	vmulps	256(%rsp), %zmm16, %zmm5        # 64-byte Folded Reload
	vmovaps	1408(%rsp), %zmm31              # 64-byte Reload
	vfmadd231ps	%zmm31, %zmm17, %zmm5   # zmm5 = (zmm17 * zmm31) + zmm5
	vextractf64x4	$1, %zmm5, %ymm8
	vaddps	%zmm8, %zmm5, %zmm5
	vextractf128	$1, %ymm5, %xmm8
	vaddps	%xmm5, %xmm8, %xmm5
	vmovaps	33728(%rsp), %zmm9              # 64-byte Reload
	vextractf64x4	$1, %zmm9, %ymm8
	vaddps	%zmm8, %zmm9, %zmm8
	vextractf128	$1, %ymm8, %xmm9
	vaddps	%xmm9, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm9         # xmm9 = xmm8[1,0]
	vaddps	%xmm9, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm9            # xmm9 = xmm8[1,1,3,3]
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm27, %xmm8, %xmm8
	vshufpd	$1, %xmm5, %xmm5, %xmm9         # xmm9 = xmm5[1,0]
	vaddps	%xmm5, %xmm9, %xmm5
	vmovshdup	%xmm5, %xmm9            # xmm9 = xmm5[1,1,3,3]
	vaddss	%xmm5, %xmm9, %xmm0
	vmovss	%xmm0, 16960(%rsp)              # 4-byte Spill
	vmulps	128(%rsp), %zmm16, %zmm5        # 64-byte Folded Reload
	vfmadd231ps	896(%rsp), %zmm17, %zmm5 # 64-byte Folded Reload
                                        # zmm5 = (zmm17 * mem) + zmm5
	vextractf64x4	$1, %zmm5, %ymm9
	vaddps	%zmm9, %zmm5, %zmm5
	vextractf128	$1, %ymm5, %xmm9
	vaddps	%xmm5, %xmm9, %xmm9
	vmovaps	16704(%rsp), %zmm10             # 64-byte Reload
	vextractf64x4	$1, %zmm10, %ymm5
	vaddps	%zmm5, %zmm10, %zmm5
	vextractf128	$1, %ymm5, %xmm10
	vaddps	%xmm5, %xmm10, %xmm5
	vshufpd	$1, %xmm5, %xmm5, %xmm10        # xmm10 = xmm5[1,0]
	vaddps	%xmm5, %xmm10, %xmm5
	vmovshdup	%xmm5, %xmm10           # xmm10 = xmm5[1,1,3,3]
	vaddss	%xmm5, %xmm10, %xmm5
	vaddss	%xmm27, %xmm5, %xmm10
	vmovaps	3584(%rsp), %zmm16              # 64-byte Reload
	vmulps	1152(%rsp), %zmm16, %zmm5       # 64-byte Folded Reload
	vmovaps	3648(%rsp), %zmm17              # 64-byte Reload
	vfmadd231ps	768(%rsp), %zmm17, %zmm5 # 64-byte Folded Reload
                                        # zmm5 = (zmm17 * mem) + zmm5
	vextractf64x4	$1, %zmm5, %ymm11
	vaddps	%zmm11, %zmm5, %zmm5
	vextractf128	$1, %ymm5, %xmm11
	vaddps	%xmm5, %xmm11, %xmm5
	vshufpd	$1, %xmm5, %xmm5, %xmm11        # xmm11 = xmm5[1,0]
	vaddps	%xmm5, %xmm11, %xmm5
	vmovshdup	%xmm5, %xmm11           # xmm11 = xmm5[1,1,3,3]
	vaddss	%xmm5, %xmm11, %xmm5
	vaddss	%xmm27, %xmm5, %xmm5
	vshufpd	$1, %xmm9, %xmm9, %xmm11        # xmm11 = xmm9[1,0]
	vaddps	%xmm11, %xmm9, %xmm9
	vmovshdup	%xmm9, %xmm11           # xmm11 = xmm9[1,1,3,3]
	vaddss	%xmm11, %xmm9, %xmm9
	vmulps	%zmm22, %zmm1, %zmm11
	vmovaps	2560(%rsp), %zmm30              # 64-byte Reload
	vfmadd231ps	%zmm30, %zmm15, %zmm11  # zmm11 = (zmm15 * zmm30) + zmm11
	vextractf64x4	$1, %zmm11, %ymm12
	vaddps	%zmm12, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm12
	vaddps	%xmm12, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm12      # xmm12 = xmm11[1,0]
	vaddps	%xmm12, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm12          # xmm12 = xmm11[1,1,3,3]
	vaddss	%xmm12, %xmm11, %xmm11
	vaddss	%xmm27, %xmm9, %xmm9
	vaddss	%xmm27, %xmm11, %xmm11
	vinsertps	$16, %xmm11, %xmm9, %xmm9 # xmm9 = xmm9[0],xmm11[0],xmm9[2,3]
	vmulps	1984(%rsp), %zmm1, %zmm11       # 64-byte Folded Reload
	vfmadd231ps	1024(%rsp), %zmm15, %zmm11 # 64-byte Folded Reload
                                        # zmm11 = (zmm15 * mem) + zmm11
	vextractf64x4	$1, %zmm11, %ymm12
	vaddps	%zmm12, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm12
	vaddps	%xmm12, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm12      # xmm12 = xmm11[1,0]
	vaddps	%xmm12, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm12          # xmm12 = xmm11[1,1,3,3]
	vaddss	%xmm12, %xmm11, %xmm11
	vaddss	%xmm27, %xmm11, %xmm11
	vinsertps	$32, %xmm11, %xmm9, %xmm9 # xmm9 = xmm9[0,1],xmm11[0],xmm9[3]
	vmulps	%zmm28, %zmm1, %zmm11
	vfmadd231ps	960(%rsp), %zmm15, %zmm11 # 64-byte Folded Reload
                                        # zmm11 = (zmm15 * mem) + zmm11
	vextractf64x4	$1, %zmm11, %ymm12
	vaddps	%zmm12, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm12
	vaddps	%xmm12, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm12      # xmm12 = xmm11[1,0]
	vaddps	%xmm12, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm12          # xmm12 = xmm11[1,1,3,3]
	vaddss	%xmm12, %xmm11, %xmm11
	vaddss	%xmm27, %xmm11, %xmm11
	vinsertps	$48, %xmm11, %xmm9, %xmm9 # xmm9 = xmm9[0,1,2],xmm11[0]
	vmovaps	%xmm9, 35840(%rsp)              # 16-byte Spill
	vmulps	640(%rsp), %zmm1, %zmm9         # 64-byte Folded Reload
	vfmadd231ps	64(%rsp), %zmm15, %zmm9 # 64-byte Folded Reload
                                        # zmm9 = (zmm15 * mem) + zmm9
	vextractf64x4	$1, %zmm9, %ymm11
	vaddps	%zmm11, %zmm9, %zmm9
	vextractf128	$1, %ymm9, %xmm11
	vaddps	%xmm11, %xmm9, %xmm9
	vshufpd	$1, %xmm9, %xmm9, %xmm11        # xmm11 = xmm9[1,0]
	vaddps	%xmm11, %xmm9, %xmm9
	vmovshdup	%xmm9, %xmm11           # xmm11 = xmm9[1,1,3,3]
	vaddss	%xmm11, %xmm9, %xmm9
	vaddss	%xmm27, %xmm9, %xmm9
	vinsertps	$16, %xmm10, %xmm9, %xmm9 # xmm9 = xmm9[0],xmm10[0],xmm9[2,3]
	vinsertps	$32, %xmm8, %xmm9, %xmm8 # xmm8 = xmm9[0,1],xmm8[0],xmm9[3]
	vinsertps	$48, %xmm6, %xmm8, %xmm6 # xmm6 = xmm8[0,1,2],xmm6[0]
	vmovaps	31296(%rsp), %zmm9              # 64-byte Reload
	vextractf64x4	$1, %zmm9, %ymm8
	vaddps	%zmm8, %zmm9, %zmm8
	vextractf128	$1, %ymm8, %xmm9
	vaddps	%xmm9, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm9         # xmm9 = xmm8[1,0]
	vaddps	%xmm9, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm9            # xmm9 = xmm8[1,1,3,3]
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm27, %xmm8, %xmm8
	vinsertps	$16, %xmm8, %xmm7, %xmm7 # xmm7 = xmm7[0],xmm8[0],xmm7[2,3]
	vinsertps	$32, %xmm2, %xmm7, %xmm2 # xmm2 = xmm7[0,1],xmm2[0],xmm7[3]
	vmovaps	12224(%rsp), %zmm8              # 64-byte Reload
	vextractf64x4	$1, %zmm8, %ymm7
	vaddps	%zmm7, %zmm8, %zmm7
	vextractf128	$1, %ymm7, %xmm8
	vaddps	%xmm7, %xmm8, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm8         # xmm8 = xmm7[1,0]
	vaddps	%xmm7, %xmm8, %xmm7
	vmovshdup	%xmm7, %xmm8            # xmm8 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm27, %xmm7, %xmm7
	vinsertps	$48, %xmm7, %xmm2, %xmm7 # xmm7 = xmm2[0,1,2],xmm7[0]
	vmovaps	3072(%rsp), %zmm29              # 64-byte Reload
	vmulps	%zmm29, %zmm1, %zmm2
	vfmadd231ps	%zmm24, %zmm15, %zmm2   # zmm2 = (zmm15 * zmm24) + zmm2
	vextractf64x4	$1, %zmm2, %ymm8
	vaddps	%zmm8, %zmm2, %zmm2
	vextractf128	$1, %ymm2, %xmm8
	vaddps	%xmm2, %xmm8, %xmm2
	vmovaps	17216(%rsp), %zmm9              # 64-byte Reload
	vextractf64x4	$1, %zmm9, %ymm8
	vaddps	%zmm8, %zmm9, %zmm8
	vextractf128	$1, %ymm8, %xmm9
	vaddps	%xmm9, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm9         # xmm9 = xmm8[1,0]
	vaddps	%xmm9, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm9            # xmm9 = xmm8[1,1,3,3]
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm27, %xmm8, %xmm8
	vshufpd	$1, %xmm2, %xmm2, %xmm9         # xmm9 = xmm2[1,0]
	vaddps	%xmm2, %xmm9, %xmm2
	vmovshdup	%xmm2, %xmm9            # xmm9 = xmm2[1,1,3,3]
	vaddss	%xmm2, %xmm9, %xmm0
	vmovss	%xmm0, 17216(%rsp)              # 4-byte Spill
	vmovaps	2112(%rsp), %zmm28              # 64-byte Reload
	vmulps	%zmm28, %zmm1, %zmm2
	vfmadd231ps	2496(%rsp), %zmm15, %zmm2 # 64-byte Folded Reload
                                        # zmm2 = (zmm15 * mem) + zmm2
	vextractf64x4	$1, %zmm2, %ymm9
	vaddps	%zmm9, %zmm2, %zmm2
	vmovaps	17408(%rsp), %zmm10             # 64-byte Reload
	vextractf64x4	$1, %zmm10, %ymm9
	vaddps	%zmm9, %zmm10, %zmm9
	vextractf128	$1, %ymm9, %xmm10
	vaddps	%xmm10, %xmm9, %xmm9
	vshufpd	$1, %xmm9, %xmm9, %xmm10        # xmm10 = xmm9[1,0]
	vaddps	%xmm10, %xmm9, %xmm9
	vmovshdup	%xmm9, %xmm10           # xmm10 = xmm9[1,1,3,3]
	vaddss	%xmm10, %xmm9, %xmm9
	vaddss	%xmm27, %xmm9, %xmm9
	vextractf128	$1, %ymm2, %xmm10
	vaddps	%xmm2, %xmm10, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm10        # xmm10 = xmm2[1,0]
	vaddps	%xmm2, %xmm10, %xmm2
	vmovshdup	%xmm2, %xmm10           # xmm10 = xmm2[1,1,3,3]
	vaddss	%xmm2, %xmm10, %xmm0
	vmovss	%xmm0, 17408(%rsp)              # 4-byte Spill
	vmulps	3136(%rsp), %zmm1, %zmm2        # 64-byte Folded Reload
	vfmadd231ps	1856(%rsp), %zmm15, %zmm2 # 64-byte Folded Reload
                                        # zmm2 = (zmm15 * mem) + zmm2
	vextractf64x4	$1, %zmm2, %ymm10
	vaddps	%zmm10, %zmm2, %zmm2
	vextractf128	$1, %ymm2, %xmm10
	vaddps	%xmm2, %xmm10, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm10        # xmm10 = xmm2[1,0]
	vaddps	%xmm2, %xmm10, %xmm2
	vmovshdup	%xmm2, %xmm10           # xmm10 = xmm2[1,1,3,3]
	vaddss	%xmm2, %xmm10, %xmm0
	vmovss	%xmm0, 16704(%rsp)              # 4-byte Spill
	vmovaps	17600(%rsp), %zmm10             # 64-byte Reload
	vextractf64x4	$1, %zmm10, %ymm2
	vaddps	%zmm2, %zmm10, %zmm2
	vextractf128	$1, %ymm2, %xmm10
	vaddps	%xmm2, %xmm10, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm10        # xmm10 = xmm2[1,0]
	vaddps	%xmm2, %xmm10, %xmm2
	vmovshdup	%xmm2, %xmm10           # xmm10 = xmm2[1,1,3,3]
	vaddss	%xmm2, %xmm10, %xmm2
	vaddss	%xmm27, %xmm2, %xmm10
	vmulps	192(%rsp), %zmm1, %zmm2         # 64-byte Folded Reload
	vfmadd231ps	1344(%rsp), %zmm15, %zmm2 # 64-byte Folded Reload
                                        # zmm2 = (zmm15 * mem) + zmm2
	vextractf64x4	$1, %zmm2, %ymm11
	vaddps	%zmm11, %zmm2, %zmm2
	vextractf128	$1, %ymm2, %xmm11
	vaddps	%xmm2, %xmm11, %xmm2
	vmulps	2432(%rsp), %zmm1, %zmm11       # 64-byte Folded Reload
	vfmadd231ps	1792(%rsp), %zmm15, %zmm11 # 64-byte Folded Reload
                                        # zmm11 = (zmm15 * mem) + zmm11
	vextractf64x4	$1, %zmm11, %ymm12
	vaddps	%zmm12, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm12
	vaddps	%xmm12, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm12      # xmm12 = xmm11[1,0]
	vaddps	%xmm12, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm12          # xmm12 = xmm11[1,1,3,3]
	vaddss	%xmm12, %xmm11, %xmm11
	vaddss	%xmm27, %xmm11, %xmm11
	vshufpd	$1, %xmm2, %xmm2, %xmm12        # xmm12 = xmm2[1,0]
	vaddps	%xmm2, %xmm12, %xmm2
	vmovshdup	%xmm2, %xmm12           # xmm12 = xmm2[1,1,3,3]
	vaddss	%xmm2, %xmm12, %xmm2
	vmulps	%zmm26, %zmm1, %zmm12
	vfmadd231ps	1728(%rsp), %zmm15, %zmm12 # 64-byte Folded Reload
                                        # zmm12 = (zmm15 * mem) + zmm12
	vextractf64x4	$1, %zmm12, %ymm13
	vaddps	%zmm13, %zmm12, %zmm12
	vextractf128	$1, %ymm12, %xmm13
	vaddps	%xmm13, %xmm12, %xmm12
	vshufpd	$1, %xmm12, %xmm12, %xmm13      # xmm13 = xmm12[1,0]
	vaddps	%xmm13, %xmm12, %xmm12
	vmovshdup	%xmm12, %xmm13          # xmm13 = xmm12[1,1,3,3]
	vaddss	%xmm13, %xmm12, %xmm12
	vmovaps	17024(%rsp), %zmm14             # 64-byte Reload
	vextractf64x4	$1, %zmm14, %ymm13
	vaddps	%zmm13, %zmm14, %zmm13
	vextractf128	$1, %ymm13, %xmm14
	vaddps	%xmm14, %xmm13, %xmm13
	vshufpd	$1, %xmm13, %xmm13, %xmm14      # xmm14 = xmm13[1,0]
	vaddps	%xmm14, %xmm13, %xmm13
	vmovshdup	%xmm13, %xmm14          # xmm14 = xmm13[1,1,3,3]
	vaddss	%xmm14, %xmm13, %xmm13
	vaddss	%xmm27, %xmm2, %xmm14
	vaddss	%xmm27, %xmm12, %xmm12
	vaddss	%xmm27, %xmm13, %xmm0
	vmovaps	%xmm0, 17024(%rsp)              # 16-byte Spill
	vmulps	2304(%rsp), %zmm1, %zmm0        # 64-byte Folded Reload
	vmovaps	%zmm15, %zmm13
	vmovaps	1536(%rsp), %zmm26              # 64-byte Reload
	vfmadd231ps	%zmm26, %zmm15, %zmm0   # zmm0 = (zmm15 * zmm26) + zmm0
	vmovaps	%zmm0, 31232(%rsp)              # 64-byte Spill
	vmulps	%zmm19, %zmm1, %zmm15
	vfmadd231ps	%zmm20, %zmm13, %zmm15  # zmm15 = (zmm13 * zmm20) + zmm15
	vmovaps	%zmm15, 31616(%rsp)             # 64-byte Spill
	vmovaps	%zmm13, %zmm15
	vmulps	2240(%rsp), %zmm1, %zmm13       # 64-byte Folded Reload
	vfmadd231ps	%zmm25, %zmm15, %zmm13  # zmm13 = (zmm15 * zmm25) + zmm13
	vextractf64x4	$1, %zmm13, %ymm15
	vaddps	%zmm15, %zmm13, %zmm13
	vextractf128	$1, %ymm13, %xmm15
	vaddps	%xmm15, %xmm13, %xmm13
	vshufpd	$1, %xmm13, %xmm13, %xmm15      # xmm15 = xmm13[1,0]
	vaddps	%xmm15, %xmm13, %xmm13
	vmovshdup	%xmm13, %xmm15          # xmm15 = xmm13[1,1,3,3]
	vaddss	%xmm15, %xmm13, %xmm13
	vaddss	%xmm27, %xmm13, %xmm13
	vinsertps	$16, %xmm13, %xmm14, %xmm13 # xmm13 = xmm14[0],xmm13[0],xmm14[2,3]
	vinsertps	$32, %xmm12, %xmm13, %xmm12 # xmm12 = xmm13[0,1],xmm12[0],xmm13[3]
	vinsertps	$48, %xmm11, %xmm12, %xmm0 # xmm0 = xmm12[0,1,2],xmm11[0]
	vmovaps	%xmm0, 33280(%rsp)              # 16-byte Spill
	vinsertf128	$1, %xmm4, %ymm3, %ymm0
	vmovaps	%zmm0, 31296(%rsp)              # 64-byte Spill
	vinsertf128	$1, %xmm7, %ymm6, %ymm0
	vmovaps	%zmm0, 32064(%rsp)              # 64-byte Spill
	vmovaps	1600(%rsp), %zmm2               # 64-byte Reload
	vmulps	%zmm2, %zmm16, %zmm3
	vmovaps	%zmm17, %zmm13
	vfmadd231ps	1664(%rsp), %zmm17, %zmm3 # 64-byte Folded Reload
                                        # zmm3 = (zmm17 * mem) + zmm3
	vextractf64x4	$1, %zmm3, %ymm4
	vaddps	%zmm4, %zmm3, %zmm3
	vextractf128	$1, %ymm3, %xmm4
	vaddps	%xmm4, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm4         # xmm4 = xmm3[1,0]
	vaddps	%xmm4, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm4            # xmm4 = xmm3[1,1,3,3]
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm27, %xmm3, %xmm3
	vinsertps	$16, %xmm10, %xmm3, %xmm3 # xmm3 = xmm3[0],xmm10[0],xmm3[2,3]
	vinsertps	$32, %xmm9, %xmm3, %xmm3 # xmm3 = xmm3[0,1],xmm9[0],xmm3[3]
	vinsertps	$48, %xmm8, %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],xmm8[0]
	vmovaps	17344(%rsp), %zmm6              # 64-byte Reload
	vextractf64x4	$1, %zmm6, %ymm4
	vaddps	%zmm4, %zmm6, %zmm4
	vextractf128	$1, %ymm4, %xmm6
	vaddps	%xmm6, %xmm4, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm6         # xmm6 = xmm4[1,0]
	vaddps	%xmm6, %xmm4, %xmm4
	vmovshdup	%xmm4, %xmm6            # xmm6 = xmm4[1,1,3,3]
	vaddss	%xmm6, %xmm4, %xmm4
	vaddss	%xmm27, %xmm4, %xmm4
	vinsertps	$16, %xmm4, %xmm5, %xmm4 # xmm4 = xmm5[0],xmm4[0],xmm5[2,3]
	vinsertps	$32, 12480(%rsp), %xmm4, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm4[0,1],mem[0],xmm4[3]
	vmovaps	36288(%rsp), %zmm5              # 64-byte Reload
	vextractf64x4	$1, %zmm5, %ymm4
	vaddps	%zmm4, %zmm5, %zmm4
	vextractf128	$1, %ymm4, %xmm5
	vaddps	%xmm5, %xmm4, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm5         # xmm5 = xmm4[1,0]
	vaddps	%xmm5, %xmm4, %xmm4
	vmovshdup	%xmm4, %xmm5            # xmm5 = xmm4[1,1,3,3]
	vaddss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm27, %xmm4, %xmm4
	vinsertps	$48, %xmm4, %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],xmm4[0]
	vmovaps	448(%rsp), %zmm24               # 64-byte Reload
	vmulps	%zmm24, %zmm16, %zmm4
	vmovaps	512(%rsp), %zmm25               # 64-byte Reload
	vfmadd231ps	%zmm25, %zmm17, %zmm4   # zmm4 = (zmm17 * zmm25) + zmm4
	vextractf64x4	$1, %zmm4, %ymm5
	vaddps	%zmm5, %zmm4, %zmm4
	vextractf128	$1, %ymm4, %xmm5
	vaddps	%xmm5, %xmm4, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm5         # xmm5 = xmm4[1,0]
	vaddps	%xmm5, %xmm4, %xmm4
	vmovaps	12672(%rsp), %zmm6              # 64-byte Reload
	vextractf64x4	$1, %zmm6, %ymm5
	vaddps	%zmm5, %zmm6, %zmm5
	vextractf128	$1, %ymm5, %xmm6
	vaddps	%xmm6, %xmm5, %xmm5
	vshufpd	$1, %xmm5, %xmm5, %xmm6         # xmm6 = xmm5[1,0]
	vaddps	%xmm6, %xmm5, %xmm5
	vmovshdup	%xmm5, %xmm6            # xmm6 = xmm5[1,1,3,3]
	vaddss	%xmm6, %xmm5, %xmm5
	vaddss	%xmm27, %xmm5, %xmm5
	vmovaps	1280(%rsp), %zmm19              # 64-byte Reload
	vmulps	%zmm19, %zmm16, %zmm6
	vfmadd231ps	%zmm21, %zmm17, %zmm6   # zmm6 = (zmm17 * zmm21) + zmm6
	vextractf64x4	$1, %zmm6, %ymm7
	vaddps	%zmm7, %zmm6, %zmm6
	vextractf128	$1, %ymm6, %xmm7
	vaddps	%xmm7, %xmm6, %xmm6
	vshufpd	$1, %xmm6, %xmm6, %xmm7         # xmm7 = xmm6[1,0]
	vaddps	%xmm7, %xmm6, %xmm6
	vmovshdup	%xmm6, %xmm7            # xmm7 = xmm6[1,1,3,3]
	vaddss	%xmm7, %xmm6, %xmm6
	vaddss	%xmm27, %xmm6, %xmm6
	vmovshdup	%xmm4, %xmm7            # xmm7 = xmm4[1,1,3,3]
	vaddss	%xmm7, %xmm4, %xmm4
	vmovss	%xmm4, 3520(%rsp)               # 4-byte Spill
	vmulps	%zmm23, %zmm16, %zmm4
	vfmadd231ps	%zmm18, %zmm17, %zmm4   # zmm4 = (zmm17 * zmm18) + zmm4
	vextractf64x4	$1, %zmm4, %ymm7
	vaddps	%zmm7, %zmm4, %zmm4
	vextractf128	$1, %ymm4, %xmm7
	vaddps	%xmm7, %xmm4, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm7         # xmm7 = xmm4[1,0]
	vaddps	%xmm7, %xmm4, %xmm4
	vmovaps	32832(%rsp), %zmm8              # 64-byte Reload
	vextractf64x4	$1, %zmm8, %ymm7
	vaddps	%zmm7, %zmm8, %zmm7
	vextractf128	$1, %ymm7, %xmm8
	vaddps	%xmm7, %xmm8, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm8         # xmm8 = xmm7[1,0]
	vaddps	%xmm7, %xmm8, %xmm7
	vmovshdup	%xmm7, %xmm8            # xmm8 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm27, %xmm7, %xmm7
	vmovshdup	%xmm4, %xmm8            # xmm8 = xmm4[1,1,3,3]
	vaddss	%xmm4, %xmm8, %xmm4
	vmovss	%xmm4, 3456(%rsp)               # 4-byte Spill
	vmulps	256(%rsp), %zmm16, %zmm4        # 64-byte Folded Reload
	vfmadd231ps	%zmm31, %zmm17, %zmm4   # zmm4 = (zmm17 * zmm31) + zmm4
	vextractf64x4	$1, %zmm4, %ymm8
	vaddps	%zmm8, %zmm4, %zmm4
	vextractf128	$1, %ymm4, %xmm8
	vaddps	%xmm4, %xmm8, %xmm4
	vmovaps	34176(%rsp), %zmm9              # 64-byte Reload
	vextractf64x4	$1, %zmm9, %ymm8
	vaddps	%zmm8, %zmm9, %zmm8
	vextractf128	$1, %ymm8, %xmm9
	vaddps	%xmm9, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm9         # xmm9 = xmm8[1,0]
	vaddps	%xmm9, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm9            # xmm9 = xmm8[1,1,3,3]
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm27, %xmm8, %xmm8
	vshufpd	$1, %xmm4, %xmm4, %xmm9         # xmm9 = xmm4[1,0]
	vaddps	%xmm4, %xmm9, %xmm4
	vmovshdup	%xmm4, %xmm9            # xmm9 = xmm4[1,1,3,3]
	vaddss	%xmm4, %xmm9, %xmm4
	vmovss	%xmm4, 17600(%rsp)              # 4-byte Spill
	vmovaps	128(%rsp), %zmm23               # 64-byte Reload
	vmulps	%zmm23, %zmm16, %zmm4
	vmovaps	896(%rsp), %zmm31               # 64-byte Reload
	vfmadd231ps	%zmm31, %zmm17, %zmm4   # zmm4 = (zmm17 * zmm31) + zmm4
	vextractf64x4	$1, %zmm4, %ymm9
	vaddps	%zmm9, %zmm4, %zmm4
	vextractf128	$1, %ymm4, %xmm9
	vaddps	%xmm4, %xmm9, %xmm9
	vmovaps	35648(%rsp), %zmm10             # 64-byte Reload
	vextractf64x4	$1, %zmm10, %ymm4
	vaddps	%zmm4, %zmm10, %zmm4
	vextractf128	$1, %ymm4, %xmm10
	vaddps	%xmm4, %xmm10, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm10        # xmm10 = xmm4[1,0]
	vaddps	%xmm4, %xmm10, %xmm4
	vmovshdup	%xmm4, %xmm10           # xmm10 = xmm4[1,1,3,3]
	vaddss	%xmm4, %xmm10, %xmm4
	vaddss	%xmm27, %xmm4, %xmm10
	vmovaps	4736(%rsp), %zmm17              # 64-byte Reload
	vmovaps	1152(%rsp), %zmm20              # 64-byte Reload
	vmulps	%zmm20, %zmm17, %zmm4
	vmovaps	4800(%rsp), %zmm18              # 64-byte Reload
	vmovaps	768(%rsp), %zmm22               # 64-byte Reload
	vfmadd231ps	%zmm22, %zmm18, %zmm4   # zmm4 = (zmm18 * zmm22) + zmm4
	vextractf64x4	$1, %zmm4, %ymm11
	vaddps	%zmm11, %zmm4, %zmm4
	vextractf128	$1, %ymm4, %xmm11
	vaddps	%xmm4, %xmm11, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm11        # xmm11 = xmm4[1,0]
	vaddps	%xmm4, %xmm11, %xmm4
	vmovshdup	%xmm4, %xmm11           # xmm11 = xmm4[1,1,3,3]
	vaddss	%xmm4, %xmm11, %xmm4
	vaddss	%xmm27, %xmm4, %xmm4
	vshufpd	$1, %xmm9, %xmm9, %xmm11        # xmm11 = xmm9[1,0]
	vaddps	%xmm11, %xmm9, %xmm9
	vmovshdup	%xmm9, %xmm11           # xmm11 = xmm9[1,1,3,3]
	vaddss	%xmm11, %xmm9, %xmm9
	vmovaps	2368(%rsp), %zmm21              # 64-byte Reload
	vmulps	%zmm21, %zmm16, %zmm11
	vfmadd231ps	%zmm30, %zmm13, %zmm11  # zmm11 = (zmm13 * zmm30) + zmm11
	vextractf64x4	$1, %zmm11, %ymm12
	vaddps	%zmm12, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm12
	vaddps	%xmm12, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm12      # xmm12 = xmm11[1,0]
	vaddps	%xmm12, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm12          # xmm12 = xmm11[1,1,3,3]
	vaddss	%xmm12, %xmm11, %xmm11
	vaddss	%xmm27, %xmm9, %xmm9
	vaddss	%xmm27, %xmm11, %xmm11
	vinsertps	$16, %xmm11, %xmm9, %xmm9 # xmm9 = xmm9[0],xmm11[0],xmm9[2,3]
	vmovaps	1984(%rsp), %zmm30              # 64-byte Reload
	vmulps	%zmm30, %zmm16, %zmm11
	vfmadd231ps	1024(%rsp), %zmm13, %zmm11 # 64-byte Folded Reload
                                        # zmm11 = (zmm13 * mem) + zmm11
	vextractf64x4	$1, %zmm11, %ymm12
	vaddps	%zmm12, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm12
	vaddps	%xmm12, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm12      # xmm12 = xmm11[1,0]
	vaddps	%xmm12, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm12          # xmm12 = xmm11[1,1,3,3]
	vaddss	%xmm12, %xmm11, %xmm11
	vaddss	%xmm27, %xmm11, %xmm11
	vinsertps	$32, %xmm11, %xmm9, %xmm9 # xmm9 = xmm9[0,1],xmm11[0],xmm9[3]
	vmulps	832(%rsp), %zmm16, %zmm11       # 64-byte Folded Reload
	vfmadd231ps	960(%rsp), %zmm13, %zmm11 # 64-byte Folded Reload
                                        # zmm11 = (zmm13 * mem) + zmm11
	vextractf64x4	$1, %zmm11, %ymm12
	vaddps	%zmm12, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm12
	vaddps	%xmm12, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm12      # xmm12 = xmm11[1,0]
	vaddps	%xmm12, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm12          # xmm12 = xmm11[1,1,3,3]
	vaddss	%xmm12, %xmm11, %xmm11
	vaddss	%xmm27, %xmm11, %xmm11
	vinsertps	$48, %xmm11, %xmm9, %xmm9 # xmm9 = xmm9[0,1,2],xmm11[0]
	vmovaps	%xmm9, 12672(%rsp)              # 16-byte Spill
	vmulps	640(%rsp), %zmm16, %zmm9        # 64-byte Folded Reload
	vfmadd231ps	64(%rsp), %zmm13, %zmm9 # 64-byte Folded Reload
                                        # zmm9 = (zmm13 * mem) + zmm9
	vextractf64x4	$1, %zmm9, %ymm11
	vaddps	%zmm11, %zmm9, %zmm9
	vextractf128	$1, %ymm9, %xmm11
	vaddps	%xmm11, %xmm9, %xmm9
	vshufpd	$1, %xmm9, %xmm9, %xmm11        # xmm11 = xmm9[1,0]
	vaddps	%xmm11, %xmm9, %xmm9
	vmovshdup	%xmm9, %xmm11           # xmm11 = xmm9[1,1,3,3]
	vaddss	%xmm11, %xmm9, %xmm9
	vaddss	%xmm27, %xmm9, %xmm9
	vinsertps	$16, %xmm10, %xmm9, %xmm9 # xmm9 = xmm9[0],xmm10[0],xmm9[2,3]
	vinsertps	$32, %xmm8, %xmm9, %xmm8 # xmm8 = xmm9[0,1],xmm8[0],xmm9[3]
	vinsertps	$48, %xmm7, %xmm8, %xmm8 # xmm8 = xmm8[0,1,2],xmm7[0]
	vmovaps	13760(%rsp), %zmm9              # 64-byte Reload
	vextractf64x4	$1, %zmm9, %ymm7
	vaddps	%zmm7, %zmm9, %zmm7
	vextractf128	$1, %ymm7, %xmm9
	vaddps	%xmm7, %xmm9, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm9         # xmm9 = xmm7[1,0]
	vaddps	%xmm7, %xmm9, %xmm7
	vmovshdup	%xmm7, %xmm9            # xmm9 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm9, %xmm7
	vaddss	%xmm27, %xmm7, %xmm7
	vinsertps	$16, %xmm7, %xmm6, %xmm6 # xmm6 = xmm6[0],xmm7[0],xmm6[2,3]
	vinsertps	$32, %xmm5, %xmm6, %xmm5 # xmm5 = xmm6[0,1],xmm5[0],xmm6[3]
	vmovaps	9088(%rsp), %zmm7               # 64-byte Reload
	vextractf64x4	$1, %zmm7, %ymm6
	vaddps	%zmm6, %zmm7, %zmm6
	vextractf128	$1, %ymm6, %xmm7
	vaddps	%xmm7, %xmm6, %xmm6
	vshufpd	$1, %xmm6, %xmm6, %xmm7         # xmm7 = xmm6[1,0]
	vaddps	%xmm7, %xmm6, %xmm6
	vmovshdup	%xmm6, %xmm7            # xmm7 = xmm6[1,1,3,3]
	vaddss	%xmm7, %xmm6, %xmm6
	vaddss	%xmm27, %xmm6, %xmm6
	vinsertps	$48, %xmm6, %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],xmm6[0]
	vmulps	%zmm29, %zmm16, %zmm6
	vfmadd231ps	1472(%rsp), %zmm13, %zmm6 # 64-byte Folded Reload
                                        # zmm6 = (zmm13 * mem) + zmm6
	vextractf64x4	$1, %zmm6, %ymm7
	vaddps	%zmm7, %zmm6, %zmm6
	vextractf128	$1, %ymm6, %xmm7
	vaddps	%xmm7, %xmm6, %xmm6
	vmovaps	17664(%rsp), %zmm9              # 64-byte Reload
	vextractf64x4	$1, %zmm9, %ymm7
	vaddps	%zmm7, %zmm9, %zmm7
	vextractf128	$1, %ymm7, %xmm9
	vaddps	%xmm7, %xmm9, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm9         # xmm9 = xmm7[1,0]
	vaddps	%xmm7, %xmm9, %xmm7
	vmovshdup	%xmm7, %xmm9            # xmm9 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm9, %xmm7
	vaddss	%xmm27, %xmm7, %xmm9
	vshufpd	$1, %xmm6, %xmm6, %xmm7         # xmm7 = xmm6[1,0]
	vaddps	%xmm7, %xmm6, %xmm6
	vmovshdup	%xmm6, %xmm7            # xmm7 = xmm6[1,1,3,3]
	vaddss	%xmm7, %xmm6, %xmm6
	vmovss	%xmm6, 17664(%rsp)              # 4-byte Spill
	vmulps	%zmm28, %zmm16, %zmm6
	vfmadd231ps	2496(%rsp), %zmm13, %zmm6 # 64-byte Folded Reload
                                        # zmm6 = (zmm13 * mem) + zmm6
	vextractf64x4	$1, %zmm6, %ymm7
	vaddps	%zmm7, %zmm6, %zmm6
	vmovaps	17792(%rsp), %zmm10             # 64-byte Reload
	vextractf64x4	$1, %zmm10, %ymm7
	vaddps	%zmm7, %zmm10, %zmm7
	vextractf128	$1, %ymm7, %xmm10
	vaddps	%xmm7, %xmm10, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm10        # xmm10 = xmm7[1,0]
	vaddps	%xmm7, %xmm10, %xmm7
	vmovshdup	%xmm7, %xmm10           # xmm10 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm10, %xmm7
	vaddss	%xmm27, %xmm7, %xmm10
	vextractf128	$1, %ymm6, %xmm7
	vaddps	%xmm7, %xmm6, %xmm6
	vshufpd	$1, %xmm6, %xmm6, %xmm7         # xmm7 = xmm6[1,0]
	vaddps	%xmm7, %xmm6, %xmm6
	vmovshdup	%xmm6, %xmm7            # xmm7 = xmm6[1,1,3,3]
	vaddss	%xmm7, %xmm6, %xmm6
	vmovss	%xmm6, 17792(%rsp)              # 4-byte Spill
	vmulps	3136(%rsp), %zmm16, %zmm6       # 64-byte Folded Reload
	vfmadd231ps	1856(%rsp), %zmm13, %zmm6 # 64-byte Folded Reload
                                        # zmm6 = (zmm13 * mem) + zmm6
	vextractf64x4	$1, %zmm6, %ymm7
	vaddps	%zmm7, %zmm6, %zmm6
	vextractf128	$1, %ymm6, %xmm7
	vaddps	%xmm7, %xmm6, %xmm6
	vshufpd	$1, %xmm6, %xmm6, %xmm7         # xmm7 = xmm6[1,0]
	vaddps	%xmm7, %xmm6, %xmm6
	vmovshdup	%xmm6, %xmm7            # xmm7 = xmm6[1,1,3,3]
	vaddss	%xmm7, %xmm6, %xmm6
	vmovss	%xmm6, 17344(%rsp)              # 4-byte Spill
	vmovaps	17920(%rsp), %zmm11             # 64-byte Reload
	vextractf64x4	$1, %zmm11, %ymm7
	vaddps	%zmm7, %zmm11, %zmm7
	vextractf128	$1, %ymm7, %xmm11
	vaddps	%xmm7, %xmm11, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm11        # xmm11 = xmm7[1,0]
	vaddps	%xmm7, %xmm11, %xmm7
	vmovshdup	%xmm7, %xmm11           # xmm11 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm11, %xmm7
	vaddss	%xmm27, %xmm7, %xmm11
	vmulps	192(%rsp), %zmm16, %zmm7        # 64-byte Folded Reload
	vmovaps	%zmm13, %zmm6
	vfmadd231ps	1344(%rsp), %zmm13, %zmm7 # 64-byte Folded Reload
                                        # zmm7 = (zmm13 * mem) + zmm7
	vextractf64x4	$1, %zmm7, %ymm12
	vaddps	%zmm12, %zmm7, %zmm7
	vextractf128	$1, %ymm7, %xmm12
	vaddps	%xmm7, %xmm12, %xmm7
	vmovaps	2432(%rsp), %zmm28              # 64-byte Reload
	vmulps	%zmm28, %zmm16, %zmm12
	vfmadd231ps	1792(%rsp), %zmm13, %zmm12 # 64-byte Folded Reload
                                        # zmm12 = (zmm13 * mem) + zmm12
	vextractf64x4	$1, %zmm12, %ymm13
	vaddps	%zmm13, %zmm12, %zmm12
	vextractf128	$1, %ymm12, %xmm13
	vaddps	%xmm13, %xmm12, %xmm12
	vshufpd	$1, %xmm12, %xmm12, %xmm13      # xmm13 = xmm12[1,0]
	vaddps	%xmm13, %xmm12, %xmm12
	vmovshdup	%xmm12, %xmm13          # xmm13 = xmm12[1,1,3,3]
	vaddss	%xmm13, %xmm12, %xmm12
	vaddss	%xmm27, %xmm12, %xmm12
	vshufpd	$1, %xmm7, %xmm7, %xmm13        # xmm13 = xmm7[1,0]
	vaddps	%xmm7, %xmm13, %xmm7
	vmovshdup	%xmm7, %xmm13           # xmm13 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm13, %xmm7
	vmulps	2176(%rsp), %zmm16, %zmm13      # 64-byte Folded Reload
	vfmadd231ps	1728(%rsp), %zmm6, %zmm13 # 64-byte Folded Reload
                                        # zmm13 = (zmm6 * mem) + zmm13
	vextractf64x4	$1, %zmm13, %ymm14
	vaddps	%zmm14, %zmm13, %zmm13
	vextractf128	$1, %ymm13, %xmm14
	vaddps	%xmm14, %xmm13, %xmm13
	vshufpd	$1, %xmm13, %xmm13, %xmm14      # xmm14 = xmm13[1,0]
	vaddps	%xmm14, %xmm13, %xmm13
	vmovshdup	%xmm13, %xmm14          # xmm14 = xmm13[1,1,3,3]
	vaddss	%xmm14, %xmm13, %xmm13
	vmovaps	36032(%rsp), %zmm15             # 64-byte Reload
	vextractf64x4	$1, %zmm15, %ymm14
	vaddps	%zmm14, %zmm15, %zmm14
	vextractf128	$1, %ymm14, %xmm15
	vaddps	%xmm15, %xmm14, %xmm14
	vshufpd	$1, %xmm14, %xmm14, %xmm15      # xmm15 = xmm14[1,0]
	vaddps	%xmm15, %xmm14, %xmm14
	vmovshdup	%xmm14, %xmm15          # xmm15 = xmm14[1,1,3,3]
	vaddss	%xmm15, %xmm14, %xmm14
	vaddss	%xmm27, %xmm7, %xmm15
	vaddss	%xmm27, %xmm13, %xmm13
	vaddss	%xmm27, %xmm14, %xmm7
	vmulps	2304(%rsp), %zmm16, %zmm14      # 64-byte Folded Reload
	vfmadd231ps	%zmm26, %zmm6, %zmm14   # zmm14 = (zmm6 * zmm26) + zmm14
	vmovaps	%zmm14, 12416(%rsp)             # 64-byte Spill
	vmulps	576(%rsp), %zmm16, %zmm14       # 64-byte Folded Reload
	vfmadd231ps	1920(%rsp), %zmm6, %zmm14 # 64-byte Folded Reload
                                        # zmm14 = (zmm6 * mem) + zmm14
	vmovaps	%zmm14, 31168(%rsp)             # 64-byte Spill
	vmulps	2240(%rsp), %zmm16, %zmm14      # 64-byte Folded Reload
	vfmadd231ps	1088(%rsp), %zmm6, %zmm14 # 64-byte Folded Reload
                                        # zmm14 = (zmm6 * mem) + zmm14
	vextractf64x4	$1, %zmm14, %ymm16
	vaddps	%zmm16, %zmm14, %zmm14
	vextractf32x4	$1, %ymm14, %xmm16
	vaddps	%xmm16, %xmm14, %xmm14
	vshufpd	$1, %xmm14, %xmm14, %xmm16      # xmm16 = xmm14[1,0]
	vaddps	%xmm16, %xmm14, %xmm14
	vmovshdup	%xmm14, %xmm16          # xmm16 = xmm14[1,1,3,3]
	vaddss	%xmm16, %xmm14, %xmm14
	vaddss	%xmm27, %xmm14, %xmm14
	vinsertps	$16, %xmm14, %xmm15, %xmm14 # xmm14 = xmm15[0],xmm14[0],xmm15[2,3]
	vinsertps	$32, %xmm13, %xmm14, %xmm13 # xmm13 = xmm14[0,1],xmm13[0],xmm14[3]
	vinsertps	$48, %xmm12, %xmm13, %xmm1 # xmm1 = xmm13[0,1,2],xmm12[0]
	vmovaps	%xmm1, 12544(%rsp)              # 16-byte Spill
	vinsertf128	$1, %xmm0, %ymm3, %ymm0
	vmovaps	%zmm0, 31104(%rsp)              # 64-byte Spill
	vinsertf128	$1, %xmm5, %ymm8, %ymm0
	vmovaps	%zmm0, 12480(%rsp)              # 64-byte Spill
	vmovaps	%zmm17, %zmm1
	vmulps	%zmm2, %zmm17, %zmm0
	vfmadd231ps	1664(%rsp), %zmm18, %zmm0 # 64-byte Folded Reload
                                        # zmm0 = (zmm18 * mem) + zmm0
	vextractf64x4	$1, %zmm0, %ymm3
	vaddps	%zmm3, %zmm0, %zmm0
	vextractf128	$1, %ymm0, %xmm3
	vaddps	%xmm3, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm3         # xmm3 = xmm0[1,0]
	vaddps	%xmm3, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm3            # xmm3 = xmm0[1,1,3,3]
	vaddss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm27, %xmm0, %xmm0
	vinsertps	$16, %xmm11, %xmm0, %xmm0 # xmm0 = xmm0[0],xmm11[0],xmm0[2,3]
	vinsertps	$32, %xmm10, %xmm0, %xmm0 # xmm0 = xmm0[0,1],xmm10[0],xmm0[3]
	vinsertps	$48, %xmm9, %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],xmm9[0]
	vmovaps	%ymm0, 3648(%rsp)               # 32-byte Spill
	vmovaps	17728(%rsp), %zmm0              # 64-byte Reload
	vextractf64x4	$1, %zmm0, %ymm3
	vaddps	%zmm3, %zmm0, %zmm3
	vextractf128	$1, %ymm3, %xmm5
	vaddps	%xmm5, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm5         # xmm5 = xmm3[1,0]
	vaddps	%xmm5, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm5            # xmm5 = xmm3[1,1,3,3]
	vaddss	%xmm5, %xmm3, %xmm3
	vaddss	%xmm27, %xmm3, %xmm3
	vinsertps	$16, %xmm3, %xmm4, %xmm3 # xmm3 = xmm4[0],xmm3[0],xmm4[2,3]
	vinsertps	$32, 17024(%rsp), %xmm3, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm3[0,1],mem[0],xmm3[3]
	vmovaps	17472(%rsp), %zmm0              # 64-byte Reload
	vextractf64x4	$1, %zmm0, %ymm3
	vaddps	%zmm3, %zmm0, %zmm3
	vextractf128	$1, %ymm3, %xmm4
	vaddps	%xmm4, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm4         # xmm4 = xmm3[1,0]
	vaddps	%xmm4, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm4            # xmm4 = xmm3[1,1,3,3]
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm27, %xmm3, %xmm3
	vinsertps	$48, %xmm3, %xmm2, %xmm0 # xmm0 = xmm2[0,1,2],xmm3[0]
	vmovaps	%xmm0, 3584(%rsp)               # 16-byte Spill
	vmulps	%zmm24, %zmm17, %zmm3
	vmovaps	%zmm18, %zmm0
	vfmadd231ps	%zmm25, %zmm18, %zmm3   # zmm3 = (zmm18 * zmm25) + zmm3
	vextractf64x4	$1, %zmm3, %ymm4
	vaddps	%zmm4, %zmm3, %zmm3
	vextractf128	$1, %ymm3, %xmm4
	vaddps	%xmm4, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm4         # xmm4 = xmm3[1,0]
	vaddps	%xmm4, %xmm3, %xmm3
	vmovaps	13056(%rsp), %zmm2              # 64-byte Reload
	vextractf64x4	$1, %zmm2, %ymm4
	vaddps	%zmm4, %zmm2, %zmm4
	vextractf128	$1, %ymm4, %xmm5
	vaddps	%xmm5, %xmm4, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm5         # xmm5 = xmm4[1,0]
	vaddps	%xmm5, %xmm4, %xmm4
	vmovshdup	%xmm4, %xmm5            # xmm5 = xmm4[1,1,3,3]
	vaddss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm27, %xmm4, %xmm8
	vmulps	%zmm19, %zmm17, %zmm4
	vfmadd231ps	1216(%rsp), %zmm18, %zmm4 # 64-byte Folded Reload
                                        # zmm4 = (zmm18 * mem) + zmm4
	vextractf64x4	$1, %zmm4, %ymm5
	vaddps	%zmm5, %zmm4, %zmm4
	vextractf128	$1, %ymm4, %xmm5
	vaddps	%xmm5, %xmm4, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm5         # xmm5 = xmm4[1,0]
	vaddps	%xmm5, %xmm4, %xmm4
	vmovshdup	%xmm4, %xmm5            # xmm5 = xmm4[1,1,3,3]
	vaddss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm27, %xmm4, %xmm10
	vmovshdup	%xmm3, %xmm4            # xmm4 = xmm3[1,1,3,3]
	vaddss	%xmm4, %xmm3, %xmm2
	vmovss	%xmm2, 17920(%rsp)              # 4-byte Spill
	vmulps	2048(%rsp), %zmm17, %zmm3       # 64-byte Folded Reload
	vfmadd231ps	704(%rsp), %zmm18, %zmm3 # 64-byte Folded Reload
                                        # zmm3 = (zmm18 * mem) + zmm3
	vextractf64x4	$1, %zmm3, %ymm5
	vaddps	%zmm5, %zmm3, %zmm3
	vextractf128	$1, %ymm3, %xmm5
	vaddps	%xmm5, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm5         # xmm5 = xmm3[1,0]
	vaddps	%xmm5, %xmm3, %xmm3
	vmovaps	15424(%rsp), %zmm2              # 64-byte Reload
	vextractf64x4	$1, %zmm2, %ymm5
	vaddps	%zmm5, %zmm2, %zmm5
	vextractf128	$1, %ymm5, %xmm9
	vaddps	%xmm5, %xmm9, %xmm5
	vshufpd	$1, %xmm5, %xmm5, %xmm9         # xmm9 = xmm5[1,0]
	vaddps	%xmm5, %xmm9, %xmm5
	vmovshdup	%xmm5, %xmm9            # xmm9 = xmm5[1,1,3,3]
	vaddss	%xmm5, %xmm9, %xmm5
	vaddss	%xmm27, %xmm5, %xmm9
	vmovshdup	%xmm3, %xmm5            # xmm5 = xmm3[1,1,3,3]
	vaddss	%xmm5, %xmm3, %xmm2
	vmovss	%xmm2, 17728(%rsp)              # 4-byte Spill
	vmulps	256(%rsp), %zmm17, %zmm3        # 64-byte Folded Reload
	vfmadd231ps	1408(%rsp), %zmm18, %zmm3 # 64-byte Folded Reload
                                        # zmm3 = (zmm18 * mem) + zmm3
	vextractf64x4	$1, %zmm3, %ymm11
	vaddps	%zmm11, %zmm3, %zmm3
	vextractf128	$1, %ymm3, %xmm11
	vaddps	%xmm3, %xmm11, %xmm3
	vmovaps	16768(%rsp), %zmm2              # 64-byte Reload
	vextractf64x4	$1, %zmm2, %ymm11
	vaddps	%zmm11, %zmm2, %zmm11
	vextractf128	$1, %ymm11, %xmm12
	vaddps	%xmm12, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm12      # xmm12 = xmm11[1,0]
	vaddps	%xmm12, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm12          # xmm12 = xmm11[1,1,3,3]
	vaddss	%xmm12, %xmm11, %xmm11
	vaddss	%xmm27, %xmm11, %xmm11
	vshufpd	$1, %xmm3, %xmm3, %xmm12        # xmm12 = xmm3[1,0]
	vaddps	%xmm3, %xmm12, %xmm3
	vmovshdup	%xmm3, %xmm12           # xmm12 = xmm3[1,1,3,3]
	vaddss	%xmm3, %xmm12, %xmm2
	vmovss	%xmm2, 17472(%rsp)              # 4-byte Spill
	vmulps	%zmm23, %zmm17, %zmm12
	vfmadd231ps	%zmm31, %zmm18, %zmm12  # zmm12 = (zmm18 * zmm31) + zmm12
	vextractf64x4	$1, %zmm12, %ymm13
	vaddps	%zmm13, %zmm12, %zmm12
	vextractf128	$1, %ymm12, %xmm13
	vaddps	%xmm13, %xmm12, %xmm13
	vmovaps	36224(%rsp), %zmm2              # 64-byte Reload
	vextractf64x4	$1, %zmm2, %ymm12
	vaddps	%zmm12, %zmm2, %zmm12
	vextractf128	$1, %ymm12, %xmm14
	vaddps	%xmm14, %xmm12, %xmm12
	vshufpd	$1, %xmm12, %xmm12, %xmm14      # xmm14 = xmm12[1,0]
	vaddps	%xmm14, %xmm12, %xmm12
	vmovshdup	%xmm12, %xmm14          # xmm14 = xmm12[1,1,3,3]
	vaddss	%xmm14, %xmm12, %xmm12
	vaddss	%xmm27, %xmm12, %xmm14
	vmovaps	6144(%rsp), %zmm26              # 64-byte Reload
	vmulps	%zmm20, %zmm26, %zmm12
	vmovaps	6208(%rsp), %zmm31              # 64-byte Reload
	vfmadd231ps	%zmm22, %zmm31, %zmm12  # zmm12 = (zmm31 * zmm22) + zmm12
	vextractf64x4	$1, %zmm12, %ymm15
	vaddps	%zmm15, %zmm12, %zmm12
	vextractf128	$1, %ymm12, %xmm15
	vaddps	%xmm15, %xmm12, %xmm12
	vshufpd	$1, %xmm12, %xmm12, %xmm15      # xmm15 = xmm12[1,0]
	vaddps	%xmm15, %xmm12, %xmm12
	vmovshdup	%xmm12, %xmm15          # xmm15 = xmm12[1,1,3,3]
	vaddss	%xmm15, %xmm12, %xmm12
	vaddss	%xmm27, %xmm12, %xmm12
	vshufpd	$1, %xmm13, %xmm13, %xmm15      # xmm15 = xmm13[1,0]
	vaddps	%xmm15, %xmm13, %xmm13
	vmovshdup	%xmm13, %xmm15          # xmm15 = xmm13[1,1,3,3]
	vaddss	%xmm15, %xmm13, %xmm13
	vmulps	%zmm21, %zmm17, %zmm15
	vfmadd231ps	2560(%rsp), %zmm18, %zmm15 # 64-byte Folded Reload
                                        # zmm15 = (zmm18 * mem) + zmm15
	vextractf64x4	$1, %zmm15, %ymm16
	vaddps	%zmm16, %zmm15, %zmm15
	vextractf32x4	$1, %ymm15, %xmm16
	vaddps	%xmm16, %xmm15, %xmm15
	vshufpd	$1, %xmm15, %xmm15, %xmm16      # xmm16 = xmm15[1,0]
	vaddps	%xmm16, %xmm15, %xmm15
	vmovshdup	%xmm15, %xmm16          # xmm16 = xmm15[1,1,3,3]
	vaddss	%xmm16, %xmm15, %xmm15
	vaddss	%xmm27, %xmm13, %xmm13
	vaddss	%xmm27, %xmm15, %xmm15
	vinsertps	$16, %xmm15, %xmm13, %xmm13 # xmm13 = xmm13[0],xmm15[0],xmm13[2,3]
	vmovaps	%zmm30, %zmm4
	vmulps	%zmm30, %zmm17, %zmm15
	vmovaps	1024(%rsp), %zmm19              # 64-byte Reload
	vfmadd231ps	%zmm19, %zmm18, %zmm15  # zmm15 = (zmm18 * zmm19) + zmm15
	vextractf64x4	$1, %zmm15, %ymm16
	vaddps	%zmm16, %zmm15, %zmm15
	vextractf32x4	$1, %ymm15, %xmm16
	vaddps	%xmm16, %xmm15, %xmm15
	vshufpd	$1, %xmm15, %xmm15, %xmm16      # xmm16 = xmm15[1,0]
	vaddps	%xmm16, %xmm15, %xmm15
	vmovshdup	%xmm15, %xmm16          # xmm16 = xmm15[1,1,3,3]
	vaddss	%xmm16, %xmm15, %xmm15
	vaddss	%xmm27, %xmm15, %xmm15
	vinsertps	$32, %xmm15, %xmm13, %xmm13 # xmm13 = xmm13[0,1],xmm15[0],xmm13[3]
	vmovaps	832(%rsp), %zmm5                # 64-byte Reload
	vmulps	%zmm5, %zmm17, %zmm15
	vmovaps	960(%rsp), %zmm30               # 64-byte Reload
	vfmadd231ps	%zmm30, %zmm18, %zmm15  # zmm15 = (zmm18 * zmm30) + zmm15
	vextractf64x4	$1, %zmm15, %ymm16
	vaddps	%zmm16, %zmm15, %zmm15
	vextractf32x4	$1, %ymm15, %xmm16
	vaddps	%xmm16, %xmm15, %xmm15
	vshufpd	$1, %xmm15, %xmm15, %xmm16      # xmm16 = xmm15[1,0]
	vaddps	%xmm16, %xmm15, %xmm15
	vmovshdup	%xmm15, %xmm16          # xmm16 = xmm15[1,1,3,3]
	vaddss	%xmm16, %xmm15, %xmm15
	vaddss	%xmm27, %xmm15, %xmm15
	vinsertps	$48, %xmm15, %xmm13, %xmm2 # xmm2 = xmm13[0,1,2],xmm15[0]
	vmovaps	%xmm2, 30656(%rsp)              # 16-byte Spill
	vmovaps	640(%rsp), %zmm2                # 64-byte Reload
	vmulps	%zmm2, %zmm17, %zmm13
	vmovaps	64(%rsp), %zmm6                 # 64-byte Reload
	vfmadd231ps	%zmm6, %zmm18, %zmm13   # zmm13 = (zmm18 * zmm6) + zmm13
	vextractf64x4	$1, %zmm13, %ymm15
	vaddps	%zmm15, %zmm13, %zmm13
	vextractf128	$1, %ymm13, %xmm15
	vaddps	%xmm15, %xmm13, %xmm13
	vshufpd	$1, %xmm13, %xmm13, %xmm15      # xmm15 = xmm13[1,0]
	vaddps	%xmm15, %xmm13, %xmm13
	vmovshdup	%xmm13, %xmm15          # xmm15 = xmm13[1,1,3,3]
	vaddss	%xmm15, %xmm13, %xmm13
	vaddss	%xmm27, %xmm13, %xmm13
	vinsertps	$16, %xmm14, %xmm13, %xmm13 # xmm13 = xmm13[0],xmm14[0],xmm13[2,3]
	vinsertps	$32, %xmm11, %xmm13, %xmm11 # xmm11 = xmm13[0,1],xmm11[0],xmm13[3]
	vinsertps	$48, %xmm9, %xmm11, %xmm9 # xmm9 = xmm11[0,1,2],xmm9[0]
	vmovaps	14144(%rsp), %zmm3              # 64-byte Reload
	vextractf64x4	$1, %zmm3, %ymm11
	vaddps	%zmm11, %zmm3, %zmm11
	vextractf128	$1, %ymm11, %xmm13
	vaddps	%xmm13, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm13      # xmm13 = xmm11[1,0]
	vaddps	%xmm13, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm13          # xmm13 = xmm11[1,1,3,3]
	vaddss	%xmm13, %xmm11, %xmm11
	vaddss	%xmm27, %xmm11, %xmm11
	vinsertps	$16, %xmm11, %xmm10, %xmm10 # xmm10 = xmm10[0],xmm11[0],xmm10[2,3]
	vinsertps	$32, %xmm8, %xmm10, %xmm8 # xmm8 = xmm10[0,1],xmm8[0],xmm10[3]
	vmovaps	9280(%rsp), %zmm3               # 64-byte Reload
	vextractf64x4	$1, %zmm3, %ymm10
	vaddps	%zmm10, %zmm3, %zmm10
	vextractf128	$1, %ymm10, %xmm11
	vaddps	%xmm11, %xmm10, %xmm10
	vshufpd	$1, %xmm10, %xmm10, %xmm11      # xmm11 = xmm10[1,0]
	vaddps	%xmm11, %xmm10, %xmm10
	vmovshdup	%xmm10, %xmm11          # xmm11 = xmm10[1,1,3,3]
	vaddss	%xmm11, %xmm10, %xmm10
	vaddss	%xmm27, %xmm10, %xmm10
	vinsertps	$48, %xmm10, %xmm8, %xmm11 # xmm11 = xmm8[0,1,2],xmm10[0]
	vmulps	%zmm2, %zmm26, %zmm8
	vfmadd231ps	%zmm6, %zmm31, %zmm8    # zmm8 = (zmm31 * zmm6) + zmm8
	vextractf64x4	$1, %zmm8, %ymm10
	vaddps	%zmm10, %zmm8, %zmm8
	vextractf128	$1, %ymm8, %xmm10
	vaddps	%xmm10, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm10        # xmm10 = xmm8[1,0]
	vaddps	%xmm10, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm10           # xmm10 = xmm8[1,1,3,3]
	vaddss	%xmm10, %xmm8, %xmm8
	vaddss	%xmm27, %xmm8, %xmm8
	vinsertps	$16, %xmm7, %xmm8, %xmm7 # xmm7 = xmm8[0],xmm7[0],xmm8[2,3]
	vinsertps	$32, 9408(%rsp), %xmm7, %xmm7 # 16-byte Folded Reload
                                        # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$48, 9344(%rsp), %xmm7, %xmm7 # 16-byte Folded Reload
                                        # xmm7 = xmm7[0,1,2],mem[0]
	vinsertf128	$1, 12992(%rsp), %ymm7, %ymm2 # 16-byte Folded Reload
	vmovaps	%zmm2, 12160(%rsp)              # 64-byte Spill
	vmulps	%zmm29, %zmm17, %zmm7
	vfmadd231ps	1472(%rsp), %zmm18, %zmm7 # 64-byte Folded Reload
                                        # zmm7 = (zmm18 * mem) + zmm7
	vextractf64x4	$1, %zmm7, %ymm8
	vaddps	%zmm8, %zmm7, %zmm7
	vextractf128	$1, %ymm7, %xmm8
	vaddps	%xmm7, %xmm8, %xmm7
	vmovaps	17856(%rsp), %zmm2              # 64-byte Reload
	vextractf64x4	$1, %zmm2, %ymm8
	vaddps	%zmm8, %zmm2, %zmm8
	vextractf128	$1, %ymm8, %xmm10
	vaddps	%xmm10, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm10        # xmm10 = xmm8[1,0]
	vaddps	%xmm10, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm10           # xmm10 = xmm8[1,1,3,3]
	vaddss	%xmm10, %xmm8, %xmm8
	vaddss	%xmm27, %xmm8, %xmm8
	vinsertps	$16, %xmm8, %xmm12, %xmm8 # xmm8 = xmm12[0],xmm8[0],xmm12[2,3]
	vinsertps	$32, 9536(%rsp), %xmm8, %xmm8 # 16-byte Folded Reload
                                        # xmm8 = xmm8[0,1],mem[0],xmm8[3]
	vinsertps	$48, 9472(%rsp), %xmm8, %xmm12 # 16-byte Folded Reload
                                        # xmm12 = xmm8[0,1,2],mem[0]
	vshufpd	$1, %xmm7, %xmm7, %xmm8         # xmm8 = xmm7[1,0]
	vaddps	%xmm7, %xmm8, %xmm7
	vmovshdup	%xmm7, %xmm8            # xmm8 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm8, %xmm2
	vmovss	%xmm2, 17856(%rsp)              # 4-byte Spill
	vmovaps	2112(%rsp), %zmm2               # 64-byte Reload
	vmulps	%zmm2, %zmm17, %zmm8
	vmovaps	2496(%rsp), %zmm6               # 64-byte Reload
	vfmadd231ps	%zmm6, %zmm18, %zmm8    # zmm8 = (zmm18 * zmm6) + zmm8
	vextractf64x4	$1, %zmm8, %ymm10
	vaddps	%zmm10, %zmm8, %zmm8
	vmovaps	17984(%rsp), %zmm7              # 64-byte Reload
	vextractf64x4	$1, %zmm7, %ymm10
	vaddps	%zmm10, %zmm7, %zmm10
	vextractf128	$1, %ymm10, %xmm13
	vaddps	%xmm13, %xmm10, %xmm10
	vshufpd	$1, %xmm10, %xmm10, %xmm13      # xmm13 = xmm10[1,0]
	vaddps	%xmm13, %xmm10, %xmm10
	vmovshdup	%xmm10, %xmm13          # xmm13 = xmm10[1,1,3,3]
	vaddss	%xmm13, %xmm10, %xmm10
	vaddss	%xmm27, %xmm10, %xmm14
	vextractf128	$1, %ymm8, %xmm10
	vaddps	%xmm10, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm10        # xmm10 = xmm8[1,0]
	vaddps	%xmm10, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm10           # xmm10 = xmm8[1,1,3,3]
	vaddss	%xmm10, %xmm8, %xmm3
	vmovss	%xmm3, 17984(%rsp)              # 4-byte Spill
	vmovaps	192(%rsp), %zmm3                # 64-byte Reload
	vmulps	%zmm3, %zmm17, %zmm10
	vmovaps	1344(%rsp), %zmm8               # 64-byte Reload
	vfmadd231ps	%zmm8, %zmm18, %zmm10   # zmm10 = (zmm18 * zmm8) + zmm10
	vextractf64x4	$1, %zmm10, %ymm13
	vaddps	%zmm13, %zmm10, %zmm10
	vmovaps	18048(%rsp), %zmm15             # 64-byte Reload
	vextractf64x4	$1, %zmm15, %ymm13
	vaddps	%zmm13, %zmm15, %zmm13
	vextractf128	$1, %ymm13, %xmm15
	vaddps	%xmm15, %xmm13, %xmm13
	vshufpd	$1, %xmm13, %xmm13, %xmm15      # xmm15 = xmm13[1,0]
	vaddps	%xmm15, %xmm13, %xmm13
	vmovshdup	%xmm13, %xmm15          # xmm15 = xmm13[1,1,3,3]
	vaddss	%xmm15, %xmm13, %xmm13
	vaddss	%xmm27, %xmm13, %xmm15
	vmovaps	2176(%rsp), %zmm18              # 64-byte Reload
	vmulps	%zmm18, %zmm17, %zmm13
	vmovaps	1728(%rsp), %zmm7               # 64-byte Reload
	vfmadd231ps	%zmm7, %zmm0, %zmm13    # zmm13 = (zmm0 * zmm7) + zmm13
	vextractf64x4	$1, %zmm13, %ymm16
	vaddps	%zmm16, %zmm13, %zmm13
	vextractf32x4	$1, %ymm13, %xmm16
	vaddps	%xmm16, %xmm13, %xmm13
	vshufpd	$1, %xmm13, %xmm13, %xmm16      # xmm16 = xmm13[1,0]
	vaddps	%xmm16, %xmm13, %xmm13
	vmovshdup	%xmm13, %xmm16          # xmm16 = xmm13[1,1,3,3]
	vaddss	%xmm16, %xmm13, %xmm13
	vaddss	%xmm27, %xmm13, %xmm13
	vextractf32x4	$1, %ymm10, %xmm16
	vaddps	%xmm16, %xmm10, %xmm10
	vmulps	%zmm28, %zmm26, %zmm16
	vmovaps	1792(%rsp), %zmm28              # 64-byte Reload
	vfmadd231ps	%zmm31, %zmm28, %zmm16  # zmm16 = (zmm28 * zmm31) + zmm16
	vextractf64x4	$1, %zmm16, %ymm17
	vaddps	%zmm17, %zmm16, %zmm16
	vextractf32x4	$1, %ymm16, %xmm17
	vaddps	%xmm17, %xmm16, %xmm16
	vshufpd	$1, %xmm16, %xmm16, %xmm17      # xmm17 = xmm16[1,0]
	vaddps	%xmm17, %xmm16, %xmm16
	vmovshdup	%xmm16, %xmm17          # xmm17 = xmm16[1,1,3,3]
	vaddss	%xmm17, %xmm16, %xmm16
	vaddss	%xmm27, %xmm16, %xmm16
	vmulps	%zmm18, %zmm26, %zmm17
	vfmadd231ps	%zmm7, %zmm31, %zmm17   # zmm17 = (zmm31 * zmm7) + zmm17
	vextractf64x4	$1, %zmm17, %ymm22
	vaddps	%zmm22, %zmm17, %zmm17
	vextractf32x4	$1, %ymm17, %xmm22
	vaddps	%xmm22, %xmm17, %xmm17
	vshufpd	$1, %xmm17, %xmm17, %xmm22      # xmm22 = xmm17[1,0]
	vaddps	%xmm22, %xmm17, %xmm17
	vmovshdup	%xmm17, %xmm22          # xmm22 = xmm17[1,1,3,3]
	vaddss	%xmm22, %xmm17, %xmm17
	vaddss	%xmm27, %xmm17, %xmm17
	vshufpd	$1, %xmm10, %xmm10, %xmm22      # xmm22 = xmm10[1,0]
	vaddps	%xmm22, %xmm10, %xmm10
	vmulps	%zmm3, %zmm26, %zmm22
	vfmadd231ps	%zmm8, %zmm31, %zmm22   # zmm22 = (zmm31 * zmm8) + zmm22
	vextractf64x4	$1, %zmm22, %ymm23
	vaddps	%zmm23, %zmm22, %zmm22
	vextractf32x4	$1, %ymm22, %xmm23
	vaddps	%xmm23, %xmm22, %xmm22
	vshufpd	$1, %xmm22, %xmm22, %xmm23      # xmm23 = xmm22[1,0]
	vaddps	%xmm23, %xmm22, %xmm22
	vmovshdup	%xmm22, %xmm23          # xmm23 = xmm22[1,1,3,3]
	vaddss	%xmm23, %xmm22, %xmm22
	vaddss	%xmm27, %xmm22, %xmm22
	vmovshdup	%xmm10, %xmm23          # xmm23 = xmm10[1,1,3,3]
	vaddss	%xmm23, %xmm10, %xmm10
	vmovaps	3136(%rsp), %zmm21              # 64-byte Reload
	vmulps	%zmm21, %zmm1, %zmm23
	vmovaps	1856(%rsp), %zmm8               # 64-byte Reload
	vfmadd231ps	%zmm8, %zmm0, %zmm23    # zmm23 = (zmm0 * zmm8) + zmm23
	vaddss	%xmm27, %xmm10, %xmm24
	vextractf64x4	$1, %zmm23, %ymm10
	vaddps	%zmm10, %zmm23, %zmm10
	vextractf32x4	$1, %ymm10, %xmm23
	vaddps	%xmm23, %xmm10, %xmm10
	vmovaps	2240(%rsp), %zmm18              # 64-byte Reload
	vmulps	%zmm18, %zmm26, %zmm23
	vmovaps	1088(%rsp), %zmm7               # 64-byte Reload
	vfmadd231ps	%zmm7, %zmm31, %zmm23   # zmm23 = (zmm31 * zmm7) + zmm23
	vextractf64x4	$1, %zmm23, %ymm25
	vaddps	%zmm25, %zmm23, %zmm23
	vextractf32x4	$1, %ymm23, %xmm25
	vaddps	%xmm25, %xmm23, %xmm23
	vshufpd	$1, %xmm23, %xmm23, %xmm25      # xmm25 = xmm23[1,0]
	vaddps	%xmm25, %xmm23, %xmm23
	vmovshdup	%xmm23, %xmm25          # xmm25 = xmm23[1,1,3,3]
	vaddss	%xmm25, %xmm23, %xmm23
	vaddss	%xmm27, %xmm23, %xmm23
	vinsertps	$16, %xmm23, %xmm22, %xmm22 # xmm22 = xmm22[0],xmm23[0],xmm22[2,3]
	vinsertps	$32, %xmm17, %xmm22, %xmm17 # xmm17 = xmm22[0,1],xmm17[0],xmm22[3]
	vinsertps	$48, %xmm16, %xmm17, %xmm3 # xmm3 = xmm17[0,1,2],xmm16[0]
	vmovaps	%xmm3, 29888(%rsp)              # 16-byte Spill
	vshufpd	$1, %xmm10, %xmm10, %xmm16      # xmm16 = xmm10[1,0]
	vaddps	%xmm16, %xmm10, %xmm10
	vmovshdup	%xmm10, %xmm16          # xmm16 = xmm10[1,1,3,3]
	vaddss	%xmm16, %xmm10, %xmm3
	vmovss	%xmm3, 18048(%rsp)              # 4-byte Spill
	vmulps	%zmm18, %zmm1, %zmm16
	vfmadd231ps	%zmm7, %zmm0, %zmm16    # zmm16 = (zmm0 * zmm7) + zmm16
	vextractf64x4	$1, %zmm16, %ymm17
	vaddps	%zmm17, %zmm16, %zmm16
	vextractf32x4	$1, %ymm16, %xmm17
	vaddps	%xmm17, %xmm16, %xmm16
	vshufpd	$1, %xmm16, %xmm16, %xmm17      # xmm17 = xmm16[1,0]
	vaddps	%xmm17, %xmm16, %xmm16
	vmovshdup	%xmm16, %xmm17          # xmm17 = xmm16[1,1,3,3]
	vaddss	%xmm17, %xmm16, %xmm16
	vaddss	%xmm27, %xmm16, %xmm16
	vinsertps	$16, %xmm16, %xmm24, %xmm16 # xmm16 = xmm24[0],xmm16[0],xmm24[2,3]
	vinsertps	$32, %xmm13, %xmm16, %xmm13 # xmm13 = xmm16[0,1],xmm13[0],xmm16[3]
	vinsertf128	$1, %xmm11, %ymm9, %ymm3
	vmovaps	%zmm3, 29568(%rsp)              # 64-byte Spill
	vmovaps	1600(%rsp), %zmm23              # 64-byte Reload
	vmulps	%zmm23, %zmm26, %zmm9
	vmovaps	1664(%rsp), %zmm20              # 64-byte Reload
	vfmadd231ps	%zmm20, %zmm31, %zmm9   # zmm9 = (zmm31 * zmm20) + zmm9
	vextractf64x4	$1, %zmm9, %ymm11
	vaddps	%zmm11, %zmm9, %zmm9
	vextractf128	$1, %ymm9, %xmm11
	vaddps	%xmm11, %xmm9, %xmm9
	vshufpd	$1, %xmm9, %xmm9, %xmm11        # xmm11 = xmm9[1,0]
	vaddps	%xmm11, %xmm9, %xmm9
	vmovshdup	%xmm9, %xmm11           # xmm11 = xmm9[1,1,3,3]
	vaddss	%xmm11, %xmm9, %xmm9
	vaddss	%xmm27, %xmm9, %xmm9
	vinsertps	$16, %xmm15, %xmm9, %xmm9 # xmm9 = xmm9[0],xmm15[0],xmm9[2,3]
	vinsertps	$32, %xmm14, %xmm9, %xmm9 # xmm9 = xmm9[0,1],xmm14[0],xmm9[3]
	vmovaps	36992(%rsp), %zmm14             # 64-byte Reload
	vextractf64x4	$1, %zmm14, %ymm11
	vaddps	%zmm11, %zmm14, %zmm11
	vextractf128	$1, %ymm11, %xmm14
	vaddps	%xmm14, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm14      # xmm14 = xmm11[1,0]
	vaddps	%xmm14, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm14          # xmm14 = xmm11[1,1,3,3]
	vaddss	%xmm14, %xmm11, %xmm11
	vaddss	%xmm27, %xmm11, %xmm11
	vinsertps	$48, %xmm11, %xmm9, %xmm9 # xmm9 = xmm9[0,1,2],xmm11[0]
	vmulps	128(%rsp), %zmm26, %zmm11       # 64-byte Folded Reload
	vfmadd231ps	896(%rsp), %zmm31, %zmm11 # 64-byte Folded Reload
                                        # zmm11 = (zmm31 * mem) + zmm11
	vextractf64x4	$1, %zmm11, %ymm14
	vaddps	%zmm14, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm14
	vaddps	%xmm14, %xmm11, %xmm11
	vinsertf128	$1, %xmm12, %ymm9, %ymm3
	vmovaps	%zmm3, 29504(%rsp)              # 64-byte Spill
	vshufpd	$1, %xmm11, %xmm11, %xmm9       # xmm9 = xmm11[1,0]
	vaddps	%xmm9, %xmm11, %xmm9
	vmovshdup	%xmm9, %xmm11           # xmm11 = xmm9[1,1,3,3]
	vaddss	%xmm11, %xmm9, %xmm9
	vmovaps	2368(%rsp), %zmm17              # 64-byte Reload
	vmulps	%zmm17, %zmm26, %zmm11
	vmovaps	%zmm31, %zmm15
	vmovaps	2560(%rsp), %zmm16              # 64-byte Reload
	vfmadd231ps	%zmm16, %zmm31, %zmm11  # zmm11 = (zmm31 * zmm16) + zmm11
	vextractf64x4	$1, %zmm11, %ymm12
	vaddps	%zmm12, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm12
	vaddps	%xmm12, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm12      # xmm12 = xmm11[1,0]
	vaddps	%xmm12, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm12          # xmm12 = xmm11[1,1,3,3]
	vaddss	%xmm12, %xmm11, %xmm11
	vaddss	%xmm27, %xmm9, %xmm9
	vaddss	%xmm27, %xmm11, %xmm11
	vinsertps	$16, %xmm11, %xmm9, %xmm9 # xmm9 = xmm9[0],xmm11[0],xmm9[2,3]
	vmulps	%zmm4, %zmm26, %zmm11
	vfmadd231ps	%zmm19, %zmm31, %zmm11  # zmm11 = (zmm31 * zmm19) + zmm11
	vextractf64x4	$1, %zmm11, %ymm12
	vaddps	%zmm12, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm12
	vaddps	%xmm12, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm12      # xmm12 = xmm11[1,0]
	vaddps	%xmm12, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm12          # xmm12 = xmm11[1,1,3,3]
	vaddss	%xmm12, %xmm11, %xmm11
	vaddss	%xmm27, %xmm11, %xmm11
	vinsertps	$32, %xmm11, %xmm9, %xmm9 # xmm9 = xmm9[0,1],xmm11[0],xmm9[3]
	vmulps	%zmm5, %zmm26, %zmm11
	vfmadd231ps	%zmm30, %zmm31, %zmm11  # zmm11 = (zmm31 * zmm30) + zmm11
	vextractf64x4	$1, %zmm11, %ymm12
	vaddps	%zmm12, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm12
	vaddps	%xmm12, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm12      # xmm12 = xmm11[1,0]
	vaddps	%xmm12, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm12          # xmm12 = xmm11[1,1,3,3]
	vaddss	%xmm12, %xmm11, %xmm11
	vaddss	%xmm27, %xmm11, %xmm11
	vinsertps	$48, %xmm11, %xmm9, %xmm3 # xmm3 = xmm9[0,1,2],xmm11[0]
	vmovaps	%xmm3, 29248(%rsp)              # 16-byte Spill
	vmovaps	%zmm26, %zmm14
	vmulps	%zmm21, %zmm26, %zmm9
	vfmadd231ps	%zmm8, %zmm31, %zmm9    # zmm9 = (zmm31 * zmm8) + zmm9
	vmovaps	%zmm8, %zmm31
	vextractf64x4	$1, %zmm9, %ymm11
	vaddps	%zmm11, %zmm9, %zmm9
	vextractf128	$1, %ymm9, %xmm11
	vaddps	%xmm11, %xmm9, %xmm9
	vshufpd	$1, %xmm9, %xmm9, %xmm11        # xmm11 = xmm9[1,0]
	vaddps	%xmm11, %xmm9, %xmm9
	vmovshdup	%xmm9, %xmm11           # xmm11 = xmm9[1,1,3,3]
	vaddss	%xmm11, %xmm9, %xmm9
	vmovaps	448(%rsp), %zmm7                # 64-byte Reload
	vmulps	%zmm7, %zmm26, %zmm11
	vfmadd231ps	512(%rsp), %zmm15, %zmm11 # 64-byte Folded Reload
                                        # zmm11 = (zmm15 * mem) + zmm11
	vaddss	%xmm27, %xmm9, %xmm3
	vmovaps	%xmm3, 29056(%rsp)              # 16-byte Spill
	vextractf64x4	$1, %zmm11, %ymm9
	vaddps	%zmm9, %zmm11, %zmm9
	vmovaps	%zmm2, %zmm18
	vmulps	%zmm2, %zmm26, %zmm11
	vmovaps	%zmm6, %zmm24
	vfmadd231ps	%zmm6, %zmm15, %zmm11   # zmm11 = (zmm15 * zmm6) + zmm11
	vextractf64x4	$1, %zmm11, %ymm12
	vaddps	%zmm12, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm12
	vaddps	%xmm12, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm12      # xmm12 = xmm11[1,0]
	vaddps	%xmm12, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm12          # xmm12 = xmm11[1,1,3,3]
	vaddss	%xmm12, %xmm11, %xmm11
	vaddss	%xmm27, %xmm11, %xmm3
	vmovaps	%xmm3, 28928(%rsp)              # 16-byte Spill
	vmulps	%zmm29, %zmm26, %zmm11
	vmovaps	1472(%rsp), %zmm25              # 64-byte Reload
	vfmadd231ps	%zmm25, %zmm15, %zmm11  # zmm11 = (zmm15 * zmm25) + zmm11
	vextractf64x4	$1, %zmm11, %ymm12
	vaddps	%zmm12, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm12
	vaddps	%xmm12, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm12      # xmm12 = xmm11[1,0]
	vaddps	%xmm12, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm12          # xmm12 = xmm11[1,1,3,3]
	vaddss	%xmm12, %xmm11, %xmm11
	vaddss	%xmm27, %xmm11, %xmm3
	vmovaps	%xmm3, 28800(%rsp)              # 16-byte Spill
	vextractf128	$1, %ymm9, %xmm11
	vaddps	%xmm11, %xmm9, %xmm9
	vmovaps	256(%rsp), %zmm8                # 64-byte Reload
	vmulps	%zmm8, %zmm26, %zmm11
	vmovaps	1408(%rsp), %zmm10              # 64-byte Reload
	vfmadd231ps	%zmm10, %zmm15, %zmm11  # zmm11 = (zmm15 * zmm10) + zmm11
	vextractf64x4	$1, %zmm11, %ymm12
	vaddps	%zmm12, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm12
	vaddps	%xmm12, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm12      # xmm12 = xmm11[1,0]
	vaddps	%xmm12, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm12          # xmm12 = xmm11[1,1,3,3]
	vaddss	%xmm12, %xmm11, %xmm11
	vaddss	%xmm27, %xmm11, %xmm3
	vmovaps	%xmm3, 5504(%rsp)               # 16-byte Spill
	vmovaps	2048(%rsp), %zmm6               # 64-byte Reload
	vmulps	%zmm6, %zmm26, %zmm11
	vmovaps	704(%rsp), %zmm4                # 64-byte Reload
	vfmadd231ps	%zmm4, %zmm15, %zmm11   # zmm11 = (zmm15 * zmm4) + zmm11
	vextractf64x4	$1, %zmm11, %ymm12
	vaddps	%zmm12, %zmm11, %zmm11
	vextractf128	$1, %ymm11, %xmm12
	vaddps	%xmm12, %xmm11, %xmm11
	vshufpd	$1, %xmm11, %xmm11, %xmm12      # xmm12 = xmm11[1,0]
	vaddps	%xmm12, %xmm11, %xmm11
	vmovshdup	%xmm11, %xmm12          # xmm12 = xmm11[1,1,3,3]
	vaddss	%xmm12, %xmm11, %xmm11
	vaddss	%xmm27, %xmm11, %xmm11
	vmovaps	%xmm11, 28544(%rsp)             # 16-byte Spill
	vshufpd	$1, %xmm9, %xmm9, %xmm11        # xmm11 = xmm9[1,0]
	vaddps	%xmm11, %xmm9, %xmm9
	vmovshdup	%xmm9, %xmm11           # xmm11 = xmm9[1,1,3,3]
	vaddss	%xmm11, %xmm9, %xmm9
	vmovaps	2304(%rsp), %zmm12              # 64-byte Reload
	vmulps	%zmm12, %zmm1, %zmm26
	vmovaps	%zmm0, %zmm11
	vmovaps	1536(%rsp), %zmm22              # 64-byte Reload
	vfmadd231ps	%zmm22, %zmm0, %zmm26   # zmm26 = (zmm0 * zmm22) + zmm26
	vmovaps	%zmm26, 44352(%rsp)             # 64-byte Spill
	vaddss	%xmm27, %xmm9, %xmm0
	vmovaps	%xmm0, 28480(%rsp)              # 16-byte Spill
	vmovaps	576(%rsp), %zmm30               # 64-byte Reload
	vmulps	%zmm30, %zmm1, %zmm9
	vmovaps	1920(%rsp), %zmm0               # 64-byte Reload
	vfmadd231ps	%zmm0, %zmm11, %zmm9    # zmm9 = (zmm11 * zmm0) + zmm9
	vmovaps	%zmm9, 28160(%rsp)              # 64-byte Spill
	vmovaps	2432(%rsp), %zmm3               # 64-byte Reload
	vmulps	%zmm3, %zmm1, %zmm9
	vfmadd231ps	%zmm11, %zmm28, %zmm9   # zmm9 = (zmm28 * zmm11) + zmm9
	vextractf64x4	$1, %zmm9, %ymm11
	vaddps	%zmm11, %zmm9, %zmm9
	vextractf128	$1, %ymm9, %xmm11
	vaddps	%xmm11, %xmm9, %xmm9
	vshufpd	$1, %xmm9, %xmm9, %xmm11        # xmm11 = xmm9[1,0]
	vaddps	%xmm11, %xmm9, %xmm9
	vmovshdup	%xmm9, %xmm11           # xmm11 = xmm9[1,1,3,3]
	vaddss	%xmm11, %xmm9, %xmm9
	vaddss	%xmm27, %xmm9, %xmm9
	vinsertps	$48, %xmm9, %xmm13, %xmm1 # xmm1 = xmm13[0,1,2],xmm9[0]
	vmovaps	%xmm1, 5488(%rsp)               # 16-byte Spill
	vmovaps	3648(%rsp), %ymm1               # 32-byte Reload
	vinsertf128	$1, 3584(%rsp), %ymm1, %ymm1 # 16-byte Folded Reload
	vmovaps	%zmm1, 28224(%rsp)              # 64-byte Spill
	vmulps	%zmm12, %zmm14, %zmm1
	vfmadd231ps	%zmm22, %zmm15, %zmm1   # zmm1 = (zmm15 * zmm22) + zmm1
	vmovaps	%zmm1, 28288(%rsp)              # 64-byte Spill
	vmulps	%zmm30, %zmm14, %zmm1
	vmovaps	%zmm0, %zmm9
	vfmadd231ps	%zmm0, %zmm15, %zmm1    # zmm1 = (zmm15 * zmm0) + zmm1
	vmovaps	%zmm1, 28352(%rsp)              # 64-byte Spill
	vmovaps	7744(%rsp), %zmm0               # 64-byte Reload
	vmulps	%zmm12, %zmm0, %zmm2
	vmovaps	6272(%rsp), %zmm1               # 64-byte Reload
	vfmadd231ps	%zmm22, %zmm1, %zmm2    # zmm2 = (zmm1 * zmm22) + zmm2
	vmovaps	%zmm2, 28416(%rsp)              # 64-byte Spill
	vmulps	%zmm30, %zmm0, %zmm0
	vfmadd231ps	%zmm9, %zmm1, %zmm0     # zmm0 = (zmm1 * zmm9) + zmm0
	vmovaps	%zmm0, 28608(%rsp)              # 64-byte Spill
	vaddss	18048(%rsp), %xmm27, %xmm0      # 4-byte Folded Reload
	vmovaps	%xmm0, 11920(%rsp)              # 16-byte Spill
	vmovaps	6336(%rsp), %zmm0               # 64-byte Reload
	vmulps	%zmm12, %zmm0, %zmm2
	vmovaps	6400(%rsp), %zmm1               # 64-byte Reload
	vfmadd231ps	%zmm22, %zmm1, %zmm2    # zmm2 = (zmm1 * zmm22) + zmm2
	vmovaps	%zmm2, 28672(%rsp)              # 64-byte Spill
	vmulps	%zmm30, %zmm0, %zmm0
	vfmadd231ps	%zmm9, %zmm1, %zmm0     # zmm0 = (zmm1 * zmm9) + zmm0
	vmovaps	%zmm0, 28736(%rsp)              # 64-byte Spill
	vaddss	17856(%rsp), %xmm27, %xmm0      # 4-byte Folded Reload
	vmovaps	%xmm0, 11904(%rsp)              # 16-byte Spill
	vaddss	17984(%rsp), %xmm27, %xmm0      # 4-byte Folded Reload
	vmovaps	%xmm0, 11888(%rsp)              # 16-byte Spill
	vaddss	17920(%rsp), %xmm27, %xmm0      # 4-byte Folded Reload
	vmovaps	%xmm0, 11872(%rsp)              # 16-byte Spill
	vaddss	17728(%rsp), %xmm27, %xmm0      # 4-byte Folded Reload
	vmovaps	%xmm0, 11856(%rsp)              # 16-byte Spill
	vaddss	17472(%rsp), %xmm27, %xmm0      # 4-byte Folded Reload
	vmovaps	%xmm0, 11840(%rsp)              # 16-byte Spill
	vmovaps	4864(%rsp), %zmm0               # 64-byte Reload
	vmulps	%zmm12, %zmm0, %zmm2
	vmovaps	6784(%rsp), %zmm1               # 64-byte Reload
	vfmadd231ps	%zmm22, %zmm1, %zmm2    # zmm2 = (zmm1 * zmm22) + zmm2
	vmovaps	%zmm2, 28864(%rsp)              # 64-byte Spill
	vmulps	%zmm30, %zmm0, %zmm0
	vfmadd231ps	%zmm9, %zmm1, %zmm0     # zmm0 = (zmm1 * zmm9) + zmm0
	vmovaps	%zmm0, 28992(%rsp)              # 64-byte Spill
	vmovaps	6720(%rsp), %zmm0               # 64-byte Reload
	vmulps	%zmm12, %zmm0, %zmm2
	vmovaps	6464(%rsp), %zmm1               # 64-byte Reload
	vfmadd231ps	%zmm22, %zmm1, %zmm2    # zmm2 = (zmm1 * zmm22) + zmm2
	vmovaps	%zmm2, 29120(%rsp)              # 64-byte Spill
	vmulps	%zmm30, %zmm0, %zmm0
	vfmadd231ps	%zmm9, %zmm1, %zmm0     # zmm0 = (zmm1 * zmm9) + zmm0
	vmovaps	%zmm0, 29184(%rsp)              # 64-byte Spill
	vmovaps	6848(%rsp), %zmm0               # 64-byte Reload
	vmulps	%zmm12, %zmm0, %zmm2
	vmovaps	7808(%rsp), %zmm1               # 64-byte Reload
	vfmadd231ps	%zmm22, %zmm1, %zmm2    # zmm2 = (zmm1 * zmm22) + zmm2
	vmovaps	%zmm2, 29312(%rsp)              # 64-byte Spill
	vmulps	%zmm30, %zmm0, %zmm0
	vfmadd231ps	%zmm9, %zmm1, %zmm0     # zmm0 = (zmm1 * zmm9) + zmm0
	vmovaps	%zmm0, 29376(%rsp)              # 64-byte Spill
	vaddss	17344(%rsp), %xmm27, %xmm0      # 4-byte Folded Reload
	vmovaps	%xmm0, 11824(%rsp)              # 16-byte Spill
	vmovaps	6912(%rsp), %zmm0               # 64-byte Reload
	vmulps	%zmm12, %zmm0, %zmm2
	vmovaps	7872(%rsp), %zmm1               # 64-byte Reload
	vfmadd231ps	%zmm22, %zmm1, %zmm2    # zmm2 = (zmm1 * zmm22) + zmm2
	vmovaps	%zmm2, 29440(%rsp)              # 64-byte Spill
	vmulps	%zmm30, %zmm0, %zmm0
	vfmadd231ps	%zmm9, %zmm1, %zmm0     # zmm0 = (zmm1 * zmm9) + zmm0
	vmovaps	%zmm0, 29632(%rsp)              # 64-byte Spill
	vaddss	17792(%rsp), %xmm27, %xmm0      # 4-byte Folded Reload
	vmovaps	%xmm0, 11808(%rsp)              # 16-byte Spill
	vmovaps	8512(%rsp), %zmm0               # 64-byte Reload
	vmulps	%zmm12, %zmm0, %zmm2
	vmovaps	8576(%rsp), %zmm1               # 64-byte Reload
	vfmadd231ps	%zmm22, %zmm1, %zmm2    # zmm2 = (zmm1 * zmm22) + zmm2
	vmovaps	%zmm2, 29696(%rsp)              # 64-byte Spill
	vmulps	%zmm30, %zmm0, %zmm0
	vfmadd231ps	%zmm9, %zmm1, %zmm0     # zmm0 = (zmm1 * zmm9) + zmm0
	vmovaps	%zmm0, 29760(%rsp)              # 64-byte Spill
	vaddss	3456(%rsp), %xmm27, %xmm0       # 4-byte Folded Reload
	vmovaps	%xmm0, 11776(%rsp)              # 16-byte Spill
	vaddss	17600(%rsp), %xmm27, %xmm0      # 4-byte Folded Reload
	vmovaps	%xmm0, 11760(%rsp)              # 16-byte Spill
	vaddss	17664(%rsp), %xmm27, %xmm0      # 4-byte Folded Reload
	vmovaps	%xmm0, 11792(%rsp)              # 16-byte Spill
	vmovaps	7936(%rsp), %zmm0               # 64-byte Reload
	vmulps	%zmm12, %zmm0, %zmm2
	vmovaps	6976(%rsp), %zmm1               # 64-byte Reload
	vfmadd231ps	%zmm22, %zmm1, %zmm2    # zmm2 = (zmm1 * zmm22) + zmm2
	vmovaps	%zmm2, 29824(%rsp)              # 64-byte Spill
	vmulps	%zmm30, %zmm0, %zmm0
	vfmadd231ps	%zmm9, %zmm1, %zmm0     # zmm0 = (zmm1 * zmm9) + zmm0
	vmovaps	%zmm0, 29952(%rsp)              # 64-byte Spill
	vmovaps	8640(%rsp), %zmm1               # 64-byte Reload
	vmulps	%zmm12, %zmm1, %zmm2
	vmovaps	8000(%rsp), %zmm0               # 64-byte Reload
	vfmadd231ps	%zmm22, %zmm0, %zmm2    # zmm2 = (zmm0 * zmm22) + zmm2
	vmovaps	%zmm2, 12032(%rsp)              # 64-byte Spill
	vmovaps	%zmm22, %zmm5
	vmulps	%zmm30, %zmm1, %zmm1
	vfmadd231ps	%zmm9, %zmm0, %zmm1     # zmm1 = (zmm0 * zmm9) + zmm1
	vmovaps	%zmm1, 30016(%rsp)              # 64-byte Spill
	vmovaps	9792(%rsp), %zmm1               # 64-byte Reload
	vextractf64x4	$1, %zmm1, %ymm0
	vaddps	%zmm0, %zmm1, %zmm0
	vextractf128	$1, %ymm0, %xmm2
	vaddps	%xmm2, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm2         # xmm2 = xmm0[1,0]
	vaddps	%xmm2, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm2            # xmm2 = xmm0[1,1,3,3]
	vaddss	%xmm2, %xmm0, %xmm0
	vmovss	%xmm0, 444(%rsp)                # 4-byte Spill
	vaddss	3392(%rsp), %xmm27, %xmm0       # 4-byte Folded Reload
	vmovaps	%xmm0, 11680(%rsp)              # 16-byte Spill
	vaddss	3328(%rsp), %xmm27, %xmm0       # 4-byte Folded Reload
	vmovaps	%xmm0, 11664(%rsp)              # 16-byte Spill
	vaddss	16960(%rsp), %xmm27, %xmm0      # 4-byte Folded Reload
	vmovaps	%xmm0, 11648(%rsp)              # 16-byte Spill
	vaddss	3520(%rsp), %xmm27, %xmm0       # 4-byte Folded Reload
	vmovaps	%xmm0, 11744(%rsp)              # 16-byte Spill
	vaddss	17216(%rsp), %xmm27, %xmm0      # 4-byte Folded Reload
	vmovaps	%xmm0, 11728(%rsp)              # 16-byte Spill
	vaddss	17408(%rsp), %xmm27, %xmm0      # 4-byte Folded Reload
	vmovaps	%xmm0, 11712(%rsp)              # 16-byte Spill
	vaddss	16704(%rsp), %xmm27, %xmm0      # 4-byte Folded Reload
	vmovaps	%xmm0, 11696(%rsp)              # 16-byte Spill
	vmovaps	8064(%rsp), %zmm0               # 64-byte Reload
	vmovaps	%zmm23, %zmm11
	vmulps	%zmm23, %zmm0, %zmm2
	vmovaps	8128(%rsp), %zmm1               # 64-byte Reload
	vmovaps	%zmm20, %zmm22
	vfmadd231ps	%zmm20, %zmm1, %zmm2    # zmm2 = (zmm1 * zmm20) + zmm2
	vmovaps	%zmm2, 30080(%rsp)              # 64-byte Spill
	vmulps	%zmm12, %zmm0, %zmm2
	vmovaps	%zmm12, %zmm14
	vfmadd231ps	%zmm5, %zmm1, %zmm2     # zmm2 = (zmm1 * zmm5) + zmm2
	vmovaps	%zmm5, %zmm12
	vmovaps	%zmm2, 30144(%rsp)              # 64-byte Spill
	vmulps	%zmm17, %zmm0, %zmm2
	vfmadd231ps	%zmm16, %zmm1, %zmm2    # zmm2 = (zmm1 * zmm16) + zmm2
	vmovaps	%zmm2, 12096(%rsp)              # 64-byte Spill
	vmovaps	640(%rsp), %zmm20               # 64-byte Reload
	vmulps	%zmm20, %zmm0, %zmm2
	vmovaps	64(%rsp), %zmm5                 # 64-byte Reload
	vfmadd231ps	%zmm5, %zmm1, %zmm2     # zmm2 = (zmm1 * zmm5) + zmm2
	vmovaps	%zmm2, 30208(%rsp)              # 64-byte Spill
	vmulps	%zmm30, %zmm0, %zmm2
	vfmadd231ps	%zmm9, %zmm1, %zmm2     # zmm2 = (zmm1 * zmm9) + zmm2
	vmovaps	%zmm2, 30272(%rsp)              # 64-byte Spill
	vmovaps	%zmm29, %zmm19
	vmulps	%zmm29, %zmm0, %zmm2
	vfmadd231ps	%zmm25, %zmm1, %zmm2    # zmm2 = (zmm1 * zmm25) + zmm2
	vmovaps	%zmm2, 30336(%rsp)              # 64-byte Spill
	vmulps	%zmm18, %zmm0, %zmm2
	vfmadd231ps	%zmm24, %zmm1, %zmm2    # zmm2 = (zmm1 * zmm24) + zmm2
	vmovaps	%zmm2, 30400(%rsp)              # 64-byte Spill
	vmulps	%zmm21, %zmm0, %zmm2
	vmovaps	%zmm31, %zmm13
	vfmadd231ps	%zmm31, %zmm1, %zmm2    # zmm2 = (zmm1 * zmm31) + zmm2
	vmovaps	%zmm2, 30464(%rsp)              # 64-byte Spill
	vmovaps	192(%rsp), %zmm9                # 64-byte Reload
	vmulps	%zmm9, %zmm0, %zmm2
	vmovaps	1344(%rsp), %zmm29              # 64-byte Reload
	vfmadd231ps	%zmm29, %zmm1, %zmm2    # zmm2 = (zmm1 * zmm29) + zmm2
	vmovaps	%zmm2, 30528(%rsp)              # 64-byte Spill
	vmovaps	2240(%rsp), %zmm26              # 64-byte Reload
	vmulps	%zmm26, %zmm0, %zmm2
	vfmadd231ps	1088(%rsp), %zmm1, %zmm2 # 64-byte Folded Reload
                                        # zmm2 = (zmm1 * mem) + zmm2
	vmovaps	%zmm2, 30592(%rsp)              # 64-byte Spill
	vmovaps	2176(%rsp), %zmm27              # 64-byte Reload
	vmulps	%zmm27, %zmm0, %zmm2
	vmovaps	1728(%rsp), %zmm30              # 64-byte Reload
	vfmadd231ps	%zmm30, %zmm1, %zmm2    # zmm2 = (zmm1 * zmm30) + zmm2
	vmovaps	%zmm2, 30720(%rsp)              # 64-byte Spill
	vmulps	%zmm3, %zmm0, %zmm0
	vfmadd231ps	%zmm1, %zmm28, %zmm0    # zmm0 = (zmm28 * zmm1) + zmm0
	vmovaps	%zmm0, 30784(%rsp)              # 64-byte Spill
	vxorps	%xmm1, %xmm1, %xmm1
	vaddss	16128(%rsp), %xmm1, %xmm0       # 4-byte Folded Reload
	vmovaps	%xmm0, 11488(%rsp)              # 16-byte Spill
	vaddss	13568(%rsp), %xmm1, %xmm0       # 4-byte Folded Reload
	vmovaps	%xmm0, 11472(%rsp)              # 16-byte Spill
	vaddss	14720(%rsp), %xmm1, %xmm0       # 4-byte Folded Reload
	vmovaps	%xmm0, 11456(%rsp)              # 16-byte Spill
	vaddss	15872(%rsp), %xmm1, %xmm0       # 4-byte Folded Reload
	vmovaps	%xmm0, 11536(%rsp)              # 16-byte Spill
	vaddss	12608(%rsp), %xmm1, %xmm0       # 4-byte Folded Reload
	vmovaps	%xmm0, 11520(%rsp)              # 16-byte Spill
	vaddss	9216(%rsp), %xmm1, %xmm0        # 4-byte Folded Reload
	vmovaps	%xmm0, 11504(%rsp)              # 16-byte Spill
	vaddss	3264(%rsp), %xmm1, %xmm0        # 4-byte Folded Reload
	vmovaps	%xmm0, 11584(%rsp)              # 16-byte Spill
	vaddss	3200(%rsp), %xmm1, %xmm0        # 4-byte Folded Reload
	vmovaps	%xmm0, 11568(%rsp)              # 16-byte Spill
	vaddss	9152(%rsp), %xmm1, %xmm0        # 4-byte Folded Reload
	vmovaps	%xmm0, 11552(%rsp)              # 16-byte Spill
	vaddss	16064(%rsp), %xmm1, %xmm0       # 4-byte Folded Reload
	vmovaps	%xmm0, 11632(%rsp)              # 16-byte Spill
	vaddss	12352(%rsp), %xmm1, %xmm0       # 4-byte Folded Reload
	vmovaps	%xmm0, 11616(%rsp)              # 16-byte Spill
	vaddss	12288(%rsp), %xmm1, %xmm0       # 4-byte Folded Reload
	vxorps	%xmm23, %xmm23, %xmm23
	vmovaps	%xmm0, 11600(%rsp)              # 16-byte Spill
	vmovaps	6528(%rsp), %zmm0               # 64-byte Reload
	vmulps	%zmm11, %zmm0, %zmm2
	vmovaps	4928(%rsp), %zmm1               # 64-byte Reload
	vfmadd231ps	%zmm22, %zmm1, %zmm2    # zmm2 = (zmm1 * zmm22) + zmm2
	vmovaps	%zmm2, 12224(%rsp)              # 64-byte Spill
	vmovaps	1152(%rsp), %zmm24              # 64-byte Reload
	vmulps	%zmm24, %zmm0, %zmm2
	vmovaps	768(%rsp), %zmm31               # 64-byte Reload
	vfmadd231ps	%zmm31, %zmm1, %zmm2    # zmm2 = (zmm1 * zmm31) + zmm2
	vmovaps	%zmm2, 30848(%rsp)              # 64-byte Spill
	vmulps	%zmm14, %zmm0, %zmm2
	vfmadd231ps	%zmm12, %zmm1, %zmm2    # zmm2 = (zmm1 * zmm12) + zmm2
	vmovaps	%zmm2, 30912(%rsp)              # 64-byte Spill
	vmovaps	%zmm7, %zmm12
	vmulps	%zmm7, %zmm0, %zmm2
	vmovaps	512(%rsp), %zmm11               # 64-byte Reload
	vfmadd231ps	%zmm11, %zmm1, %zmm2    # zmm2 = (zmm1 * zmm11) + zmm2
	vmovaps	%zmm2, 30976(%rsp)              # 64-byte Spill
	vmovaps	%zmm6, %zmm28
	vmulps	%zmm6, %zmm0, %zmm2
	vfmadd231ps	%zmm4, %zmm1, %zmm2     # zmm2 = (zmm1 * zmm4) + zmm2
	vmovaps	%zmm2, 31040(%rsp)              # 64-byte Spill
	vmovaps	%zmm8, %zmm15
	vmulps	%zmm8, %zmm0, %zmm2
	vfmadd231ps	%zmm10, %zmm1, %zmm2    # zmm2 = (zmm1 * zmm10) + zmm2
	vmovaps	%zmm2, 12288(%rsp)              # 64-byte Spill
	vmovaps	128(%rsp), %zmm14               # 64-byte Reload
	vmulps	%zmm14, %zmm0, %zmm2
	vmovaps	896(%rsp), %zmm7                # 64-byte Reload
	vfmadd231ps	%zmm7, %zmm1, %zmm2     # zmm2 = (zmm1 * zmm7) + zmm2
	vmovaps	%zmm2, 9088(%rsp)               # 64-byte Spill
	vmulps	%zmm17, %zmm0, %zmm2
	vfmadd231ps	%zmm16, %zmm1, %zmm2    # zmm2 = (zmm1 * zmm16) + zmm2
	vmovaps	%zmm2, 12352(%rsp)              # 64-byte Spill
	vmovaps	1984(%rsp), %zmm8               # 64-byte Reload
	vmulps	%zmm8, %zmm0, %zmm2
	vmovaps	1024(%rsp), %zmm6               # 64-byte Reload
	vfmadd231ps	%zmm6, %zmm1, %zmm2     # zmm2 = (zmm1 * zmm6) + zmm2
	vmovaps	%zmm2, 9152(%rsp)               # 64-byte Spill
	vmovaps	832(%rsp), %zmm2                # 64-byte Reload
	vmulps	%zmm2, %zmm0, %zmm4
	vmovaps	960(%rsp), %zmm3                # 64-byte Reload
	vfmadd231ps	%zmm3, %zmm1, %zmm4     # zmm4 = (zmm1 * zmm3) + zmm4
	vmovaps	%zmm4, 9216(%rsp)               # 64-byte Spill
	vmulps	%zmm20, %zmm0, %zmm4
	vfmadd231ps	%zmm5, %zmm1, %zmm4     # zmm4 = (zmm1 * zmm5) + zmm4
	vmovaps	%zmm4, 9280(%rsp)               # 64-byte Spill
	vmovaps	1280(%rsp), %zmm4               # 64-byte Reload
	vmulps	%zmm4, %zmm0, %zmm16
	vmovaps	1216(%rsp), %zmm5               # 64-byte Reload
	vfmadd231ps	%zmm5, %zmm1, %zmm16    # zmm16 = (zmm1 * zmm5) + zmm16
	vmovaps	%zmm16, 12608(%rsp)             # 64-byte Spill
	vmulps	576(%rsp), %zmm0, %zmm16        # 64-byte Folded Reload
	vmovaps	1920(%rsp), %zmm20              # 64-byte Reload
	vfmadd231ps	%zmm20, %zmm1, %zmm16   # zmm16 = (zmm1 * zmm20) + zmm16
	vmovaps	%zmm16, 9344(%rsp)              # 64-byte Spill
	vmulps	%zmm19, %zmm0, %zmm16
	vfmadd231ps	%zmm25, %zmm1, %zmm16   # zmm16 = (zmm1 * zmm25) + zmm16
	vmovaps	%zmm16, 9408(%rsp)              # 64-byte Spill
	vmulps	%zmm18, %zmm0, %zmm16
	vfmadd231ps	2496(%rsp), %zmm1, %zmm16 # 64-byte Folded Reload
                                        # zmm16 = (zmm1 * mem) + zmm16
	vmovaps	%zmm16, 9472(%rsp)              # 64-byte Spill
	vmulps	%zmm21, %zmm0, %zmm16
	vfmadd231ps	%zmm13, %zmm1, %zmm16   # zmm16 = (zmm1 * zmm13) + zmm16
	vmovaps	%zmm16, 9536(%rsp)              # 64-byte Spill
	vmulps	%zmm9, %zmm0, %zmm13
	vfmadd231ps	%zmm29, %zmm1, %zmm13   # zmm13 = (zmm1 * zmm29) + zmm13
	vmovaps	%zmm13, 9792(%rsp)              # 64-byte Spill
	vmulps	%zmm26, %zmm0, %zmm9
	vmovaps	1088(%rsp), %zmm26              # 64-byte Reload
	vfmadd231ps	%zmm26, %zmm1, %zmm9    # zmm9 = (zmm1 * zmm26) + zmm9
	vmovaps	%zmm9, 12992(%rsp)              # 64-byte Spill
	vmulps	%zmm27, %zmm0, %zmm9
	vmovaps	%zmm30, %zmm17
	vfmadd231ps	%zmm30, %zmm1, %zmm9    # zmm9 = (zmm1 * zmm30) + zmm9
	vmovaps	%zmm9, 13056(%rsp)              # 64-byte Spill
	vmovaps	2432(%rsp), %zmm25              # 64-byte Reload
	vmulps	%zmm25, %zmm0, %zmm0
	vfmadd231ps	1792(%rsp), %zmm1, %zmm0 # 64-byte Folded Reload
                                        # zmm0 = (zmm1 * mem) + zmm0
	vmovaps	%zmm0, 13568(%rsp)              # 64-byte Spill
	vaddss	15616(%rsp), %xmm23, %xmm0      # 4-byte Folded Reload
	vmovaps	%xmm0, 11344(%rsp)              # 16-byte Spill
	vaddss	15360(%rsp), %xmm23, %xmm0      # 4-byte Folded Reload
	vmovaps	%xmm0, 11328(%rsp)              # 16-byte Spill
	vaddss	14080(%rsp), %xmm23, %xmm0      # 4-byte Folded Reload
	vmovaps	%xmm0, 11312(%rsp)              # 16-byte Spill
	vaddss	14592(%rsp), %xmm23, %xmm0      # 4-byte Folded Reload
	vmovaps	%xmm0, 11392(%rsp)              # 16-byte Spill
	vaddss	12800(%rsp), %xmm23, %xmm0      # 4-byte Folded Reload
	vmovaps	%xmm0, 11376(%rsp)              # 16-byte Spill
	vaddss	9664(%rsp), %xmm23, %xmm0       # 4-byte Folded Reload
	vmovaps	%xmm0, 11360(%rsp)              # 16-byte Spill
	vaddss	16000(%rsp), %xmm23, %xmm0      # 4-byte Folded Reload
	vmovaps	%xmm0, 11440(%rsp)              # 16-byte Spill
	vaddss	15936(%rsp), %xmm23, %xmm0      # 4-byte Folded Reload
	vmovaps	%xmm0, 11424(%rsp)              # 16-byte Spill
	vaddss	14464(%rsp), %xmm23, %xmm0      # 4-byte Folded Reload
	vmovaps	%xmm0, 11408(%rsp)              # 16-byte Spill
	vaddss	15232(%rsp), %xmm23, %xmm0      # 4-byte Folded Reload
	vmovaps	%xmm0, 12800(%rsp)              # 16-byte Spill
	vaddss	13248(%rsp), %xmm23, %xmm0      # 4-byte Folded Reload
	vmovaps	%xmm0, 13248(%rsp)              # 16-byte Spill
	vaddss	9600(%rsp), %xmm23, %xmm0       # 4-byte Folded Reload
	vmovaps	%xmm0, 9664(%rsp)               # 16-byte Spill
	vmovaps	6592(%rsp), %zmm0               # 64-byte Reload
	vmulps	1600(%rsp), %zmm0, %zmm9        # 64-byte Folded Reload
	vmovaps	4992(%rsp), %zmm1               # 64-byte Reload
	vfmadd231ps	%zmm22, %zmm1, %zmm9    # zmm9 = (zmm1 * zmm22) + zmm9
	vmovaps	%zmm9, 13760(%rsp)              # 64-byte Spill
	vmulps	%zmm24, %zmm0, %zmm9
	vfmadd231ps	%zmm31, %zmm1, %zmm9    # zmm9 = (zmm1 * zmm31) + zmm9
	vmovaps	%zmm9, 14080(%rsp)              # 64-byte Spill
	vmovaps	2304(%rsp), %zmm30              # 64-byte Reload
	vmulps	%zmm30, %zmm0, %zmm13
	vmovaps	1536(%rsp), %zmm16              # 64-byte Reload
	vfmadd231ps	%zmm16, %zmm1, %zmm13   # zmm13 = (zmm1 * zmm16) + zmm13
	vmovaps	%zmm13, 14144(%rsp)             # 64-byte Spill
	vmulps	%zmm12, %zmm0, %zmm13
	vfmadd231ps	%zmm11, %zmm1, %zmm13   # zmm13 = (zmm1 * zmm11) + zmm13
	vmovaps	%zmm13, 14464(%rsp)             # 64-byte Spill
	vmovaps	%zmm11, %zmm27
	vmulps	%zmm28, %zmm0, %zmm13
	vmovaps	704(%rsp), %zmm31               # 64-byte Reload
	vfmadd231ps	%zmm31, %zmm1, %zmm13   # zmm13 = (zmm1 * zmm31) + zmm13
	vmovaps	%zmm13, 14592(%rsp)             # 64-byte Spill
	vmulps	%zmm15, %zmm0, %zmm13
	vfmadd231ps	%zmm10, %zmm1, %zmm13   # zmm13 = (zmm1 * zmm10) + zmm13
	vmovaps	%zmm13, 32832(%rsp)             # 64-byte Spill
	vmovaps	%zmm10, %zmm22
	vmulps	%zmm14, %zmm0, %zmm13
	vfmadd231ps	%zmm7, %zmm1, %zmm13    # zmm13 = (zmm1 * zmm7) + zmm13
	vmovaps	%zmm13, 14720(%rsp)             # 64-byte Spill
	vmovaps	%zmm7, %zmm10
	vmovaps	2368(%rsp), %zmm11              # 64-byte Reload
	vmulps	%zmm11, %zmm0, %zmm13
	vfmadd231ps	2560(%rsp), %zmm1, %zmm13 # 64-byte Folded Reload
                                        # zmm13 = (zmm1 * mem) + zmm13
	vmovaps	%zmm13, 15232(%rsp)             # 64-byte Spill
	vmulps	%zmm8, %zmm0, %zmm13
	vmovaps	%zmm8, %zmm12
	vfmadd231ps	%zmm6, %zmm1, %zmm13    # zmm13 = (zmm1 * zmm6) + zmm13
	vmovaps	%zmm13, 15360(%rsp)             # 64-byte Spill
	vmovaps	%zmm6, %zmm14
	vmulps	%zmm2, %zmm0, %zmm13
	vmovaps	%zmm2, %zmm9
	vfmadd231ps	%zmm3, %zmm1, %zmm13    # zmm13 = (zmm1 * zmm3) + zmm13
	vmovaps	%zmm13, 15424(%rsp)             # 64-byte Spill
	vmovaps	640(%rsp), %zmm3                # 64-byte Reload
	vmulps	%zmm3, %zmm0, %zmm13
	vfmadd231ps	64(%rsp), %zmm1, %zmm13 # 64-byte Folded Reload
                                        # zmm13 = (zmm1 * mem) + zmm13
	vmovaps	%zmm13, 15616(%rsp)             # 64-byte Spill
	vmulps	%zmm4, %zmm0, %zmm13
	vfmadd231ps	%zmm5, %zmm1, %zmm13    # zmm13 = (zmm1 * zmm5) + zmm13
	vmovaps	%zmm13, 15872(%rsp)             # 64-byte Spill
	vmovaps	576(%rsp), %zmm29               # 64-byte Reload
	vmulps	%zmm29, %zmm0, %zmm13
	vmovaps	%zmm20, %zmm5
	vfmadd231ps	%zmm20, %zmm1, %zmm13   # zmm13 = (zmm1 * zmm20) + zmm13
	vmovaps	%zmm13, 15936(%rsp)             # 64-byte Spill
	vmulps	%zmm19, %zmm0, %zmm13
	vmovaps	1472(%rsp), %zmm15              # 64-byte Reload
	vfmadd231ps	%zmm15, %zmm1, %zmm13   # zmm13 = (zmm1 * zmm15) + zmm13
	vmovaps	%zmm13, 33728(%rsp)             # 64-byte Spill
	vmulps	%zmm18, %zmm0, %zmm20
	vmovaps	2496(%rsp), %zmm13              # 64-byte Reload
	vfmadd231ps	%zmm13, %zmm1, %zmm20   # zmm20 = (zmm1 * zmm13) + zmm20
	vmovaps	%zmm20, 16000(%rsp)             # 64-byte Spill
	vmulps	%zmm21, %zmm0, %zmm20
	vfmadd231ps	1856(%rsp), %zmm1, %zmm20 # 64-byte Folded Reload
                                        # zmm20 = (zmm1 * mem) + zmm20
	vmovaps	%zmm20, 16064(%rsp)             # 64-byte Spill
	vmulps	192(%rsp), %zmm0, %zmm20        # 64-byte Folded Reload
	vfmadd231ps	1344(%rsp), %zmm1, %zmm20 # 64-byte Folded Reload
                                        # zmm20 = (zmm1 * mem) + zmm20
	vmovaps	%zmm20, 16128(%rsp)             # 64-byte Spill
	vmulps	2240(%rsp), %zmm0, %zmm20       # 64-byte Folded Reload
	vmovaps	%zmm26, %zmm8
	vfmadd231ps	%zmm26, %zmm1, %zmm20   # zmm20 = (zmm1 * zmm26) + zmm20
	vmovaps	%zmm20, 34176(%rsp)             # 64-byte Spill
	vmovaps	2176(%rsp), %zmm4               # 64-byte Reload
	vmulps	%zmm4, %zmm0, %zmm20
	vfmadd231ps	%zmm17, %zmm1, %zmm20   # zmm20 = (zmm1 * zmm17) + zmm20
	vmovaps	%zmm20, 16704(%rsp)             # 64-byte Spill
	vmulps	%zmm25, %zmm0, %zmm0
	vmovaps	1792(%rsp), %zmm2               # 64-byte Reload
	vfmadd231ps	%zmm1, %zmm2, %zmm0     # zmm0 = (zmm2 * zmm1) + zmm0
	vmovaps	%zmm0, 16768(%rsp)              # 64-byte Spill
	vaddss	14528(%rsp), %xmm23, %xmm0      # 4-byte Folded Reload
	vmovaps	%xmm0, 9600(%rsp)               # 16-byte Spill
	vaddss	14400(%rsp), %xmm23, %xmm0      # 4-byte Folded Reload
	vmovaps	%xmm0, 11296(%rsp)              # 16-byte Spill
	vaddss	13376(%rsp), %xmm23, %xmm0      # 4-byte Folded Reload
	vmovaps	%xmm0, 11280(%rsp)              # 16-byte Spill
	vaddss	13888(%rsp), %xmm23, %xmm0      # 4-byte Folded Reload
	vmovaps	%xmm0, 13376(%rsp)              # 16-byte Spill
	vaddss	9856(%rsp), %xmm23, %xmm0       # 4-byte Folded Reload
	vmovaps	%xmm0, 9856(%rsp)               # 16-byte Spill
	vaddss	12864(%rsp), %xmm23, %xmm0      # 4-byte Folded Reload
	vmovaps	%xmm0, 12864(%rsp)              # 16-byte Spill
	vaddss	14784(%rsp), %xmm23, %xmm0      # 4-byte Folded Reload
	vmovaps	%xmm0, 14400(%rsp)              # 16-byte Spill
	vaddss	14656(%rsp), %xmm23, %xmm0      # 4-byte Folded Reload
	vmovaps	%xmm0, 13888(%rsp)              # 16-byte Spill
	vaddss	13696(%rsp), %xmm23, %xmm0      # 4-byte Folded Reload
	vmovaps	%xmm0, 13696(%rsp)              # 16-byte Spill
	vaddss	14336(%rsp), %xmm23, %xmm0      # 4-byte Folded Reload
	vmovaps	%xmm0, 14784(%rsp)              # 16-byte Spill
	vaddss	9728(%rsp), %xmm23, %xmm0       # 4-byte Folded Reload
	vmovaps	%xmm0, 14656(%rsp)              # 16-byte Spill
	vaddss	12736(%rsp), %xmm23, %xmm0      # 4-byte Folded Reload
	vmovaps	%xmm0, 14528(%rsp)              # 16-byte Spill
	vmovaps	5056(%rsp), %zmm0               # 64-byte Reload
	vmovaps	1600(%rsp), %zmm6               # 64-byte Reload
	vmulps	%zmm6, %zmm0, %zmm26
	vmovaps	5120(%rsp), %zmm1               # 64-byte Reload
	vmovaps	1664(%rsp), %zmm7               # 64-byte Reload
	vfmadd231ps	%zmm7, %zmm1, %zmm26    # zmm26 = (zmm1 * zmm7) + zmm26
	vmovaps	%zmm26, 35648(%rsp)             # 64-byte Spill
	vmulps	%zmm24, %zmm0, %zmm26
	vfmadd231ps	768(%rsp), %zmm1, %zmm26 # 64-byte Folded Reload
                                        # zmm26 = (zmm1 * mem) + zmm26
	vmovaps	%zmm26, 36032(%rsp)             # 64-byte Spill
	vmulps	%zmm30, %zmm0, %zmm26
	vfmadd231ps	%zmm16, %zmm1, %zmm26   # zmm26 = (zmm1 * zmm16) + zmm26
	vmovaps	%zmm26, 16960(%rsp)             # 64-byte Spill
	vmulps	448(%rsp), %zmm0, %zmm26        # 64-byte Folded Reload
	vfmadd231ps	%zmm27, %zmm1, %zmm26   # zmm26 = (zmm1 * zmm27) + zmm26
	vmovaps	%zmm26, 36224(%rsp)             # 64-byte Spill
	vmulps	%zmm28, %zmm0, %zmm26
	vfmadd231ps	%zmm31, %zmm1, %zmm26   # zmm26 = (zmm1 * zmm31) + zmm26
	vmovaps	%zmm26, 36288(%rsp)             # 64-byte Spill
	vmulps	256(%rsp), %zmm0, %zmm30        # 64-byte Folded Reload
	vfmadd231ps	%zmm22, %zmm1, %zmm30   # zmm30 = (zmm1 * zmm22) + zmm30
	vmovaps	%zmm30, 17024(%rsp)             # 64-byte Spill
	vmulps	128(%rsp), %zmm0, %zmm30        # 64-byte Folded Reload
	vfmadd231ps	%zmm10, %zmm1, %zmm30   # zmm30 = (zmm1 * zmm10) + zmm30
	vmovaps	%zmm10, %zmm22
	vmovaps	%zmm30, 17216(%rsp)             # 64-byte Spill
	vmovaps	%zmm11, %zmm17
	vmulps	%zmm11, %zmm0, %zmm30
	vmovaps	2560(%rsp), %zmm10              # 64-byte Reload
	vfmadd231ps	%zmm10, %zmm1, %zmm30   # zmm30 = (zmm1 * zmm10) + zmm30
	vmovaps	%zmm30, 17344(%rsp)             # 64-byte Spill
	vmulps	%zmm12, %zmm0, %zmm30
	vmovaps	%zmm12, %zmm27
	vfmadd231ps	%zmm14, %zmm1, %zmm30   # zmm30 = (zmm1 * zmm14) + zmm30
	vmovaps	%zmm30, 17408(%rsp)             # 64-byte Spill
	vmulps	%zmm9, %zmm0, %zmm30
	vmovaps	960(%rsp), %zmm16               # 64-byte Reload
	vfmadd231ps	%zmm16, %zmm1, %zmm30   # zmm30 = (zmm1 * zmm16) + zmm30
	vmovaps	%zmm30, 17472(%rsp)             # 64-byte Spill
	vmulps	%zmm3, %zmm0, %zmm30
	vmovaps	64(%rsp), %zmm3                 # 64-byte Reload
	vfmadd231ps	%zmm3, %zmm1, %zmm30    # zmm30 = (zmm1 * zmm3) + zmm30
	vmovaps	%zmm30, 17600(%rsp)             # 64-byte Spill
	vmovaps	1280(%rsp), %zmm9               # 64-byte Reload
	vmulps	%zmm9, %zmm0, %zmm30
	vmovaps	1216(%rsp), %zmm11              # 64-byte Reload
	vfmadd231ps	%zmm11, %zmm1, %zmm30   # zmm30 = (zmm1 * zmm11) + zmm30
	vmovaps	%zmm30, 17664(%rsp)             # 64-byte Spill
	vmulps	%zmm29, %zmm0, %zmm30
	vfmadd231ps	%zmm5, %zmm1, %zmm30    # zmm30 = (zmm1 * zmm5) + zmm30
	vmovaps	%zmm30, 17728(%rsp)             # 64-byte Spill
	vmulps	%zmm19, %zmm0, %zmm30
	vmovaps	%zmm19, %zmm26
	vfmadd231ps	%zmm15, %zmm1, %zmm30   # zmm30 = (zmm1 * zmm15) + zmm30
	vmovaps	%zmm30, 17792(%rsp)             # 64-byte Spill
	vmulps	%zmm18, %zmm0, %zmm30
	vfmadd231ps	%zmm13, %zmm1, %zmm30   # zmm30 = (zmm1 * zmm13) + zmm30
	vmovaps	%zmm30, 17856(%rsp)             # 64-byte Spill
	vmulps	%zmm21, %zmm0, %zmm30
	vmovaps	1856(%rsp), %zmm15              # 64-byte Reload
	vfmadd231ps	%zmm15, %zmm1, %zmm30   # zmm30 = (zmm1 * zmm15) + zmm30
	vmovaps	%zmm30, 17920(%rsp)             # 64-byte Spill
	vmovaps	192(%rsp), %zmm13               # 64-byte Reload
	vmulps	%zmm13, %zmm0, %zmm30
	vmovaps	1344(%rsp), %zmm18              # 64-byte Reload
	vfmadd231ps	%zmm18, %zmm1, %zmm30   # zmm30 = (zmm1 * zmm18) + zmm30
	vmovaps	%zmm30, 17984(%rsp)             # 64-byte Spill
	vmovaps	2240(%rsp), %zmm31              # 64-byte Reload
	vmulps	%zmm31, %zmm0, %zmm30
	vfmadd231ps	%zmm8, %zmm1, %zmm30    # zmm30 = (zmm1 * zmm8) + zmm30
	vmovaps	%zmm30, 36992(%rsp)             # 64-byte Spill
	vmulps	%zmm4, %zmm0, %zmm30
	vmovaps	1728(%rsp), %zmm20              # 64-byte Reload
	vfmadd231ps	%zmm20, %zmm1, %zmm30   # zmm30 = (zmm1 * zmm20) + zmm30
	vmovaps	%zmm30, 18048(%rsp)             # 64-byte Spill
	vmulps	%zmm25, %zmm0, %zmm0
	vfmadd231ps	%zmm1, %zmm2, %zmm0     # zmm0 = (zmm2 * zmm1) + zmm0
	vmovaps	%zmm0, 8512(%rsp)               # 64-byte Spill
	vaddss	13824(%rsp), %xmm23, %xmm0      # 4-byte Folded Reload
	vmovaps	%xmm0, 9728(%rsp)               # 16-byte Spill
	vaddss	13632(%rsp), %xmm23, %xmm0      # 4-byte Folded Reload
	vmovaps	%xmm0, 12736(%rsp)              # 16-byte Spill
	vaddss	13504(%rsp), %xmm23, %xmm0      # 4-byte Folded Reload
	vmovaps	%xmm0, 11264(%rsp)              # 16-byte Spill
	vaddss	9984(%rsp), %xmm23, %xmm0       # 4-byte Folded Reload
	vmovaps	%xmm0, 9984(%rsp)               # 16-byte Spill
	vaddss	13312(%rsp), %xmm23, %xmm0      # 4-byte Folded Reload
	vmovaps	%xmm0, 13312(%rsp)              # 16-byte Spill
	vaddss	13184(%rsp), %xmm23, %xmm0      # 4-byte Folded Reload
	vmovaps	%xmm0, 13184(%rsp)              # 16-byte Spill
	vaddss	14016(%rsp), %xmm23, %xmm0      # 4-byte Folded Reload
	vmovaps	%xmm0, 13824(%rsp)              # 16-byte Spill
	vaddss	13952(%rsp), %xmm23, %xmm0      # 4-byte Folded Reload
	vmovaps	%xmm0, 13632(%rsp)              # 16-byte Spill
	vaddss	13120(%rsp), %xmm23, %xmm0      # 4-byte Folded Reload
	vmovaps	%xmm0, 13504(%rsp)              # 16-byte Spill
	vaddss	13440(%rsp), %xmm23, %xmm0      # 4-byte Folded Reload
	vmovaps	%xmm0, 14336(%rsp)              # 16-byte Spill
	vaddss	9920(%rsp), %xmm23, %xmm0       # 4-byte Folded Reload
	vmovaps	%xmm0, 14016(%rsp)              # 16-byte Spill
	vaddss	12928(%rsp), %xmm23, %xmm0      # 4-byte Folded Reload
	vmovaps	%xmm0, 13952(%rsp)              # 16-byte Spill
	vmovaps	5184(%rsp), %zmm0               # 64-byte Reload
	vmulps	%zmm6, %zmm0, %zmm30
	vmovaps	5248(%rsp), %zmm1               # 64-byte Reload
	vfmadd231ps	%zmm7, %zmm1, %zmm30    # zmm30 = (zmm1 * zmm7) + zmm30
	vmovaps	%zmm30, 8576(%rsp)              # 64-byte Spill
	vmulps	%zmm24, %zmm0, %zmm30
	vmovaps	768(%rsp), %zmm8                # 64-byte Reload
	vfmadd231ps	%zmm8, %zmm1, %zmm30    # zmm30 = (zmm1 * zmm8) + zmm30
	vmovaps	%zmm30, 8640(%rsp)              # 64-byte Spill
	vmovaps	2304(%rsp), %zmm24              # 64-byte Reload
	vmulps	%zmm24, %zmm0, %zmm30
	vmovaps	1536(%rsp), %zmm4               # 64-byte Reload
	vfmadd231ps	%zmm4, %zmm1, %zmm30    # zmm30 = (zmm1 * zmm4) + zmm30
	vmovaps	%zmm30, 7744(%rsp)              # 64-byte Spill
	vmovaps	448(%rsp), %zmm5                # 64-byte Reload
	vmulps	%zmm5, %zmm0, %zmm30
	vmovaps	512(%rsp), %zmm12               # 64-byte Reload
	vfmadd231ps	%zmm12, %zmm1, %zmm30   # zmm30 = (zmm1 * zmm12) + zmm30
	vmovaps	%zmm30, 7808(%rsp)              # 64-byte Spill
	vmulps	%zmm28, %zmm0, %zmm30
	vmovaps	704(%rsp), %zmm19               # 64-byte Reload
	vfmadd231ps	%zmm19, %zmm1, %zmm30   # zmm30 = (zmm1 * zmm19) + zmm30
	vmovaps	%zmm30, 7872(%rsp)              # 64-byte Spill
	vmovaps	256(%rsp), %zmm6                # 64-byte Reload
	vmulps	%zmm6, %zmm0, %zmm30
	vmovaps	1408(%rsp), %zmm14              # 64-byte Reload
	vfmadd231ps	%zmm14, %zmm1, %zmm30   # zmm30 = (zmm1 * zmm14) + zmm30
	vmovaps	%zmm30, 7936(%rsp)              # 64-byte Spill
	vmovaps	128(%rsp), %zmm7                # 64-byte Reload
	vmulps	%zmm7, %zmm0, %zmm30
	vfmadd231ps	%zmm22, %zmm1, %zmm30   # zmm30 = (zmm1 * zmm22) + zmm30
	vmovaps	%zmm30, 8000(%rsp)              # 64-byte Spill
	vmulps	%zmm17, %zmm0, %zmm30
	vfmadd231ps	%zmm10, %zmm1, %zmm30   # zmm30 = (zmm1 * zmm10) + zmm30
	vmovaps	%zmm30, 8064(%rsp)              # 64-byte Spill
	vmulps	%zmm27, %zmm0, %zmm30
	vmovaps	1024(%rsp), %zmm10              # 64-byte Reload
	vfmadd231ps	%zmm10, %zmm1, %zmm30   # zmm30 = (zmm1 * zmm10) + zmm30
	vmovaps	%zmm30, 8128(%rsp)              # 64-byte Spill
	vmulps	832(%rsp), %zmm0, %zmm30        # 64-byte Folded Reload
	vfmadd231ps	%zmm16, %zmm1, %zmm30   # zmm30 = (zmm1 * zmm16) + zmm30
	vmovaps	%zmm30, 6720(%rsp)              # 64-byte Spill
	vmovaps	640(%rsp), %zmm29               # 64-byte Reload
	vmulps	%zmm29, %zmm0, %zmm30
	vfmadd231ps	%zmm3, %zmm1, %zmm30    # zmm30 = (zmm1 * zmm3) + zmm30
	vmovaps	%zmm30, 6784(%rsp)              # 64-byte Spill
	vmulps	%zmm9, %zmm0, %zmm30
	vfmadd231ps	%zmm11, %zmm1, %zmm30   # zmm30 = (zmm1 * zmm11) + zmm30
	vmovaps	%zmm30, 6848(%rsp)              # 64-byte Spill
	vmovaps	576(%rsp), %zmm23               # 64-byte Reload
	vmulps	%zmm23, %zmm0, %zmm30
	vmovaps	1920(%rsp), %zmm17              # 64-byte Reload
	vfmadd231ps	%zmm17, %zmm1, %zmm30   # zmm30 = (zmm1 * zmm17) + zmm30
	vmovaps	%zmm30, 6912(%rsp)              # 64-byte Spill
	vmulps	%zmm26, %zmm0, %zmm30
	vfmadd231ps	1472(%rsp), %zmm1, %zmm30 # 64-byte Folded Reload
                                        # zmm30 = (zmm1 * mem) + zmm30
	vmovaps	%zmm30, 6976(%rsp)              # 64-byte Spill
	vmovaps	2112(%rsp), %zmm16              # 64-byte Reload
	vmulps	%zmm16, %zmm0, %zmm30
	vmovaps	2496(%rsp), %zmm26              # 64-byte Reload
	vfmadd231ps	%zmm26, %zmm1, %zmm30   # zmm30 = (zmm1 * zmm26) + zmm30
	vmovaps	%zmm30, 6144(%rsp)              # 64-byte Spill
	vmulps	%zmm21, %zmm0, %zmm30
	vfmadd231ps	%zmm15, %zmm1, %zmm30   # zmm30 = (zmm1 * zmm15) + zmm30
	vmovaps	%zmm30, 6208(%rsp)              # 64-byte Spill
	vmulps	%zmm13, %zmm0, %zmm30
	vfmadd231ps	%zmm18, %zmm1, %zmm30   # zmm30 = (zmm1 * zmm18) + zmm30
	vmovaps	%zmm30, 6272(%rsp)              # 64-byte Spill
	vmulps	%zmm31, %zmm0, %zmm30
	vmovaps	1088(%rsp), %zmm25              # 64-byte Reload
	vfmadd231ps	%zmm25, %zmm1, %zmm30   # zmm30 = (zmm1 * zmm25) + zmm30
	vmovaps	%zmm30, 6336(%rsp)              # 64-byte Spill
	vmovaps	2176(%rsp), %zmm2               # 64-byte Reload
	vmulps	%zmm2, %zmm0, %zmm30
	vfmadd231ps	%zmm20, %zmm1, %zmm30   # zmm30 = (zmm1 * zmm20) + zmm30
	vmovaps	%zmm20, %zmm22
	vmovaps	%zmm30, 6400(%rsp)              # 64-byte Spill
	vmovaps	2432(%rsp), %zmm9               # 64-byte Reload
	vmulps	%zmm9, %zmm0, %zmm0
	vmovaps	1792(%rsp), %zmm11              # 64-byte Reload
	vfmadd231ps	%zmm1, %zmm11, %zmm0    # zmm0 = (zmm11 * zmm1) + zmm0
	vmovaps	%zmm0, 6464(%rsp)               # 64-byte Spill
	vxorps	%xmm0, %xmm0, %xmm0
	vaddss	10816(%rsp), %xmm0, %xmm1       # 4-byte Folded Reload
	vmovaps	%xmm1, 12928(%rsp)              # 16-byte Spill
	vaddss	10752(%rsp), %xmm0, %xmm1       # 4-byte Folded Reload
	vmovaps	%xmm1, 11232(%rsp)              # 16-byte Spill
	vaddss	10688(%rsp), %xmm0, %xmm1       # 4-byte Folded Reload
	vmovaps	%xmm1, 11248(%rsp)              # 16-byte Spill
	vaddss	10560(%rsp), %xmm0, %xmm1       # 4-byte Folded Reload
	vmovaps	%xmm1, 13440(%rsp)              # 16-byte Spill
	vaddss	10496(%rsp), %xmm0, %xmm1       # 4-byte Folded Reload
	vmovaps	%xmm1, 13120(%rsp)              # 16-byte Spill
	vaddss	10432(%rsp), %xmm0, %xmm1       # 4-byte Folded Reload
	vmovaps	%xmm1, 9920(%rsp)               # 16-byte Spill
	vaddss	18688(%rsp), %xmm0, %xmm1       # 4-byte Folded Reload
	vmovaps	%xmm1, 10560(%rsp)              # 16-byte Spill
	vaddss	37696(%rsp), %xmm0, %xmm1       # 4-byte Folded Reload
	vmovaps	%xmm1, 10496(%rsp)              # 16-byte Spill
	vaddss	37632(%rsp), %xmm0, %xmm1       # 4-byte Folded Reload
	vmovaps	%xmm1, 10432(%rsp)              # 16-byte Spill
	vaddss	31488(%rsp), %xmm0, %xmm1       # 4-byte Folded Reload
	vmovaps	%xmm1, 10816(%rsp)              # 16-byte Spill
	vaddss	31424(%rsp), %xmm0, %xmm1       # 4-byte Folded Reload
	vmovaps	%xmm1, 10752(%rsp)              # 16-byte Spill
	vaddss	31360(%rsp), %xmm0, %xmm0       # 4-byte Folded Reload
	vmovaps	%xmm0, 10688(%rsp)              # 16-byte Spill
	vxorps	%xmm27, %xmm27, %xmm27
	vmovaps	5312(%rsp), %zmm0               # 64-byte Reload
	vmovaps	1600(%rsp), %zmm18              # 64-byte Reload
	vmulps	%zmm18, %zmm0, %zmm30
	vmovaps	5376(%rsp), %zmm1               # 64-byte Reload
	vmovaps	1664(%rsp), %zmm20              # 64-byte Reload
	vfmadd231ps	%zmm20, %zmm1, %zmm30   # zmm30 = (zmm1 * zmm20) + zmm30
	vmovaps	%zmm30, 6528(%rsp)              # 64-byte Spill
	vmovaps	1152(%rsp), %zmm15              # 64-byte Reload
	vmulps	%zmm15, %zmm0, %zmm30
	vfmadd231ps	%zmm8, %zmm1, %zmm30    # zmm30 = (zmm1 * zmm8) + zmm30
	vmovaps	%zmm30, 6592(%rsp)              # 64-byte Spill
	vmulps	%zmm24, %zmm0, %zmm30
	vfmadd231ps	%zmm4, %zmm1, %zmm30    # zmm30 = (zmm1 * zmm4) + zmm30
	vmovaps	%zmm30, 3200(%rsp)              # 64-byte Spill
	vmulps	%zmm5, %zmm0, %zmm30
	vfmadd231ps	%zmm12, %zmm1, %zmm30   # zmm30 = (zmm1 * zmm12) + zmm30
	vmovaps	%zmm30, 3264(%rsp)              # 64-byte Spill
	vmulps	%zmm28, %zmm0, %zmm30
	vfmadd231ps	%zmm19, %zmm1, %zmm30   # zmm30 = (zmm1 * zmm19) + zmm30
	vmovaps	%zmm30, 3328(%rsp)              # 64-byte Spill
	vmulps	%zmm6, %zmm0, %zmm30
	vfmadd231ps	%zmm14, %zmm1, %zmm30   # zmm30 = (zmm1 * zmm14) + zmm30
	vmovaps	%zmm30, 3392(%rsp)              # 64-byte Spill
	vmulps	%zmm7, %zmm0, %zmm30
	vmovaps	896(%rsp), %zmm7                # 64-byte Reload
	vfmadd231ps	%zmm7, %zmm1, %zmm30    # zmm30 = (zmm1 * zmm7) + zmm30
	vmovaps	%zmm30, 3456(%rsp)              # 64-byte Spill
	vmovaps	2368(%rsp), %zmm13              # 64-byte Reload
	vmulps	%zmm13, %zmm0, %zmm30
	vmovaps	2560(%rsp), %zmm14              # 64-byte Reload
	vfmadd231ps	%zmm14, %zmm1, %zmm30   # zmm30 = (zmm1 * zmm14) + zmm30
	vmovaps	%zmm30, 3520(%rsp)              # 64-byte Spill
	vmovaps	1984(%rsp), %zmm12              # 64-byte Reload
	vmulps	%zmm12, %zmm0, %zmm30
	vfmadd231ps	%zmm10, %zmm1, %zmm30   # zmm30 = (zmm1 * zmm10) + zmm30
	vmovaps	%zmm30, 3584(%rsp)              # 64-byte Spill
	vmovaps	832(%rsp), %zmm8                # 64-byte Reload
	vmulps	%zmm8, %zmm0, %zmm30
	vmovaps	960(%rsp), %zmm5                # 64-byte Reload
	vfmadd231ps	%zmm5, %zmm1, %zmm30    # zmm30 = (zmm1 * zmm5) + zmm30
	vmovaps	%zmm30, 3648(%rsp)              # 64-byte Spill
	vmovaps	%zmm29, %zmm6
	vmulps	%zmm29, %zmm0, %zmm30
	vfmadd231ps	%zmm3, %zmm1, %zmm30    # zmm30 = (zmm1 * zmm3) + zmm30
	vmovaps	%zmm3, %zmm24
	vmovaps	%zmm30, 4736(%rsp)              # 64-byte Spill
	vmovaps	1280(%rsp), %zmm3               # 64-byte Reload
	vmulps	%zmm3, %zmm0, %zmm30
	vmovaps	1216(%rsp), %zmm4               # 64-byte Reload
	vfmadd231ps	%zmm4, %zmm1, %zmm30    # zmm30 = (zmm1 * zmm4) + zmm30
	vmovaps	%zmm30, 4800(%rsp)              # 64-byte Spill
	vmulps	%zmm23, %zmm0, %zmm30
	vfmadd231ps	%zmm17, %zmm1, %zmm30   # zmm30 = (zmm1 * zmm17) + zmm30
	vmovaps	%zmm30, 4864(%rsp)              # 64-byte Spill
	vmovaps	3072(%rsp), %zmm19              # 64-byte Reload
	vmulps	%zmm19, %zmm0, %zmm30
	vmovaps	1472(%rsp), %zmm29              # 64-byte Reload
	vfmadd231ps	%zmm29, %zmm1, %zmm30   # zmm30 = (zmm1 * zmm29) + zmm30
	vmovaps	%zmm30, 4928(%rsp)              # 64-byte Spill
	vmulps	%zmm16, %zmm0, %zmm30
	vfmadd231ps	%zmm26, %zmm1, %zmm30   # zmm30 = (zmm1 * zmm26) + zmm30
	vmovaps	%zmm30, 4992(%rsp)              # 64-byte Spill
	vmulps	%zmm21, %zmm0, %zmm30
	vmovaps	1856(%rsp), %zmm10              # 64-byte Reload
	vfmadd231ps	%zmm10, %zmm1, %zmm30   # zmm30 = (zmm1 * zmm10) + zmm30
	vmovaps	%zmm30, 5056(%rsp)              # 64-byte Spill
	vmovaps	192(%rsp), %zmm17               # 64-byte Reload
	vmulps	%zmm17, %zmm0, %zmm30
	vmovaps	1344(%rsp), %zmm16              # 64-byte Reload
	vfmadd231ps	%zmm16, %zmm1, %zmm30   # zmm30 = (zmm1 * zmm16) + zmm30
	vmovaps	%zmm30, 5120(%rsp)              # 64-byte Spill
	vmulps	%zmm31, %zmm0, %zmm30
	vfmadd231ps	%zmm25, %zmm1, %zmm30   # zmm30 = (zmm1 * zmm25) + zmm30
	vmovaps	%zmm30, 5184(%rsp)              # 64-byte Spill
	vmulps	%zmm2, %zmm0, %zmm30
	vfmadd231ps	%zmm22, %zmm1, %zmm30   # zmm30 = (zmm1 * zmm22) + zmm30
	vmovaps	%zmm22, %zmm25
	vmovaps	%zmm30, 5248(%rsp)              # 64-byte Spill
	vmulps	%zmm9, %zmm0, %zmm0
	vmovaps	%zmm9, %zmm30
	vfmadd231ps	%zmm1, %zmm11, %zmm0    # zmm0 = (zmm11 * zmm1) + zmm0
	vmovaps	%zmm0, 5312(%rsp)               # 64-byte Spill
	vmovaps	7040(%rsp), %zmm0               # 64-byte Reload
	vmulps	%zmm18, %zmm0, %zmm23
	vmovaps	7104(%rsp), %zmm1               # 64-byte Reload
	vfmadd231ps	%zmm20, %zmm1, %zmm23   # zmm23 = (zmm1 * zmm20) + zmm23
	vmovaps	%zmm23, 5376(%rsp)              # 64-byte Spill
	vmulps	%zmm15, %zmm0, %zmm22
	vfmadd231ps	768(%rsp), %zmm1, %zmm22 # 64-byte Folded Reload
                                        # zmm22 = (zmm1 * mem) + zmm22
	vmovaps	%zmm22, 1600(%rsp)              # 64-byte Spill
	vaddss	8768(%rsp), %xmm27, %xmm2       # 4-byte Folded Reload
	vmovaps	%xmm2, 18688(%rsp)              # 16-byte Spill
	vaddss	19200(%rsp), %xmm27, %xmm2      # 4-byte Folded Reload
	vmovaps	%xmm2, 8768(%rsp)               # 16-byte Spill
	vmulps	2304(%rsp), %zmm0, %zmm9        # 64-byte Folded Reload
	vfmadd231ps	1536(%rsp), %zmm1, %zmm9 # 64-byte Folded Reload
                                        # zmm9 = (zmm1 * mem) + zmm9
	vmovaps	%zmm9, 1664(%rsp)               # 64-byte Spill
	vmulps	448(%rsp), %zmm0, %zmm9         # 64-byte Folded Reload
	vfmadd231ps	512(%rsp), %zmm1, %zmm9 # 64-byte Folded Reload
                                        # zmm9 = (zmm1 * mem) + zmm9
	vmovaps	%zmm9, 1152(%rsp)               # 64-byte Spill
	vaddss	19584(%rsp), %xmm27, %xmm22     # 4-byte Folded Reload
	vaddss	10944(%rsp), %xmm27, %xmm23     # 4-byte Folded Reload
	vmulps	%zmm28, %zmm0, %zmm9
	vfmadd231ps	704(%rsp), %zmm1, %zmm9 # 64-byte Folded Reload
                                        # zmm9 = (zmm1 * mem) + zmm9
	vmovaps	%zmm9, 768(%rsp)                # 64-byte Spill
	vmulps	256(%rsp), %zmm0, %zmm9         # 64-byte Folded Reload
	vfmadd231ps	1408(%rsp), %zmm1, %zmm9 # 64-byte Folded Reload
                                        # zmm9 = (zmm1 * mem) + zmm9
	vmovaps	%zmm9, 704(%rsp)                # 64-byte Spill
	vaddss	8256(%rsp), %xmm27, %xmm18      # 4-byte Folded Reload
	vaddss	8192(%rsp), %xmm27, %xmm15      # 4-byte Folded Reload
	vmulps	128(%rsp), %zmm0, %zmm9         # 64-byte Folded Reload
	vfmadd231ps	%zmm7, %zmm1, %zmm9     # zmm9 = (zmm1 * zmm7) + zmm9
	vmovaps	%zmm9, 448(%rsp)                # 64-byte Spill
	vmulps	%zmm13, %zmm0, %zmm7
	vfmadd231ps	%zmm14, %zmm1, %zmm7    # zmm7 = (zmm1 * zmm14) + zmm7
	vmovaps	%zmm7, 512(%rsp)                # 64-byte Spill
	vmulps	%zmm12, %zmm0, %zmm7
	vfmadd231ps	1024(%rsp), %zmm1, %zmm7 # 64-byte Folded Reload
                                        # zmm7 = (zmm1 * mem) + zmm7
	vmovaps	%zmm7, 256(%rsp)                # 64-byte Spill
	vmulps	%zmm8, %zmm0, %zmm2
	vfmadd231ps	%zmm5, %zmm1, %zmm2     # zmm2 = (zmm1 * zmm5) + zmm2
	vmovaps	%zmm2, 1408(%rsp)               # 64-byte Spill
	vmulps	%zmm6, %zmm0, %zmm28
	vfmadd231ps	%zmm24, %zmm1, %zmm28   # zmm28 = (zmm1 * zmm24) + zmm28
	vmulps	%zmm3, %zmm0, %zmm2
	vfmadd231ps	%zmm4, %zmm1, %zmm2     # zmm2 = (zmm1 * zmm4) + zmm2
	vmovaps	%zmm2, 1024(%rsp)               # 64-byte Spill
	vaddss	7168(%rsp), %xmm27, %xmm6       # 4-byte Folded Reload
	vmulps	576(%rsp), %zmm0, %zmm2         # 64-byte Folded Reload
	vfmadd231ps	1920(%rsp), %zmm1, %zmm2 # 64-byte Folded Reload
                                        # zmm2 = (zmm1 * mem) + zmm2
	vmovaps	%zmm2, 832(%rsp)                # 64-byte Spill
	vmulps	%zmm19, %zmm0, %zmm14
	vfmadd231ps	%zmm29, %zmm1, %zmm14   # zmm14 = (zmm1 * zmm29) + zmm14
	vaddss	7296(%rsp), %xmm27, %xmm5       # 4-byte Folded Reload
	vaddss	7232(%rsp), %xmm27, %xmm4       # 4-byte Folded Reload
	vmulps	2112(%rsp), %zmm0, %zmm2        # 64-byte Folded Reload
	vfmadd231ps	%zmm26, %zmm1, %zmm2    # zmm2 = (zmm1 * zmm26) + zmm2
	vmovaps	%zmm2, 960(%rsp)                # 64-byte Spill
	vmulps	%zmm21, %zmm0, %zmm13
	vfmadd231ps	%zmm10, %zmm1, %zmm13   # zmm13 = (zmm1 * zmm10) + zmm13
	vaddss	7552(%rsp), %xmm27, %xmm2       # 4-byte Folded Reload
	vaddss	7488(%rsp), %xmm27, %xmm3       # 4-byte Folded Reload
	vmulps	%zmm17, %zmm0, %zmm12
	vfmadd231ps	%zmm16, %zmm1, %zmm12   # zmm12 = (zmm1 * zmm16) + zmm12
	vmulps	%zmm31, %zmm0, %zmm19
	vfmadd231ps	1088(%rsp), %zmm1, %zmm19 # 64-byte Folded Reload
                                        # zmm19 = (zmm1 * mem) + zmm19
	vaddss	7680(%rsp), %xmm27, %xmm29      # 4-byte Folded Reload
	vaddss	444(%rsp), %xmm27, %xmm7        # 4-byte Folded Reload
	vmulps	2176(%rsp), %zmm0, %zmm10       # 64-byte Folded Reload
	vfmadd231ps	%zmm25, %zmm1, %zmm10   # zmm10 = (zmm1 * zmm25) + zmm10
	vmulps	%zmm30, %zmm0, %zmm25
	vfmadd231ps	%zmm11, %zmm1, %zmm25   # zmm25 = (zmm1 * zmm11) + zmm25
	vmovaps	7360(%rsp), %zmm0               # 64-byte Reload
	vextractf64x4	$1, %zmm0, %ymm21
	vaddps	%zmm21, %zmm0, %zmm21
	vextractf32x4	$1, %ymm21, %xmm20
	vaddps	%xmm20, %xmm21, %xmm20
	vshufpd	$1, %xmm20, %xmm20, %xmm21      # xmm21 = xmm20[1,0]
	vaddps	%xmm21, %xmm20, %xmm20
	vmovshdup	%xmm20, %xmm21          # xmm21 = xmm20[1,1,3,3]
	vaddss	%xmm21, %xmm20, %xmm20
	vaddss	%xmm27, %xmm20, %xmm20
	vinsertps	$16, %xmm29, %xmm20, %xmm20 # xmm20 = xmm20[0],xmm29[0],xmm20[2,3]
	vinsertps	$32, %xmm2, %xmm20, %xmm0 # xmm0 = xmm20[0,1],xmm2[0],xmm20[3]
	vinsertps	$48, %xmm3, %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],xmm3[0]
	vmovaps	7424(%rsp), %zmm2               # 64-byte Reload
	vextractf64x4	$1, %zmm2, %ymm1
	vaddps	%zmm1, %zmm2, %zmm1
	vextractf32x4	$1, %ymm1, %xmm20
	vaddps	%xmm20, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm20        # xmm20 = xmm1[1,0]
	vaddps	%xmm20, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm20           # xmm20 = xmm1[1,1,3,3]
	vaddss	%xmm20, %xmm1, %xmm1
	vaddss	%xmm27, %xmm1, %xmm1
	vinsertps	$16, %xmm5, %xmm1, %xmm1 # xmm1 = xmm1[0],xmm5[0],xmm1[2,3]
	vinsertps	$32, %xmm4, %xmm1, %xmm1 # xmm1 = xmm1[0,1],xmm4[0],xmm1[3]
	vinsertps	$48, %xmm6, %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],xmm6[0]
	vinsertf128	$1, 7616(%rsp), %ymm0, %ymm0 # 16-byte Folded Reload
	vmovaps	5824(%rsp), %zmm2               # 64-byte Reload
	vinsertf64x4	$1, %ymm0, %zmm2, %zmm0
	vmovaps	24000(%rsp), %zmm2              # 64-byte Reload
	vaddps	%zmm0, %zmm2, %zmm2
	vmovaps	%zmm2, 24000(%rsp)              # 64-byte Spill
	vinsertf128	$1, 5952(%rsp), %ymm1, %ymm0 # 16-byte Folded Reload
	vmovaps	5888(%rsp), %zmm1               # 64-byte Reload
	vinsertf64x4	$1, %ymm0, %zmm1, %zmm0
	vmovaps	19968(%rsp), %zmm1              # 64-byte Reload
	vaddps	%zmm0, %zmm1, %zmm1
	vmovaps	%zmm1, 19968(%rsp)              # 64-byte Spill
	vmovaps	11008(%rsp), %zmm1              # 64-byte Reload
	vextractf64x4	$1, %zmm1, %ymm0
	vaddps	%zmm0, %zmm1, %zmm0
	vextractf128	$1, %ymm0, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm1         # xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm27, %xmm0, %xmm0
	vinsertps	$16, %xmm18, %xmm0, %xmm0 # xmm0 = xmm0[0],xmm18[0],xmm0[2,3]
	vinsertps	$32, %xmm15, %xmm0, %xmm0 # xmm0 = xmm0[0,1],xmm15[0],xmm0[3]
	vinsertps	$48, %xmm22, %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],xmm22[0]
	vmovaps	19264(%rsp), %zmm2              # 64-byte Reload
	vextractf64x4	$1, %zmm2, %ymm1
	vaddps	%zmm1, %zmm2, %zmm1
	vextractf128	$1, %ymm1, %xmm2
	vaddps	%xmm2, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm2         # xmm2 = xmm1[1,0]
	vaddps	%xmm2, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm2            # xmm2 = xmm1[1,1,3,3]
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm27, %xmm1, %xmm1
	vinsertps	$16, %xmm23, %xmm1, %xmm1 # xmm1 = xmm1[0],xmm23[0],xmm1[2,3]
	vinsertps	$32, 18688(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 8768(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, 5760(%rsp), %ymm0, %ymm0 # 16-byte Folded Reload
	vmovaps	8832(%rsp), %zmm2               # 64-byte Reload
	vinsertf64x4	$1, %ymm0, %zmm2, %zmm0
	vmovaps	20096(%rsp), %zmm2              # 64-byte Reload
	vaddps	%zmm0, %zmm2, %zmm2
	vmovaps	%zmm2, 20096(%rsp)              # 64-byte Spill
	vinsertf128	$1, 3776(%rsp), %ymm1, %ymm0 # 16-byte Folded Reload
	vmovaps	3712(%rsp), %zmm1               # 64-byte Reload
	vinsertf64x4	$1, %ymm0, %zmm1, %zmm0
	vmovaps	20032(%rsp), %zmm1              # 64-byte Reload
	vaddps	%zmm0, %zmm1, %zmm1
	vmovaps	%zmm1, 20032(%rsp)              # 64-byte Spill
	vmovaps	19072(%rsp), %zmm1              # 64-byte Reload
	vextractf64x4	$1, %zmm1, %ymm0
	vaddps	%zmm0, %zmm1, %zmm0
	vextractf128	$1, %ymm0, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm1         # xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm27, %xmm0, %xmm0
	vinsertps	$16, 12928(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, 11232(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, 11248(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	19328(%rsp), %zmm2              # 64-byte Reload
	vextractf64x4	$1, %zmm2, %ymm1
	vaddps	%zmm1, %zmm2, %zmm1
	vextractf128	$1, %ymm1, %xmm2
	vaddps	%xmm2, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm2         # xmm2 = xmm1[1,0]
	vaddps	%xmm2, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm2            # xmm2 = xmm1[1,1,3,3]
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm27, %xmm1, %xmm1
	vinsertps	$16, 13440(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, 13120(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 9920(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, 8704(%rsp), %ymm0, %ymm0 # 16-byte Folded Reload
	vmovaps	19136(%rsp), %zmm2              # 64-byte Reload
	vinsertf64x4	$1, %ymm0, %zmm2, %zmm0
	vmovaps	20224(%rsp), %zmm2              # 64-byte Reload
	vaddps	%zmm0, %zmm2, %zmm2
	vmovaps	%zmm2, 20224(%rsp)              # 64-byte Spill
	vinsertf128	$1, 3904(%rsp), %ymm1, %ymm0 # 16-byte Folded Reload
	vmovaps	3840(%rsp), %zmm1               # 64-byte Reload
	vinsertf64x4	$1, %ymm0, %zmm1, %zmm0
	vmovaps	20160(%rsp), %zmm1              # 64-byte Reload
	vaddps	%zmm0, %zmm1, %zmm1
	vmovaps	%zmm1, 20160(%rsp)              # 64-byte Spill
	vmovaps	37760(%rsp), %zmm1              # 64-byte Reload
	vextractf64x4	$1, %zmm1, %ymm0
	vaddps	%zmm0, %zmm1, %zmm0
	vextractf128	$1, %ymm0, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm1         # xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm27, %xmm0, %xmm0
	vinsertps	$16, 10560(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, 10496(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, 10432(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	19392(%rsp), %zmm2              # 64-byte Reload
	vextractf64x4	$1, %zmm2, %ymm1
	vaddps	%zmm1, %zmm2, %zmm1
	vextractf128	$1, %ymm1, %xmm2
	vaddps	%xmm2, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm2         # xmm2 = xmm1[1,0]
	vaddps	%xmm2, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm2            # xmm2 = xmm1[1,1,3,3]
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm27, %xmm1, %xmm1
	vinsertps	$16, 10816(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, 10752(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 10688(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, 10624(%rsp), %ymm0, %ymm0 # 16-byte Folded Reload
	vmovaps	19008(%rsp), %zmm2              # 64-byte Reload
	vinsertf64x4	$1, %ymm0, %zmm2, %zmm0
	vmovaps	20352(%rsp), %zmm2              # 64-byte Reload
	vaddps	%zmm0, %zmm2, %zmm2
	vmovaps	%zmm2, 20352(%rsp)              # 64-byte Spill
	vinsertf128	$1, 4032(%rsp), %ymm1, %ymm0 # 16-byte Folded Reload
	vmovaps	3968(%rsp), %zmm1               # 64-byte Reload
	vinsertf64x4	$1, %ymm0, %zmm1, %zmm0
	vmovaps	20288(%rsp), %zmm1              # 64-byte Reload
	vaddps	%zmm0, %zmm1, %zmm1
	vmovaps	%zmm1, 20288(%rsp)              # 64-byte Spill
	vmovaps	31680(%rsp), %zmm1              # 64-byte Reload
	vextractf64x4	$1, %zmm1, %ymm0
	vaddps	%zmm0, %zmm1, %zmm0
	vextractf128	$1, %ymm0, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm1         # xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm27, %xmm0, %xmm0
	vinsertps	$16, 9728(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, 12736(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, 11264(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	19456(%rsp), %zmm2              # 64-byte Reload
	vextractf64x4	$1, %zmm2, %ymm1
	vaddps	%zmm1, %zmm2, %zmm1
	vextractf128	$1, %ymm1, %xmm2
	vaddps	%xmm2, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm2         # xmm2 = xmm1[1,0]
	vaddps	%xmm2, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm2            # xmm2 = xmm1[1,1,3,3]
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm27, %xmm1, %xmm1
	vinsertps	$16, 9984(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, 13312(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 13184(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, 31552(%rsp), %ymm0, %ymm0 # 16-byte Folded Reload
	vmovaps	31936(%rsp), %zmm2              # 64-byte Reload
	vinsertf64x4	$1, %ymm0, %zmm2, %zmm0
	vmovaps	20480(%rsp), %zmm2              # 64-byte Reload
	vaddps	%zmm0, %zmm2, %zmm2
	vmovaps	%zmm2, 20480(%rsp)              # 64-byte Spill
	vinsertf128	$1, 4160(%rsp), %ymm1, %ymm0 # 16-byte Folded Reload
	vmovaps	4096(%rsp), %zmm1               # 64-byte Reload
	vinsertf64x4	$1, %ymm0, %zmm1, %zmm0
	vmovaps	20416(%rsp), %zmm1              # 64-byte Reload
	vaddps	%zmm0, %zmm1, %zmm1
	vmovaps	%zmm1, 20416(%rsp)              # 64-byte Spill
	vmovaps	32384(%rsp), %zmm1              # 64-byte Reload
	vextractf64x4	$1, %zmm1, %ymm0
	vaddps	%zmm0, %zmm1, %zmm0
	vextractf128	$1, %ymm0, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm1         # xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm27, %xmm0, %xmm0
	vinsertps	$16, 13824(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, 13632(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, 13504(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	19520(%rsp), %zmm2              # 64-byte Reload
	vextractf64x4	$1, %zmm2, %ymm1
	vaddps	%zmm1, %zmm2, %zmm1
	vextractf128	$1, %ymm1, %xmm2
	vaddps	%xmm2, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm2         # xmm2 = xmm1[1,0]
	vaddps	%xmm2, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm2            # xmm2 = xmm1[1,1,3,3]
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm27, %xmm1, %xmm1
	vinsertps	$16, 14336(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, 14016(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 13952(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, 31808(%rsp), %ymm0, %ymm0 # 16-byte Folded Reload
	vmovaps	32768(%rsp), %zmm2              # 64-byte Reload
	vinsertf64x4	$1, %ymm0, %zmm2, %zmm0
	vmovaps	20608(%rsp), %zmm2              # 64-byte Reload
	vaddps	%zmm0, %zmm2, %zmm2
	vmovaps	%zmm2, 20608(%rsp)              # 64-byte Spill
	vinsertf128	$1, 4288(%rsp), %ymm1, %ymm0 # 16-byte Folded Reload
	vmovaps	4224(%rsp), %zmm1               # 64-byte Reload
	vinsertf64x4	$1, %ymm0, %zmm1, %zmm0
	vmovaps	20544(%rsp), %zmm1              # 64-byte Reload
	vaddps	%zmm0, %zmm1, %zmm1
	vmovaps	%zmm1, 20544(%rsp)              # 64-byte Spill
	vmovaps	32960(%rsp), %zmm1              # 64-byte Reload
	vextractf64x4	$1, %zmm1, %ymm0
	vaddps	%zmm0, %zmm1, %zmm0
	vextractf128	$1, %ymm0, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm1         # xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm27, %xmm0, %xmm0
	vinsertps	$16, 9600(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, 11296(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, 11280(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	19648(%rsp), %zmm2              # 64-byte Reload
	vextractf64x4	$1, %zmm2, %ymm1
	vaddps	%zmm1, %zmm2, %zmm1
	vextractf128	$1, %ymm1, %xmm2
	vaddps	%xmm2, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm2         # xmm2 = xmm1[1,0]
	vaddps	%xmm2, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm2            # xmm2 = xmm1[1,1,3,3]
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm27, %xmm1, %xmm1
	vinsertps	$16, 13376(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, 9856(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 12864(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, 32640(%rsp), %ymm0, %ymm0 # 16-byte Folded Reload
	vmovaps	33088(%rsp), %zmm2              # 64-byte Reload
	vinsertf64x4	$1, %ymm0, %zmm2, %zmm0
	vmovaps	20736(%rsp), %zmm2              # 64-byte Reload
	vaddps	%zmm0, %zmm2, %zmm2
	vmovaps	%zmm2, 20736(%rsp)              # 64-byte Spill
	vinsertf128	$1, 6080(%rsp), %ymm1, %ymm0 # 16-byte Folded Reload
	vmovaps	6016(%rsp), %zmm1               # 64-byte Reload
	vinsertf64x4	$1, %ymm0, %zmm1, %zmm0
	vmovaps	20672(%rsp), %zmm1              # 64-byte Reload
	vaddps	%zmm0, %zmm1, %zmm1
	vmovaps	%zmm1, 20672(%rsp)              # 64-byte Spill
	vmovaps	33472(%rsp), %zmm1              # 64-byte Reload
	vextractf64x4	$1, %zmm1, %ymm0
	vaddps	%zmm0, %zmm1, %zmm0
	vextractf128	$1, %ymm0, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm1         # xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm27, %xmm0, %xmm0
	vinsertps	$16, 14400(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, 13888(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, 13696(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	19712(%rsp), %zmm2              # 64-byte Reload
	vextractf64x4	$1, %zmm2, %ymm1
	vaddps	%zmm1, %zmm2, %zmm1
	vextractf128	$1, %ymm1, %xmm2
	vaddps	%xmm2, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm2         # xmm2 = xmm1[1,0]
	vaddps	%xmm2, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm2            # xmm2 = xmm1[1,1,3,3]
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm27, %xmm1, %xmm1
	vinsertps	$16, 14784(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, 14656(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 14528(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, 33024(%rsp), %ymm0, %ymm0 # 16-byte Folded Reload
	vmovaps	33600(%rsp), %zmm2              # 64-byte Reload
	vinsertf64x4	$1, %ymm0, %zmm2, %zmm0
	vmovaps	20864(%rsp), %zmm2              # 64-byte Reload
	vaddps	%zmm0, %zmm2, %zmm2
	vmovaps	%zmm2, 20864(%rsp)              # 64-byte Spill
	vinsertf128	$1, 4416(%rsp), %ymm1, %ymm0 # 16-byte Folded Reload
	vmovaps	4352(%rsp), %zmm1               # 64-byte Reload
	vinsertf64x4	$1, %ymm0, %zmm1, %zmm0
	vmovaps	20800(%rsp), %zmm1              # 64-byte Reload
	vaddps	%zmm0, %zmm1, %zmm1
	vmovaps	%zmm1, 20800(%rsp)              # 64-byte Spill
	vmovaps	33984(%rsp), %zmm1              # 64-byte Reload
	vextractf64x4	$1, %zmm1, %ymm0
	vaddps	%zmm0, %zmm1, %zmm0
	vextractf128	$1, %ymm0, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm1         # xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm27, %xmm0, %xmm0
	vinsertps	$16, 11344(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, 11328(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, 11312(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	19776(%rsp), %zmm2              # 64-byte Reload
	vextractf64x4	$1, %zmm2, %ymm1
	vaddps	%zmm1, %zmm2, %zmm1
	vextractf128	$1, %ymm1, %xmm2
	vaddps	%xmm2, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm2         # xmm2 = xmm1[1,0]
	vaddps	%xmm2, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm2            # xmm2 = xmm1[1,1,3,3]
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm27, %xmm1, %xmm1
	vinsertps	$16, 11392(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, 11376(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 11360(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, 33536(%rsp), %ymm0, %ymm0 # 16-byte Folded Reload
	vmovaps	34112(%rsp), %zmm2              # 64-byte Reload
	vinsertf64x4	$1, %ymm0, %zmm2, %zmm0
	vmovaps	20992(%rsp), %zmm2              # 64-byte Reload
	vaddps	%zmm0, %zmm2, %zmm2
	vmovaps	%zmm2, 20992(%rsp)              # 64-byte Spill
	vinsertf128	$1, 4544(%rsp), %ymm1, %ymm0 # 16-byte Folded Reload
	vmovaps	4480(%rsp), %zmm1               # 64-byte Reload
	vinsertf64x4	$1, %ymm0, %zmm1, %zmm0
	vmovaps	20928(%rsp), %zmm1              # 64-byte Reload
	vaddps	%zmm0, %zmm1, %zmm1
	vmovaps	%zmm1, 20928(%rsp)              # 64-byte Spill
	vmovaps	34624(%rsp), %zmm1              # 64-byte Reload
	vextractf64x4	$1, %zmm1, %ymm0
	vaddps	%zmm0, %zmm1, %zmm0
	vextractf128	$1, %ymm0, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm1         # xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm27, %xmm0, %xmm0
	vinsertps	$16, 11440(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, 11424(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, 11408(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	35200(%rsp), %zmm2              # 64-byte Reload
	vextractf64x4	$1, %zmm2, %ymm1
	vaddps	%zmm1, %zmm2, %zmm1
	vextractf128	$1, %ymm1, %xmm2
	vaddps	%xmm2, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm2         # xmm2 = xmm1[1,0]
	vaddps	%xmm2, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm2            # xmm2 = xmm1[1,1,3,3]
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm27, %xmm1, %xmm1
	vinsertps	$16, 12800(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, 13248(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 9664(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, 34048(%rsp), %ymm0, %ymm0 # 16-byte Folded Reload
	vmovaps	34944(%rsp), %zmm2              # 64-byte Reload
	vinsertf64x4	$1, %ymm0, %zmm2, %zmm0
	vmovaps	21120(%rsp), %zmm2              # 64-byte Reload
	vaddps	%zmm0, %zmm2, %zmm2
	vmovaps	%zmm2, 21120(%rsp)              # 64-byte Spill
	vinsertf128	$1, 4672(%rsp), %ymm1, %ymm0 # 16-byte Folded Reload
	vmovaps	4608(%rsp), %zmm1               # 64-byte Reload
	vinsertf64x4	$1, %ymm0, %zmm1, %zmm0
	vmovaps	21056(%rsp), %zmm1              # 64-byte Reload
	vaddps	%zmm0, %zmm1, %zmm1
	vmovaps	%zmm1, 21056(%rsp)              # 64-byte Spill
	vmovaps	34240(%rsp), %zmm1              # 64-byte Reload
	vextractf64x4	$1, %zmm1, %ymm0
	vaddps	%zmm0, %zmm1, %zmm0
	vextractf128	$1, %ymm0, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm1         # xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm27, %xmm0, %xmm0
	vinsertps	$16, 11488(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, 11472(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, 11456(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	35392(%rsp), %zmm2              # 64-byte Reload
	vextractf64x4	$1, %zmm2, %ymm1
	vaddps	%zmm1, %zmm2, %zmm1
	vextractf128	$1, %ymm1, %xmm2
	vaddps	%xmm2, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm2         # xmm2 = xmm1[1,0]
	vaddps	%xmm2, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm2            # xmm2 = xmm1[1,1,3,3]
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm27, %xmm1, %xmm1
	vinsertps	$16, 11536(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, 11520(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 11504(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, 34752(%rsp), %ymm0, %ymm0 # 16-byte Folded Reload
	vmovaps	35072(%rsp), %zmm2              # 64-byte Reload
	vinsertf64x4	$1, %ymm0, %zmm2, %zmm0
	vmovaps	21248(%rsp), %zmm2              # 64-byte Reload
	vaddps	%zmm0, %zmm2, %zmm2
	vmovaps	%zmm2, 21248(%rsp)              # 64-byte Spill
	vinsertf128	$1, 35968(%rsp), %ymm1, %ymm0 # 16-byte Folded Reload
	vmovaps	35584(%rsp), %zmm1              # 64-byte Reload
	vinsertf64x4	$1, %ymm0, %zmm1, %zmm0
	vmovaps	21184(%rsp), %zmm1              # 64-byte Reload
	vaddps	%zmm0, %zmm1, %zmm1
	vmovaps	%zmm1, 21184(%rsp)              # 64-byte Spill
	vmovaps	35008(%rsp), %zmm1              # 64-byte Reload
	vextractf64x4	$1, %zmm1, %ymm0
	vaddps	%zmm0, %zmm1, %zmm0
	vextractf128	$1, %ymm0, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm1         # xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm27, %xmm0, %xmm0
	vinsertps	$16, 11584(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, 11568(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, 11552(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	36096(%rsp), %zmm2              # 64-byte Reload
	vextractf64x4	$1, %zmm2, %ymm1
	vaddps	%zmm1, %zmm2, %zmm1
	vextractf128	$1, %ymm1, %xmm2
	vaddps	%xmm2, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm2         # xmm2 = xmm1[1,0]
	vaddps	%xmm2, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm2            # xmm2 = xmm1[1,1,3,3]
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm27, %xmm1, %xmm1
	vinsertps	$16, 11632(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, 11616(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 11600(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, 35520(%rsp), %ymm0, %ymm0 # 16-byte Folded Reload
	vmovaps	35904(%rsp), %zmm2              # 64-byte Reload
	vinsertf64x4	$1, %ymm0, %zmm2, %zmm0
	vmovaps	21376(%rsp), %zmm2              # 64-byte Reload
	vaddps	%zmm0, %zmm2, %zmm2
	vmovaps	%zmm2, 21376(%rsp)              # 64-byte Spill
	vinsertf128	$1, 36352(%rsp), %ymm1, %ymm0 # 16-byte Folded Reload
	vmovaps	36160(%rsp), %zmm1              # 64-byte Reload
	vinsertf64x4	$1, %ymm0, %zmm1, %zmm0
	vmovaps	21312(%rsp), %zmm1              # 64-byte Reload
	vaddps	%zmm0, %zmm1, %zmm1
	vmovaps	%zmm1, 21312(%rsp)              # 64-byte Spill
	vmovaps	31232(%rsp), %zmm1              # 64-byte Reload
	vextractf64x4	$1, %zmm1, %ymm0
	vaddps	%zmm0, %zmm1, %zmm0
	vextractf128	$1, %ymm0, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm1         # xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm27, %xmm0, %xmm0
	vinsertps	$16, 11680(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, 11664(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, 11648(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	31616(%rsp), %zmm2              # 64-byte Reload
	vextractf64x4	$1, %zmm2, %ymm1
	vaddps	%zmm1, %zmm2, %zmm1
	vextractf128	$1, %ymm1, %xmm2
	vaddps	%xmm2, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm2         # xmm2 = xmm1[1,0]
	vaddps	%xmm2, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm2            # xmm2 = xmm1[1,1,3,3]
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm27, %xmm1, %xmm1
	vinsertps	$16, 11728(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, 11712(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 11696(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, 35840(%rsp), %ymm0, %ymm0 # 16-byte Folded Reload
	vmovaps	31296(%rsp), %zmm2              # 64-byte Reload
	vinsertf64x4	$1, %ymm0, %zmm2, %zmm0
	vmovaps	21504(%rsp), %zmm2              # 64-byte Reload
	vaddps	%zmm0, %zmm2, %zmm2
	vmovaps	%zmm2, 21504(%rsp)              # 64-byte Spill
	vinsertf128	$1, 33280(%rsp), %ymm1, %ymm0 # 16-byte Folded Reload
	vmovaps	32064(%rsp), %zmm1              # 64-byte Reload
	vinsertf64x4	$1, %ymm0, %zmm1, %zmm0
	vmovaps	21440(%rsp), %zmm1              # 64-byte Reload
	vaddps	%zmm0, %zmm1, %zmm1
	vmovaps	%zmm1, 21440(%rsp)              # 64-byte Spill
	vmovaps	12416(%rsp), %zmm1              # 64-byte Reload
	vextractf64x4	$1, %zmm1, %ymm0
	vaddps	%zmm0, %zmm1, %zmm0
	vextractf128	$1, %ymm0, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm1         # xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm27, %xmm0, %xmm0
	vinsertps	$16, 11744(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, 11776(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, 11760(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	31168(%rsp), %zmm2              # 64-byte Reload
	vextractf64x4	$1, %zmm2, %ymm1
	vaddps	%zmm1, %zmm2, %zmm1
	vextractf128	$1, %ymm1, %xmm2
	vaddps	%xmm2, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm2         # xmm2 = xmm1[1,0]
	vaddps	%xmm2, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm2            # xmm2 = xmm1[1,1,3,3]
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm27, %xmm1, %xmm1
	vinsertps	$16, 11792(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, 11808(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 11824(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, 12672(%rsp), %ymm0, %ymm0 # 16-byte Folded Reload
	vmovaps	31104(%rsp), %zmm2              # 64-byte Reload
	vinsertf64x4	$1, %ymm0, %zmm2, %zmm0
	vmovaps	21632(%rsp), %zmm2              # 64-byte Reload
	vaddps	%zmm0, %zmm2, %zmm2
	vmovaps	%zmm2, 21632(%rsp)              # 64-byte Spill
	vinsertf128	$1, 12544(%rsp), %ymm1, %ymm0 # 16-byte Folded Reload
	vmovaps	12480(%rsp), %zmm1              # 64-byte Reload
	vinsertf64x4	$1, %ymm0, %zmm1, %zmm0
	vmovaps	21568(%rsp), %zmm1              # 64-byte Reload
	vaddps	%zmm0, %zmm1, %zmm1
	vmovaps	%zmm1, 21568(%rsp)              # 64-byte Spill
	vmovaps	44352(%rsp), %zmm1              # 64-byte Reload
	vextractf64x4	$1, %zmm1, %ymm0
	vaddps	%zmm0, %zmm1, %zmm0
	vextractf128	$1, %ymm0, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm1         # xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm27, %xmm0, %xmm0
	vinsertps	$16, 11872(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, 11856(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, 11840(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	28160(%rsp), %zmm2              # 64-byte Reload
	vextractf64x4	$1, %zmm2, %ymm1
	vaddps	%zmm1, %zmm2, %zmm1
	vextractf128	$1, %ymm1, %xmm2
	vaddps	%xmm2, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm2         # xmm2 = xmm1[1,0]
	vaddps	%xmm2, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm2            # xmm2 = xmm1[1,1,3,3]
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm27, %xmm1, %xmm1
	vinsertps	$16, 11904(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, 11888(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 11920(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, 30656(%rsp), %ymm0, %ymm0 # 16-byte Folded Reload
	vmovaps	28224(%rsp), %zmm2              # 64-byte Reload
	vinsertf64x4	$1, %ymm0, %zmm2, %zmm0
	vmovaps	21760(%rsp), %zmm2              # 64-byte Reload
	vaddps	%zmm0, %zmm2, %zmm2
	vmovaps	%zmm2, 21760(%rsp)              # 64-byte Spill
	vinsertf128	$1, 5488(%rsp), %ymm1, %ymm0 # 16-byte Folded Reload
	vmovaps	29568(%rsp), %zmm1              # 64-byte Reload
	vinsertf64x4	$1, %ymm0, %zmm1, %zmm0
	vmovaps	21696(%rsp), %zmm1              # 64-byte Reload
	vaddps	%zmm0, %zmm1, %zmm1
	vmovaps	%zmm1, 21696(%rsp)              # 64-byte Spill
	vmovaps	28288(%rsp), %zmm1              # 64-byte Reload
	vextractf64x4	$1, %zmm1, %ymm0
	vaddps	%zmm0, %zmm1, %zmm0
	vextractf128	$1, %ymm0, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm1         # xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm27, %xmm0, %xmm0
	vinsertps	$16, 28480(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, 28544(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, 5504(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	28352(%rsp), %zmm2              # 64-byte Reload
	vextractf64x4	$1, %zmm2, %ymm1
	vaddps	%zmm1, %zmm2, %zmm1
	vextractf128	$1, %ymm1, %xmm2
	vaddps	%xmm2, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm2         # xmm2 = xmm1[1,0]
	vaddps	%xmm2, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm2            # xmm2 = xmm1[1,1,3,3]
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm27, %xmm1, %xmm1
	vinsertps	$16, 28800(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, 28928(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 29056(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, 29248(%rsp), %ymm0, %ymm0 # 16-byte Folded Reload
	vmovaps	29504(%rsp), %zmm2              # 64-byte Reload
	vinsertf64x4	$1, %ymm0, %zmm2, %zmm0
	vmovaps	21888(%rsp), %zmm2              # 64-byte Reload
	vaddps	%zmm0, %zmm2, %zmm2
	vmovaps	%zmm2, 21888(%rsp)              # 64-byte Spill
	vinsertf128	$1, 29888(%rsp), %ymm1, %ymm0 # 16-byte Folded Reload
	vmovaps	12160(%rsp), %zmm1              # 64-byte Reload
	vinsertf64x4	$1, %ymm0, %zmm1, %zmm0
	vmovaps	21824(%rsp), %zmm1              # 64-byte Reload
	vaddps	%zmm0, %zmm1, %zmm1
	vmovaps	%zmm1, 21824(%rsp)              # 64-byte Spill
	vmovaps	28416(%rsp), %zmm1              # 64-byte Reload
	vextractf64x4	$1, %zmm1, %ymm0
	vaddps	%zmm0, %zmm1, %zmm0
	vextractf128	$1, %ymm0, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm1         # xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm27, %xmm0, %xmm0
	vinsertps	$16, 352(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, 10048(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, 336(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	28608(%rsp), %zmm2              # 64-byte Reload
	vextractf64x4	$1, %zmm2, %ymm1
	vaddps	%zmm1, %zmm2, %zmm1
	vextractf128	$1, %ymm1, %xmm2
	vaddps	%xmm2, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm2         # xmm2 = xmm1[1,0]
	vaddps	%xmm2, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm2            # xmm2 = xmm1[1,1,3,3]
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm27, %xmm1, %xmm1
	vinsertps	$16, 10112(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, 2656(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 14208(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, 32000(%rsp), %ymm0, %ymm0 # 16-byte Folded Reload
	vmovaps	14272(%rsp), %zmm2              # 64-byte Reload
	vinsertf64x4	$1, %ymm0, %zmm2, %zmm0
	vmovaps	22016(%rsp), %zmm2              # 64-byte Reload
	vaddps	%zmm0, %zmm2, %zmm2
	vmovaps	%zmm2, 22016(%rsp)              # 64-byte Spill
	vinsertf128	$1, 31744(%rsp), %ymm1, %ymm0 # 16-byte Folded Reload
	vmovaps	31872(%rsp), %zmm1              # 64-byte Reload
	vinsertf64x4	$1, %ymm0, %zmm1, %zmm0
	vmovaps	21952(%rsp), %zmm1              # 64-byte Reload
	vaddps	%zmm0, %zmm1, %zmm1
	vmovaps	%zmm1, 21952(%rsp)              # 64-byte Spill
	vmovaps	28672(%rsp), %zmm1              # 64-byte Reload
	vextractf64x4	$1, %zmm1, %ymm0
	vaddps	%zmm0, %zmm1, %zmm0
	vextractf128	$1, %ymm0, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm1         # xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm27, %xmm0, %xmm0
	vinsertps	$16, 32128(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, 32192(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, 32256(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	28736(%rsp), %zmm2              # 64-byte Reload
	vextractf64x4	$1, %zmm2, %ymm1
	vaddps	%zmm1, %zmm2, %zmm1
	vextractf128	$1, %ymm1, %xmm2
	vaddps	%xmm2, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm2         # xmm2 = xmm1[1,0]
	vaddps	%xmm2, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm2            # xmm2 = xmm1[1,1,3,3]
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm27, %xmm1, %xmm1
	vinsertps	$16, 32320(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, 32448(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 32512(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, 32576(%rsp), %ymm0, %ymm0 # 16-byte Folded Reload
	vmovaps	32704(%rsp), %zmm2              # 64-byte Reload
	vinsertf64x4	$1, %ymm0, %zmm2, %zmm0
	vmovaps	22144(%rsp), %zmm2              # 64-byte Reload
	vaddps	%zmm0, %zmm2, %zmm2
	vmovaps	%zmm2, 22144(%rsp)              # 64-byte Spill
	vinsertf128	$1, 32896(%rsp), %ymm1, %ymm0 # 16-byte Folded Reload
	vmovaps	33792(%rsp), %zmm1              # 64-byte Reload
	vinsertf64x4	$1, %ymm0, %zmm1, %zmm0
	vmovaps	22080(%rsp), %zmm1              # 64-byte Reload
	vaddps	%zmm0, %zmm1, %zmm1
	vmovaps	%zmm1, 22080(%rsp)              # 64-byte Spill
	vmovaps	28864(%rsp), %zmm1              # 64-byte Reload
	vextractf64x4	$1, %zmm1, %ymm0
	vaddps	%zmm0, %zmm1, %zmm0
	vextractf128	$1, %ymm0, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm1         # xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm27, %xmm0, %xmm0
	vinsertps	$16, 384(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, 368(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, 2672(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	28992(%rsp), %zmm2              # 64-byte Reload
	vextractf64x4	$1, %zmm2, %ymm1
	vaddps	%zmm1, %zmm2, %zmm1
	vextractf128	$1, %ymm1, %xmm2
	vaddps	%xmm2, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm2         # xmm2 = xmm1[1,0]
	vaddps	%xmm2, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm2            # xmm2 = xmm1[1,1,3,3]
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm27, %xmm1, %xmm1
	vinsertps	$16, 15040(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, 14976(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 14912(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, 33664(%rsp), %ymm0, %ymm0 # 16-byte Folded Reload
	vmovaps	15744(%rsp), %zmm2              # 64-byte Reload
	vinsertf64x4	$1, %ymm0, %zmm2, %zmm0
	vmovaps	22272(%rsp), %zmm2              # 64-byte Reload
	vaddps	%zmm0, %zmm2, %zmm2
	vmovaps	%zmm2, 22272(%rsp)              # 64-byte Spill
	vinsertf128	$1, 33408(%rsp), %ymm1, %ymm0 # 16-byte Folded Reload
	vmovaps	15808(%rsp), %zmm1              # 64-byte Reload
	vinsertf64x4	$1, %ymm0, %zmm1, %zmm0
	vmovaps	22208(%rsp), %zmm1              # 64-byte Reload
	vaddps	%zmm0, %zmm1, %zmm1
	vmovaps	%zmm1, 22208(%rsp)              # 64-byte Spill
	vmovaps	29120(%rsp), %zmm1              # 64-byte Reload
	vextractf64x4	$1, %zmm1, %ymm0
	vaddps	%zmm0, %zmm1, %zmm0
	vextractf128	$1, %ymm0, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm1         # xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm27, %xmm0, %xmm0
	vinsertps	$16, 15168(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, 15104(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, 15296(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	29184(%rsp), %zmm2              # 64-byte Reload
	vextractf64x4	$1, %zmm2, %ymm1
	vaddps	%zmm1, %zmm2, %zmm1
	vextractf128	$1, %ymm1, %xmm2
	vaddps	%xmm2, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm2         # xmm2 = xmm1[1,0]
	vaddps	%xmm2, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm2            # xmm2 = xmm1[1,1,3,3]
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm27, %xmm1, %xmm1
	vinsertps	$16, 33152(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, 33216(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 15488(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, 15552(%rsp), %ymm0, %ymm0 # 16-byte Folded Reload
	vmovaps	33344(%rsp), %zmm2              # 64-byte Reload
	vinsertf64x4	$1, %ymm0, %zmm2, %zmm0
	vmovaps	22400(%rsp), %zmm2              # 64-byte Reload
	vaddps	%zmm0, %zmm2, %zmm2
	vmovaps	%zmm2, 22400(%rsp)              # 64-byte Spill
	vinsertf128	$1, 35776(%rsp), %ymm1, %ymm0 # 16-byte Folded Reload
	vmovaps	35712(%rsp), %zmm1              # 64-byte Reload
	vinsertf64x4	$1, %ymm0, %zmm1, %zmm0
	vmovaps	22336(%rsp), %zmm1              # 64-byte Reload
	vaddps	%zmm0, %zmm1, %zmm1
	vmovaps	%zmm1, 22336(%rsp)              # 64-byte Spill
	vmovaps	29312(%rsp), %zmm1              # 64-byte Reload
	vextractf64x4	$1, %zmm1, %ymm0
	vaddps	%zmm0, %zmm1, %zmm0
	vextractf128	$1, %ymm0, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm1         # xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm27, %xmm0, %xmm0
	vinsertps	$16, 2704(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, 2688(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, 16192(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	29376(%rsp), %zmm2              # 64-byte Reload
	vextractf64x4	$1, %zmm2, %ymm1
	vaddps	%zmm1, %zmm2, %zmm1
	vextractf128	$1, %ymm1, %xmm2
	vaddps	%xmm2, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm2         # xmm2 = xmm1[1,0]
	vaddps	%xmm2, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm2            # xmm2 = xmm1[1,1,3,3]
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm27, %xmm1, %xmm1
	vinsertps	$16, 16256(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, 2720(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 400(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, 34880(%rsp), %ymm1, %ymm1 # 16-byte Folded Reload
	vmovaps	34816(%rsp), %zmm2              # 64-byte Reload
	vinsertf64x4	$1, %ymm1, %zmm2, %zmm1
	vmovaps	22464(%rsp), %zmm2              # 64-byte Reload
	vaddps	%zmm1, %zmm2, %zmm2
	vmovaps	%zmm2, 22464(%rsp)              # 64-byte Spill
	vinsertf128	$1, 35456(%rsp), %ymm0, %ymm0 # 16-byte Folded Reload
	vmovaps	16640(%rsp), %zmm1              # 64-byte Reload
	vinsertf64x4	$1, %ymm0, %zmm1, %zmm0
	vmovaps	22528(%rsp), %zmm1              # 64-byte Reload
	vaddps	%zmm0, %zmm1, %zmm1
	vmovaps	%zmm1, 22528(%rsp)              # 64-byte Spill
	vmovaps	29440(%rsp), %zmm1              # 64-byte Reload
	vextractf64x4	$1, %zmm1, %ymm0
	vaddps	%zmm0, %zmm1, %zmm0
	vextractf128	$1, %ymm0, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm1         # xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm27, %xmm0, %xmm0
	vinsertps	$16, 34304(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, 16448(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, 16384(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	29632(%rsp), %zmm2              # 64-byte Reload
	vextractf64x4	$1, %zmm2, %ymm1
	vaddps	%zmm1, %zmm2, %zmm1
	vextractf128	$1, %ymm1, %xmm2
	vaddps	%xmm2, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm2         # xmm2 = xmm1[1,0]
	vaddps	%xmm2, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm2            # xmm2 = xmm1[1,1,3,3]
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm27, %xmm1, %xmm1
	vinsertps	$16, 34368(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, 34496(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 34560(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, 16512(%rsp), %ymm1, %ymm1 # 16-byte Folded Reload
	vmovaps	34688(%rsp), %zmm2              # 64-byte Reload
	vinsertf64x4	$1, %ymm1, %zmm2, %zmm1
	vmovaps	22592(%rsp), %zmm2              # 64-byte Reload
	vaddps	%zmm1, %zmm2, %zmm2
	vmovaps	%zmm2, 22592(%rsp)              # 64-byte Spill
	vinsertf128	$1, 35136(%rsp), %ymm0, %ymm0 # 16-byte Folded Reload
	vmovaps	16832(%rsp), %zmm1              # 64-byte Reload
	vinsertf64x4	$1, %ymm0, %zmm1, %zmm0
	vmovaps	22656(%rsp), %zmm1              # 64-byte Reload
	vaddps	%zmm0, %zmm1, %zmm1
	vmovaps	%zmm1, 22656(%rsp)              # 64-byte Spill
	vmovaps	29696(%rsp), %zmm1              # 64-byte Reload
	vextractf64x4	$1, %zmm1, %ymm0
	vaddps	%zmm0, %zmm1, %zmm0
	vextractf128	$1, %ymm0, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm1         # xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm27, %xmm0, %xmm0
	vinsertps	$16, 5552(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, 5536(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, 5520(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	29760(%rsp), %zmm2              # 64-byte Reload
	vextractf64x4	$1, %zmm2, %ymm1
	vaddps	%zmm1, %zmm2, %zmm1
	vextractf128	$1, %ymm1, %xmm2
	vaddps	%xmm2, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm2         # xmm2 = xmm1[1,0]
	vaddps	%xmm2, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm2            # xmm2 = xmm1[1,1,3,3]
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm27, %xmm1, %xmm1
	vinsertps	$16, 2736(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, 5584(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 5568(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, 37504(%rsp), %ymm1, %ymm1 # 16-byte Folded Reload
	vmovaps	36928(%rsp), %zmm2              # 64-byte Reload
	vinsertf64x4	$1, %ymm1, %zmm2, %zmm1
	vmovaps	22720(%rsp), %zmm2              # 64-byte Reload
	vaddps	%zmm1, %zmm2, %zmm2
	vmovaps	%zmm2, 22720(%rsp)              # 64-byte Spill
	vinsertf128	$1, 18816(%rsp), %ymm0, %ymm0 # 16-byte Folded Reload
	vmovaps	18560(%rsp), %zmm1              # 64-byte Reload
	vinsertf64x4	$1, %ymm0, %zmm1, %zmm0
	vmovaps	22784(%rsp), %zmm1              # 64-byte Reload
	vaddps	%zmm0, %zmm1, %zmm1
	vmovaps	%zmm1, 22784(%rsp)              # 64-byte Spill
	vmovaps	29824(%rsp), %zmm1              # 64-byte Reload
	vextractf64x4	$1, %zmm1, %ymm0
	vaddps	%zmm0, %zmm1, %zmm0
	vextractf128	$1, %ymm0, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm1         # xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm27, %xmm0, %xmm0
	vinsertps	$16, 2784(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, 2768(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, 2752(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	29952(%rsp), %zmm2              # 64-byte Reload
	vextractf64x4	$1, %zmm2, %ymm1
	vaddps	%zmm1, %zmm2, %zmm1
	vextractf128	$1, %ymm1, %xmm2
	vaddps	%xmm2, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm2         # xmm2 = xmm1[1,0]
	vaddps	%xmm2, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm2            # xmm2 = xmm1[1,1,3,3]
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm27, %xmm1, %xmm1
	vinsertps	$16, 2832(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, 2816(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 2800(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, 36672(%rsp), %ymm1, %ymm1 # 16-byte Folded Reload
	vmovaps	36608(%rsp), %zmm2              # 64-byte Reload
	vinsertf64x4	$1, %ymm1, %zmm2, %zmm1
	vmovaps	22848(%rsp), %zmm2              # 64-byte Reload
	vaddps	%zmm1, %zmm2, %zmm2
	vmovaps	%zmm2, 22848(%rsp)              # 64-byte Spill
	vinsertf128	$1, 36864(%rsp), %ymm0, %ymm0 # 16-byte Folded Reload
	vmovaps	36736(%rsp), %zmm1              # 64-byte Reload
	vinsertf64x4	$1, %ymm0, %zmm1, %zmm0
	vmovaps	22912(%rsp), %zmm1              # 64-byte Reload
	vaddps	%zmm0, %zmm1, %zmm1
	vmovaps	%zmm1, 22912(%rsp)              # 64-byte Spill
	vmovaps	12032(%rsp), %zmm1              # 64-byte Reload
	vextractf64x4	$1, %zmm1, %ymm0
	vaddps	%zmm0, %zmm1, %zmm0
	vextractf128	$1, %ymm0, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm1         # xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm27, %xmm0, %xmm0
	vinsertps	$16, 2848(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, 2864(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, 17152(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	30016(%rsp), %zmm2              # 64-byte Reload
	vextractf64x4	$1, %zmm2, %ymm1
	vaddps	%zmm1, %zmm2, %zmm1
	vextractf128	$1, %ymm1, %xmm2
	vaddps	%xmm2, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm2         # xmm2 = xmm1[1,0]
	vaddps	%xmm2, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm2            # xmm2 = xmm1[1,1,3,3]
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm27, %xmm1, %xmm1
	vinsertps	$16, 2896(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, 2880(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 17280(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, 36416(%rsp), %ymm1, %ymm1 # 16-byte Folded Reload
	vmovaps	36480(%rsp), %zmm2              # 64-byte Reload
	vinsertf64x4	$1, %ymm1, %zmm2, %zmm1
	vmovaps	22976(%rsp), %zmm2              # 64-byte Reload
	vaddps	%zmm1, %zmm2, %zmm2
	vmovaps	%zmm2, 22976(%rsp)              # 64-byte Spill
	vinsertf128	$1, 17536(%rsp), %ymm0, %ymm0 # 16-byte Folded Reload
	vmovaps	36544(%rsp), %zmm1              # 64-byte Reload
	vinsertf64x4	$1, %ymm0, %zmm1, %zmm0
	vmovaps	23040(%rsp), %zmm1              # 64-byte Reload
	vaddps	%zmm0, %zmm1, %zmm1
	vmovaps	%zmm1, 23040(%rsp)              # 64-byte Spill
	vmovaps	30080(%rsp), %zmm1              # 64-byte Reload
	vextractf64x4	$1, %zmm1, %ymm0
	vaddps	%zmm0, %zmm1, %zmm0
	vextractf128	$1, %ymm0, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm1         # xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm0
	vaddss	%xmm27, %xmm0, %xmm0
	vinsertps	$16, 36800(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, 18624(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, 18880(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
                                        # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	44416(%rsp), %zmm2              # 64-byte Reload
	vextractf64x4	$1, %zmm2, %ymm1
	vaddps	%zmm1, %zmm2, %zmm1
	vextractf128	$1, %ymm1, %xmm2
	vaddps	%xmm2, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm2         # xmm2 = xmm1[1,0]
	vaddps	%xmm2, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm2            # xmm2 = xmm1[1,1,3,3]
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm27, %xmm1, %xmm1
	vmovaps	5632(%rsp), %xmm2               # 16-byte Reload
	vinsertps	$16, %xmm1, %xmm2, %xmm1 # xmm1 = xmm2[0],xmm1[0],xmm2[2,3]
	vinsertps	$32, 5616(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 5600(%rsp), %xmm1, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = xmm1[0,1,2],mem[0]
	vmovaps	30144(%rsp), %zmm3              # 64-byte Reload
	vextractf64x4	$1, %zmm3, %ymm1
	vaddps	%zmm1, %zmm3, %zmm1
	vextractf128	$1, %ymm1, %xmm3
	vaddps	%xmm3, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm3         # xmm3 = xmm1[1,0]
	vaddps	%xmm3, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm3            # xmm3 = xmm1[1,1,3,3]
	vaddss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm27, %xmm1, %xmm1
	vinsertps	$16, 18176(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, 18112(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 18240(%rsp), %xmm1, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = xmm1[0,1,2],mem[0]
	vmovaps	12096(%rsp), %zmm4              # 64-byte Reload
	vextractf64x4	$1, %zmm4, %ymm1
	vaddps	%zmm1, %zmm4, %zmm1
	vextractf128	$1, %ymm1, %xmm4
	vaddps	%xmm4, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm4         # xmm4 = xmm1[1,0]
	vaddps	%xmm4, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm4            # xmm4 = xmm1[1,1,3,3]
	vaddss	%xmm4, %xmm1, %xmm1
	vaddss	%xmm27, %xmm1, %xmm1
	vmovaps	18304(%rsp), %xmm4              # 16-byte Reload
	vinsertps	$16, %xmm1, %xmm4, %xmm1 # xmm1 = xmm4[0],xmm1[0],xmm4[2,3]
	vinsertps	$32, 37056(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 11936(%rsp), %xmm1, %xmm4 # 16-byte Folded Reload
                                        # xmm4 = xmm1[0,1,2],mem[0]
	vmovaps	30208(%rsp), %zmm5              # 64-byte Reload
	vextractf64x4	$1, %zmm5, %ymm1
	vaddps	%zmm1, %zmm5, %zmm1
	vextractf128	$1, %ymm1, %xmm5
	vaddps	%xmm5, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm5         # xmm5 = xmm1[1,0]
	vaddps	%xmm5, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm5            # xmm5 = xmm1[1,1,3,3]
	vaddss	%xmm5, %xmm1, %xmm1
	vaddss	%xmm27, %xmm1, %xmm1
	vinsertps	$16, 11984(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, 11968(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 11952(%rsp), %xmm1, %xmm5 # 16-byte Folded Reload
                                        # xmm5 = xmm1[0,1,2],mem[0]
	vmovaps	16576(%rsp), %zmm6              # 64-byte Reload
	vextractf64x4	$1, %zmm6, %ymm1
	vaddps	%zmm1, %zmm6, %zmm1
	vextractf128	$1, %ymm1, %xmm6
	vaddps	%xmm6, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm6         # xmm6 = xmm1[1,0]
	vaddps	%xmm6, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm6            # xmm6 = xmm1[1,1,3,3]
	vaddss	%xmm6, %xmm1, %xmm1
	vaddss	%xmm27, %xmm1, %xmm1
	vmovaps	12016(%rsp), %xmm6              # 16-byte Reload
	vinsertps	$16, %xmm1, %xmm6, %xmm1 # xmm1 = xmm6[0],xmm1[0],xmm6[2,3]
	vinsertps	$32, 12000(%rsp), %xmm1, %xmm1 # 16-byte Folded Reload
                                        # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, %xmm7, %xmm1, %xmm6 # xmm6 = xmm1[0,1,2],xmm7[0]
	vmovaps	30272(%rsp), %zmm7              # 64-byte Reload
	vextractf64x4	$1, %zmm7, %ymm1
	vaddps	%zmm1, %zmm7, %zmm1
	vextractf128	$1, %ymm1, %xmm7
	vaddps	%xmm7, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm7         # xmm7 = xmm1[1,0]
	vaddps	%xmm7, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm7            # xmm7 = xmm1[1,1,3,3]
	vaddss	%xmm7, %xmm1, %xmm1
	vmovaps	30336(%rsp), %zmm8              # 64-byte Reload
	vextractf64x4	$1, %zmm8, %ymm7
	vaddps	%zmm7, %zmm8, %zmm7
	vextractf128	$1, %ymm7, %xmm8
	vaddps	%xmm7, %xmm8, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm8         # xmm8 = xmm7[1,0]
	vaddps	%xmm7, %xmm8, %xmm7
	vmovshdup	%xmm7, %xmm8            # xmm8 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm27, %xmm1, %xmm1
	vaddss	%xmm27, %xmm7, %xmm7
	vinsertps	$16, %xmm7, %xmm1, %xmm1 # xmm1 = xmm1[0],xmm7[0],xmm1[2,3]
	vmovaps	30400(%rsp), %zmm8              # 64-byte Reload
	vextractf64x4	$1, %zmm8, %ymm7
	vaddps	%zmm7, %zmm8, %zmm7
	vextractf128	$1, %ymm7, %xmm8
	vaddps	%xmm7, %xmm8, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm8         # xmm8 = xmm7[1,0]
	vaddps	%xmm7, %xmm8, %xmm7
	vmovshdup	%xmm7, %xmm8            # xmm8 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm27, %xmm7, %xmm7
	vinsertps	$32, %xmm7, %xmm1, %xmm1 # xmm1 = xmm1[0,1],xmm7[0],xmm1[3]
	vmovaps	30464(%rsp), %zmm8              # 64-byte Reload
	vextractf64x4	$1, %zmm8, %ymm7
	vaddps	%zmm7, %zmm8, %zmm7
	vextractf128	$1, %ymm7, %xmm8
	vaddps	%xmm7, %xmm8, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm8         # xmm8 = xmm7[1,0]
	vaddps	%xmm7, %xmm8, %xmm7
	vmovshdup	%xmm7, %xmm8            # xmm8 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm27, %xmm7, %xmm7
	vinsertps	$48, %xmm7, %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],xmm7[0]
	vmovaps	30528(%rsp), %zmm8              # 64-byte Reload
	vextractf64x4	$1, %zmm8, %ymm7
	vaddps	%zmm7, %zmm8, %zmm7
	vextractf128	$1, %ymm7, %xmm8
	vaddps	%xmm7, %xmm8, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm8         # xmm8 = xmm7[1,0]
	vaddps	%xmm7, %xmm8, %xmm7
	vmovshdup	%xmm7, %xmm8            # xmm8 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm8, %xmm7
	vmovaps	30592(%rsp), %zmm9              # 64-byte Reload
	vextractf64x4	$1, %zmm9, %ymm8
	vaddps	%zmm8, %zmm9, %zmm8
	vextractf128	$1, %ymm8, %xmm9
	vaddps	%xmm9, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm9         # xmm9 = xmm8[1,0]
	vaddps	%xmm9, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm9            # xmm9 = xmm8[1,1,3,3]
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm27, %xmm7, %xmm7
	vaddss	%xmm27, %xmm8, %xmm8
	vinsertps	$16, %xmm8, %xmm7, %xmm7 # xmm7 = xmm7[0],xmm8[0],xmm7[2,3]
	vmovaps	30720(%rsp), %zmm9              # 64-byte Reload
	vextractf64x4	$1, %zmm9, %ymm8
	vaddps	%zmm8, %zmm9, %zmm8
	vextractf128	$1, %ymm8, %xmm9
	vaddps	%xmm9, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm9         # xmm9 = xmm8[1,0]
	vaddps	%xmm9, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm9            # xmm9 = xmm8[1,1,3,3]
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm27, %xmm8, %xmm8
	vinsertps	$32, %xmm8, %xmm7, %xmm7 # xmm7 = xmm7[0,1],xmm8[0],xmm7[3]
	vmovaps	30784(%rsp), %zmm9              # 64-byte Reload
	vextractf64x4	$1, %zmm9, %ymm8
	vaddps	%zmm8, %zmm9, %zmm8
	vextractf128	$1, %ymm8, %xmm9
	vaddps	%xmm9, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm9         # xmm9 = xmm8[1,0]
	vaddps	%xmm9, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm9            # xmm9 = xmm8[1,1,3,3]
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm27, %xmm8, %xmm8
	vinsertps	$48, %xmm8, %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],xmm8[0]
	vinsertf128	$1, %xmm7, %ymm1, %ymm1
	vinsertf128	$1, %xmm6, %ymm5, %ymm5
	vinsertf64x4	$1, %ymm1, %zmm5, %zmm1
	vmovaps	23104(%rsp), %zmm5              # 64-byte Reload
	vaddps	%zmm1, %zmm5, %zmm5
	vmovaps	%zmm5, 23104(%rsp)              # 64-byte Spill
	vinsertf128	$1, %xmm4, %ymm3, %ymm1
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vinsertf64x4	$1, %ymm1, %zmm0, %zmm0
	vmovaps	23168(%rsp), %zmm1              # 64-byte Reload
	vaddps	%zmm0, %zmm1, %zmm1
	vmovaps	%zmm1, 23168(%rsp)              # 64-byte Spill
	vmovaps	12224(%rsp), %zmm1              # 64-byte Reload
	vextractf64x4	$1, %zmm1, %ymm0
	vaddps	%zmm0, %zmm1, %zmm0
	vextractf128	$1, %ymm0, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm1         # xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm0
	vmovaps	44928(%rsp), %zmm2              # 64-byte Reload
	vextractf64x4	$1, %zmm2, %ymm1
	vaddps	%zmm1, %zmm2, %zmm1
	vextractf128	$1, %ymm1, %xmm2
	vaddps	%xmm2, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm2         # xmm2 = xmm1[1,0]
	vaddps	%xmm2, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm2            # xmm2 = xmm1[1,1,3,3]
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm27, %xmm0, %xmm0
	vaddss	%xmm27, %xmm1, %xmm1
	vinsertps	$16, %xmm1, %xmm0, %xmm0 # xmm0 = xmm0[0],xmm1[0],xmm0[2,3]
	vmovaps	44800(%rsp), %zmm2              # 64-byte Reload
	vextractf64x4	$1, %zmm2, %ymm1
	vaddps	%zmm1, %zmm2, %zmm1
	vextractf128	$1, %ymm1, %xmm2
	vaddps	%xmm2, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm2         # xmm2 = xmm1[1,0]
	vaddps	%xmm2, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm2            # xmm2 = xmm1[1,1,3,3]
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm27, %xmm1, %xmm1
	vinsertps	$32, %xmm1, %xmm0, %xmm0 # xmm0 = xmm0[0,1],xmm1[0],xmm0[3]
	vmovaps	44672(%rsp), %zmm2              # 64-byte Reload
	vextractf64x4	$1, %zmm2, %ymm1
	vaddps	%zmm1, %zmm2, %zmm1
	vextractf128	$1, %ymm1, %xmm2
	vaddps	%xmm2, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm2         # xmm2 = xmm1[1,0]
	vaddps	%xmm2, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm2            # xmm2 = xmm1[1,1,3,3]
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm27, %xmm1, %xmm1
	vinsertps	$48, %xmm1, %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],xmm1[0]
	vmovaps	30848(%rsp), %zmm2              # 64-byte Reload
	vextractf64x4	$1, %zmm2, %ymm1
	vaddps	%zmm1, %zmm2, %zmm1
	vextractf128	$1, %ymm1, %xmm2
	vaddps	%xmm2, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm2         # xmm2 = xmm1[1,0]
	vaddps	%xmm2, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm2            # xmm2 = xmm1[1,1,3,3]
	vaddss	%xmm2, %xmm1, %xmm1
	vmovaps	44608(%rsp), %zmm3              # 64-byte Reload
	vextractf64x4	$1, %zmm3, %ymm2
	vaddps	%zmm2, %zmm3, %zmm2
	vextractf128	$1, %ymm2, %xmm3
	vaddps	%xmm3, %xmm2, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm3         # xmm3 = xmm2[1,0]
	vaddps	%xmm3, %xmm2, %xmm2
	vmovshdup	%xmm2, %xmm3            # xmm3 = xmm2[1,1,3,3]
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm27, %xmm1, %xmm1
	vaddss	%xmm27, %xmm2, %xmm2
	vinsertps	$16, %xmm2, %xmm1, %xmm1 # xmm1 = xmm1[0],xmm2[0],xmm1[2,3]
	vmovaps	44544(%rsp), %zmm3              # 64-byte Reload
	vextractf64x4	$1, %zmm3, %ymm2
	vaddps	%zmm2, %zmm3, %zmm2
	vextractf128	$1, %ymm2, %xmm3
	vaddps	%xmm3, %xmm2, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm3         # xmm3 = xmm2[1,0]
	vaddps	%xmm3, %xmm2, %xmm2
	vmovshdup	%xmm2, %xmm3            # xmm3 = xmm2[1,1,3,3]
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm27, %xmm2, %xmm2
	vinsertps	$32, %xmm2, %xmm1, %xmm1 # xmm1 = xmm1[0,1],xmm2[0],xmm1[3]
	vmovaps	44480(%rsp), %zmm3              # 64-byte Reload
	vextractf64x4	$1, %zmm3, %ymm2
	vaddps	%zmm2, %zmm3, %zmm2
	vextractf128	$1, %ymm2, %xmm3
	vaddps	%xmm3, %xmm2, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm3         # xmm3 = xmm2[1,0]
	vaddps	%xmm3, %xmm2, %xmm2
	vmovshdup	%xmm2, %xmm3            # xmm3 = xmm2[1,1,3,3]
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm27, %xmm2, %xmm2
	vinsertps	$48, %xmm2, %xmm1, %xmm2 # xmm2 = xmm1[0,1,2],xmm2[0]
	vmovaps	30912(%rsp), %zmm3              # 64-byte Reload
	vextractf64x4	$1, %zmm3, %ymm1
	vaddps	%zmm1, %zmm3, %zmm1
	vextractf128	$1, %ymm1, %xmm3
	vaddps	%xmm3, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm3         # xmm3 = xmm1[1,0]
	vaddps	%xmm3, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm3            # xmm3 = xmm1[1,1,3,3]
	vaddss	%xmm3, %xmm1, %xmm1
	vmovaps	30976(%rsp), %zmm4              # 64-byte Reload
	vextractf64x4	$1, %zmm4, %ymm3
	vaddps	%zmm3, %zmm4, %zmm3
	vextractf128	$1, %ymm3, %xmm4
	vaddps	%xmm4, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm4         # xmm4 = xmm3[1,0]
	vaddps	%xmm4, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm4            # xmm4 = xmm3[1,1,3,3]
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm27, %xmm1, %xmm1
	vaddss	%xmm27, %xmm3, %xmm3
	vinsertps	$16, %xmm3, %xmm1, %xmm1 # xmm1 = xmm1[0],xmm3[0],xmm1[2,3]
	vmovaps	31040(%rsp), %zmm4              # 64-byte Reload
	vextractf64x4	$1, %zmm4, %ymm3
	vaddps	%zmm3, %zmm4, %zmm3
	vextractf128	$1, %ymm3, %xmm4
	vaddps	%xmm4, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm4         # xmm4 = xmm3[1,0]
	vaddps	%xmm4, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm4            # xmm4 = xmm3[1,1,3,3]
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm27, %xmm3, %xmm3
	vinsertps	$32, %xmm3, %xmm1, %xmm1 # xmm1 = xmm1[0,1],xmm3[0],xmm1[3]
	vmovaps	12288(%rsp), %zmm4              # 64-byte Reload
	vextractf64x4	$1, %zmm4, %ymm3
	vaddps	%zmm3, %zmm4, %zmm3
	vextractf128	$1, %ymm3, %xmm4
	vaddps	%xmm4, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm4         # xmm4 = xmm3[1,0]
	vaddps	%xmm4, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm4            # xmm4 = xmm3[1,1,3,3]
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm27, %xmm3, %xmm3
	vinsertps	$48, %xmm3, %xmm1, %xmm3 # xmm3 = xmm1[0,1,2],xmm3[0]
	vmovaps	9088(%rsp), %zmm4               # 64-byte Reload
	vextractf64x4	$1, %zmm4, %ymm1
	vaddps	%zmm1, %zmm4, %zmm1
	vextractf128	$1, %ymm1, %xmm4
	vaddps	%xmm4, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm4         # xmm4 = xmm1[1,0]
	vaddps	%xmm4, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm4            # xmm4 = xmm1[1,1,3,3]
	vaddss	%xmm4, %xmm1, %xmm1
	vmovaps	12352(%rsp), %zmm5              # 64-byte Reload
	vextractf64x4	$1, %zmm5, %ymm4
	vaddps	%zmm4, %zmm5, %zmm4
	vextractf128	$1, %ymm4, %xmm5
	vaddps	%xmm5, %xmm4, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm5         # xmm5 = xmm4[1,0]
	vaddps	%xmm5, %xmm4, %xmm4
	vmovshdup	%xmm4, %xmm5            # xmm5 = xmm4[1,1,3,3]
	vaddss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm27, %xmm1, %xmm1
	vaddss	%xmm27, %xmm4, %xmm4
	vinsertps	$16, %xmm4, %xmm1, %xmm1 # xmm1 = xmm1[0],xmm4[0],xmm1[2,3]
	vmovaps	9152(%rsp), %zmm5               # 64-byte Reload
	vextractf64x4	$1, %zmm5, %ymm4
	vaddps	%zmm4, %zmm5, %zmm4
	vextractf128	$1, %ymm4, %xmm5
	vaddps	%xmm5, %xmm4, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm5         # xmm5 = xmm4[1,0]
	vaddps	%xmm5, %xmm4, %xmm4
	vmovshdup	%xmm4, %xmm5            # xmm5 = xmm4[1,1,3,3]
	vaddss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm27, %xmm4, %xmm4
	vinsertps	$32, %xmm4, %xmm1, %xmm1 # xmm1 = xmm1[0,1],xmm4[0],xmm1[3]
	vmovaps	9216(%rsp), %zmm5               # 64-byte Reload
	vextractf64x4	$1, %zmm5, %ymm4
	vaddps	%zmm4, %zmm5, %zmm4
	vextractf128	$1, %ymm4, %xmm5
	vaddps	%xmm5, %xmm4, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm5         # xmm5 = xmm4[1,0]
	vaddps	%xmm5, %xmm4, %xmm4
	vmovshdup	%xmm4, %xmm5            # xmm5 = xmm4[1,1,3,3]
	vaddss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm27, %xmm4, %xmm4
	vinsertps	$48, %xmm4, %xmm1, %xmm4 # xmm4 = xmm1[0,1,2],xmm4[0]
	vmovaps	9280(%rsp), %zmm5               # 64-byte Reload
	vextractf64x4	$1, %zmm5, %ymm1
	vaddps	%zmm1, %zmm5, %zmm1
	vextractf128	$1, %ymm1, %xmm5
	vaddps	%xmm5, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm5         # xmm5 = xmm1[1,0]
	vaddps	%xmm5, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm5            # xmm5 = xmm1[1,1,3,3]
	vaddss	%xmm5, %xmm1, %xmm1
	vmovaps	37120(%rsp), %zmm6              # 64-byte Reload
	vextractf64x4	$1, %zmm6, %ymm5
	vaddps	%zmm5, %zmm6, %zmm5
	vextractf128	$1, %ymm5, %xmm6
	vaddps	%xmm6, %xmm5, %xmm5
	vshufpd	$1, %xmm5, %xmm5, %xmm6         # xmm6 = xmm5[1,0]
	vaddps	%xmm6, %xmm5, %xmm5
	vmovshdup	%xmm5, %xmm6            # xmm6 = xmm5[1,1,3,3]
	vaddss	%xmm6, %xmm5, %xmm5
	vaddss	%xmm27, %xmm1, %xmm1
	vaddss	%xmm27, %xmm5, %xmm5
	vinsertps	$16, %xmm5, %xmm1, %xmm1 # xmm1 = xmm1[0],xmm5[0],xmm1[2,3]
	vmovaps	18368(%rsp), %zmm6              # 64-byte Reload
	vextractf64x4	$1, %zmm6, %ymm5
	vaddps	%zmm5, %zmm6, %zmm5
	vextractf128	$1, %ymm5, %xmm6
	vaddps	%xmm6, %xmm5, %xmm5
	vshufpd	$1, %xmm5, %xmm5, %xmm6         # xmm6 = xmm5[1,0]
	vaddps	%xmm6, %xmm5, %xmm5
	vmovshdup	%xmm5, %xmm6            # xmm6 = xmm5[1,1,3,3]
	vaddss	%xmm6, %xmm5, %xmm5
	vaddss	%xmm27, %xmm5, %xmm5
	vinsertps	$32, %xmm5, %xmm1, %xmm1 # xmm1 = xmm1[0,1],xmm5[0],xmm1[3]
	vmovaps	6656(%rsp), %zmm6               # 64-byte Reload
	vextractf64x4	$1, %zmm6, %ymm5
	vaddps	%zmm5, %zmm6, %zmm5
	vextractf128	$1, %ymm5, %xmm6
	vaddps	%xmm6, %xmm5, %xmm5
	vshufpd	$1, %xmm5, %xmm5, %xmm6         # xmm6 = xmm5[1,0]
	vaddps	%xmm6, %xmm5, %xmm5
	vmovshdup	%xmm5, %xmm6            # xmm6 = xmm5[1,1,3,3]
	vaddss	%xmm6, %xmm5, %xmm5
	vaddss	%xmm27, %xmm5, %xmm5
	vinsertps	$48, %xmm5, %xmm1, %xmm5 # xmm5 = xmm1[0,1,2],xmm5[0]
	vmovaps	12608(%rsp), %zmm6              # 64-byte Reload
	vextractf64x4	$1, %zmm6, %ymm1
	vaddps	%zmm1, %zmm6, %zmm1
	vextractf128	$1, %ymm1, %xmm6
	vaddps	%xmm6, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm6         # xmm6 = xmm1[1,0]
	vaddps	%xmm6, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm6            # xmm6 = xmm1[1,1,3,3]
	vaddss	%xmm6, %xmm1, %xmm1
	vmovaps	35328(%rsp), %zmm7              # 64-byte Reload
	vextractf64x4	$1, %zmm7, %ymm6
	vaddps	%zmm6, %zmm7, %zmm6
	vextractf128	$1, %ymm6, %xmm7
	vaddps	%xmm7, %xmm6, %xmm6
	vshufpd	$1, %xmm6, %xmm6, %xmm7         # xmm7 = xmm6[1,0]
	vaddps	%xmm7, %xmm6, %xmm6
	vmovshdup	%xmm6, %xmm7            # xmm7 = xmm6[1,1,3,3]
	vaddss	%xmm7, %xmm6, %xmm6
	vaddss	%xmm27, %xmm1, %xmm1
	vaddss	%xmm27, %xmm6, %xmm6
	vinsertps	$16, %xmm6, %xmm1, %xmm1 # xmm1 = xmm1[0],xmm6[0],xmm1[2,3]
	vmovaps	33920(%rsp), %zmm7              # 64-byte Reload
	vextractf64x4	$1, %zmm7, %ymm6
	vaddps	%zmm6, %zmm7, %zmm6
	vextractf128	$1, %ymm6, %xmm7
	vaddps	%xmm7, %xmm6, %xmm6
	vshufpd	$1, %xmm6, %xmm6, %xmm7         # xmm7 = xmm6[1,0]
	vaddps	%xmm7, %xmm6, %xmm6
	vmovshdup	%xmm6, %xmm7            # xmm7 = xmm6[1,1,3,3]
	vaddss	%xmm7, %xmm6, %xmm6
	vaddss	%xmm27, %xmm6, %xmm6
	vinsertps	$32, %xmm6, %xmm1, %xmm1 # xmm1 = xmm1[0,1],xmm6[0],xmm1[3]
	vmovaps	14848(%rsp), %zmm7              # 64-byte Reload
	vextractf64x4	$1, %zmm7, %ymm6
	vaddps	%zmm6, %zmm7, %zmm6
	vextractf128	$1, %ymm6, %xmm7
	vaddps	%xmm7, %xmm6, %xmm6
	vshufpd	$1, %xmm6, %xmm6, %xmm7         # xmm7 = xmm6[1,0]
	vaddps	%xmm7, %xmm6, %xmm6
	vmovshdup	%xmm6, %xmm7            # xmm7 = xmm6[1,1,3,3]
	vaddss	%xmm7, %xmm6, %xmm6
	vaddss	%xmm27, %xmm6, %xmm6
	vinsertps	$48, %xmm6, %xmm1, %xmm6 # xmm6 = xmm1[0,1,2],xmm6[0]
	vmovaps	9344(%rsp), %zmm7               # 64-byte Reload
	vextractf64x4	$1, %zmm7, %ymm1
	vaddps	%zmm1, %zmm7, %zmm1
	vextractf128	$1, %ymm1, %xmm7
	vaddps	%xmm7, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm7         # xmm7 = xmm1[1,0]
	vaddps	%xmm7, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm7            # xmm7 = xmm1[1,1,3,3]
	vaddss	%xmm7, %xmm1, %xmm1
	vmovaps	9408(%rsp), %zmm8               # 64-byte Reload
	vextractf64x4	$1, %zmm8, %ymm7
	vaddps	%zmm7, %zmm8, %zmm7
	vextractf128	$1, %ymm7, %xmm8
	vaddps	%xmm7, %xmm8, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm8         # xmm8 = xmm7[1,0]
	vaddps	%xmm7, %xmm8, %xmm7
	vmovshdup	%xmm7, %xmm8            # xmm8 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm27, %xmm1, %xmm1
	vaddss	%xmm27, %xmm7, %xmm7
	vinsertps	$16, %xmm7, %xmm1, %xmm1 # xmm1 = xmm1[0],xmm7[0],xmm1[2,3]
	vmovaps	9472(%rsp), %zmm8               # 64-byte Reload
	vextractf64x4	$1, %zmm8, %ymm7
	vaddps	%zmm7, %zmm8, %zmm7
	vextractf128	$1, %ymm7, %xmm8
	vaddps	%xmm7, %xmm8, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm8         # xmm8 = xmm7[1,0]
	vaddps	%xmm7, %xmm8, %xmm7
	vmovshdup	%xmm7, %xmm8            # xmm8 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm27, %xmm7, %xmm7
	vinsertps	$32, %xmm7, %xmm1, %xmm1 # xmm1 = xmm1[0,1],xmm7[0],xmm1[3]
	vmovaps	9536(%rsp), %zmm8               # 64-byte Reload
	vextractf64x4	$1, %zmm8, %ymm7
	vaddps	%zmm7, %zmm8, %zmm7
	vextractf128	$1, %ymm7, %xmm8
	vaddps	%xmm7, %xmm8, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm8         # xmm8 = xmm7[1,0]
	vaddps	%xmm7, %xmm8, %xmm7
	vmovshdup	%xmm7, %xmm8            # xmm8 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm27, %xmm7, %xmm7
	vinsertps	$48, %xmm7, %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],xmm7[0]
	vmovaps	9792(%rsp), %zmm8               # 64-byte Reload
	vextractf64x4	$1, %zmm8, %ymm7
	vaddps	%zmm7, %zmm8, %zmm7
	vextractf128	$1, %ymm7, %xmm8
	vaddps	%xmm7, %xmm8, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm8         # xmm8 = xmm7[1,0]
	vaddps	%xmm7, %xmm8, %xmm7
	vmovshdup	%xmm7, %xmm8            # xmm8 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm8, %xmm7
	vmovaps	12992(%rsp), %zmm9              # 64-byte Reload
	vextractf64x4	$1, %zmm9, %ymm8
	vaddps	%zmm8, %zmm9, %zmm8
	vextractf128	$1, %ymm8, %xmm9
	vaddps	%xmm9, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm9         # xmm9 = xmm8[1,0]
	vaddps	%xmm9, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm9            # xmm9 = xmm8[1,1,3,3]
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm27, %xmm7, %xmm7
	vaddss	%xmm27, %xmm8, %xmm8
	vinsertps	$16, %xmm8, %xmm7, %xmm7 # xmm7 = xmm7[0],xmm8[0],xmm7[2,3]
	vmovaps	13056(%rsp), %zmm9              # 64-byte Reload
	vextractf64x4	$1, %zmm9, %ymm8
	vaddps	%zmm8, %zmm9, %zmm8
	vextractf128	$1, %ymm8, %xmm9
	vaddps	%xmm9, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm9         # xmm9 = xmm8[1,0]
	vaddps	%xmm9, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm9            # xmm9 = xmm8[1,1,3,3]
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm27, %xmm8, %xmm8
	vinsertps	$32, %xmm8, %xmm7, %xmm7 # xmm7 = xmm7[0,1],xmm8[0],xmm7[3]
	vmovaps	13568(%rsp), %zmm9              # 64-byte Reload
	vextractf64x4	$1, %zmm9, %ymm8
	vaddps	%zmm8, %zmm9, %zmm8
	vextractf128	$1, %ymm8, %xmm9
	vaddps	%xmm9, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm9         # xmm9 = xmm8[1,0]
	vaddps	%xmm9, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm9            # xmm9 = xmm8[1,1,3,3]
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm27, %xmm8, %xmm8
	vinsertps	$48, %xmm8, %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],xmm8[0]
	vinsertf128	$1, %xmm7, %ymm1, %ymm1
	vinsertf128	$1, %xmm6, %ymm5, %ymm5
	vinsertf64x4	$1, %ymm1, %zmm5, %zmm1
	vmovaps	23232(%rsp), %zmm5              # 64-byte Reload
	vaddps	%zmm1, %zmm5, %zmm5
	vmovaps	%zmm5, 23232(%rsp)              # 64-byte Spill
	vinsertf128	$1, %xmm4, %ymm3, %ymm1
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vinsertf64x4	$1, %ymm1, %zmm0, %zmm0
	vmovaps	23296(%rsp), %zmm1              # 64-byte Reload
	vaddps	%zmm0, %zmm1, %zmm1
	vmovaps	%zmm1, 23296(%rsp)              # 64-byte Spill
	vmovaps	13760(%rsp), %zmm1              # 64-byte Reload
	vextractf64x4	$1, %zmm1, %ymm0
	vaddps	%zmm0, %zmm1, %zmm0
	vextractf128	$1, %ymm0, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm1         # xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm0
	vmovaps	45312(%rsp), %zmm2              # 64-byte Reload
	vextractf64x4	$1, %zmm2, %ymm1
	vaddps	%zmm1, %zmm2, %zmm1
	vextractf128	$1, %ymm1, %xmm2
	vaddps	%xmm2, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm2         # xmm2 = xmm1[1,0]
	vaddps	%xmm2, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm2            # xmm2 = xmm1[1,1,3,3]
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm27, %xmm0, %xmm0
	vaddss	%xmm27, %xmm1, %xmm1
	vinsertps	$16, %xmm1, %xmm0, %xmm0 # xmm0 = xmm0[0],xmm1[0],xmm0[2,3]
	vmovaps	45184(%rsp), %zmm2              # 64-byte Reload
	vextractf64x4	$1, %zmm2, %ymm1
	vaddps	%zmm1, %zmm2, %zmm1
	vextractf128	$1, %ymm1, %xmm2
	vaddps	%xmm2, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm2         # xmm2 = xmm1[1,0]
	vaddps	%xmm2, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm2            # xmm2 = xmm1[1,1,3,3]
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm27, %xmm1, %xmm1
	vinsertps	$32, %xmm1, %xmm0, %xmm0 # xmm0 = xmm0[0,1],xmm1[0],xmm0[3]
	vmovaps	45056(%rsp), %zmm2              # 64-byte Reload
	vextractf64x4	$1, %zmm2, %ymm1
	vaddps	%zmm1, %zmm2, %zmm1
	vextractf128	$1, %ymm1, %xmm2
	vaddps	%xmm2, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm2         # xmm2 = xmm1[1,0]
	vaddps	%xmm2, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm2            # xmm2 = xmm1[1,1,3,3]
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm27, %xmm1, %xmm1
	vinsertps	$48, %xmm1, %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],xmm1[0]
	vmovaps	14080(%rsp), %zmm2              # 64-byte Reload
	vextractf64x4	$1, %zmm2, %ymm1
	vaddps	%zmm1, %zmm2, %zmm1
	vextractf128	$1, %ymm1, %xmm2
	vaddps	%xmm2, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm2         # xmm2 = xmm1[1,0]
	vaddps	%xmm2, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm2            # xmm2 = xmm1[1,1,3,3]
	vaddss	%xmm2, %xmm1, %xmm1
	vmovaps	44992(%rsp), %zmm3              # 64-byte Reload
	vextractf64x4	$1, %zmm3, %ymm2
	vaddps	%zmm2, %zmm3, %zmm2
	vextractf128	$1, %ymm2, %xmm3
	vaddps	%xmm3, %xmm2, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm3         # xmm3 = xmm2[1,0]
	vaddps	%xmm3, %xmm2, %xmm2
	vmovshdup	%xmm2, %xmm3            # xmm3 = xmm2[1,1,3,3]
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm27, %xmm1, %xmm1
	vaddss	%xmm27, %xmm2, %xmm2
	vinsertps	$16, %xmm2, %xmm1, %xmm1 # xmm1 = xmm1[0],xmm2[0],xmm1[2,3]
	vmovaps	44864(%rsp), %zmm3              # 64-byte Reload
	vextractf64x4	$1, %zmm3, %ymm2
	vaddps	%zmm2, %zmm3, %zmm2
	vextractf128	$1, %ymm2, %xmm3
	vaddps	%xmm3, %xmm2, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm3         # xmm3 = xmm2[1,0]
	vaddps	%xmm3, %xmm2, %xmm2
	vmovshdup	%xmm2, %xmm3            # xmm3 = xmm2[1,1,3,3]
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm27, %xmm2, %xmm2
	vinsertps	$32, %xmm2, %xmm1, %xmm1 # xmm1 = xmm1[0,1],xmm2[0],xmm1[3]
	vmovaps	44736(%rsp), %zmm3              # 64-byte Reload
	vextractf64x4	$1, %zmm3, %ymm2
	vaddps	%zmm2, %zmm3, %zmm2
	vextractf128	$1, %ymm2, %xmm3
	vaddps	%xmm3, %xmm2, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm3         # xmm3 = xmm2[1,0]
	vaddps	%xmm3, %xmm2, %xmm2
	vmovshdup	%xmm2, %xmm3            # xmm3 = xmm2[1,1,3,3]
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm27, %xmm2, %xmm2
	vinsertps	$48, %xmm2, %xmm1, %xmm2 # xmm2 = xmm1[0,1,2],xmm2[0]
	vmovaps	14144(%rsp), %zmm3              # 64-byte Reload
	vextractf64x4	$1, %zmm3, %ymm1
	vaddps	%zmm1, %zmm3, %zmm1
	vextractf128	$1, %ymm1, %xmm3
	vaddps	%xmm3, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm3         # xmm3 = xmm1[1,0]
	vaddps	%xmm3, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm3            # xmm3 = xmm1[1,1,3,3]
	vaddss	%xmm3, %xmm1, %xmm1
	vmovaps	14464(%rsp), %zmm4              # 64-byte Reload
	vextractf64x4	$1, %zmm4, %ymm3
	vaddps	%zmm3, %zmm4, %zmm3
	vextractf128	$1, %ymm3, %xmm4
	vaddps	%xmm4, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm4         # xmm4 = xmm3[1,0]
	vaddps	%xmm4, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm4            # xmm4 = xmm3[1,1,3,3]
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm27, %xmm1, %xmm1
	vaddss	%xmm27, %xmm3, %xmm3
	vinsertps	$16, %xmm3, %xmm1, %xmm1 # xmm1 = xmm1[0],xmm3[0],xmm1[2,3]
	vmovaps	14592(%rsp), %zmm4              # 64-byte Reload
	vextractf64x4	$1, %zmm4, %ymm3
	vaddps	%zmm3, %zmm4, %zmm3
	vextractf128	$1, %ymm3, %xmm4
	vaddps	%xmm4, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm4         # xmm4 = xmm3[1,0]
	vaddps	%xmm4, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm4            # xmm4 = xmm3[1,1,3,3]
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm27, %xmm3, %xmm3
	vinsertps	$32, %xmm3, %xmm1, %xmm1 # xmm1 = xmm1[0,1],xmm3[0],xmm1[3]
	vmovaps	32832(%rsp), %zmm4              # 64-byte Reload
	vextractf64x4	$1, %zmm4, %ymm3
	vaddps	%zmm3, %zmm4, %zmm3
	vextractf128	$1, %ymm3, %xmm4
	vaddps	%xmm4, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm4         # xmm4 = xmm3[1,0]
	vaddps	%xmm4, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm4            # xmm4 = xmm3[1,1,3,3]
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm27, %xmm3, %xmm3
	vinsertps	$48, %xmm3, %xmm1, %xmm3 # xmm3 = xmm1[0,1,2],xmm3[0]
	vmovaps	14720(%rsp), %zmm4              # 64-byte Reload
	vextractf64x4	$1, %zmm4, %ymm1
	vaddps	%zmm1, %zmm4, %zmm1
	vextractf128	$1, %ymm1, %xmm4
	vaddps	%xmm4, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm4         # xmm4 = xmm1[1,0]
	vaddps	%xmm4, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm4            # xmm4 = xmm1[1,1,3,3]
	vaddss	%xmm4, %xmm1, %xmm1
	vmovaps	15232(%rsp), %zmm5              # 64-byte Reload
	vextractf64x4	$1, %zmm5, %ymm4
	vaddps	%zmm4, %zmm5, %zmm4
	vextractf128	$1, %ymm4, %xmm5
	vaddps	%xmm5, %xmm4, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm5         # xmm5 = xmm4[1,0]
	vaddps	%xmm5, %xmm4, %xmm4
	vmovshdup	%xmm4, %xmm5            # xmm5 = xmm4[1,1,3,3]
	vaddss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm27, %xmm1, %xmm1
	vaddss	%xmm27, %xmm4, %xmm4
	vinsertps	$16, %xmm4, %xmm1, %xmm1 # xmm1 = xmm1[0],xmm4[0],xmm1[2,3]
	vmovaps	15360(%rsp), %zmm5              # 64-byte Reload
	vextractf64x4	$1, %zmm5, %ymm4
	vaddps	%zmm4, %zmm5, %zmm4
	vextractf128	$1, %ymm4, %xmm5
	vaddps	%xmm5, %xmm4, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm5         # xmm5 = xmm4[1,0]
	vaddps	%xmm5, %xmm4, %xmm4
	vmovshdup	%xmm4, %xmm5            # xmm5 = xmm4[1,1,3,3]
	vaddss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm27, %xmm4, %xmm4
	vinsertps	$32, %xmm4, %xmm1, %xmm1 # xmm1 = xmm1[0,1],xmm4[0],xmm1[3]
	vmovaps	15424(%rsp), %zmm5              # 64-byte Reload
	vextractf64x4	$1, %zmm5, %ymm4
	vaddps	%zmm4, %zmm5, %zmm4
	vextractf128	$1, %ymm4, %xmm5
	vaddps	%xmm5, %xmm4, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm5         # xmm5 = xmm4[1,0]
	vaddps	%xmm5, %xmm4, %xmm4
	vmovshdup	%xmm4, %xmm5            # xmm5 = xmm4[1,1,3,3]
	vaddss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm27, %xmm4, %xmm4
	vinsertps	$48, %xmm4, %xmm1, %xmm4 # xmm4 = xmm1[0,1,2],xmm4[0]
	vmovaps	15616(%rsp), %zmm5              # 64-byte Reload
	vextractf64x4	$1, %zmm5, %ymm1
	vaddps	%zmm1, %zmm5, %zmm1
	vextractf128	$1, %ymm1, %xmm5
	vaddps	%xmm5, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm5         # xmm5 = xmm1[1,0]
	vaddps	%xmm5, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm5            # xmm5 = xmm1[1,1,3,3]
	vaddss	%xmm5, %xmm1, %xmm1
	vmovaps	37184(%rsp), %zmm6              # 64-byte Reload
	vextractf64x4	$1, %zmm6, %ymm5
	vaddps	%zmm5, %zmm6, %zmm5
	vextractf128	$1, %ymm5, %xmm6
	vaddps	%xmm6, %xmm5, %xmm5
	vshufpd	$1, %xmm5, %xmm5, %xmm6         # xmm6 = xmm5[1,0]
	vaddps	%xmm6, %xmm5, %xmm5
	vmovshdup	%xmm5, %xmm6            # xmm6 = xmm5[1,1,3,3]
	vaddss	%xmm6, %xmm5, %xmm5
	vaddss	%xmm27, %xmm1, %xmm1
	vaddss	%xmm27, %xmm5, %xmm5
	vinsertps	$16, %xmm5, %xmm1, %xmm1 # xmm1 = xmm1[0],xmm5[0],xmm1[2,3]
	vmovaps	18432(%rsp), %zmm6              # 64-byte Reload
	vextractf64x4	$1, %zmm6, %ymm5
	vaddps	%zmm5, %zmm6, %zmm5
	vextractf128	$1, %ymm5, %xmm6
	vaddps	%xmm6, %xmm5, %xmm5
	vshufpd	$1, %xmm5, %xmm5, %xmm6         # xmm6 = xmm5[1,0]
	vaddps	%xmm6, %xmm5, %xmm5
	vmovshdup	%xmm5, %xmm6            # xmm6 = xmm5[1,1,3,3]
	vaddss	%xmm6, %xmm5, %xmm5
	vaddss	%xmm27, %xmm5, %xmm5
	vinsertps	$32, %xmm5, %xmm1, %xmm1 # xmm1 = xmm1[0,1],xmm5[0],xmm1[3]
	vmovaps	10240(%rsp), %zmm6              # 64-byte Reload
	vextractf64x4	$1, %zmm6, %ymm5
	vaddps	%zmm5, %zmm6, %zmm5
	vextractf128	$1, %ymm5, %xmm6
	vaddps	%xmm6, %xmm5, %xmm5
	vshufpd	$1, %xmm5, %xmm5, %xmm6         # xmm6 = xmm5[1,0]
	vaddps	%xmm6, %xmm5, %xmm5
	vmovshdup	%xmm5, %xmm6            # xmm6 = xmm5[1,1,3,3]
	vaddss	%xmm6, %xmm5, %xmm5
	vaddss	%xmm27, %xmm5, %xmm5
	vinsertps	$48, %xmm5, %xmm1, %xmm5 # xmm5 = xmm1[0,1,2],xmm5[0]
	vmovaps	15872(%rsp), %zmm6              # 64-byte Reload
	vextractf64x4	$1, %zmm6, %ymm1
	vaddps	%zmm1, %zmm6, %zmm1
	vextractf128	$1, %ymm1, %xmm6
	vaddps	%xmm6, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm6         # xmm6 = xmm1[1,0]
	vaddps	%xmm6, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm6            # xmm6 = xmm1[1,1,3,3]
	vaddss	%xmm6, %xmm1, %xmm1
	vmovaps	16896(%rsp), %zmm7              # 64-byte Reload
	vextractf64x4	$1, %zmm7, %ymm6
	vaddps	%zmm6, %zmm7, %zmm6
	vextractf128	$1, %ymm6, %xmm7
	vaddps	%xmm7, %xmm6, %xmm6
	vshufpd	$1, %xmm6, %xmm6, %xmm7         # xmm7 = xmm6[1,0]
	vaddps	%xmm7, %xmm6, %xmm6
	vmovshdup	%xmm6, %xmm7            # xmm7 = xmm6[1,1,3,3]
	vaddss	%xmm7, %xmm6, %xmm6
	vaddss	%xmm27, %xmm1, %xmm1
	vaddss	%xmm27, %xmm6, %xmm6
	vinsertps	$16, %xmm6, %xmm1, %xmm1 # xmm1 = xmm1[0],xmm6[0],xmm1[2,3]
	vmovaps	34432(%rsp), %zmm7              # 64-byte Reload
	vextractf64x4	$1, %zmm7, %ymm6
	vaddps	%zmm6, %zmm7, %zmm6
	vextractf128	$1, %ymm6, %xmm7
	vaddps	%xmm7, %xmm6, %xmm6
	vshufpd	$1, %xmm6, %xmm6, %xmm7         # xmm7 = xmm6[1,0]
	vaddps	%xmm7, %xmm6, %xmm6
	vmovshdup	%xmm6, %xmm7            # xmm7 = xmm6[1,1,3,3]
	vaddss	%xmm7, %xmm6, %xmm6
	vaddss	%xmm27, %xmm6, %xmm6
	vinsertps	$32, %xmm6, %xmm1, %xmm1 # xmm1 = xmm1[0,1],xmm6[0],xmm1[3]
	vmovaps	15680(%rsp), %zmm7              # 64-byte Reload
	vextractf64x4	$1, %zmm7, %ymm6
	vaddps	%zmm6, %zmm7, %zmm6
	vextractf128	$1, %ymm6, %xmm7
	vaddps	%xmm7, %xmm6, %xmm6
	vshufpd	$1, %xmm6, %xmm6, %xmm7         # xmm7 = xmm6[1,0]
	vaddps	%xmm7, %xmm6, %xmm6
	vmovshdup	%xmm6, %xmm7            # xmm7 = xmm6[1,1,3,3]
	vaddss	%xmm7, %xmm6, %xmm6
	vaddss	%xmm27, %xmm6, %xmm6
	vinsertps	$48, %xmm6, %xmm1, %xmm6 # xmm6 = xmm1[0,1,2],xmm6[0]
	vmovaps	15936(%rsp), %zmm7              # 64-byte Reload
	vextractf64x4	$1, %zmm7, %ymm1
	vaddps	%zmm1, %zmm7, %zmm1
	vextractf128	$1, %ymm1, %xmm7
	vaddps	%xmm7, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm7         # xmm7 = xmm1[1,0]
	vaddps	%xmm7, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm7            # xmm7 = xmm1[1,1,3,3]
	vaddss	%xmm7, %xmm1, %xmm1
	vmovaps	33728(%rsp), %zmm8              # 64-byte Reload
	vextractf64x4	$1, %zmm8, %ymm7
	vaddps	%zmm7, %zmm8, %zmm7
	vextractf128	$1, %ymm7, %xmm8
	vaddps	%xmm7, %xmm8, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm8         # xmm8 = xmm7[1,0]
	vaddps	%xmm7, %xmm8, %xmm7
	vmovshdup	%xmm7, %xmm8            # xmm8 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm27, %xmm1, %xmm1
	vaddss	%xmm27, %xmm7, %xmm7
	vinsertps	$16, %xmm7, %xmm1, %xmm1 # xmm1 = xmm1[0],xmm7[0],xmm1[2,3]
	vmovaps	16000(%rsp), %zmm8              # 64-byte Reload
	vextractf64x4	$1, %zmm8, %ymm7
	vaddps	%zmm7, %zmm8, %zmm7
	vextractf128	$1, %ymm7, %xmm8
	vaddps	%xmm7, %xmm8, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm8         # xmm8 = xmm7[1,0]
	vaddps	%xmm7, %xmm8, %xmm7
	vmovshdup	%xmm7, %xmm8            # xmm8 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm27, %xmm7, %xmm7
	vinsertps	$32, %xmm7, %xmm1, %xmm1 # xmm1 = xmm1[0,1],xmm7[0],xmm1[3]
	vmovaps	16064(%rsp), %zmm8              # 64-byte Reload
	vextractf64x4	$1, %zmm8, %ymm7
	vaddps	%zmm7, %zmm8, %zmm7
	vextractf128	$1, %ymm7, %xmm8
	vaddps	%xmm7, %xmm8, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm8         # xmm8 = xmm7[1,0]
	vaddps	%xmm7, %xmm8, %xmm7
	vmovshdup	%xmm7, %xmm8            # xmm8 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm27, %xmm7, %xmm7
	vinsertps	$48, %xmm7, %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],xmm7[0]
	vmovaps	16128(%rsp), %zmm8              # 64-byte Reload
	vextractf64x4	$1, %zmm8, %ymm7
	vaddps	%zmm7, %zmm8, %zmm7
	vextractf128	$1, %ymm7, %xmm8
	vaddps	%xmm7, %xmm8, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm8         # xmm8 = xmm7[1,0]
	vaddps	%xmm7, %xmm8, %xmm7
	vmovshdup	%xmm7, %xmm8            # xmm8 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm8, %xmm7
	vmovaps	34176(%rsp), %zmm9              # 64-byte Reload
	vextractf64x4	$1, %zmm9, %ymm8
	vaddps	%zmm8, %zmm9, %zmm8
	vextractf128	$1, %ymm8, %xmm9
	vaddps	%xmm9, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm9         # xmm9 = xmm8[1,0]
	vaddps	%xmm9, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm9            # xmm9 = xmm8[1,1,3,3]
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm27, %xmm7, %xmm7
	vaddss	%xmm27, %xmm8, %xmm8
	vinsertps	$16, %xmm8, %xmm7, %xmm7 # xmm7 = xmm7[0],xmm8[0],xmm7[2,3]
	vmovaps	16704(%rsp), %zmm9              # 64-byte Reload
	vextractf64x4	$1, %zmm9, %ymm8
	vaddps	%zmm8, %zmm9, %zmm8
	vextractf128	$1, %ymm8, %xmm9
	vaddps	%xmm9, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm9         # xmm9 = xmm8[1,0]
	vaddps	%xmm9, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm9            # xmm9 = xmm8[1,1,3,3]
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm27, %xmm8, %xmm8
	vinsertps	$32, %xmm8, %xmm7, %xmm7 # xmm7 = xmm7[0,1],xmm8[0],xmm7[3]
	vmovaps	16768(%rsp), %zmm9              # 64-byte Reload
	vextractf64x4	$1, %zmm9, %ymm8
	vaddps	%zmm8, %zmm9, %zmm8
	vextractf128	$1, %ymm8, %xmm9
	vaddps	%xmm9, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm9         # xmm9 = xmm8[1,0]
	vaddps	%xmm9, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm9            # xmm9 = xmm8[1,1,3,3]
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm27, %xmm8, %xmm8
	vinsertps	$48, %xmm8, %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],xmm8[0]
	vinsertf128	$1, %xmm7, %ymm1, %ymm1
	vinsertf128	$1, %xmm6, %ymm5, %ymm5
	vinsertf64x4	$1, %ymm1, %zmm5, %zmm1
	vmovaps	23360(%rsp), %zmm5              # 64-byte Reload
	vaddps	%zmm1, %zmm5, %zmm5
	vmovaps	%zmm5, 23360(%rsp)              # 64-byte Spill
	vinsertf128	$1, %xmm4, %ymm3, %ymm1
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vinsertf64x4	$1, %ymm1, %zmm0, %zmm0
	vmovaps	23424(%rsp), %zmm1              # 64-byte Reload
	vaddps	%zmm0, %zmm1, %zmm1
	vmovaps	%zmm1, 23424(%rsp)              # 64-byte Spill
	vmovaps	35648(%rsp), %zmm1              # 64-byte Reload
	vextractf64x4	$1, %zmm1, %ymm0
	vaddps	%zmm0, %zmm1, %zmm0
	vextractf128	$1, %ymm0, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm1         # xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm0
	vmovaps	45696(%rsp), %zmm2              # 64-byte Reload
	vextractf64x4	$1, %zmm2, %ymm1
	vaddps	%zmm1, %zmm2, %zmm1
	vextractf128	$1, %ymm1, %xmm2
	vaddps	%xmm2, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm2         # xmm2 = xmm1[1,0]
	vaddps	%xmm2, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm2            # xmm2 = xmm1[1,1,3,3]
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm27, %xmm0, %xmm0
	vaddss	%xmm27, %xmm1, %xmm1
	vinsertps	$16, %xmm1, %xmm0, %xmm0 # xmm0 = xmm0[0],xmm1[0],xmm0[2,3]
	vmovaps	45568(%rsp), %zmm2              # 64-byte Reload
	vextractf64x4	$1, %zmm2, %ymm1
	vaddps	%zmm1, %zmm2, %zmm1
	vextractf128	$1, %ymm1, %xmm2
	vaddps	%xmm2, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm2         # xmm2 = xmm1[1,0]
	vaddps	%xmm2, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm2            # xmm2 = xmm1[1,1,3,3]
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm27, %xmm1, %xmm1
	vinsertps	$32, %xmm1, %xmm0, %xmm0 # xmm0 = xmm0[0,1],xmm1[0],xmm0[3]
	vmovaps	45440(%rsp), %zmm2              # 64-byte Reload
	vextractf64x4	$1, %zmm2, %ymm1
	vaddps	%zmm1, %zmm2, %zmm1
	vextractf128	$1, %ymm1, %xmm2
	vaddps	%xmm2, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm2         # xmm2 = xmm1[1,0]
	vaddps	%xmm2, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm2            # xmm2 = xmm1[1,1,3,3]
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm27, %xmm1, %xmm1
	vinsertps	$48, %xmm1, %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],xmm1[0]
	vmovaps	36032(%rsp), %zmm2              # 64-byte Reload
	vextractf64x4	$1, %zmm2, %ymm1
	vaddps	%zmm1, %zmm2, %zmm1
	vextractf128	$1, %ymm1, %xmm2
	vaddps	%xmm2, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm2         # xmm2 = xmm1[1,0]
	vaddps	%xmm2, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm2            # xmm2 = xmm1[1,1,3,3]
	vaddss	%xmm2, %xmm1, %xmm1
	vmovaps	45376(%rsp), %zmm3              # 64-byte Reload
	vextractf64x4	$1, %zmm3, %ymm2
	vaddps	%zmm2, %zmm3, %zmm2
	vextractf128	$1, %ymm2, %xmm3
	vaddps	%xmm3, %xmm2, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm3         # xmm3 = xmm2[1,0]
	vaddps	%xmm3, %xmm2, %xmm2
	vmovshdup	%xmm2, %xmm3            # xmm3 = xmm2[1,1,3,3]
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm27, %xmm1, %xmm1
	vaddss	%xmm27, %xmm2, %xmm2
	vinsertps	$16, %xmm2, %xmm1, %xmm1 # xmm1 = xmm1[0],xmm2[0],xmm1[2,3]
	vmovaps	45248(%rsp), %zmm3              # 64-byte Reload
	vextractf64x4	$1, %zmm3, %ymm2
	vaddps	%zmm2, %zmm3, %zmm2
	vextractf128	$1, %ymm2, %xmm3
	vaddps	%xmm3, %xmm2, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm3         # xmm3 = xmm2[1,0]
	vaddps	%xmm3, %xmm2, %xmm2
	vmovshdup	%xmm2, %xmm3            # xmm3 = xmm2[1,1,3,3]
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm27, %xmm2, %xmm2
	vinsertps	$32, %xmm2, %xmm1, %xmm1 # xmm1 = xmm1[0,1],xmm2[0],xmm1[3]
	vmovaps	45120(%rsp), %zmm3              # 64-byte Reload
	vextractf64x4	$1, %zmm3, %ymm2
	vaddps	%zmm2, %zmm3, %zmm2
	vextractf128	$1, %ymm2, %xmm3
	vaddps	%xmm3, %xmm2, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm3         # xmm3 = xmm2[1,0]
	vaddps	%xmm3, %xmm2, %xmm2
	vmovshdup	%xmm2, %xmm3            # xmm3 = xmm2[1,1,3,3]
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm27, %xmm2, %xmm2
	vinsertps	$48, %xmm2, %xmm1, %xmm2 # xmm2 = xmm1[0,1,2],xmm2[0]
	vmovaps	16960(%rsp), %zmm3              # 64-byte Reload
	vextractf64x4	$1, %zmm3, %ymm1
	vaddps	%zmm1, %zmm3, %zmm1
	vextractf128	$1, %ymm1, %xmm3
	vaddps	%xmm3, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm3         # xmm3 = xmm1[1,0]
	vaddps	%xmm3, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm3            # xmm3 = xmm1[1,1,3,3]
	vaddss	%xmm3, %xmm1, %xmm1
	vmovaps	36224(%rsp), %zmm4              # 64-byte Reload
	vextractf64x4	$1, %zmm4, %ymm3
	vaddps	%zmm3, %zmm4, %zmm3
	vextractf128	$1, %ymm3, %xmm4
	vaddps	%xmm4, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm4         # xmm4 = xmm3[1,0]
	vaddps	%xmm4, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm4            # xmm4 = xmm3[1,1,3,3]
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm27, %xmm1, %xmm1
	vaddss	%xmm27, %xmm3, %xmm3
	vinsertps	$16, %xmm3, %xmm1, %xmm1 # xmm1 = xmm1[0],xmm3[0],xmm1[2,3]
	vmovaps	36288(%rsp), %zmm4              # 64-byte Reload
	vextractf64x4	$1, %zmm4, %ymm3
	vaddps	%zmm3, %zmm4, %zmm3
	vextractf128	$1, %ymm3, %xmm4
	vaddps	%xmm4, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm4         # xmm4 = xmm3[1,0]
	vaddps	%xmm4, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm4            # xmm4 = xmm3[1,1,3,3]
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm27, %xmm3, %xmm3
	vinsertps	$32, %xmm3, %xmm1, %xmm1 # xmm1 = xmm1[0,1],xmm3[0],xmm1[3]
	vmovaps	17024(%rsp), %zmm4              # 64-byte Reload
	vextractf64x4	$1, %zmm4, %ymm3
	vaddps	%zmm3, %zmm4, %zmm3
	vextractf128	$1, %ymm3, %xmm4
	vaddps	%xmm4, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm4         # xmm4 = xmm3[1,0]
	vaddps	%xmm4, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm4            # xmm4 = xmm3[1,1,3,3]
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm27, %xmm3, %xmm3
	vinsertps	$48, %xmm3, %xmm1, %xmm3 # xmm3 = xmm1[0,1,2],xmm3[0]
	vmovaps	17216(%rsp), %zmm4              # 64-byte Reload
	vextractf64x4	$1, %zmm4, %ymm1
	vaddps	%zmm1, %zmm4, %zmm1
	vextractf128	$1, %ymm1, %xmm4
	vaddps	%xmm4, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm4         # xmm4 = xmm1[1,0]
	vaddps	%xmm4, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm4            # xmm4 = xmm1[1,1,3,3]
	vaddss	%xmm4, %xmm1, %xmm1
	vmovaps	17344(%rsp), %zmm5              # 64-byte Reload
	vextractf64x4	$1, %zmm5, %ymm4
	vaddps	%zmm4, %zmm5, %zmm4
	vextractf128	$1, %ymm4, %xmm5
	vaddps	%xmm5, %xmm4, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm5         # xmm5 = xmm4[1,0]
	vaddps	%xmm5, %xmm4, %xmm4
	vmovshdup	%xmm4, %xmm5            # xmm5 = xmm4[1,1,3,3]
	vaddss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm27, %xmm1, %xmm1
	vaddss	%xmm27, %xmm4, %xmm4
	vinsertps	$16, %xmm4, %xmm1, %xmm1 # xmm1 = xmm1[0],xmm4[0],xmm1[2,3]
	vmovaps	17408(%rsp), %zmm5              # 64-byte Reload
	vextractf64x4	$1, %zmm5, %ymm4
	vaddps	%zmm4, %zmm5, %zmm4
	vextractf128	$1, %ymm4, %xmm5
	vaddps	%xmm5, %xmm4, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm5         # xmm5 = xmm4[1,0]
	vaddps	%xmm5, %xmm4, %xmm4
	vmovshdup	%xmm4, %xmm5            # xmm5 = xmm4[1,1,3,3]
	vaddss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm27, %xmm4, %xmm4
	vinsertps	$32, %xmm4, %xmm1, %xmm1 # xmm1 = xmm1[0,1],xmm4[0],xmm1[3]
	vmovaps	17472(%rsp), %zmm5              # 64-byte Reload
	vextractf64x4	$1, %zmm5, %ymm4
	vaddps	%zmm4, %zmm5, %zmm4
	vextractf128	$1, %ymm4, %xmm5
	vaddps	%xmm5, %xmm4, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm5         # xmm5 = xmm4[1,0]
	vaddps	%xmm5, %xmm4, %xmm4
	vmovshdup	%xmm4, %xmm5            # xmm5 = xmm4[1,1,3,3]
	vaddss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm27, %xmm4, %xmm4
	vinsertps	$48, %xmm4, %xmm1, %xmm4 # xmm4 = xmm1[0,1,2],xmm4[0]
	vmovaps	17600(%rsp), %zmm5              # 64-byte Reload
	vextractf64x4	$1, %zmm5, %ymm1
	vaddps	%zmm1, %zmm5, %zmm1
	vextractf128	$1, %ymm1, %xmm5
	vaddps	%xmm5, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm5         # xmm5 = xmm1[1,0]
	vaddps	%xmm5, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm5            # xmm5 = xmm1[1,1,3,3]
	vaddss	%xmm5, %xmm1, %xmm1
	vmovaps	37248(%rsp), %zmm6              # 64-byte Reload
	vextractf64x4	$1, %zmm6, %ymm5
	vaddps	%zmm5, %zmm6, %zmm5
	vextractf128	$1, %ymm5, %xmm6
	vaddps	%xmm6, %xmm5, %xmm5
	vshufpd	$1, %xmm5, %xmm5, %xmm6         # xmm6 = xmm5[1,0]
	vaddps	%xmm6, %xmm5, %xmm5
	vmovshdup	%xmm5, %xmm6            # xmm6 = xmm5[1,1,3,3]
	vaddss	%xmm6, %xmm5, %xmm5
	vaddss	%xmm27, %xmm1, %xmm1
	vaddss	%xmm27, %xmm5, %xmm5
	vinsertps	$16, %xmm5, %xmm1, %xmm1 # xmm1 = xmm1[0],xmm5[0],xmm1[2,3]
	vmovaps	37440(%rsp), %zmm6              # 64-byte Reload
	vextractf64x4	$1, %zmm6, %ymm5
	vaddps	%zmm5, %zmm6, %zmm5
	vextractf128	$1, %ymm5, %xmm6
	vaddps	%xmm6, %xmm5, %xmm5
	vshufpd	$1, %xmm5, %xmm5, %xmm6         # xmm6 = xmm5[1,0]
	vaddps	%xmm6, %xmm5, %xmm5
	vmovshdup	%xmm5, %xmm6            # xmm6 = xmm5[1,1,3,3]
	vaddss	%xmm6, %xmm5, %xmm5
	vaddss	%xmm27, %xmm5, %xmm5
	vinsertps	$32, %xmm5, %xmm1, %xmm1 # xmm1 = xmm1[0,1],xmm5[0],xmm1[3]
	vmovaps	10304(%rsp), %zmm6              # 64-byte Reload
	vextractf64x4	$1, %zmm6, %ymm5
	vaddps	%zmm5, %zmm6, %zmm5
	vextractf128	$1, %ymm5, %xmm6
	vaddps	%xmm6, %xmm5, %xmm5
	vshufpd	$1, %xmm5, %xmm5, %xmm6         # xmm6 = xmm5[1,0]
	vaddps	%xmm6, %xmm5, %xmm5
	vmovshdup	%xmm5, %xmm6            # xmm6 = xmm5[1,1,3,3]
	vaddss	%xmm6, %xmm5, %xmm5
	vaddss	%xmm27, %xmm5, %xmm5
	vinsertps	$48, %xmm5, %xmm1, %xmm5 # xmm5 = xmm1[0,1,2],xmm5[0]
	vmovaps	17664(%rsp), %zmm6              # 64-byte Reload
	vextractf64x4	$1, %zmm6, %ymm1
	vaddps	%zmm1, %zmm6, %zmm1
	vextractf128	$1, %ymm1, %xmm6
	vaddps	%xmm6, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm6         # xmm6 = xmm1[1,0]
	vaddps	%xmm6, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm6            # xmm6 = xmm1[1,1,3,3]
	vaddss	%xmm6, %xmm1, %xmm1
	vmovaps	17088(%rsp), %zmm7              # 64-byte Reload
	vextractf64x4	$1, %zmm7, %ymm6
	vaddps	%zmm6, %zmm7, %zmm6
	vextractf128	$1, %ymm6, %xmm7
	vaddps	%xmm7, %xmm6, %xmm6
	vshufpd	$1, %xmm6, %xmm6, %xmm7         # xmm7 = xmm6[1,0]
	vaddps	%xmm7, %xmm6, %xmm6
	vmovshdup	%xmm6, %xmm7            # xmm7 = xmm6[1,1,3,3]
	vaddss	%xmm7, %xmm6, %xmm6
	vaddss	%xmm27, %xmm1, %xmm1
	vaddss	%xmm27, %xmm6, %xmm6
	vinsertps	$16, %xmm6, %xmm1, %xmm1 # xmm1 = xmm1[0],xmm6[0],xmm1[2,3]
	vmovaps	35264(%rsp), %zmm7              # 64-byte Reload
	vextractf64x4	$1, %zmm7, %ymm6
	vaddps	%zmm6, %zmm7, %zmm6
	vextractf128	$1, %ymm6, %xmm7
	vaddps	%xmm7, %xmm6, %xmm6
	vshufpd	$1, %xmm6, %xmm6, %xmm7         # xmm7 = xmm6[1,0]
	vaddps	%xmm7, %xmm6, %xmm6
	vmovshdup	%xmm6, %xmm7            # xmm7 = xmm6[1,1,3,3]
	vaddss	%xmm7, %xmm6, %xmm6
	vaddss	%xmm27, %xmm6, %xmm6
	vinsertps	$32, %xmm6, %xmm1, %xmm1 # xmm1 = xmm1[0,1],xmm6[0],xmm1[3]
	vmovaps	33856(%rsp), %zmm7              # 64-byte Reload
	vextractf64x4	$1, %zmm7, %ymm6
	vaddps	%zmm6, %zmm7, %zmm6
	vextractf128	$1, %ymm6, %xmm7
	vaddps	%xmm7, %xmm6, %xmm6
	vshufpd	$1, %xmm6, %xmm6, %xmm7         # xmm7 = xmm6[1,0]
	vaddps	%xmm7, %xmm6, %xmm6
	vmovshdup	%xmm6, %xmm7            # xmm7 = xmm6[1,1,3,3]
	vaddss	%xmm7, %xmm6, %xmm6
	vaddss	%xmm27, %xmm6, %xmm6
	vinsertps	$48, %xmm6, %xmm1, %xmm6 # xmm6 = xmm1[0,1,2],xmm6[0]
	vmovaps	17728(%rsp), %zmm7              # 64-byte Reload
	vextractf64x4	$1, %zmm7, %ymm1
	vaddps	%zmm1, %zmm7, %zmm1
	vextractf128	$1, %ymm1, %xmm7
	vaddps	%xmm7, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm7         # xmm7 = xmm1[1,0]
	vaddps	%xmm7, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm7            # xmm7 = xmm1[1,1,3,3]
	vaddss	%xmm7, %xmm1, %xmm1
	vmovaps	17792(%rsp), %zmm8              # 64-byte Reload
	vextractf64x4	$1, %zmm8, %ymm7
	vaddps	%zmm7, %zmm8, %zmm7
	vextractf128	$1, %ymm7, %xmm8
	vaddps	%xmm7, %xmm8, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm8         # xmm8 = xmm7[1,0]
	vaddps	%xmm7, %xmm8, %xmm7
	vmovshdup	%xmm7, %xmm8            # xmm8 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm27, %xmm1, %xmm1
	vaddss	%xmm27, %xmm7, %xmm7
	vinsertps	$16, %xmm7, %xmm1, %xmm1 # xmm1 = xmm1[0],xmm7[0],xmm1[2,3]
	vmovaps	17856(%rsp), %zmm8              # 64-byte Reload
	vextractf64x4	$1, %zmm8, %ymm7
	vaddps	%zmm7, %zmm8, %zmm7
	vextractf128	$1, %ymm7, %xmm8
	vaddps	%xmm7, %xmm8, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm8         # xmm8 = xmm7[1,0]
	vaddps	%xmm7, %xmm8, %xmm7
	vmovshdup	%xmm7, %xmm8            # xmm8 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm27, %xmm7, %xmm7
	vinsertps	$32, %xmm7, %xmm1, %xmm1 # xmm1 = xmm1[0,1],xmm7[0],xmm1[3]
	vmovaps	17920(%rsp), %zmm8              # 64-byte Reload
	vextractf64x4	$1, %zmm8, %ymm7
	vaddps	%zmm7, %zmm8, %zmm7
	vextractf128	$1, %ymm7, %xmm8
	vaddps	%xmm7, %xmm8, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm8         # xmm8 = xmm7[1,0]
	vaddps	%xmm7, %xmm8, %xmm7
	vmovshdup	%xmm7, %xmm8            # xmm8 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm27, %xmm7, %xmm7
	vinsertps	$48, %xmm7, %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],xmm7[0]
	vmovaps	17984(%rsp), %zmm8              # 64-byte Reload
	vextractf64x4	$1, %zmm8, %ymm7
	vaddps	%zmm7, %zmm8, %zmm7
	vextractf128	$1, %ymm7, %xmm8
	vaddps	%xmm7, %xmm8, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm8         # xmm8 = xmm7[1,0]
	vaddps	%xmm7, %xmm8, %xmm7
	vmovshdup	%xmm7, %xmm8            # xmm8 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm8, %xmm7
	vmovaps	36992(%rsp), %zmm9              # 64-byte Reload
	vextractf64x4	$1, %zmm9, %ymm8
	vaddps	%zmm8, %zmm9, %zmm8
	vextractf128	$1, %ymm8, %xmm9
	vaddps	%xmm9, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm9         # xmm9 = xmm8[1,0]
	vaddps	%xmm9, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm9            # xmm9 = xmm8[1,1,3,3]
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm27, %xmm7, %xmm7
	vaddss	%xmm27, %xmm8, %xmm8
	vinsertps	$16, %xmm8, %xmm7, %xmm7 # xmm7 = xmm7[0],xmm8[0],xmm7[2,3]
	vmovaps	18048(%rsp), %zmm9              # 64-byte Reload
	vextractf64x4	$1, %zmm9, %ymm8
	vaddps	%zmm8, %zmm9, %zmm8
	vextractf128	$1, %ymm8, %xmm9
	vaddps	%xmm9, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm9         # xmm9 = xmm8[1,0]
	vaddps	%xmm9, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm9            # xmm9 = xmm8[1,1,3,3]
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm27, %xmm8, %xmm8
	vinsertps	$32, %xmm8, %xmm7, %xmm7 # xmm7 = xmm7[0,1],xmm8[0],xmm7[3]
	vmovaps	8512(%rsp), %zmm9               # 64-byte Reload
	vextractf64x4	$1, %zmm9, %ymm8
	vaddps	%zmm8, %zmm9, %zmm8
	vextractf128	$1, %ymm8, %xmm9
	vaddps	%xmm9, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm9         # xmm9 = xmm8[1,0]
	vaddps	%xmm9, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm9            # xmm9 = xmm8[1,1,3,3]
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm27, %xmm8, %xmm8
	vinsertps	$48, %xmm8, %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],xmm8[0]
	vinsertf128	$1, %xmm7, %ymm1, %ymm1
	vinsertf128	$1, %xmm6, %ymm5, %ymm5
	vinsertf64x4	$1, %ymm1, %zmm5, %zmm1
	vmovaps	23488(%rsp), %zmm5              # 64-byte Reload
	vaddps	%zmm1, %zmm5, %zmm5
	vmovaps	%zmm5, 23488(%rsp)              # 64-byte Spill
	vinsertf128	$1, %xmm4, %ymm3, %ymm1
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vinsertf64x4	$1, %ymm1, %zmm0, %zmm0
	vmovaps	23552(%rsp), %zmm1              # 64-byte Reload
	vaddps	%zmm0, %zmm1, %zmm1
	vmovaps	%zmm1, 23552(%rsp)              # 64-byte Spill
	vmovaps	8576(%rsp), %zmm1               # 64-byte Reload
	vextractf64x4	$1, %zmm1, %ymm0
	vaddps	%zmm0, %zmm1, %zmm0
	vextractf128	$1, %ymm0, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm1         # xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm0
	vmovaps	46080(%rsp), %zmm2              # 64-byte Reload
	vextractf64x4	$1, %zmm2, %ymm1
	vaddps	%zmm1, %zmm2, %zmm1
	vextractf128	$1, %ymm1, %xmm2
	vaddps	%xmm2, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm2         # xmm2 = xmm1[1,0]
	vaddps	%xmm2, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm2            # xmm2 = xmm1[1,1,3,3]
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm27, %xmm0, %xmm0
	vaddss	%xmm27, %xmm1, %xmm1
	vinsertps	$16, %xmm1, %xmm0, %xmm0 # xmm0 = xmm0[0],xmm1[0],xmm0[2,3]
	vmovaps	45952(%rsp), %zmm2              # 64-byte Reload
	vextractf64x4	$1, %zmm2, %ymm1
	vaddps	%zmm1, %zmm2, %zmm1
	vextractf128	$1, %ymm1, %xmm2
	vaddps	%xmm2, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm2         # xmm2 = xmm1[1,0]
	vaddps	%xmm2, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm2            # xmm2 = xmm1[1,1,3,3]
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm27, %xmm1, %xmm1
	vinsertps	$32, %xmm1, %xmm0, %xmm0 # xmm0 = xmm0[0,1],xmm1[0],xmm0[3]
	vmovaps	45824(%rsp), %zmm2              # 64-byte Reload
	vextractf64x4	$1, %zmm2, %ymm1
	vaddps	%zmm1, %zmm2, %zmm1
	vextractf128	$1, %ymm1, %xmm2
	vaddps	%xmm2, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm2         # xmm2 = xmm1[1,0]
	vaddps	%xmm2, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm2            # xmm2 = xmm1[1,1,3,3]
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm27, %xmm1, %xmm1
	vinsertps	$48, %xmm1, %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],xmm1[0]
	vmovaps	8640(%rsp), %zmm2               # 64-byte Reload
	vextractf64x4	$1, %zmm2, %ymm1
	vaddps	%zmm1, %zmm2, %zmm1
	vextractf128	$1, %ymm1, %xmm2
	vaddps	%xmm2, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm2         # xmm2 = xmm1[1,0]
	vaddps	%xmm2, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm2            # xmm2 = xmm1[1,1,3,3]
	vaddss	%xmm2, %xmm1, %xmm1
	vmovaps	45760(%rsp), %zmm3              # 64-byte Reload
	vextractf64x4	$1, %zmm3, %ymm2
	vaddps	%zmm2, %zmm3, %zmm2
	vextractf128	$1, %ymm2, %xmm3
	vaddps	%xmm3, %xmm2, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm3         # xmm3 = xmm2[1,0]
	vaddps	%xmm3, %xmm2, %xmm2
	vmovshdup	%xmm2, %xmm3            # xmm3 = xmm2[1,1,3,3]
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm27, %xmm1, %xmm1
	vaddss	%xmm27, %xmm2, %xmm2
	vinsertps	$16, %xmm2, %xmm1, %xmm1 # xmm1 = xmm1[0],xmm2[0],xmm1[2,3]
	vmovaps	45632(%rsp), %zmm3              # 64-byte Reload
	vextractf64x4	$1, %zmm3, %ymm2
	vaddps	%zmm2, %zmm3, %zmm2
	vextractf128	$1, %ymm2, %xmm3
	vaddps	%xmm3, %xmm2, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm3         # xmm3 = xmm2[1,0]
	vaddps	%xmm3, %xmm2, %xmm2
	vmovshdup	%xmm2, %xmm3            # xmm3 = xmm2[1,1,3,3]
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm27, %xmm2, %xmm2
	vinsertps	$32, %xmm2, %xmm1, %xmm1 # xmm1 = xmm1[0,1],xmm2[0],xmm1[3]
	vmovaps	45504(%rsp), %zmm3              # 64-byte Reload
	vextractf64x4	$1, %zmm3, %ymm2
	vaddps	%zmm2, %zmm3, %zmm2
	vextractf128	$1, %ymm2, %xmm3
	vaddps	%xmm3, %xmm2, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm3         # xmm3 = xmm2[1,0]
	vaddps	%xmm3, %xmm2, %xmm2
	vmovshdup	%xmm2, %xmm3            # xmm3 = xmm2[1,1,3,3]
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm27, %xmm2, %xmm2
	vinsertps	$48, %xmm2, %xmm1, %xmm2 # xmm2 = xmm1[0,1,2],xmm2[0]
	vmovaps	7744(%rsp), %zmm3               # 64-byte Reload
	vextractf64x4	$1, %zmm3, %ymm1
	vaddps	%zmm1, %zmm3, %zmm1
	vextractf128	$1, %ymm1, %xmm3
	vaddps	%xmm3, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm3         # xmm3 = xmm1[1,0]
	vaddps	%xmm3, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm3            # xmm3 = xmm1[1,1,3,3]
	vaddss	%xmm3, %xmm1, %xmm1
	vmovaps	7808(%rsp), %zmm4               # 64-byte Reload
	vextractf64x4	$1, %zmm4, %ymm3
	vaddps	%zmm3, %zmm4, %zmm3
	vextractf128	$1, %ymm3, %xmm4
	vaddps	%xmm4, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm4         # xmm4 = xmm3[1,0]
	vaddps	%xmm4, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm4            # xmm4 = xmm3[1,1,3,3]
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm27, %xmm1, %xmm1
	vaddss	%xmm27, %xmm3, %xmm3
	vinsertps	$16, %xmm3, %xmm1, %xmm1 # xmm1 = xmm1[0],xmm3[0],xmm1[2,3]
	vmovaps	7872(%rsp), %zmm4               # 64-byte Reload
	vextractf64x4	$1, %zmm4, %ymm3
	vaddps	%zmm3, %zmm4, %zmm3
	vextractf128	$1, %ymm3, %xmm4
	vaddps	%xmm4, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm4         # xmm4 = xmm3[1,0]
	vaddps	%xmm4, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm4            # xmm4 = xmm3[1,1,3,3]
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm27, %xmm3, %xmm3
	vinsertps	$32, %xmm3, %xmm1, %xmm1 # xmm1 = xmm1[0,1],xmm3[0],xmm1[3]
	vmovaps	7936(%rsp), %zmm4               # 64-byte Reload
	vextractf64x4	$1, %zmm4, %ymm3
	vaddps	%zmm3, %zmm4, %zmm3
	vextractf128	$1, %ymm3, %xmm4
	vaddps	%xmm4, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm4         # xmm4 = xmm3[1,0]
	vaddps	%xmm4, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm4            # xmm4 = xmm3[1,1,3,3]
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm27, %xmm3, %xmm3
	vinsertps	$48, %xmm3, %xmm1, %xmm3 # xmm3 = xmm1[0,1,2],xmm3[0]
	vmovaps	8000(%rsp), %zmm4               # 64-byte Reload
	vextractf64x4	$1, %zmm4, %ymm1
	vaddps	%zmm1, %zmm4, %zmm1
	vextractf128	$1, %ymm1, %xmm4
	vaddps	%xmm4, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm4         # xmm4 = xmm1[1,0]
	vaddps	%xmm4, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm4            # xmm4 = xmm1[1,1,3,3]
	vaddss	%xmm4, %xmm1, %xmm1
	vmovaps	8064(%rsp), %zmm5               # 64-byte Reload
	vextractf64x4	$1, %zmm5, %ymm4
	vaddps	%zmm4, %zmm5, %zmm4
	vextractf128	$1, %ymm4, %xmm5
	vaddps	%xmm5, %xmm4, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm5         # xmm5 = xmm4[1,0]
	vaddps	%xmm5, %xmm4, %xmm4
	vmovshdup	%xmm4, %xmm5            # xmm5 = xmm4[1,1,3,3]
	vaddss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm27, %xmm1, %xmm1
	vaddss	%xmm27, %xmm4, %xmm4
	vinsertps	$16, %xmm4, %xmm1, %xmm1 # xmm1 = xmm1[0],xmm4[0],xmm1[2,3]
	vmovaps	8128(%rsp), %zmm5               # 64-byte Reload
	vextractf64x4	$1, %zmm5, %ymm4
	vaddps	%zmm4, %zmm5, %zmm4
	vextractf128	$1, %ymm4, %xmm5
	vaddps	%xmm5, %xmm4, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm5         # xmm5 = xmm4[1,0]
	vaddps	%xmm5, %xmm4, %xmm4
	vmovshdup	%xmm4, %xmm5            # xmm5 = xmm4[1,1,3,3]
	vaddss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm27, %xmm4, %xmm4
	vinsertps	$32, %xmm4, %xmm1, %xmm1 # xmm1 = xmm1[0,1],xmm4[0],xmm1[3]
	vmovaps	6720(%rsp), %zmm5               # 64-byte Reload
	vextractf64x4	$1, %zmm5, %ymm4
	vaddps	%zmm4, %zmm5, %zmm4
	vextractf128	$1, %ymm4, %xmm5
	vaddps	%xmm5, %xmm4, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm5         # xmm5 = xmm4[1,0]
	vaddps	%xmm5, %xmm4, %xmm4
	vmovshdup	%xmm4, %xmm5            # xmm5 = xmm4[1,1,3,3]
	vaddss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm27, %xmm4, %xmm4
	vinsertps	$48, %xmm4, %xmm1, %xmm4 # xmm4 = xmm1[0,1,2],xmm4[0]
	vmovaps	6784(%rsp), %zmm5               # 64-byte Reload
	vextractf64x4	$1, %zmm5, %ymm1
	vaddps	%zmm1, %zmm5, %zmm1
	vextractf128	$1, %ymm1, %xmm5
	vaddps	%xmm5, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm5         # xmm5 = xmm1[1,0]
	vaddps	%xmm5, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm5            # xmm5 = xmm1[1,1,3,3]
	vaddss	%xmm5, %xmm1, %xmm1
	vmovaps	37312(%rsp), %zmm6              # 64-byte Reload
	vextractf64x4	$1, %zmm6, %ymm5
	vaddps	%zmm5, %zmm6, %zmm5
	vextractf128	$1, %ymm5, %xmm6
	vaddps	%xmm6, %xmm5, %xmm5
	vshufpd	$1, %xmm5, %xmm5, %xmm6         # xmm6 = xmm5[1,0]
	vaddps	%xmm6, %xmm5, %xmm5
	vmovshdup	%xmm5, %xmm6            # xmm6 = xmm5[1,1,3,3]
	vaddss	%xmm6, %xmm5, %xmm5
	vaddss	%xmm27, %xmm1, %xmm1
	vaddss	%xmm27, %xmm5, %xmm5
	vinsertps	$16, %xmm5, %xmm1, %xmm1 # xmm1 = xmm1[0],xmm5[0],xmm1[2,3]
	vmovaps	37568(%rsp), %zmm6              # 64-byte Reload
	vextractf64x4	$1, %zmm6, %ymm5
	vaddps	%zmm5, %zmm6, %zmm5
	vextractf128	$1, %ymm5, %xmm6
	vaddps	%xmm6, %xmm5, %xmm5
	vshufpd	$1, %xmm5, %xmm5, %xmm6         # xmm6 = xmm5[1,0]
	vaddps	%xmm6, %xmm5, %xmm5
	vmovshdup	%xmm5, %xmm6            # xmm6 = xmm5[1,1,3,3]
	vaddss	%xmm6, %xmm5, %xmm5
	vaddss	%xmm27, %xmm5, %xmm5
	vinsertps	$32, %xmm5, %xmm1, %xmm1 # xmm1 = xmm1[0,1],xmm5[0],xmm1[3]
	vmovaps	18944(%rsp), %zmm6              # 64-byte Reload
	vextractf64x4	$1, %zmm6, %ymm5
	vaddps	%zmm5, %zmm6, %zmm5
	vextractf128	$1, %ymm5, %xmm6
	vaddps	%xmm6, %xmm5, %xmm5
	vshufpd	$1, %xmm5, %xmm5, %xmm6         # xmm6 = xmm5[1,0]
	vaddps	%xmm6, %xmm5, %xmm5
	vmovshdup	%xmm5, %xmm6            # xmm6 = xmm5[1,1,3,3]
	vaddss	%xmm6, %xmm5, %xmm5
	vaddss	%xmm27, %xmm5, %xmm5
	vinsertps	$48, %xmm5, %xmm1, %xmm5 # xmm5 = xmm1[0,1,2],xmm5[0]
	vmovaps	6848(%rsp), %zmm6               # 64-byte Reload
	vextractf64x4	$1, %zmm6, %ymm1
	vaddps	%zmm1, %zmm6, %zmm1
	vextractf128	$1, %ymm1, %xmm6
	vaddps	%xmm6, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm6         # xmm6 = xmm1[1,0]
	vaddps	%xmm6, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm6            # xmm6 = xmm1[1,1,3,3]
	vaddss	%xmm6, %xmm1, %xmm1
	vmovaps	10368(%rsp), %zmm7              # 64-byte Reload
	vextractf64x4	$1, %zmm7, %ymm6
	vaddps	%zmm6, %zmm7, %zmm6
	vextractf128	$1, %ymm6, %xmm7
	vaddps	%xmm7, %xmm6, %xmm6
	vshufpd	$1, %xmm6, %xmm6, %xmm7         # xmm7 = xmm6[1,0]
	vaddps	%xmm7, %xmm6, %xmm6
	vmovshdup	%xmm6, %xmm7            # xmm7 = xmm6[1,1,3,3]
	vaddss	%xmm7, %xmm6, %xmm6
	vaddss	%xmm27, %xmm1, %xmm1
	vaddss	%xmm27, %xmm6, %xmm6
	vinsertps	$16, %xmm6, %xmm1, %xmm1 # xmm1 = xmm1[0],xmm6[0],xmm1[2,3]
	vmovaps	10880(%rsp), %zmm7              # 64-byte Reload
	vextractf64x4	$1, %zmm7, %ymm6
	vaddps	%zmm6, %zmm7, %zmm6
	vextractf128	$1, %ymm6, %xmm7
	vaddps	%xmm7, %xmm6, %xmm6
	vshufpd	$1, %xmm6, %xmm6, %xmm7         # xmm7 = xmm6[1,0]
	vaddps	%xmm7, %xmm6, %xmm6
	vmovshdup	%xmm6, %xmm7            # xmm7 = xmm6[1,1,3,3]
	vaddss	%xmm7, %xmm6, %xmm6
	vaddss	%xmm27, %xmm6, %xmm6
	vinsertps	$32, %xmm6, %xmm1, %xmm1 # xmm1 = xmm1[0,1],xmm6[0],xmm1[3]
	vmovaps	16320(%rsp), %zmm7              # 64-byte Reload
	vextractf64x4	$1, %zmm7, %ymm6
	vaddps	%zmm6, %zmm7, %zmm6
	vextractf128	$1, %ymm6, %xmm7
	vaddps	%xmm7, %xmm6, %xmm6
	vshufpd	$1, %xmm6, %xmm6, %xmm7         # xmm7 = xmm6[1,0]
	vaddps	%xmm7, %xmm6, %xmm6
	vmovshdup	%xmm6, %xmm7            # xmm7 = xmm6[1,1,3,3]
	vaddss	%xmm7, %xmm6, %xmm6
	vaddss	%xmm27, %xmm6, %xmm6
	vinsertps	$48, %xmm6, %xmm1, %xmm6 # xmm6 = xmm1[0,1,2],xmm6[0]
	vmovaps	6912(%rsp), %zmm7               # 64-byte Reload
	vextractf64x4	$1, %zmm7, %ymm1
	vaddps	%zmm1, %zmm7, %zmm1
	vextractf128	$1, %ymm1, %xmm7
	vaddps	%xmm7, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm7         # xmm7 = xmm1[1,0]
	vaddps	%xmm7, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm7            # xmm7 = xmm1[1,1,3,3]
	vaddss	%xmm7, %xmm1, %xmm1
	vmovaps	6976(%rsp), %zmm8               # 64-byte Reload
	vextractf64x4	$1, %zmm8, %ymm7
	vaddps	%zmm7, %zmm8, %zmm7
	vextractf128	$1, %ymm7, %xmm8
	vaddps	%xmm7, %xmm8, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm8         # xmm8 = xmm7[1,0]
	vaddps	%xmm7, %xmm8, %xmm7
	vmovshdup	%xmm7, %xmm8            # xmm8 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm27, %xmm1, %xmm1
	vaddss	%xmm27, %xmm7, %xmm7
	vinsertps	$16, %xmm7, %xmm1, %xmm1 # xmm1 = xmm1[0],xmm7[0],xmm1[2,3]
	vmovaps	6144(%rsp), %zmm8               # 64-byte Reload
	vextractf64x4	$1, %zmm8, %ymm7
	vaddps	%zmm7, %zmm8, %zmm7
	vextractf128	$1, %ymm7, %xmm8
	vaddps	%xmm7, %xmm8, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm8         # xmm8 = xmm7[1,0]
	vaddps	%xmm7, %xmm8, %xmm7
	vmovshdup	%xmm7, %xmm8            # xmm8 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm27, %xmm7, %xmm7
	vinsertps	$32, %xmm7, %xmm1, %xmm1 # xmm1 = xmm1[0,1],xmm7[0],xmm1[3]
	vmovaps	6208(%rsp), %zmm8               # 64-byte Reload
	vextractf64x4	$1, %zmm8, %ymm7
	vaddps	%zmm7, %zmm8, %zmm7
	vextractf128	$1, %ymm7, %xmm8
	vaddps	%xmm7, %xmm8, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm8         # xmm8 = xmm7[1,0]
	vaddps	%xmm7, %xmm8, %xmm7
	vmovshdup	%xmm7, %xmm8            # xmm8 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm27, %xmm7, %xmm7
	vinsertps	$48, %xmm7, %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],xmm7[0]
	vmovaps	6272(%rsp), %zmm8               # 64-byte Reload
	vextractf64x4	$1, %zmm8, %ymm7
	vaddps	%zmm7, %zmm8, %zmm7
	vextractf128	$1, %ymm7, %xmm8
	vaddps	%xmm7, %xmm8, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm8         # xmm8 = xmm7[1,0]
	vaddps	%xmm7, %xmm8, %xmm7
	vmovshdup	%xmm7, %xmm8            # xmm8 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm8, %xmm7
	vmovaps	6336(%rsp), %zmm9               # 64-byte Reload
	vextractf64x4	$1, %zmm9, %ymm8
	vaddps	%zmm8, %zmm9, %zmm8
	vextractf128	$1, %ymm8, %xmm9
	vaddps	%xmm9, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm9         # xmm9 = xmm8[1,0]
	vaddps	%xmm9, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm9            # xmm9 = xmm8[1,1,3,3]
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm27, %xmm7, %xmm7
	vaddss	%xmm27, %xmm8, %xmm8
	vinsertps	$16, %xmm8, %xmm7, %xmm7 # xmm7 = xmm7[0],xmm8[0],xmm7[2,3]
	vmovaps	6400(%rsp), %zmm9               # 64-byte Reload
	vextractf64x4	$1, %zmm9, %ymm8
	vaddps	%zmm8, %zmm9, %zmm8
	vextractf128	$1, %ymm8, %xmm9
	vaddps	%xmm9, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm9         # xmm9 = xmm8[1,0]
	vaddps	%xmm9, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm9            # xmm9 = xmm8[1,1,3,3]
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm27, %xmm8, %xmm8
	vinsertps	$32, %xmm8, %xmm7, %xmm7 # xmm7 = xmm7[0,1],xmm8[0],xmm7[3]
	vmovaps	6464(%rsp), %zmm9               # 64-byte Reload
	vextractf64x4	$1, %zmm9, %ymm8
	vaddps	%zmm8, %zmm9, %zmm8
	vextractf128	$1, %ymm8, %xmm9
	vaddps	%xmm9, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm9         # xmm9 = xmm8[1,0]
	vaddps	%xmm9, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm9            # xmm9 = xmm8[1,1,3,3]
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm27, %xmm8, %xmm8
	vinsertps	$48, %xmm8, %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],xmm8[0]
	vinsertf128	$1, %xmm7, %ymm1, %ymm1
	vinsertf128	$1, %xmm6, %ymm5, %ymm5
	vinsertf64x4	$1, %ymm1, %zmm5, %zmm1
	vmovaps	23616(%rsp), %zmm5              # 64-byte Reload
	vaddps	%zmm1, %zmm5, %zmm5
	vmovaps	%zmm5, 23616(%rsp)              # 64-byte Spill
	vinsertf128	$1, %xmm4, %ymm3, %ymm1
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vinsertf64x4	$1, %ymm1, %zmm0, %zmm0
	vmovaps	23680(%rsp), %zmm1              # 64-byte Reload
	vaddps	%zmm0, %zmm1, %zmm1
	vmovaps	%zmm1, 23680(%rsp)              # 64-byte Spill
	vmovaps	6528(%rsp), %zmm1               # 64-byte Reload
	vextractf64x4	$1, %zmm1, %ymm0
	vaddps	%zmm0, %zmm1, %zmm0
	vextractf128	$1, %ymm0, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm1         # xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm0
	vmovaps	46400(%rsp), %zmm2              # 64-byte Reload
	vextractf64x4	$1, %zmm2, %ymm1
	vaddps	%zmm1, %zmm2, %zmm1
	vextractf128	$1, %ymm1, %xmm2
	vaddps	%xmm2, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm2         # xmm2 = xmm1[1,0]
	vaddps	%xmm2, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm2            # xmm2 = xmm1[1,1,3,3]
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm27, %xmm0, %xmm0
	vaddss	%xmm27, %xmm1, %xmm1
	vinsertps	$16, %xmm1, %xmm0, %xmm0 # xmm0 = xmm0[0],xmm1[0],xmm0[2,3]
	vmovaps	19840(%rsp), %zmm2              # 64-byte Reload
	vextractf64x4	$1, %zmm2, %ymm1
	vaddps	%zmm1, %zmm2, %zmm1
	vextractf128	$1, %ymm1, %xmm2
	vaddps	%xmm2, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm2         # xmm2 = xmm1[1,0]
	vaddps	%xmm2, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm2            # xmm2 = xmm1[1,1,3,3]
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm27, %xmm1, %xmm1
	vinsertps	$32, %xmm1, %xmm0, %xmm0 # xmm0 = xmm0[0,1],xmm1[0],xmm0[3]
	vmovaps	46208(%rsp), %zmm2              # 64-byte Reload
	vextractf64x4	$1, %zmm2, %ymm1
	vaddps	%zmm1, %zmm2, %zmm1
	vextractf128	$1, %ymm1, %xmm2
	vaddps	%xmm2, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm2         # xmm2 = xmm1[1,0]
	vaddps	%xmm2, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm2            # xmm2 = xmm1[1,1,3,3]
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm27, %xmm1, %xmm1
	vinsertps	$48, %xmm1, %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],xmm1[0]
	vmovaps	6592(%rsp), %zmm2               # 64-byte Reload
	vextractf64x4	$1, %zmm2, %ymm1
	vaddps	%zmm1, %zmm2, %zmm1
	vextractf128	$1, %ymm1, %xmm2
	vaddps	%xmm2, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm2         # xmm2 = xmm1[1,0]
	vaddps	%xmm2, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm2            # xmm2 = xmm1[1,1,3,3]
	vaddss	%xmm2, %xmm1, %xmm1
	vmovaps	46144(%rsp), %zmm3              # 64-byte Reload
	vextractf64x4	$1, %zmm3, %ymm2
	vaddps	%zmm2, %zmm3, %zmm2
	vextractf128	$1, %ymm2, %xmm3
	vaddps	%xmm3, %xmm2, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm3         # xmm3 = xmm2[1,0]
	vaddps	%xmm3, %xmm2, %xmm2
	vmovshdup	%xmm2, %xmm3            # xmm3 = xmm2[1,1,3,3]
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm27, %xmm1, %xmm1
	vaddss	%xmm27, %xmm2, %xmm2
	vinsertps	$16, %xmm2, %xmm1, %xmm1 # xmm1 = xmm1[0],xmm2[0],xmm1[2,3]
	vmovaps	46016(%rsp), %zmm3              # 64-byte Reload
	vextractf64x4	$1, %zmm3, %ymm2
	vaddps	%zmm2, %zmm3, %zmm2
	vextractf128	$1, %ymm2, %xmm3
	vaddps	%xmm3, %xmm2, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm3         # xmm3 = xmm2[1,0]
	vaddps	%xmm3, %xmm2, %xmm2
	vmovshdup	%xmm2, %xmm3            # xmm3 = xmm2[1,1,3,3]
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm27, %xmm2, %xmm2
	vinsertps	$32, %xmm2, %xmm1, %xmm1 # xmm1 = xmm1[0,1],xmm2[0],xmm1[3]
	vmovaps	45888(%rsp), %zmm3              # 64-byte Reload
	vextractf64x4	$1, %zmm3, %ymm2
	vaddps	%zmm2, %zmm3, %zmm2
	vextractf128	$1, %ymm2, %xmm3
	vaddps	%xmm3, %xmm2, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm3         # xmm3 = xmm2[1,0]
	vaddps	%xmm3, %xmm2, %xmm2
	vmovshdup	%xmm2, %xmm3            # xmm3 = xmm2[1,1,3,3]
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm27, %xmm2, %xmm2
	vinsertps	$48, %xmm2, %xmm1, %xmm2 # xmm2 = xmm1[0,1,2],xmm2[0]
	vmovaps	3200(%rsp), %zmm3               # 64-byte Reload
	vextractf64x4	$1, %zmm3, %ymm1
	vaddps	%zmm1, %zmm3, %zmm1
	vextractf128	$1, %ymm1, %xmm3
	vaddps	%xmm3, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm3         # xmm3 = xmm1[1,0]
	vaddps	%xmm3, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm3            # xmm3 = xmm1[1,1,3,3]
	vaddss	%xmm3, %xmm1, %xmm1
	vmovaps	3264(%rsp), %zmm4               # 64-byte Reload
	vextractf64x4	$1, %zmm4, %ymm3
	vaddps	%zmm3, %zmm4, %zmm3
	vextractf128	$1, %ymm3, %xmm4
	vaddps	%xmm4, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm4         # xmm4 = xmm3[1,0]
	vaddps	%xmm4, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm4            # xmm4 = xmm3[1,1,3,3]
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm27, %xmm1, %xmm1
	vaddss	%xmm27, %xmm3, %xmm3
	vinsertps	$16, %xmm3, %xmm1, %xmm1 # xmm1 = xmm1[0],xmm3[0],xmm1[2,3]
	vmovaps	3328(%rsp), %zmm4               # 64-byte Reload
	vextractf64x4	$1, %zmm4, %ymm3
	vaddps	%zmm3, %zmm4, %zmm3
	vextractf128	$1, %ymm3, %xmm4
	vaddps	%xmm4, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm4         # xmm4 = xmm3[1,0]
	vaddps	%xmm4, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm4            # xmm4 = xmm3[1,1,3,3]
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm27, %xmm3, %xmm3
	vinsertps	$32, %xmm3, %xmm1, %xmm1 # xmm1 = xmm1[0,1],xmm3[0],xmm1[3]
	vmovaps	3392(%rsp), %zmm4               # 64-byte Reload
	vextractf64x4	$1, %zmm4, %ymm3
	vaddps	%zmm3, %zmm4, %zmm3
	vextractf128	$1, %ymm3, %xmm4
	vaddps	%xmm4, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm4         # xmm4 = xmm3[1,0]
	vaddps	%xmm4, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm4            # xmm4 = xmm3[1,1,3,3]
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm27, %xmm3, %xmm3
	vinsertps	$48, %xmm3, %xmm1, %xmm3 # xmm3 = xmm1[0,1,2],xmm3[0]
	vmovaps	3456(%rsp), %zmm4               # 64-byte Reload
	vextractf64x4	$1, %zmm4, %ymm1
	vaddps	%zmm1, %zmm4, %zmm1
	vextractf128	$1, %ymm1, %xmm4
	vaddps	%xmm4, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm4         # xmm4 = xmm1[1,0]
	vaddps	%xmm4, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm4            # xmm4 = xmm1[1,1,3,3]
	vaddss	%xmm4, %xmm1, %xmm1
	vmovaps	3520(%rsp), %zmm5               # 64-byte Reload
	vextractf64x4	$1, %zmm5, %ymm4
	vaddps	%zmm4, %zmm5, %zmm4
	vextractf128	$1, %ymm4, %xmm5
	vaddps	%xmm5, %xmm4, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm5         # xmm5 = xmm4[1,0]
	vaddps	%xmm5, %xmm4, %xmm4
	vmovshdup	%xmm4, %xmm5            # xmm5 = xmm4[1,1,3,3]
	vaddss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm27, %xmm1, %xmm1
	vaddss	%xmm27, %xmm4, %xmm4
	vinsertps	$16, %xmm4, %xmm1, %xmm1 # xmm1 = xmm1[0],xmm4[0],xmm1[2,3]
	vmovaps	3584(%rsp), %zmm5               # 64-byte Reload
	vextractf64x4	$1, %zmm5, %ymm4
	vaddps	%zmm4, %zmm5, %zmm4
	vextractf128	$1, %ymm4, %xmm5
	vaddps	%xmm5, %xmm4, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm5         # xmm5 = xmm4[1,0]
	vaddps	%xmm5, %xmm4, %xmm4
	vmovshdup	%xmm4, %xmm5            # xmm5 = xmm4[1,1,3,3]
	vaddss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm27, %xmm4, %xmm4
	vinsertps	$32, %xmm4, %xmm1, %xmm1 # xmm1 = xmm1[0,1],xmm4[0],xmm1[3]
	vmovaps	3648(%rsp), %zmm5               # 64-byte Reload
	vextractf64x4	$1, %zmm5, %ymm4
	vaddps	%zmm4, %zmm5, %zmm4
	vextractf128	$1, %ymm4, %xmm5
	vaddps	%xmm5, %xmm4, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm5         # xmm5 = xmm4[1,0]
	vaddps	%xmm5, %xmm4, %xmm4
	vmovshdup	%xmm4, %xmm5            # xmm5 = xmm4[1,1,3,3]
	vaddss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm27, %xmm4, %xmm4
	vinsertps	$48, %xmm4, %xmm1, %xmm4 # xmm4 = xmm1[0,1,2],xmm4[0]
	vmovaps	4736(%rsp), %zmm5               # 64-byte Reload
	vextractf64x4	$1, %zmm5, %ymm1
	vaddps	%zmm1, %zmm5, %zmm1
	vextractf128	$1, %ymm1, %xmm5
	vaddps	%xmm5, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm5         # xmm5 = xmm1[1,0]
	vaddps	%xmm5, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm5            # xmm5 = xmm1[1,1,3,3]
	vaddss	%xmm5, %xmm1, %xmm1
	vmovaps	37376(%rsp), %zmm6              # 64-byte Reload
	vextractf64x4	$1, %zmm6, %ymm5
	vaddps	%zmm5, %zmm6, %zmm5
	vextractf128	$1, %ymm5, %xmm6
	vaddps	%xmm6, %xmm5, %xmm5
	vshufpd	$1, %xmm5, %xmm5, %xmm6         # xmm6 = xmm5[1,0]
	vaddps	%xmm6, %xmm5, %xmm5
	vmovshdup	%xmm5, %xmm6            # xmm6 = xmm5[1,1,3,3]
	vaddss	%xmm6, %xmm5, %xmm5
	vaddss	%xmm27, %xmm1, %xmm1
	vaddss	%xmm27, %xmm5, %xmm5
	vinsertps	$16, %xmm5, %xmm1, %xmm1 # xmm1 = xmm1[0],xmm5[0],xmm1[2,3]
	vmovaps	10176(%rsp), %zmm6              # 64-byte Reload
	vextractf64x4	$1, %zmm6, %ymm5
	vaddps	%zmm5, %zmm6, %zmm5
	vextractf128	$1, %ymm5, %xmm6
	vaddps	%xmm6, %xmm5, %xmm5
	vshufpd	$1, %xmm5, %xmm5, %xmm6         # xmm6 = xmm5[1,0]
	vaddps	%xmm6, %xmm5, %xmm5
	vmovshdup	%xmm5, %xmm6            # xmm6 = xmm5[1,1,3,3]
	vaddss	%xmm6, %xmm5, %xmm5
	vaddss	%xmm27, %xmm5, %xmm5
	vinsertps	$32, %xmm5, %xmm1, %xmm1 # xmm1 = xmm1[0,1],xmm5[0],xmm1[3]
	vmovaps	11136(%rsp), %zmm6              # 64-byte Reload
	vextractf64x4	$1, %zmm6, %ymm5
	vaddps	%zmm5, %zmm6, %zmm5
	vextractf128	$1, %ymm5, %xmm6
	vaddps	%xmm6, %xmm5, %xmm5
	vshufpd	$1, %xmm5, %xmm5, %xmm6         # xmm6 = xmm5[1,0]
	vaddps	%xmm6, %xmm5, %xmm5
	vmovshdup	%xmm5, %xmm6            # xmm6 = xmm5[1,1,3,3]
	vaddss	%xmm6, %xmm5, %xmm5
	vaddss	%xmm27, %xmm5, %xmm5
	vinsertps	$48, %xmm5, %xmm1, %xmm5 # xmm5 = xmm1[0,1,2],xmm5[0]
	vmovaps	4800(%rsp), %zmm6               # 64-byte Reload
	vextractf64x4	$1, %zmm6, %ymm1
	vaddps	%zmm1, %zmm6, %zmm1
	vextractf128	$1, %ymm1, %xmm6
	vaddps	%xmm6, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm6         # xmm6 = xmm1[1,0]
	vaddps	%xmm6, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm6            # xmm6 = xmm1[1,1,3,3]
	vaddss	%xmm6, %xmm1, %xmm1
	vmovaps	8896(%rsp), %zmm7               # 64-byte Reload
	vextractf64x4	$1, %zmm7, %ymm6
	vaddps	%zmm6, %zmm7, %zmm6
	vextractf128	$1, %ymm6, %xmm7
	vaddps	%xmm7, %xmm6, %xmm6
	vshufpd	$1, %xmm6, %xmm6, %xmm7         # xmm7 = xmm6[1,0]
	vaddps	%xmm7, %xmm6, %xmm6
	vmovshdup	%xmm6, %xmm7            # xmm7 = xmm6[1,1,3,3]
	vaddss	%xmm7, %xmm6, %xmm6
	vaddss	%xmm27, %xmm1, %xmm1
	vaddss	%xmm27, %xmm6, %xmm6
	vinsertps	$16, %xmm6, %xmm1, %xmm1 # xmm1 = xmm1[0],xmm6[0],xmm1[2,3]
	vmovaps	8320(%rsp), %zmm7               # 64-byte Reload
	vextractf64x4	$1, %zmm7, %ymm6
	vaddps	%zmm6, %zmm7, %zmm6
	vextractf128	$1, %ymm6, %xmm7
	vaddps	%xmm7, %xmm6, %xmm6
	vshufpd	$1, %xmm6, %xmm6, %xmm7         # xmm7 = xmm6[1,0]
	vaddps	%xmm7, %xmm6, %xmm6
	vmovshdup	%xmm6, %xmm7            # xmm7 = xmm6[1,1,3,3]
	vaddss	%xmm7, %xmm6, %xmm6
	vaddss	%xmm27, %xmm6, %xmm6
	vinsertps	$32, %xmm6, %xmm1, %xmm1 # xmm1 = xmm1[0,1],xmm6[0],xmm1[3]
	vmovaps	11072(%rsp), %zmm7              # 64-byte Reload
	vextractf64x4	$1, %zmm7, %ymm6
	vaddps	%zmm6, %zmm7, %zmm6
	vextractf128	$1, %ymm6, %xmm7
	vaddps	%xmm7, %xmm6, %xmm6
	vshufpd	$1, %xmm6, %xmm6, %xmm7         # xmm7 = xmm6[1,0]
	vaddps	%xmm7, %xmm6, %xmm6
	vmovshdup	%xmm6, %xmm7            # xmm7 = xmm6[1,1,3,3]
	vaddss	%xmm7, %xmm6, %xmm6
	vaddss	%xmm27, %xmm6, %xmm6
	vinsertps	$48, %xmm6, %xmm1, %xmm6 # xmm6 = xmm1[0,1,2],xmm6[0]
	vmovaps	4864(%rsp), %zmm7               # 64-byte Reload
	vextractf64x4	$1, %zmm7, %ymm1
	vaddps	%zmm1, %zmm7, %zmm1
	vextractf128	$1, %ymm1, %xmm7
	vaddps	%xmm7, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm7         # xmm7 = xmm1[1,0]
	vaddps	%xmm7, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm7            # xmm7 = xmm1[1,1,3,3]
	vaddss	%xmm7, %xmm1, %xmm1
	vmovaps	4928(%rsp), %zmm8               # 64-byte Reload
	vextractf64x4	$1, %zmm8, %ymm7
	vaddps	%zmm7, %zmm8, %zmm7
	vextractf128	$1, %ymm7, %xmm8
	vaddps	%xmm7, %xmm8, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm8         # xmm8 = xmm7[1,0]
	vaddps	%xmm7, %xmm8, %xmm7
	vmovshdup	%xmm7, %xmm8            # xmm8 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm27, %xmm1, %xmm1
	vaddss	%xmm27, %xmm7, %xmm7
	vinsertps	$16, %xmm7, %xmm1, %xmm1 # xmm1 = xmm1[0],xmm7[0],xmm1[2,3]
	vmovaps	4992(%rsp), %zmm8               # 64-byte Reload
	vextractf64x4	$1, %zmm8, %ymm7
	vaddps	%zmm7, %zmm8, %zmm7
	vextractf128	$1, %ymm7, %xmm8
	vaddps	%xmm7, %xmm8, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm8         # xmm8 = xmm7[1,0]
	vaddps	%xmm7, %xmm8, %xmm7
	vmovshdup	%xmm7, %xmm8            # xmm8 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm27, %xmm7, %xmm7
	vinsertps	$32, %xmm7, %xmm1, %xmm1 # xmm1 = xmm1[0,1],xmm7[0],xmm1[3]
	vmovaps	5056(%rsp), %zmm8               # 64-byte Reload
	vextractf64x4	$1, %zmm8, %ymm7
	vaddps	%zmm7, %zmm8, %zmm7
	vextractf128	$1, %ymm7, %xmm8
	vaddps	%xmm7, %xmm8, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm8         # xmm8 = xmm7[1,0]
	vaddps	%xmm7, %xmm8, %xmm7
	vmovshdup	%xmm7, %xmm8            # xmm8 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm27, %xmm7, %xmm7
	vinsertps	$48, %xmm7, %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],xmm7[0]
	vmovaps	5120(%rsp), %zmm8               # 64-byte Reload
	vextractf64x4	$1, %zmm8, %ymm7
	vaddps	%zmm7, %zmm8, %zmm7
	vextractf128	$1, %ymm7, %xmm8
	vaddps	%xmm7, %xmm8, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm8         # xmm8 = xmm7[1,0]
	vaddps	%xmm7, %xmm8, %xmm7
	vmovshdup	%xmm7, %xmm8            # xmm8 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm8, %xmm7
	vmovaps	5184(%rsp), %zmm9               # 64-byte Reload
	vextractf64x4	$1, %zmm9, %ymm8
	vaddps	%zmm8, %zmm9, %zmm8
	vextractf128	$1, %ymm8, %xmm9
	vaddps	%xmm9, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm9         # xmm9 = xmm8[1,0]
	vaddps	%xmm9, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm9            # xmm9 = xmm8[1,1,3,3]
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm27, %xmm7, %xmm7
	vaddss	%xmm27, %xmm8, %xmm8
	vinsertps	$16, %xmm8, %xmm7, %xmm7 # xmm7 = xmm7[0],xmm8[0],xmm7[2,3]
	vmovaps	5248(%rsp), %zmm9               # 64-byte Reload
	vextractf64x4	$1, %zmm9, %ymm8
	vaddps	%zmm8, %zmm9, %zmm8
	vextractf128	$1, %ymm8, %xmm9
	vaddps	%xmm9, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm9         # xmm9 = xmm8[1,0]
	vaddps	%xmm9, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm9            # xmm9 = xmm8[1,1,3,3]
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm27, %xmm8, %xmm8
	vinsertps	$32, %xmm8, %xmm7, %xmm7 # xmm7 = xmm7[0,1],xmm8[0],xmm7[3]
	vmovaps	5312(%rsp), %zmm9               # 64-byte Reload
	vextractf64x4	$1, %zmm9, %ymm8
	vaddps	%zmm8, %zmm9, %zmm8
	vextractf128	$1, %ymm8, %xmm9
	vaddps	%xmm9, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm9         # xmm9 = xmm8[1,0]
	vaddps	%xmm9, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm9            # xmm9 = xmm8[1,1,3,3]
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm27, %xmm8, %xmm8
	vinsertps	$48, %xmm8, %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],xmm8[0]
	vinsertf128	$1, %xmm7, %ymm1, %ymm1
	vinsertf128	$1, %xmm6, %ymm5, %ymm5
	vinsertf64x4	$1, %ymm1, %zmm5, %zmm1
	vmovaps	23744(%rsp), %zmm5              # 64-byte Reload
	vaddps	%zmm1, %zmm5, %zmm5
	vmovaps	%zmm5, 23744(%rsp)              # 64-byte Spill
	vinsertf128	$1, %xmm4, %ymm3, %ymm1
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vinsertf64x4	$1, %ymm1, %zmm0, %zmm0
	vmovaps	23808(%rsp), %zmm1              # 64-byte Reload
	vaddps	%zmm0, %zmm1, %zmm1
	vmovaps	%zmm1, 23808(%rsp)              # 64-byte Spill
	vmovaps	5376(%rsp), %zmm1               # 64-byte Reload
	vextractf64x4	$1, %zmm1, %ymm0
	vaddps	%zmm0, %zmm1, %zmm0
	vextractf128	$1, %ymm0, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vshufpd	$1, %xmm0, %xmm0, %xmm1         # xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm0
	vmovaps	46656(%rsp), %zmm2              # 64-byte Reload
	vextractf64x4	$1, %zmm2, %ymm1
	vaddps	%zmm1, %zmm2, %zmm1
	vextractf128	$1, %ymm1, %xmm2
	vaddps	%xmm2, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm2         # xmm2 = xmm1[1,0]
	vaddps	%xmm2, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm2            # xmm2 = xmm1[1,1,3,3]
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm27, %xmm0, %xmm0
	vaddss	%xmm27, %xmm1, %xmm1
	vinsertps	$16, %xmm1, %xmm0, %xmm0 # xmm0 = xmm0[0],xmm1[0],xmm0[2,3]
	vmovaps	46592(%rsp), %zmm2              # 64-byte Reload
	vextractf64x4	$1, %zmm2, %ymm1
	vaddps	%zmm1, %zmm2, %zmm1
	vextractf128	$1, %ymm1, %xmm2
	vaddps	%xmm2, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm2         # xmm2 = xmm1[1,0]
	vaddps	%xmm2, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm2            # xmm2 = xmm1[1,1,3,3]
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm27, %xmm1, %xmm1
	vinsertps	$32, %xmm1, %xmm0, %xmm0 # xmm0 = xmm0[0,1],xmm1[0],xmm0[3]
	vmovaps	46528(%rsp), %zmm2              # 64-byte Reload
	vextractf64x4	$1, %zmm2, %ymm1
	vaddps	%zmm1, %zmm2, %zmm1
	vextractf128	$1, %ymm1, %xmm2
	vaddps	%xmm2, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm2         # xmm2 = xmm1[1,0]
	vaddps	%xmm2, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm2            # xmm2 = xmm1[1,1,3,3]
	vaddss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm27, %xmm1, %xmm1
	vinsertps	$48, %xmm1, %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],xmm1[0]
	vmovaps	1600(%rsp), %zmm2               # 64-byte Reload
	vextractf64x4	$1, %zmm2, %ymm1
	vaddps	%zmm1, %zmm2, %zmm1
	vextractf128	$1, %ymm1, %xmm2
	vaddps	%xmm2, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm2         # xmm2 = xmm1[1,0]
	vaddps	%xmm2, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm2            # xmm2 = xmm1[1,1,3,3]
	vaddss	%xmm2, %xmm1, %xmm1
	vmovaps	46464(%rsp), %zmm3              # 64-byte Reload
	vextractf64x4	$1, %zmm3, %ymm2
	vaddps	%zmm2, %zmm3, %zmm2
	vextractf128	$1, %ymm2, %xmm3
	vaddps	%xmm3, %xmm2, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm3         # xmm3 = xmm2[1,0]
	vaddps	%xmm3, %xmm2, %xmm2
	vmovshdup	%xmm2, %xmm3            # xmm3 = xmm2[1,1,3,3]
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm27, %xmm1, %xmm1
	vaddss	%xmm27, %xmm2, %xmm2
	vinsertps	$16, %xmm2, %xmm1, %xmm1 # xmm1 = xmm1[0],xmm2[0],xmm1[2,3]
	vmovaps	46336(%rsp), %zmm3              # 64-byte Reload
	vextractf64x4	$1, %zmm3, %ymm2
	vaddps	%zmm2, %zmm3, %zmm2
	vextractf128	$1, %ymm2, %xmm3
	vaddps	%xmm3, %xmm2, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm3         # xmm3 = xmm2[1,0]
	vaddps	%xmm3, %xmm2, %xmm2
	vmovshdup	%xmm2, %xmm3            # xmm3 = xmm2[1,1,3,3]
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm27, %xmm2, %xmm2
	vinsertps	$32, %xmm2, %xmm1, %xmm1 # xmm1 = xmm1[0,1],xmm2[0],xmm1[3]
	vmovaps	46272(%rsp), %zmm3              # 64-byte Reload
	vextractf64x4	$1, %zmm3, %ymm2
	vaddps	%zmm2, %zmm3, %zmm2
	vextractf128	$1, %ymm2, %xmm3
	vaddps	%xmm3, %xmm2, %xmm2
	vshufpd	$1, %xmm2, %xmm2, %xmm3         # xmm3 = xmm2[1,0]
	vaddps	%xmm3, %xmm2, %xmm2
	vmovshdup	%xmm2, %xmm3            # xmm3 = xmm2[1,1,3,3]
	vaddss	%xmm3, %xmm2, %xmm2
	vaddss	%xmm27, %xmm2, %xmm2
	vinsertps	$48, %xmm2, %xmm1, %xmm2 # xmm2 = xmm1[0,1,2],xmm2[0]
	vmovaps	1664(%rsp), %zmm3               # 64-byte Reload
	vextractf64x4	$1, %zmm3, %ymm1
	vaddps	%zmm1, %zmm3, %zmm1
	vextractf128	$1, %ymm1, %xmm3
	vaddps	%xmm3, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm3         # xmm3 = xmm1[1,0]
	vaddps	%xmm3, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm3            # xmm3 = xmm1[1,1,3,3]
	vaddss	%xmm3, %xmm1, %xmm1
	vmovaps	1152(%rsp), %zmm4               # 64-byte Reload
	vextractf64x4	$1, %zmm4, %ymm3
	vaddps	%zmm3, %zmm4, %zmm3
	vextractf128	$1, %ymm3, %xmm4
	vaddps	%xmm4, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm4         # xmm4 = xmm3[1,0]
	vaddps	%xmm4, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm4            # xmm4 = xmm3[1,1,3,3]
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm27, %xmm1, %xmm1
	vaddss	%xmm27, %xmm3, %xmm3
	vinsertps	$16, %xmm3, %xmm1, %xmm1 # xmm1 = xmm1[0],xmm3[0],xmm1[2,3]
	vmovaps	768(%rsp), %zmm4                # 64-byte Reload
	vextractf64x4	$1, %zmm4, %ymm3
	vaddps	%zmm3, %zmm4, %zmm3
	vextractf128	$1, %ymm3, %xmm4
	vaddps	%xmm4, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm4         # xmm4 = xmm3[1,0]
	vaddps	%xmm4, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm4            # xmm4 = xmm3[1,1,3,3]
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm27, %xmm3, %xmm3
	vinsertps	$32, %xmm3, %xmm1, %xmm1 # xmm1 = xmm1[0,1],xmm3[0],xmm1[3]
	vmovaps	704(%rsp), %zmm4                # 64-byte Reload
	vextractf64x4	$1, %zmm4, %ymm3
	vaddps	%zmm3, %zmm4, %zmm3
	vextractf128	$1, %ymm3, %xmm4
	vaddps	%xmm4, %xmm3, %xmm3
	vshufpd	$1, %xmm3, %xmm3, %xmm4         # xmm4 = xmm3[1,0]
	vaddps	%xmm4, %xmm3, %xmm3
	vmovshdup	%xmm3, %xmm4            # xmm4 = xmm3[1,1,3,3]
	vaddss	%xmm4, %xmm3, %xmm3
	vaddss	%xmm27, %xmm3, %xmm3
	vinsertps	$48, %xmm3, %xmm1, %xmm3 # xmm3 = xmm1[0,1,2],xmm3[0]
	vmovaps	448(%rsp), %zmm4                # 64-byte Reload
	vextractf64x4	$1, %zmm4, %ymm1
	vaddps	%zmm1, %zmm4, %zmm1
	vextractf128	$1, %ymm1, %xmm4
	vaddps	%xmm4, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm4         # xmm4 = xmm1[1,0]
	vaddps	%xmm4, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm4            # xmm4 = xmm1[1,1,3,3]
	vaddss	%xmm4, %xmm1, %xmm1
	vmovaps	512(%rsp), %zmm5                # 64-byte Reload
	vextractf64x4	$1, %zmm5, %ymm4
	vaddps	%zmm4, %zmm5, %zmm4
	vextractf128	$1, %ymm4, %xmm5
	vaddps	%xmm5, %xmm4, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm5         # xmm5 = xmm4[1,0]
	vaddps	%xmm5, %xmm4, %xmm4
	vmovshdup	%xmm4, %xmm5            # xmm5 = xmm4[1,1,3,3]
	vaddss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm27, %xmm1, %xmm1
	vaddss	%xmm27, %xmm4, %xmm4
	vinsertps	$16, %xmm4, %xmm1, %xmm1 # xmm1 = xmm1[0],xmm4[0],xmm1[2,3]
	vmovaps	256(%rsp), %zmm5                # 64-byte Reload
	vextractf64x4	$1, %zmm5, %ymm4
	vaddps	%zmm4, %zmm5, %zmm4
	vextractf128	$1, %ymm4, %xmm5
	vaddps	%xmm5, %xmm4, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm5         # xmm5 = xmm4[1,0]
	vaddps	%xmm5, %xmm4, %xmm4
	vmovshdup	%xmm4, %xmm5            # xmm5 = xmm4[1,1,3,3]
	vaddss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm27, %xmm4, %xmm4
	vinsertps	$32, %xmm4, %xmm1, %xmm1 # xmm1 = xmm1[0,1],xmm4[0],xmm1[3]
	vmovaps	1408(%rsp), %zmm5               # 64-byte Reload
	vextractf64x4	$1, %zmm5, %ymm4
	vaddps	%zmm4, %zmm5, %zmm4
	vextractf128	$1, %ymm4, %xmm5
	vaddps	%xmm5, %xmm4, %xmm4
	vshufpd	$1, %xmm4, %xmm4, %xmm5         # xmm5 = xmm4[1,0]
	vaddps	%xmm5, %xmm4, %xmm4
	vmovshdup	%xmm4, %xmm5            # xmm5 = xmm4[1,1,3,3]
	vaddss	%xmm5, %xmm4, %xmm4
	vaddss	%xmm27, %xmm4, %xmm4
	vinsertps	$48, %xmm4, %xmm1, %xmm4 # xmm4 = xmm1[0,1,2],xmm4[0]
	vextractf64x4	$1, %zmm28, %ymm1
	vaddps	%zmm1, %zmm28, %zmm1
	vextractf128	$1, %ymm1, %xmm5
	vaddps	%xmm5, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm5         # xmm5 = xmm1[1,0]
	vaddps	%xmm5, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm5            # xmm5 = xmm1[1,1,3,3]
	vaddss	%xmm5, %xmm1, %xmm1
	vmovaps	18496(%rsp), %zmm6              # 64-byte Reload
	vextractf64x4	$1, %zmm6, %ymm5
	vaddps	%zmm5, %zmm6, %zmm5
	vextractf128	$1, %ymm5, %xmm6
	vaddps	%xmm6, %xmm5, %xmm5
	vshufpd	$1, %xmm5, %xmm5, %xmm6         # xmm6 = xmm5[1,0]
	vaddps	%xmm6, %xmm5, %xmm5
	vmovshdup	%xmm5, %xmm6            # xmm6 = xmm5[1,1,3,3]
	vaddss	%xmm6, %xmm5, %xmm5
	vaddss	%xmm27, %xmm1, %xmm1
	vaddss	%xmm27, %xmm5, %xmm5
	vinsertps	$16, %xmm5, %xmm1, %xmm1 # xmm1 = xmm1[0],xmm5[0],xmm1[2,3]
	vmovaps	18752(%rsp), %zmm6              # 64-byte Reload
	vextractf64x4	$1, %zmm6, %ymm5
	vaddps	%zmm5, %zmm6, %zmm5
	vextractf128	$1, %ymm5, %xmm6
	vaddps	%xmm6, %xmm5, %xmm5
	vshufpd	$1, %xmm5, %xmm5, %xmm6         # xmm6 = xmm5[1,0]
	vaddps	%xmm6, %xmm5, %xmm5
	vmovshdup	%xmm5, %xmm6            # xmm6 = xmm5[1,1,3,3]
	vaddss	%xmm6, %xmm5, %xmm5
	vaddss	%xmm27, %xmm5, %xmm5
	vinsertps	$32, %xmm5, %xmm1, %xmm1 # xmm1 = xmm1[0,1],xmm5[0],xmm1[3]
	vmovaps	9024(%rsp), %zmm6               # 64-byte Reload
	vextractf64x4	$1, %zmm6, %ymm5
	vaddps	%zmm5, %zmm6, %zmm5
	vextractf128	$1, %ymm5, %xmm6
	vaddps	%xmm6, %xmm5, %xmm5
	vshufpd	$1, %xmm5, %xmm5, %xmm6         # xmm6 = xmm5[1,0]
	vaddps	%xmm6, %xmm5, %xmm5
	vmovshdup	%xmm5, %xmm6            # xmm6 = xmm5[1,1,3,3]
	vaddss	%xmm6, %xmm5, %xmm5
	vaddss	%xmm27, %xmm5, %xmm5
	vinsertps	$48, %xmm5, %xmm1, %xmm5 # xmm5 = xmm1[0,1,2],xmm5[0]
	vmovaps	1024(%rsp), %zmm6               # 64-byte Reload
	vextractf64x4	$1, %zmm6, %ymm1
	vaddps	%zmm1, %zmm6, %zmm1
	vextractf128	$1, %ymm1, %xmm6
	vaddps	%xmm6, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm6         # xmm6 = xmm1[1,0]
	vaddps	%xmm6, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm6            # xmm6 = xmm1[1,1,3,3]
	vaddss	%xmm6, %xmm1, %xmm1
	vmovaps	8960(%rsp), %zmm7               # 64-byte Reload
	vextractf64x4	$1, %zmm7, %ymm6
	vaddps	%zmm6, %zmm7, %zmm6
	vextractf128	$1, %ymm6, %xmm7
	vaddps	%xmm7, %xmm6, %xmm6
	vshufpd	$1, %xmm6, %xmm6, %xmm7         # xmm7 = xmm6[1,0]
	vaddps	%xmm7, %xmm6, %xmm6
	vmovshdup	%xmm6, %xmm7            # xmm7 = xmm6[1,1,3,3]
	vaddss	%xmm7, %xmm6, %xmm6
	vaddss	%xmm27, %xmm1, %xmm1
	vaddss	%xmm27, %xmm6, %xmm6
	vinsertps	$16, %xmm6, %xmm1, %xmm1 # xmm1 = xmm1[0],xmm6[0],xmm1[2,3]
	vmovaps	8448(%rsp), %zmm7               # 64-byte Reload
	vextractf64x4	$1, %zmm7, %ymm6
	vaddps	%zmm6, %zmm7, %zmm6
	vextractf128	$1, %ymm6, %xmm7
	vaddps	%xmm7, %xmm6, %xmm6
	vshufpd	$1, %xmm6, %xmm6, %xmm7         # xmm7 = xmm6[1,0]
	vaddps	%xmm7, %xmm6, %xmm6
	vmovshdup	%xmm6, %xmm7            # xmm7 = xmm6[1,1,3,3]
	vaddss	%xmm7, %xmm6, %xmm6
	vaddss	%xmm27, %xmm6, %xmm6
	vinsertps	$32, %xmm6, %xmm1, %xmm1 # xmm1 = xmm1[0,1],xmm6[0],xmm1[3]
	vmovaps	8384(%rsp), %zmm7               # 64-byte Reload
	vextractf64x4	$1, %zmm7, %ymm6
	vaddps	%zmm6, %zmm7, %zmm6
	vextractf128	$1, %ymm6, %xmm7
	vaddps	%xmm7, %xmm6, %xmm6
	vshufpd	$1, %xmm6, %xmm6, %xmm7         # xmm7 = xmm6[1,0]
	vaddps	%xmm7, %xmm6, %xmm6
	vmovshdup	%xmm6, %xmm7            # xmm7 = xmm6[1,1,3,3]
	vaddss	%xmm7, %xmm6, %xmm6
	vaddss	%xmm27, %xmm6, %xmm6
	vinsertps	$48, %xmm6, %xmm1, %xmm6 # xmm6 = xmm1[0,1,2],xmm6[0]
	vmovaps	832(%rsp), %zmm7                # 64-byte Reload
	vextractf64x4	$1, %zmm7, %ymm1
	vaddps	%zmm1, %zmm7, %zmm1
	vextractf128	$1, %ymm1, %xmm7
	vaddps	%xmm7, %xmm1, %xmm1
	vshufpd	$1, %xmm1, %xmm1, %xmm7         # xmm7 = xmm1[1,0]
	vaddps	%xmm7, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm7            # xmm7 = xmm1[1,1,3,3]
	vaddss	%xmm7, %xmm1, %xmm1
	vextractf64x4	$1, %zmm14, %ymm7
	vaddps	%zmm7, %zmm14, %zmm7
	vextractf128	$1, %ymm7, %xmm8
	vaddps	%xmm7, %xmm8, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm8         # xmm8 = xmm7[1,0]
	vaddps	%xmm7, %xmm8, %xmm7
	vmovshdup	%xmm7, %xmm8            # xmm8 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm27, %xmm1, %xmm1
	vaddss	%xmm27, %xmm7, %xmm7
	vinsertps	$16, %xmm7, %xmm1, %xmm1 # xmm1 = xmm1[0],xmm7[0],xmm1[2,3]
	vmovaps	960(%rsp), %zmm8                # 64-byte Reload
	vextractf64x4	$1, %zmm8, %ymm7
	vaddps	%zmm7, %zmm8, %zmm7
	vextractf128	$1, %ymm7, %xmm8
	vaddps	%xmm7, %xmm8, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm8         # xmm8 = xmm7[1,0]
	vaddps	%xmm7, %xmm8, %xmm7
	vmovshdup	%xmm7, %xmm8            # xmm8 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm27, %xmm7, %xmm7
	vinsertps	$32, %xmm7, %xmm1, %xmm1 # xmm1 = xmm1[0,1],xmm7[0],xmm1[3]
	vextractf64x4	$1, %zmm13, %ymm7
	vaddps	%zmm7, %zmm13, %zmm7
	vextractf128	$1, %ymm7, %xmm8
	vaddps	%xmm7, %xmm8, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm8         # xmm8 = xmm7[1,0]
	vaddps	%xmm7, %xmm8, %xmm7
	vmovshdup	%xmm7, %xmm8            # xmm8 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm8, %xmm7
	vaddss	%xmm27, %xmm7, %xmm7
	vinsertps	$48, %xmm7, %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],xmm7[0]
	vextractf64x4	$1, %zmm12, %ymm7
	vaddps	%zmm7, %zmm12, %zmm7
	vextractf128	$1, %ymm7, %xmm8
	vaddps	%xmm7, %xmm8, %xmm7
	vshufpd	$1, %xmm7, %xmm7, %xmm8         # xmm8 = xmm7[1,0]
	vaddps	%xmm7, %xmm8, %xmm7
	vmovshdup	%xmm7, %xmm8            # xmm8 = xmm7[1,1,3,3]
	vaddss	%xmm7, %xmm8, %xmm7
	vextractf64x4	$1, %zmm19, %ymm8
	vaddps	%zmm8, %zmm19, %zmm8
	vextractf128	$1, %ymm8, %xmm9
	vaddps	%xmm9, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm9         # xmm9 = xmm8[1,0]
	vaddps	%xmm9, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm9            # xmm9 = xmm8[1,1,3,3]
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm27, %xmm7, %xmm7
	vaddss	%xmm27, %xmm8, %xmm8
	vinsertps	$16, %xmm8, %xmm7, %xmm7 # xmm7 = xmm7[0],xmm8[0],xmm7[2,3]
	vextractf64x4	$1, %zmm10, %ymm8
	vaddps	%zmm8, %zmm10, %zmm8
	vextractf128	$1, %ymm8, %xmm9
	vaddps	%xmm9, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm9         # xmm9 = xmm8[1,0]
	vaddps	%xmm9, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm9            # xmm9 = xmm8[1,1,3,3]
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm27, %xmm8, %xmm8
	vinsertps	$32, %xmm8, %xmm7, %xmm7 # xmm7 = xmm7[0,1],xmm8[0],xmm7[3]
	vextractf64x4	$1, %zmm25, %ymm8
	vaddps	%zmm8, %zmm25, %zmm8
	vextractf128	$1, %ymm8, %xmm9
	vaddps	%xmm9, %xmm8, %xmm8
	vshufpd	$1, %xmm8, %xmm8, %xmm9         # xmm9 = xmm8[1,0]
	vaddps	%xmm9, %xmm8, %xmm8
	vmovshdup	%xmm8, %xmm9            # xmm9 = xmm8[1,1,3,3]
	vaddss	%xmm9, %xmm8, %xmm8
	vaddss	%xmm27, %xmm8, %xmm8
	vinsertps	$48, %xmm8, %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],xmm8[0]
	vinsertf128	$1, %xmm7, %ymm1, %ymm1
	vinsertf128	$1, %xmm6, %ymm5, %ymm5
	vinsertf64x4	$1, %ymm1, %zmm5, %zmm1
	vmovaps	23872(%rsp), %zmm5              # 64-byte Reload
	vaddps	%zmm1, %zmm5, %zmm5
	vmovaps	%zmm5, 23872(%rsp)              # 64-byte Spill
	vinsertf128	$1, %xmm4, %ymm3, %ymm1
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vinsertf64x4	$1, %ymm1, %zmm0, %zmm0
	vmovaps	23936(%rsp), %zmm1              # 64-byte Reload
	vaddps	%zmm0, %zmm1, %zmm1
	vmovaps	%zmm1, 23936(%rsp)              # 64-byte Spill
	movq	5664(%rsp), %r14                # 8-byte Reload
	.loc	1 228 18                        # 03-matrix-multiplication-cpu.py:228:18
	subq	$-128, %r14
	movq	5672(%rsp), %rdx                # 8-byte Reload
	subq	$-128, %rdx
	movq	5680(%rsp), %r12                # 8-byte Reload
	subq	$-128, %r12
	movq	5688(%rsp), %r8                 # 8-byte Reload
	subq	$-128, %r8
	movq	5696(%rsp), %r13                # 8-byte Reload
	subq	$-128, %r13
	movq	5704(%rsp), %r10                # 8-byte Reload
	subq	$-128, %r10
	movq	5712(%rsp), %rdi                # 8-byte Reload
	subq	$-128, %rdi
	movq	5720(%rsp), %r15                # 8-byte Reload
	subq	$-128, %r15
	subq	$-128, 2920(%rsp)               # 8-byte Folded Spill
	subq	$-128, 2928(%rsp)               # 8-byte Folded Spill
	subq	$-128, 2936(%rsp)               # 8-byte Folded Spill
	subq	$-128, 2944(%rsp)               # 8-byte Folded Spill
	subq	$-128, 2952(%rsp)               # 8-byte Folded Spill
	subq	$-128, 2960(%rsp)               # 8-byte Folded Spill
	subq	$-128, 2968(%rsp)               # 8-byte Folded Spill
	subq	$-128, 2976(%rsp)               # 8-byte Folded Spill
	subq	$-128, 2984(%rsp)               # 8-byte Folded Spill
	subq	$-128, 2992(%rsp)               # 8-byte Folded Spill
	subq	$-128, 3000(%rsp)               # 8-byte Folded Spill
	subq	$-128, 3008(%rsp)               # 8-byte Folded Spill
	subq	$-128, 3016(%rsp)               # 8-byte Folded Spill
	subq	$-128, 3064(%rsp)               # 8-byte Folded Spill
	movq	5728(%rsp), %r9                 # 8-byte Reload
	subq	$-128, %r9
	movq	5736(%rsp), %rsi                # 8-byte Reload
	subq	$-128, %rsi
	movq	5744(%rsp), %r11                # 8-byte Reload
	subq	$-128, %r11
	movq	5752(%rsp), %rbx                # 8-byte Reload
	subq	$-128, %rbx
	subq	$-128, 3024(%rsp)               # 8-byte Folded Spill
	subq	$-128, 3032(%rsp)               # 8-byte Folded Spill
	subq	$-128, 3040(%rsp)               # 8-byte Folded Spill
	subq	$-128, 3048(%rsp)               # 8-byte Folded Spill
	subq	$-128, 3056(%rsp)               # 8-byte Folded Spill
	movq	432(%rsp), %rax                 # 8-byte Reload
	subq	$-128, %rax
	vmovdqa64	46720(%rsp), %zmm14     # 64-byte Reload
	vmovdqa64	44288(%rsp), %zmm16     # 64-byte Reload
	.loc	1 229 18                        # 03-matrix-multiplication-cpu.py:229:18
	vpaddq	%zmm16, %zmm14, %zmm14
	vmovdqa64	46784(%rsp), %zmm9      # 64-byte Reload
	vpaddq	%zmm16, %zmm9, %zmm9
	vmovdqa64	46848(%rsp), %zmm13     # 64-byte Reload
	vpaddq	%zmm16, %zmm13, %zmm13
	vmovdqa64	46912(%rsp), %zmm12     # 64-byte Reload
	vpaddq	%zmm16, %zmm12, %zmm12
	vmovdqa64	46976(%rsp), %zmm10     # 64-byte Reload
	vpaddq	%zmm16, %zmm10, %zmm10
	vmovdqa64	47040(%rsp), %zmm28     # 64-byte Reload
	vpaddq	%zmm16, %zmm28, %zmm28
	vmovdqa64	47104(%rsp), %zmm8      # 64-byte Reload
	vpaddq	%zmm16, %zmm8, %zmm8
	vmovdqa64	47168(%rsp), %zmm31     # 64-byte Reload
	vpaddq	%zmm16, %zmm31, %zmm31
	vmovdqa64	47232(%rsp), %zmm30     # 64-byte Reload
	vpaddq	%zmm16, %zmm30, %zmm30
	vmovdqa64	47296(%rsp), %zmm23     # 64-byte Reload
	vpaddq	%zmm16, %zmm23, %zmm23
	vmovdqa64	47360(%rsp), %zmm29     # 64-byte Reload
	vpaddq	%zmm16, %zmm29, %zmm29
	vmovdqa64	47424(%rsp), %zmm26     # 64-byte Reload
	vpaddq	%zmm16, %zmm26, %zmm26
	vmovdqa64	47488(%rsp), %zmm25     # 64-byte Reload
	vpaddq	%zmm16, %zmm25, %zmm25
	vmovdqa64	47552(%rsp), %zmm19     # 64-byte Reload
	vpaddq	%zmm16, %zmm19, %zmm19
	vmovdqa64	47616(%rsp), %zmm24     # 64-byte Reload
	vpaddq	%zmm16, %zmm24, %zmm24
	vmovdqa64	47680(%rsp), %zmm22     # 64-byte Reload
	vpaddq	%zmm16, %zmm22, %zmm22
	vmovdqa64	47744(%rsp), %zmm21     # 64-byte Reload
	vpaddq	%zmm16, %zmm21, %zmm21
	vmovdqa64	47808(%rsp), %zmm15     # 64-byte Reload
	vpaddq	%zmm16, %zmm15, %zmm15
	vmovdqa64	47872(%rsp), %zmm20     # 64-byte Reload
	vpaddq	%zmm16, %zmm20, %zmm20
	vmovdqa64	47936(%rsp), %zmm18     # 64-byte Reload
	vpaddq	%zmm16, %zmm18, %zmm18
	vmovdqa64	48000(%rsp), %zmm17     # 64-byte Reload
	vpaddq	%zmm16, %zmm17, %zmm17
	vmovdqa64	48064(%rsp), %zmm11     # 64-byte Reload
	vpaddq	%zmm16, %zmm11, %zmm11
	vmovdqa64	38080(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 38080(%rsp)      # 64-byte Spill
	vmovdqa64	38144(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 38144(%rsp)      # 64-byte Spill
	vmovdqa64	38208(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 38208(%rsp)      # 64-byte Spill
	vmovdqa64	38272(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 38272(%rsp)      # 64-byte Spill
	vmovdqa64	38336(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 38336(%rsp)      # 64-byte Spill
	vmovdqa64	38400(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 38400(%rsp)      # 64-byte Spill
	vmovdqa64	38464(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 38464(%rsp)      # 64-byte Spill
	vmovdqa64	38528(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 38528(%rsp)      # 64-byte Spill
	vmovdqa64	38592(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 38592(%rsp)      # 64-byte Spill
	vmovdqa64	38656(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 38656(%rsp)      # 64-byte Spill
	vmovdqa64	38720(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 38720(%rsp)      # 64-byte Spill
	vmovdqa64	38784(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 38784(%rsp)      # 64-byte Spill
	vmovdqa64	38848(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 38848(%rsp)      # 64-byte Spill
	vmovdqa64	38912(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 38912(%rsp)      # 64-byte Spill
	vmovdqa64	38976(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 38976(%rsp)      # 64-byte Spill
	vmovdqa64	39040(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 39040(%rsp)      # 64-byte Spill
	vmovdqa64	39104(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 39104(%rsp)      # 64-byte Spill
	vmovdqa64	39168(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 39168(%rsp)      # 64-byte Spill
	vmovdqa64	39232(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 39232(%rsp)      # 64-byte Spill
	vmovdqa64	39296(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 39296(%rsp)      # 64-byte Spill
	vmovdqa64	39360(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 39360(%rsp)      # 64-byte Spill
	vmovdqa64	39424(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 39424(%rsp)      # 64-byte Spill
	vmovdqa64	39488(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 39488(%rsp)      # 64-byte Spill
	vmovdqa64	39552(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 39552(%rsp)      # 64-byte Spill
	vmovdqa64	39616(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 39616(%rsp)      # 64-byte Spill
	vmovdqa64	39680(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 39680(%rsp)      # 64-byte Spill
	vmovdqa64	39744(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 39744(%rsp)      # 64-byte Spill
	vmovdqa64	39808(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 39808(%rsp)      # 64-byte Spill
	vmovdqa64	39872(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 39872(%rsp)      # 64-byte Spill
	vmovdqa64	39936(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 39936(%rsp)      # 64-byte Spill
	vmovdqa64	40000(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 40000(%rsp)      # 64-byte Spill
	vmovdqa64	40064(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 40064(%rsp)      # 64-byte Spill
	vmovdqa64	40128(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 40128(%rsp)      # 64-byte Spill
	vmovdqa64	40192(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 40192(%rsp)      # 64-byte Spill
	vmovdqa64	40256(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 40256(%rsp)      # 64-byte Spill
	vmovdqa64	40320(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 40320(%rsp)      # 64-byte Spill
	vmovdqa64	40384(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 40384(%rsp)      # 64-byte Spill
	vmovdqa64	40448(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 40448(%rsp)      # 64-byte Spill
	vmovdqa64	40512(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 40512(%rsp)      # 64-byte Spill
	vmovdqa64	40576(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 40576(%rsp)      # 64-byte Spill
	vmovdqa64	40640(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 40640(%rsp)      # 64-byte Spill
	vmovdqa64	40704(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 40704(%rsp)      # 64-byte Spill
	vmovdqa64	40768(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 40768(%rsp)      # 64-byte Spill
	vmovdqa64	40832(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 40832(%rsp)      # 64-byte Spill
	vmovdqa64	40896(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 40896(%rsp)      # 64-byte Spill
	vmovdqa64	40960(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 40960(%rsp)      # 64-byte Spill
	vmovdqa64	41024(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 41024(%rsp)      # 64-byte Spill
	vmovdqa64	41088(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 41088(%rsp)      # 64-byte Spill
	vmovdqa64	41152(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 41152(%rsp)      # 64-byte Spill
	vmovdqa64	41216(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 41216(%rsp)      # 64-byte Spill
	vmovdqa64	41280(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 41280(%rsp)      # 64-byte Spill
	vmovdqa64	41344(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 41344(%rsp)      # 64-byte Spill
	vmovdqa64	41408(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 41408(%rsp)      # 64-byte Spill
	vmovdqa64	41472(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 41472(%rsp)      # 64-byte Spill
	vmovdqa64	41536(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 41536(%rsp)      # 64-byte Spill
	vmovdqa64	41600(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 41600(%rsp)      # 64-byte Spill
	vmovdqa64	41664(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 41664(%rsp)      # 64-byte Spill
	vmovdqa64	41728(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 41728(%rsp)      # 64-byte Spill
	vmovdqa64	41792(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 41792(%rsp)      # 64-byte Spill
	vmovdqa64	41856(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 41856(%rsp)      # 64-byte Spill
	vmovdqa64	41920(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 41920(%rsp)      # 64-byte Spill
	vmovdqa64	41984(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 41984(%rsp)      # 64-byte Spill
	vmovdqa64	42048(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 42048(%rsp)      # 64-byte Spill
	vmovdqa64	42112(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 42112(%rsp)      # 64-byte Spill
	vmovdqa64	42176(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 42176(%rsp)      # 64-byte Spill
	vmovdqa64	42240(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 42240(%rsp)      # 64-byte Spill
	vmovdqa64	42304(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 42304(%rsp)      # 64-byte Spill
	vmovdqa64	42368(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 42368(%rsp)      # 64-byte Spill
	vmovdqa64	42432(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 42432(%rsp)      # 64-byte Spill
	vmovdqa64	42496(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 42496(%rsp)      # 64-byte Spill
	vmovdqa64	42560(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 42560(%rsp)      # 64-byte Spill
	vmovdqa64	42624(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 42624(%rsp)      # 64-byte Spill
	vmovdqa64	42688(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 42688(%rsp)      # 64-byte Spill
	vmovdqa64	42752(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 42752(%rsp)      # 64-byte Spill
	vmovdqa64	42816(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 42816(%rsp)      # 64-byte Spill
	vmovdqa64	48128(%rsp), %zmm7      # 64-byte Reload
	vpaddq	%zmm16, %zmm7, %zmm7
	vmovdqa64	48192(%rsp), %zmm6      # 64-byte Reload
	vpaddq	%zmm16, %zmm6, %zmm6
	vmovdqa64	48256(%rsp), %zmm5      # 64-byte Reload
	vpaddq	%zmm16, %zmm5, %zmm5
	vmovdqa64	48320(%rsp), %zmm4      # 64-byte Reload
	vpaddq	%zmm16, %zmm4, %zmm4
	vmovdqa64	37824(%rsp), %zmm3      # 64-byte Reload
	vpaddq	%zmm16, %zmm3, %zmm3
	vmovdqa64	37888(%rsp), %zmm2      # 64-byte Reload
	vpaddq	%zmm16, %zmm2, %zmm2
	vmovdqa64	42880(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	%zmm0, 42880(%rsp)      # 64-byte Spill
	vmovdqa64	37952(%rsp), %zmm1      # 64-byte Reload
	vpaddq	%zmm16, %zmm1, %zmm1
	vmovdqa64	38016(%rsp), %zmm0      # 64-byte Reload
	vpaddq	%zmm16, %zmm0, %zmm0
	vmovdqa64	42944(%rsp), %zmm27     # 64-byte Reload
	vpaddq	%zmm16, %zmm27, %zmm27
	vmovdqa64	%zmm27, 42944(%rsp)     # 64-byte Spill
	vmovdqa64	43008(%rsp), %zmm27     # 64-byte Reload
	vpaddq	%zmm16, %zmm27, %zmm27
	vmovdqa64	%zmm27, 43008(%rsp)     # 64-byte Spill
	vmovdqa64	43072(%rsp), %zmm27     # 64-byte Reload
	vpaddq	%zmm16, %zmm27, %zmm27
	vmovdqa64	%zmm27, 43072(%rsp)     # 64-byte Spill
	vmovdqa64	43136(%rsp), %zmm27     # 64-byte Reload
	vpaddq	%zmm16, %zmm27, %zmm27
	vmovdqa64	%zmm27, 43136(%rsp)     # 64-byte Spill
	vmovdqa64	43200(%rsp), %zmm27     # 64-byte Reload
	vpaddq	%zmm16, %zmm27, %zmm27
	vmovdqa64	%zmm27, 43200(%rsp)     # 64-byte Spill
	vmovdqa64	43264(%rsp), %zmm27     # 64-byte Reload
	vpaddq	%zmm16, %zmm27, %zmm27
	vmovdqa64	%zmm27, 43264(%rsp)     # 64-byte Spill
	vmovdqa64	43328(%rsp), %zmm27     # 64-byte Reload
	vpaddq	%zmm16, %zmm27, %zmm27
	vmovdqa64	%zmm27, 43328(%rsp)     # 64-byte Spill
	vmovdqa64	43392(%rsp), %zmm27     # 64-byte Reload
	vpaddq	%zmm16, %zmm27, %zmm27
	vmovdqa64	%zmm27, 43392(%rsp)     # 64-byte Spill
	vmovdqa64	43456(%rsp), %zmm27     # 64-byte Reload
	vpaddq	%zmm16, %zmm27, %zmm27
	vmovdqa64	%zmm27, 43456(%rsp)     # 64-byte Spill
	vmovdqa64	43520(%rsp), %zmm27     # 64-byte Reload
	vpaddq	%zmm16, %zmm27, %zmm27
	vmovdqa64	%zmm27, 43520(%rsp)     # 64-byte Spill
	vmovdqa64	43584(%rsp), %zmm27     # 64-byte Reload
	vpaddq	%zmm16, %zmm27, %zmm27
	vmovdqa64	%zmm27, 43584(%rsp)     # 64-byte Spill
	vmovdqa64	43648(%rsp), %zmm27     # 64-byte Reload
	vpaddq	%zmm16, %zmm27, %zmm27
	vmovdqa64	%zmm27, 43648(%rsp)     # 64-byte Spill
	vmovdqa64	43712(%rsp), %zmm27     # 64-byte Reload
	vpaddq	%zmm16, %zmm27, %zmm27
	vmovdqa64	%zmm27, 43712(%rsp)     # 64-byte Spill
	vmovdqa64	43776(%rsp), %zmm27     # 64-byte Reload
	vpaddq	%zmm16, %zmm27, %zmm27
	vmovdqa64	%zmm27, 43776(%rsp)     # 64-byte Spill
	vmovdqa64	43840(%rsp), %zmm27     # 64-byte Reload
	vpaddq	%zmm16, %zmm27, %zmm27
	vmovdqa64	%zmm27, 43840(%rsp)     # 64-byte Spill
	vmovdqa64	43904(%rsp), %zmm27     # 64-byte Reload
	vpaddq	%zmm16, %zmm27, %zmm27
	vmovdqa64	%zmm27, 43904(%rsp)     # 64-byte Spill
	vmovdqa64	43968(%rsp), %zmm27     # 64-byte Reload
	vpaddq	%zmm16, %zmm27, %zmm27
	vmovdqa64	%zmm27, 43968(%rsp)     # 64-byte Spill
	vmovdqa64	44032(%rsp), %zmm27     # 64-byte Reload
	vpaddq	%zmm16, %zmm27, %zmm27
	vmovdqa64	%zmm27, 44032(%rsp)     # 64-byte Spill
	vmovdqa64	44096(%rsp), %zmm27     # 64-byte Reload
	vpaddq	%zmm16, %zmm27, %zmm27
	vmovdqa64	%zmm27, 44096(%rsp)     # 64-byte Spill
	vmovdqa64	44160(%rsp), %zmm27     # 64-byte Reload
	vpaddq	%zmm16, %zmm27, %zmm27
	vmovdqa64	%zmm27, 44160(%rsp)     # 64-byte Spill
	vmovdqa64	44224(%rsp), %zmm27     # 64-byte Reload
	vpaddq	%zmm16, %zmm27, %zmm27
	vmovdqa64	%zmm27, 44224(%rsp)     # 64-byte Spill
	vmovdqa64	48384(%rsp), %zmm27     # 64-byte Reload
	vpaddq	%zmm16, %zmm27, %zmm27
	movl	60(%rsp), %ecx                  # 4-byte Reload
	.loc	1 216 22                        # 03-matrix-multiplication-cpu.py:216:22
	incl	%ecx
	movl	%ecx, 60(%rsp)                  # 4-byte Spill
	cmpl	424(%rsp), %ecx                 # 4-byte Folded Reload
	movq	%rbx, %rcx
	movq	%r11, %rbx
	movq	%rsi, %r11
	movq	%r9, %rsi
	movq	3064(%rsp), %r9                 # 8-byte Reload
	jne	.LBB0_3
	jmp	.LBB0_6
.LBB0_1:
	.loc	1 0 22 is_stmt 0                # 03-matrix-multiplication-cpu.py:0:22
	vpxor	%xmm0, %xmm0, %xmm0
	vmovdqa64	%zmm0, 24000(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 19968(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 20096(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 20032(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 20224(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 20160(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 20352(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 20288(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 20480(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 20416(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 20608(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 20544(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 20736(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 20672(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 20864(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 20800(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 20992(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 20928(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 21120(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 21056(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 21248(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 21184(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 21376(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 21312(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 21504(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 21440(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 21632(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 21568(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 21760(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 21696(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 21888(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 21824(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 22016(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 21952(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 22144(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 22080(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 22272(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 22208(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 22400(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 22336(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 22528(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 22464(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 22656(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 22592(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 22784(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 22720(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 22912(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 22848(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 23040(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 22976(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 23168(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 23104(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 23296(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 23232(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 23424(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 23360(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 23552(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 23488(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 23680(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 23616(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 23808(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 23744(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 23936(%rsp)      # 64-byte Spill
	vmovdqa64	%zmm0, 23872(%rsp)      # 64-byte Spill
.LBB0_6:                                # %._crit_edge
	movl	56(%rsp), %edi                  # 4-byte Reload
	.loc	1 238 33 is_stmt 1              # 03-matrix-multiplication-cpu.py:238:33
	movl	%edi, %eax
	movl	32(%rbp), %esi
	imull	%esi, %eax
	.loc	1 238 21 is_stmt 0              # 03-matrix-multiplication-cpu.py:238:21
	cltq
	movq	5656(%rsp), %rdx                # 8-byte Reload
	leaq	(%rdx,%rax,4), %rcx
	.loc	1 238 52                        # 03-matrix-multiplication-cpu.py:238:52
	movslq	440(%rsp), %rax                 # 4-byte Folded Reload
	vmovaps	19968(%rsp), %zmm0              # 64-byte Reload
	.loc	1 243 21 is_stmt 1              # 03-matrix-multiplication-cpu.py:243:21
	vmovups	%zmm0, 64(%rcx,%rax,4)
	vmovaps	24000(%rsp), %zmm0              # 64-byte Reload
	vmovups	%zmm0, (%rcx,%rax,4)
	.loc	1 204 38                        # 03-matrix-multiplication-cpu.py:204:38
	movl	%edi, %ecx
	orl	$1, %ecx
	.loc	1 238 33                        # 03-matrix-multiplication-cpu.py:238:33
	imull	%esi, %ecx
	.loc	1 238 21 is_stmt 0              # 03-matrix-multiplication-cpu.py:238:21
	movslq	%ecx, %rcx
	leaq	(%rdx,%rcx,4), %rcx
	vmovaps	20032(%rsp), %zmm0              # 64-byte Reload
	.loc	1 243 21 is_stmt 1              # 03-matrix-multiplication-cpu.py:243:21
	vmovups	%zmm0, 64(%rcx,%rax,4)
	vmovaps	20096(%rsp), %zmm0              # 64-byte Reload
	vmovups	%zmm0, (%rcx,%rax,4)
	.loc	1 204 38                        # 03-matrix-multiplication-cpu.py:204:38
	movl	%edi, %ecx
	orl	$2, %ecx
	.loc	1 238 33                        # 03-matrix-multiplication-cpu.py:238:33
	imull	%esi, %ecx
	.loc	1 238 21 is_stmt 0              # 03-matrix-multiplication-cpu.py:238:21
	movslq	%ecx, %rcx
	leaq	(%rdx,%rcx,4), %rcx
	vmovaps	20160(%rsp), %zmm0              # 64-byte Reload
	.loc	1 243 21 is_stmt 1              # 03-matrix-multiplication-cpu.py:243:21
	vmovups	%zmm0, 64(%rcx,%rax,4)
	vmovaps	20224(%rsp), %zmm0              # 64-byte Reload
	vmovups	%zmm0, (%rcx,%rax,4)
	.loc	1 204 38                        # 03-matrix-multiplication-cpu.py:204:38
	movl	%edi, %ecx
	orl	$3, %ecx
	.loc	1 238 33                        # 03-matrix-multiplication-cpu.py:238:33
	imull	%esi, %ecx
	.loc	1 238 21 is_stmt 0              # 03-matrix-multiplication-cpu.py:238:21
	movslq	%ecx, %rcx
	leaq	(%rdx,%rcx,4), %rcx
	vmovaps	20352(%rsp), %zmm0              # 64-byte Reload
	.loc	1 243 21 is_stmt 1              # 03-matrix-multiplication-cpu.py:243:21
	vmovups	%zmm0, (%rcx,%rax,4)
	vmovaps	20288(%rsp), %zmm0              # 64-byte Reload
	vmovups	%zmm0, 64(%rcx,%rax,4)
	.loc	1 204 38                        # 03-matrix-multiplication-cpu.py:204:38
	movl	%edi, %ecx
	orl	$4, %ecx
	.loc	1 238 33                        # 03-matrix-multiplication-cpu.py:238:33
	imull	%esi, %ecx
	.loc	1 238 21 is_stmt 0              # 03-matrix-multiplication-cpu.py:238:21
	movslq	%ecx, %rcx
	leaq	(%rdx,%rcx,4), %rcx
	vmovaps	20480(%rsp), %zmm0              # 64-byte Reload
	.loc	1 243 21 is_stmt 1              # 03-matrix-multiplication-cpu.py:243:21
	vmovups	%zmm0, (%rcx,%rax,4)
	vmovaps	20416(%rsp), %zmm0              # 64-byte Reload
	vmovups	%zmm0, 64(%rcx,%rax,4)
	.loc	1 204 38                        # 03-matrix-multiplication-cpu.py:204:38
	movl	%edi, %ecx
	orl	$5, %ecx
	.loc	1 238 33                        # 03-matrix-multiplication-cpu.py:238:33
	imull	%esi, %ecx
	.loc	1 238 21 is_stmt 0              # 03-matrix-multiplication-cpu.py:238:21
	movslq	%ecx, %rcx
	leaq	(%rdx,%rcx,4), %rcx
	vmovaps	20608(%rsp), %zmm0              # 64-byte Reload
	.loc	1 243 21 is_stmt 1              # 03-matrix-multiplication-cpu.py:243:21
	vmovups	%zmm0, (%rcx,%rax,4)
	vmovaps	20544(%rsp), %zmm0              # 64-byte Reload
	vmovups	%zmm0, 64(%rcx,%rax,4)
	.loc	1 204 38                        # 03-matrix-multiplication-cpu.py:204:38
	movl	%edi, %ecx
	orl	$6, %ecx
	.loc	1 238 33                        # 03-matrix-multiplication-cpu.py:238:33
	imull	%esi, %ecx
	.loc	1 238 21 is_stmt 0              # 03-matrix-multiplication-cpu.py:238:21
	movslq	%ecx, %rcx
	leaq	(%rdx,%rcx,4), %rcx
	vmovaps	20736(%rsp), %zmm0              # 64-byte Reload
	.loc	1 243 21 is_stmt 1              # 03-matrix-multiplication-cpu.py:243:21
	vmovups	%zmm0, (%rcx,%rax,4)
	vmovaps	20672(%rsp), %zmm0              # 64-byte Reload
	vmovups	%zmm0, 64(%rcx,%rax,4)
	.loc	1 204 38                        # 03-matrix-multiplication-cpu.py:204:38
	movl	%edi, %ecx
	orl	$7, %ecx
	.loc	1 238 33                        # 03-matrix-multiplication-cpu.py:238:33
	imull	%esi, %ecx
	.loc	1 238 21 is_stmt 0              # 03-matrix-multiplication-cpu.py:238:21
	movslq	%ecx, %rcx
	leaq	(%rdx,%rcx,4), %rcx
	vmovaps	20864(%rsp), %zmm0              # 64-byte Reload
	.loc	1 243 21 is_stmt 1              # 03-matrix-multiplication-cpu.py:243:21
	vmovups	%zmm0, (%rcx,%rax,4)
	vmovaps	20800(%rsp), %zmm0              # 64-byte Reload
	vmovups	%zmm0, 64(%rcx,%rax,4)
	.loc	1 204 38                        # 03-matrix-multiplication-cpu.py:204:38
	movl	%edi, %ecx
	orl	$8, %ecx
	.loc	1 238 33                        # 03-matrix-multiplication-cpu.py:238:33
	imull	%esi, %ecx
	.loc	1 238 21 is_stmt 0              # 03-matrix-multiplication-cpu.py:238:21
	movslq	%ecx, %rcx
	leaq	(%rdx,%rcx,4), %rcx
	vmovaps	20992(%rsp), %zmm0              # 64-byte Reload
	.loc	1 243 21 is_stmt 1              # 03-matrix-multiplication-cpu.py:243:21
	vmovups	%zmm0, (%rcx,%rax,4)
	vmovaps	20928(%rsp), %zmm0              # 64-byte Reload
	vmovups	%zmm0, 64(%rcx,%rax,4)
	.loc	1 204 38                        # 03-matrix-multiplication-cpu.py:204:38
	movl	%edi, %ecx
	orl	$9, %ecx
	.loc	1 238 33                        # 03-matrix-multiplication-cpu.py:238:33
	imull	%esi, %ecx
	.loc	1 238 21 is_stmt 0              # 03-matrix-multiplication-cpu.py:238:21
	movslq	%ecx, %rcx
	leaq	(%rdx,%rcx,4), %rcx
	vmovaps	21120(%rsp), %zmm0              # 64-byte Reload
	.loc	1 243 21 is_stmt 1              # 03-matrix-multiplication-cpu.py:243:21
	vmovups	%zmm0, (%rcx,%rax,4)
	vmovaps	21056(%rsp), %zmm0              # 64-byte Reload
	vmovups	%zmm0, 64(%rcx,%rax,4)
	.loc	1 204 38                        # 03-matrix-multiplication-cpu.py:204:38
	movl	%edi, %ecx
	orl	$10, %ecx
	.loc	1 238 33                        # 03-matrix-multiplication-cpu.py:238:33
	imull	%esi, %ecx
	.loc	1 238 21 is_stmt 0              # 03-matrix-multiplication-cpu.py:238:21
	movslq	%ecx, %rcx
	leaq	(%rdx,%rcx,4), %rcx
	vmovaps	21248(%rsp), %zmm0              # 64-byte Reload
	.loc	1 243 21 is_stmt 1              # 03-matrix-multiplication-cpu.py:243:21
	vmovups	%zmm0, (%rcx,%rax,4)
	vmovaps	21184(%rsp), %zmm0              # 64-byte Reload
	vmovups	%zmm0, 64(%rcx,%rax,4)
	.loc	1 204 38                        # 03-matrix-multiplication-cpu.py:204:38
	movl	%edi, %ecx
	orl	$11, %ecx
	.loc	1 238 33                        # 03-matrix-multiplication-cpu.py:238:33
	imull	%esi, %ecx
	.loc	1 238 21 is_stmt 0              # 03-matrix-multiplication-cpu.py:238:21
	movslq	%ecx, %rcx
	leaq	(%rdx,%rcx,4), %rcx
	vmovaps	21376(%rsp), %zmm0              # 64-byte Reload
	.loc	1 243 21 is_stmt 1              # 03-matrix-multiplication-cpu.py:243:21
	vmovups	%zmm0, (%rcx,%rax,4)
	vmovaps	21312(%rsp), %zmm0              # 64-byte Reload
	vmovups	%zmm0, 64(%rcx,%rax,4)
	.loc	1 204 38                        # 03-matrix-multiplication-cpu.py:204:38
	movl	%edi, %ecx
	orl	$12, %ecx
	.loc	1 238 33                        # 03-matrix-multiplication-cpu.py:238:33
	imull	%esi, %ecx
	.loc	1 238 21 is_stmt 0              # 03-matrix-multiplication-cpu.py:238:21
	movslq	%ecx, %rcx
	leaq	(%rdx,%rcx,4), %rcx
	vmovaps	21504(%rsp), %zmm0              # 64-byte Reload
	.loc	1 243 21 is_stmt 1              # 03-matrix-multiplication-cpu.py:243:21
	vmovups	%zmm0, (%rcx,%rax,4)
	vmovaps	21440(%rsp), %zmm0              # 64-byte Reload
	vmovups	%zmm0, 64(%rcx,%rax,4)
	.loc	1 204 38                        # 03-matrix-multiplication-cpu.py:204:38
	movl	%edi, %ecx
	orl	$13, %ecx
	.loc	1 238 33                        # 03-matrix-multiplication-cpu.py:238:33
	imull	%esi, %ecx
	.loc	1 238 21 is_stmt 0              # 03-matrix-multiplication-cpu.py:238:21
	movslq	%ecx, %rcx
	leaq	(%rdx,%rcx,4), %rcx
	vmovaps	21632(%rsp), %zmm0              # 64-byte Reload
	.loc	1 243 21 is_stmt 1              # 03-matrix-multiplication-cpu.py:243:21
	vmovups	%zmm0, (%rcx,%rax,4)
	vmovaps	21568(%rsp), %zmm0              # 64-byte Reload
	vmovups	%zmm0, 64(%rcx,%rax,4)
	.loc	1 204 38                        # 03-matrix-multiplication-cpu.py:204:38
	movl	%edi, %ecx
	orl	$14, %ecx
	.loc	1 238 33                        # 03-matrix-multiplication-cpu.py:238:33
	imull	%esi, %ecx
	.loc	1 238 21 is_stmt 0              # 03-matrix-multiplication-cpu.py:238:21
	movslq	%ecx, %rcx
	leaq	(%rdx,%rcx,4), %rcx
	vmovaps	21760(%rsp), %zmm0              # 64-byte Reload
	.loc	1 243 21 is_stmt 1              # 03-matrix-multiplication-cpu.py:243:21
	vmovups	%zmm0, (%rcx,%rax,4)
	vmovaps	21696(%rsp), %zmm0              # 64-byte Reload
	vmovups	%zmm0, 64(%rcx,%rax,4)
	.loc	1 204 38                        # 03-matrix-multiplication-cpu.py:204:38
	movl	%edi, %ecx
	orl	$15, %ecx
	.loc	1 238 33                        # 03-matrix-multiplication-cpu.py:238:33
	imull	%esi, %ecx
	.loc	1 238 21 is_stmt 0              # 03-matrix-multiplication-cpu.py:238:21
	movslq	%ecx, %rcx
	leaq	(%rdx,%rcx,4), %rcx
	vmovaps	21888(%rsp), %zmm0              # 64-byte Reload
	.loc	1 243 21 is_stmt 1              # 03-matrix-multiplication-cpu.py:243:21
	vmovups	%zmm0, (%rcx,%rax,4)
	vmovaps	21824(%rsp), %zmm0              # 64-byte Reload
	vmovups	%zmm0, 64(%rcx,%rax,4)
	.loc	1 204 38                        # 03-matrix-multiplication-cpu.py:204:38
	movl	%edi, %ecx
	orl	$16, %ecx
	.loc	1 238 33                        # 03-matrix-multiplication-cpu.py:238:33
	imull	%esi, %ecx
	.loc	1 238 21 is_stmt 0              # 03-matrix-multiplication-cpu.py:238:21
	movslq	%ecx, %rcx
	leaq	(%rdx,%rcx,4), %rcx
	vmovaps	22016(%rsp), %zmm0              # 64-byte Reload
	.loc	1 243 21 is_stmt 1              # 03-matrix-multiplication-cpu.py:243:21
	vmovups	%zmm0, (%rcx,%rax,4)
	vmovaps	21952(%rsp), %zmm0              # 64-byte Reload
	vmovups	%zmm0, 64(%rcx,%rax,4)
	.loc	1 204 38                        # 03-matrix-multiplication-cpu.py:204:38
	movl	%edi, %ecx
	orl	$17, %ecx
	.loc	1 238 33                        # 03-matrix-multiplication-cpu.py:238:33
	imull	%esi, %ecx
	.loc	1 238 21 is_stmt 0              # 03-matrix-multiplication-cpu.py:238:21
	movslq	%ecx, %rcx
	leaq	(%rdx,%rcx,4), %rcx
	vmovaps	22144(%rsp), %zmm0              # 64-byte Reload
	.loc	1 243 21 is_stmt 1              # 03-matrix-multiplication-cpu.py:243:21
	vmovups	%zmm0, (%rcx,%rax,4)
	vmovaps	22080(%rsp), %zmm0              # 64-byte Reload
	vmovups	%zmm0, 64(%rcx,%rax,4)
	.loc	1 204 38                        # 03-matrix-multiplication-cpu.py:204:38
	movl	%edi, %ecx
	orl	$18, %ecx
	.loc	1 238 33                        # 03-matrix-multiplication-cpu.py:238:33
	imull	%esi, %ecx
	.loc	1 238 21 is_stmt 0              # 03-matrix-multiplication-cpu.py:238:21
	movslq	%ecx, %rcx
	leaq	(%rdx,%rcx,4), %rcx
	vmovaps	22272(%rsp), %zmm0              # 64-byte Reload
	.loc	1 243 21 is_stmt 1              # 03-matrix-multiplication-cpu.py:243:21
	vmovups	%zmm0, (%rcx,%rax,4)
	vmovaps	22208(%rsp), %zmm0              # 64-byte Reload
	vmovups	%zmm0, 64(%rcx,%rax,4)
	.loc	1 204 38                        # 03-matrix-multiplication-cpu.py:204:38
	movl	%edi, %ecx
	orl	$19, %ecx
	.loc	1 238 33                        # 03-matrix-multiplication-cpu.py:238:33
	imull	%esi, %ecx
	.loc	1 238 21 is_stmt 0              # 03-matrix-multiplication-cpu.py:238:21
	movslq	%ecx, %rcx
	leaq	(%rdx,%rcx,4), %rcx
	vmovaps	22400(%rsp), %zmm0              # 64-byte Reload
	.loc	1 243 21 is_stmt 1              # 03-matrix-multiplication-cpu.py:243:21
	vmovups	%zmm0, (%rcx,%rax,4)
	vmovaps	22336(%rsp), %zmm0              # 64-byte Reload
	vmovups	%zmm0, 64(%rcx,%rax,4)
	.loc	1 204 38                        # 03-matrix-multiplication-cpu.py:204:38
	movl	%edi, %ecx
	orl	$20, %ecx
	.loc	1 238 33                        # 03-matrix-multiplication-cpu.py:238:33
	imull	%esi, %ecx
	.loc	1 238 21 is_stmt 0              # 03-matrix-multiplication-cpu.py:238:21
	movslq	%ecx, %rcx
	leaq	(%rdx,%rcx,4), %rcx
	vmovaps	22528(%rsp), %zmm0              # 64-byte Reload
	.loc	1 243 21 is_stmt 1              # 03-matrix-multiplication-cpu.py:243:21
	vmovups	%zmm0, (%rcx,%rax,4)
	vmovaps	22464(%rsp), %zmm0              # 64-byte Reload
	vmovups	%zmm0, 64(%rcx,%rax,4)
	.loc	1 204 38                        # 03-matrix-multiplication-cpu.py:204:38
	movl	%edi, %ecx
	orl	$21, %ecx
	.loc	1 238 33                        # 03-matrix-multiplication-cpu.py:238:33
	imull	%esi, %ecx
	.loc	1 238 21 is_stmt 0              # 03-matrix-multiplication-cpu.py:238:21
	movslq	%ecx, %rcx
	leaq	(%rdx,%rcx,4), %rcx
	vmovaps	22656(%rsp), %zmm0              # 64-byte Reload
	.loc	1 243 21 is_stmt 1              # 03-matrix-multiplication-cpu.py:243:21
	vmovups	%zmm0, (%rcx,%rax,4)
	vmovaps	22592(%rsp), %zmm0              # 64-byte Reload
	vmovups	%zmm0, 64(%rcx,%rax,4)
	.loc	1 204 38                        # 03-matrix-multiplication-cpu.py:204:38
	movl	%edi, %ecx
	orl	$22, %ecx
	.loc	1 238 33                        # 03-matrix-multiplication-cpu.py:238:33
	imull	%esi, %ecx
	.loc	1 238 21 is_stmt 0              # 03-matrix-multiplication-cpu.py:238:21
	movslq	%ecx, %rcx
	leaq	(%rdx,%rcx,4), %rcx
	vmovaps	22784(%rsp), %zmm0              # 64-byte Reload
	.loc	1 243 21 is_stmt 1              # 03-matrix-multiplication-cpu.py:243:21
	vmovups	%zmm0, (%rcx,%rax,4)
	vmovaps	22720(%rsp), %zmm0              # 64-byte Reload
	vmovups	%zmm0, 64(%rcx,%rax,4)
	.loc	1 204 38                        # 03-matrix-multiplication-cpu.py:204:38
	movl	%edi, %ecx
	orl	$23, %ecx
	.loc	1 238 33                        # 03-matrix-multiplication-cpu.py:238:33
	imull	%esi, %ecx
	.loc	1 238 21 is_stmt 0              # 03-matrix-multiplication-cpu.py:238:21
	movslq	%ecx, %rcx
	leaq	(%rdx,%rcx,4), %rcx
	vmovaps	22912(%rsp), %zmm0              # 64-byte Reload
	.loc	1 243 21 is_stmt 1              # 03-matrix-multiplication-cpu.py:243:21
	vmovups	%zmm0, (%rcx,%rax,4)
	vmovaps	22848(%rsp), %zmm0              # 64-byte Reload
	vmovups	%zmm0, 64(%rcx,%rax,4)
	.loc	1 204 38                        # 03-matrix-multiplication-cpu.py:204:38
	movl	%edi, %ecx
	orl	$24, %ecx
	.loc	1 238 33                        # 03-matrix-multiplication-cpu.py:238:33
	imull	%esi, %ecx
	.loc	1 238 21 is_stmt 0              # 03-matrix-multiplication-cpu.py:238:21
	movslq	%ecx, %rcx
	leaq	(%rdx,%rcx,4), %rcx
	vmovaps	23040(%rsp), %zmm0              # 64-byte Reload
	.loc	1 243 21 is_stmt 1              # 03-matrix-multiplication-cpu.py:243:21
	vmovups	%zmm0, (%rcx,%rax,4)
	vmovaps	22976(%rsp), %zmm0              # 64-byte Reload
	vmovups	%zmm0, 64(%rcx,%rax,4)
	.loc	1 204 38                        # 03-matrix-multiplication-cpu.py:204:38
	movl	%edi, %ecx
	orl	$25, %ecx
	.loc	1 238 33                        # 03-matrix-multiplication-cpu.py:238:33
	imull	%esi, %ecx
	.loc	1 238 21 is_stmt 0              # 03-matrix-multiplication-cpu.py:238:21
	movslq	%ecx, %rcx
	leaq	(%rdx,%rcx,4), %rcx
	vmovaps	23168(%rsp), %zmm0              # 64-byte Reload
	.loc	1 243 21 is_stmt 1              # 03-matrix-multiplication-cpu.py:243:21
	vmovups	%zmm0, (%rcx,%rax,4)
	vmovaps	23104(%rsp), %zmm0              # 64-byte Reload
	vmovups	%zmm0, 64(%rcx,%rax,4)
	.loc	1 204 38                        # 03-matrix-multiplication-cpu.py:204:38
	movl	%edi, %ecx
	orl	$26, %ecx
	.loc	1 238 33                        # 03-matrix-multiplication-cpu.py:238:33
	imull	%esi, %ecx
	.loc	1 238 21 is_stmt 0              # 03-matrix-multiplication-cpu.py:238:21
	movslq	%ecx, %rcx
	leaq	(%rdx,%rcx,4), %rcx
	vmovaps	23296(%rsp), %zmm0              # 64-byte Reload
	.loc	1 243 21 is_stmt 1              # 03-matrix-multiplication-cpu.py:243:21
	vmovups	%zmm0, (%rcx,%rax,4)
	vmovaps	23232(%rsp), %zmm0              # 64-byte Reload
	vmovups	%zmm0, 64(%rcx,%rax,4)
	.loc	1 204 38                        # 03-matrix-multiplication-cpu.py:204:38
	movl	%edi, %ecx
	orl	$27, %ecx
	.loc	1 238 33                        # 03-matrix-multiplication-cpu.py:238:33
	imull	%esi, %ecx
	.loc	1 238 21 is_stmt 0              # 03-matrix-multiplication-cpu.py:238:21
	movslq	%ecx, %rcx
	leaq	(%rdx,%rcx,4), %rcx
	vmovaps	23424(%rsp), %zmm0              # 64-byte Reload
	.loc	1 243 21 is_stmt 1              # 03-matrix-multiplication-cpu.py:243:21
	vmovups	%zmm0, (%rcx,%rax,4)
	vmovaps	23360(%rsp), %zmm0              # 64-byte Reload
	vmovups	%zmm0, 64(%rcx,%rax,4)
	.loc	1 204 38                        # 03-matrix-multiplication-cpu.py:204:38
	movl	%edi, %ecx
	orl	$28, %ecx
	.loc	1 238 33                        # 03-matrix-multiplication-cpu.py:238:33
	imull	%esi, %ecx
	.loc	1 238 21 is_stmt 0              # 03-matrix-multiplication-cpu.py:238:21
	movslq	%ecx, %rcx
	leaq	(%rdx,%rcx,4), %rcx
	vmovaps	23552(%rsp), %zmm0              # 64-byte Reload
	.loc	1 243 21 is_stmt 1              # 03-matrix-multiplication-cpu.py:243:21
	vmovups	%zmm0, (%rcx,%rax,4)
	vmovaps	23488(%rsp), %zmm0              # 64-byte Reload
	vmovups	%zmm0, 64(%rcx,%rax,4)
	.loc	1 204 38                        # 03-matrix-multiplication-cpu.py:204:38
	movl	%edi, %ecx
	orl	$29, %ecx
	.loc	1 238 33                        # 03-matrix-multiplication-cpu.py:238:33
	imull	%esi, %ecx
	.loc	1 238 21 is_stmt 0              # 03-matrix-multiplication-cpu.py:238:21
	movslq	%ecx, %rcx
	leaq	(%rdx,%rcx,4), %rcx
	vmovaps	23680(%rsp), %zmm0              # 64-byte Reload
	.loc	1 243 21 is_stmt 1              # 03-matrix-multiplication-cpu.py:243:21
	vmovups	%zmm0, (%rcx,%rax,4)
	vmovaps	23616(%rsp), %zmm0              # 64-byte Reload
	vmovups	%zmm0, 64(%rcx,%rax,4)
	.loc	1 204 38                        # 03-matrix-multiplication-cpu.py:204:38
	movl	%edi, %ecx
	orl	$30, %ecx
	.loc	1 238 33                        # 03-matrix-multiplication-cpu.py:238:33
	imull	%esi, %ecx
	.loc	1 238 21 is_stmt 0              # 03-matrix-multiplication-cpu.py:238:21
	movslq	%ecx, %rcx
	leaq	(%rdx,%rcx,4), %rcx
	vmovaps	23808(%rsp), %zmm0              # 64-byte Reload
	.loc	1 243 21 is_stmt 1              # 03-matrix-multiplication-cpu.py:243:21
	vmovups	%zmm0, (%rcx,%rax,4)
	vmovaps	23744(%rsp), %zmm0              # 64-byte Reload
	vmovups	%zmm0, 64(%rcx,%rax,4)
	.loc	1 204 38                        # 03-matrix-multiplication-cpu.py:204:38
	orl	$31, %edi
	.loc	1 238 33                        # 03-matrix-multiplication-cpu.py:238:33
	imull	%esi, %edi
	.loc	1 238 21 is_stmt 0              # 03-matrix-multiplication-cpu.py:238:21
	movslq	%edi, %rcx
	leaq	(%rdx,%rcx,4), %rcx
	vmovaps	23936(%rsp), %zmm0              # 64-byte Reload
	.loc	1 243 21 is_stmt 1              # 03-matrix-multiplication-cpu.py:243:21
	vmovups	%zmm0, (%rcx,%rax,4)
	vmovaps	23872(%rsp), %zmm0              # 64-byte Reload
	vmovups	%zmm0, 64(%rcx,%rax,4)
	.loc	1 243 4 is_stmt 0               # 03-matrix-multiplication-cpu.py:243:4
	leaq	-40(%rbp), %rsp
	.loc	1 243 4 epilogue_begin          # 03-matrix-multiplication-cpu.py:243:4
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa %rsp, 8
	vzeroupper
	retq
.Ltmp7:
.Lfunc_end0:
	.size	matmul_kernel, .Lfunc_end0-matmul_kernel
	.cfi_endproc
                                        # -- End function
	.section	.debug_abbrev,"",@progbits
	.byte	1                               # Abbreviation Code
	.byte	17                              # DW_TAG_compile_unit
	.byte	1                               # DW_CHILDREN_yes
	.byte	37                              # DW_AT_producer
	.byte	14                              # DW_FORM_strp
	.byte	19                              # DW_AT_language
	.byte	5                               # DW_FORM_data2
	.byte	3                               # DW_AT_name
	.byte	14                              # DW_FORM_strp
	.byte	16                              # DW_AT_stmt_list
	.byte	23                              # DW_FORM_sec_offset
	.byte	27                              # DW_AT_comp_dir
	.byte	14                              # DW_FORM_strp
	.byte	17                              # DW_AT_low_pc
	.byte	1                               # DW_FORM_addr
	.byte	18                              # DW_AT_high_pc
	.byte	6                               # DW_FORM_data4
	.byte	0                               # EOM(1)
	.byte	0                               # EOM(2)
	.byte	2                               # Abbreviation Code
	.byte	46                              # DW_TAG_subprogram
	.byte	0                               # DW_CHILDREN_no
	.byte	3                               # DW_AT_name
	.byte	14                              # DW_FORM_strp
	.byte	32                              # DW_AT_inline
	.byte	11                              # DW_FORM_data1
	.byte	0                               # EOM(1)
	.byte	0                               # EOM(2)
	.byte	3                               # Abbreviation Code
	.byte	46                              # DW_TAG_subprogram
	.byte	1                               # DW_CHILDREN_yes
	.byte	17                              # DW_AT_low_pc
	.byte	1                               # DW_FORM_addr
	.byte	18                              # DW_AT_high_pc
	.byte	6                               # DW_FORM_data4
	.byte	49                              # DW_AT_abstract_origin
	.byte	19                              # DW_FORM_ref4
	.byte	0                               # EOM(1)
	.byte	0                               # EOM(2)
	.byte	4                               # Abbreviation Code
	.byte	29                              # DW_TAG_inlined_subroutine
	.byte	0                               # DW_CHILDREN_no
	.byte	49                              # DW_AT_abstract_origin
	.byte	19                              # DW_FORM_ref4
	.byte	17                              # DW_AT_low_pc
	.byte	1                               # DW_FORM_addr
	.byte	18                              # DW_AT_high_pc
	.byte	6                               # DW_FORM_data4
	.byte	88                              # DW_AT_call_file
	.byte	11                              # DW_FORM_data1
	.byte	89                              # DW_AT_call_line
	.byte	11                              # DW_FORM_data1
	.byte	87                              # DW_AT_call_column
	.byte	11                              # DW_FORM_data1
	.byte	0                               # EOM(1)
	.byte	0                               # EOM(2)
	.byte	0                               # EOM(3)
	.section	.debug_info,"",@progbits
.Lcu_begin0:
	.long	.Ldebug_info_end0-.Ldebug_info_start0 # Length of Unit
.Ldebug_info_start0:
	.short	4                               # DWARF version number
	.long	.debug_abbrev                   # Offset Into Abbrev. Section
	.byte	8                               # Address Size (in bytes)
	.byte	1                               # Abbrev [1] 0xb:0x74 DW_TAG_compile_unit
	.long	.Linfo_string0                  # DW_AT_producer
	.short	2                               # DW_AT_language
	.long	.Linfo_string1                  # DW_AT_name
	.long	.Lline_table_start0             # DW_AT_stmt_list
	.long	.Linfo_string2                  # DW_AT_comp_dir
	.quad	.Lfunc_begin0                   # DW_AT_low_pc
	.long	.Lfunc_end0-.Lfunc_begin0       # DW_AT_high_pc
	.byte	2                               # Abbrev [2] 0x2a:0x6 DW_TAG_subprogram
	.long	.Linfo_string3                  # DW_AT_name
	.byte	1                               # DW_AT_inline
	.byte	3                               # Abbrev [3] 0x30:0x4e DW_TAG_subprogram
	.quad	.Lfunc_begin0                   # DW_AT_low_pc
	.long	.Lfunc_end0-.Lfunc_begin0       # DW_AT_high_pc
	.long	42                              # DW_AT_abstract_origin
	.byte	4                               # Abbrev [4] 0x41:0x14 DW_TAG_inlined_subroutine
	.long	42                              # DW_AT_abstract_origin
	.quad	.Ltmp0                          # DW_AT_low_pc
	.long	.Ltmp1-.Ltmp0                   # DW_AT_high_pc
	.byte	1                               # DW_AT_call_file
	.byte	188                             # DW_AT_call_line
	.byte	27                              # DW_AT_call_column
	.byte	4                               # Abbrev [4] 0x55:0x14 DW_TAG_inlined_subroutine
	.long	42                              # DW_AT_abstract_origin
	.quad	.Ltmp1                          # DW_AT_low_pc
	.long	.Ltmp2-.Ltmp1                   # DW_AT_high_pc
	.byte	1                               # DW_AT_call_file
	.byte	189                             # DW_AT_call_line
	.byte	27                              # DW_AT_call_column
	.byte	4                               # Abbrev [4] 0x69:0x14 DW_TAG_inlined_subroutine
	.long	42                              # DW_AT_abstract_origin
	.quad	.Ltmp3                          # DW_AT_low_pc
	.long	.Ltmp4-.Ltmp3                   # DW_AT_high_pc
	.byte	1                               # DW_AT_call_file
	.byte	216                             # DW_AT_call_line
	.byte	33                              # DW_AT_call_column
	.byte	0                               # End Of Children Mark
	.byte	0                               # End Of Children Mark
.Ldebug_info_end0:
	.section	.debug_str,"MS",@progbits,1
.Linfo_string0:
	.asciz	"triton"                        # string offset=0
.Linfo_string1:
	.asciz	"03-matrix-multiplication-cpu.py" # string offset=7
.Linfo_string2:
	.asciz	"/root/triton/triton-cpu/./python/tutorials" # string offset=39
.Linfo_string3:
	.asciz	"matmul_kernel"                 # string offset=82
	.section	".note.GNU-stack","",@progbits
	.section	.debug_line,"",@progbits
.Lline_table_start0:
