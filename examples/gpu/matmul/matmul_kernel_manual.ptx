//
// Generated by LLVM NVPTX Back-End
//

.version 6.0
.target sm_30
.address_size 64

	// .globl	matmul_kernel           // -- Begin function matmul_kernel
.extern .shared .align 16 .b8 global_smem[];
                                        // @matmul_kernel
.visible .entry matmul_kernel(
	.param .u64 .ptr .global .align 1 matmul_kernel_param_0,
	.param .u64 .ptr .global .align 1 matmul_kernel_param_1,
	.param .u64 .ptr .global .align 1 matmul_kernel_param_2,
	.param .u32 matmul_kernel_param_3,
	.param .u32 matmul_kernel_param_4,
	.param .u32 matmul_kernel_param_5,
	.param .u32 matmul_kernel_param_6,
	.param .u32 matmul_kernel_param_7,
	.param .u32 matmul_kernel_param_8,
	.param .u64 .ptr .global .align 1 matmul_kernel_param_9
)
.reqntid 64, 1, 1
{
	.reg .pred 	%p<60>;
	.reg .b16 	%rs<385>;
	.reg .b32 	%r<682>;
	.reg .f32 	%f<257>;
	.reg .b64 	%rd<373>;

// %bb.0:
	ld.param.u32 	%r26, [matmul_kernel_param_8];
	ld.param.u32 	%r25, [matmul_kernel_param_4];
	ld.param.u32 	%r24, [matmul_kernel_param_3];
	ld.param.u64 	%rd149, [matmul_kernel_param_2];
	ld.param.u64 	%rd162, [matmul_kernel_param_0];
	ld.param.u64 	%rd163, [matmul_kernel_param_1];
	// begin inline asm
	mov.u32 %r27, %ctaid.x;
	// end inline asm
	add.s32 	%r122, %r24, 31;
	shr.s32 	%r123, %r122, 31;
	shr.u32 	%r124, %r123, 27;
	add.s32 	%r125, %r122, %r124;
	shr.s32 	%r126, %r125, 5;
	add.s32 	%r127, %r25, 63;
	shr.s32 	%r128, %r127, 31;
	shr.u32 	%r129, %r128, 26;
	add.s32 	%r130, %r127, %r129;
	shr.s32 	%r131, %r130, 6;
	ld.param.u32 	%r132, [matmul_kernel_param_5];
	shl.b32 	%r133, %r131, 3;
	ld.param.u32 	%r134, [matmul_kernel_param_6];
	div.s32 	%r135, %r27, %r133;
	ld.param.u32 	%r136, [matmul_kernel_param_7];
	shl.b32 	%r137, %r135, 3;
	sub.s32 	%r138, %r126, %r137;
	min.s32 	%r139, %r138, 8;
	mul.lo.s32 	%r140, %r135, %r133;
	sub.s32 	%r141, %r27, %r140;
	div.s32 	%r142, %r141, %r139;
	mul.lo.s32 	%r143, %r142, %r139;
	sub.s32 	%r144, %r141, %r143;
	add.s32 	%r145, %r137, %r144;
	shl.b32 	%r146, %r145, 5;
	mov.u32 	%r147, %tid.x;
	and.b32  	%r148, %r147, 31;
	and.b32  	%r1, %r147, 2;
	and.b32  	%r2, %r147, 4;
	shl.b32 	%r149, %r148, 3;
	and.b32  	%r3, %r147, 8;
	shl.b32 	%r150, %r147, 1;
	and.b32  	%r4, %r147, 16;
	bfe.u32 	%r151, %r147, 1, 4;
	bfe.u32 	%r5, %r147, 5, 1;
	neg.s32 	%r152, %r5;
	and.b32  	%r153, %r152, 16;
	bfe.u32 	%r154, %r147, 3, 2;
	bfe.u32 	%r155, %r148, 3, 1;
	and.b32  	%r156, %r154, 2;
	or.b32  	%r157, %r155, %r156;
	and.b32  	%r158, %r152, 4;
	or.b32  	%r159, %r157, %r158;
	or.b32  	%r160, %r151, %r153;
	or.b32  	%r161, %r146, %r160;
	or.b32  	%r6, %r146, %r159;
	or.b32  	%r7, %r6, 8;
	or.b32  	%r8, %r6, 16;
	or.b32  	%r9, %r6, 24;
	rem.s32 	%r162, %r161, %r24;
	shl.b32 	%r163, %r142, 6;
	and.b32  	%r10, %r147, 1;
	neg.s32 	%r164, %r10;
	and.b32  	%r165, %r164, 8;
	and.b32  	%r166, %r149, 16;
	or.b32  	%r167, %r165, %r166;
	and.b32  	%r168, %r149, 32;
	or.b32  	%r11, %r167, %r168;
	or.b32  	%r169, %r163, %r160;
	or.b32  	%r170, %r169, 32;
	or.b32  	%r12, %r163, %r11;
	rem.s32 	%r171, %r169, %r25;
	rem.s32 	%r172, %r170, %r25;
	mul.lo.s32 	%r173, %r162, %r134;
	and.b32  	%r13, %r164, 16;
	or.b32  	%r174, %r13, 1;
	or.b32  	%r175, %r13, 2;
	or.b32  	%r176, %r13, 3;
	or.b32  	%r177, %r13, 4;
	or.b32  	%r178, %r13, 5;
	or.b32  	%r179, %r13, 6;
	or.b32  	%r180, %r13, 7;
	or.b32  	%r181, %r13, 8;
	or.b32  	%r182, %r13, 9;
	or.b32  	%r183, %r13, 10;
	or.b32  	%r184, %r13, 11;
	or.b32  	%r185, %r13, 12;
	or.b32  	%r186, %r13, 13;
	or.b32  	%r187, %r13, 14;
	or.b32  	%r188, %r13, 15;
	add.s32 	%r189, %r173, %r13;
	add.s32 	%r190, %r173, %r174;
	add.s32 	%r191, %r173, %r175;
	add.s32 	%r192, %r173, %r176;
	add.s32 	%r193, %r173, %r177;
	add.s32 	%r194, %r173, %r178;
	add.s32 	%r195, %r173, %r179;
	add.s32 	%r196, %r173, %r180;
	add.s32 	%r197, %r173, %r181;
	add.s32 	%r198, %r173, %r182;
	add.s32 	%r199, %r173, %r183;
	add.s32 	%r200, %r173, %r184;
	add.s32 	%r201, %r173, %r185;
	add.s32 	%r202, %r173, %r186;
	add.s32 	%r203, %r173, %r187;
	add.s32 	%r204, %r173, %r188;
	cvt.s64.s32 	%rd164, %r189;
	add.s64 	%rd150, %rd162, %rd164;
	cvt.s64.s32 	%rd165, %r190;
	add.s64 	%rd166, %rd162, %rd165;
	cvt.s64.s32 	%rd167, %r191;
	add.s64 	%rd168, %rd162, %rd167;
	cvt.s64.s32 	%rd169, %r192;
	add.s64 	%rd170, %rd162, %rd169;
	cvt.s64.s32 	%rd171, %r193;
	add.s64 	%rd172, %rd162, %rd171;
	cvt.s64.s32 	%rd173, %r194;
	add.s64 	%rd174, %rd162, %rd173;
	cvt.s64.s32 	%rd175, %r195;
	add.s64 	%rd176, %rd162, %rd175;
	cvt.s64.s32 	%rd177, %r196;
	add.s64 	%rd178, %rd162, %rd177;
	cvt.s64.s32 	%rd179, %r197;
	add.s64 	%rd180, %rd162, %rd179;
	cvt.s64.s32 	%rd181, %r198;
	add.s64 	%rd182, %rd162, %rd181;
	cvt.s64.s32 	%rd183, %r199;
	add.s64 	%rd184, %rd162, %rd183;
	cvt.s64.s32 	%rd185, %r200;
	add.s64 	%rd186, %rd162, %rd185;
	cvt.s64.s32 	%rd187, %r201;
	add.s64 	%rd188, %rd162, %rd187;
	cvt.s64.s32 	%rd189, %r202;
	add.s64 	%rd190, %rd162, %rd189;
	cvt.s64.s32 	%rd191, %r203;
	add.s64 	%rd192, %rd162, %rd191;
	cvt.s64.s32 	%rd193, %r204;
	add.s64 	%rd194, %rd162, %rd193;
	mul.lo.s32 	%r205, %r171, %r136;
	mul.lo.s32 	%r206, %r172, %r136;
	add.s32 	%r207, %r13, %r205;
	add.s32 	%r208, %r174, %r205;
	add.s32 	%r209, %r175, %r205;
	add.s32 	%r210, %r176, %r205;
	add.s32 	%r211, %r177, %r205;
	add.s32 	%r212, %r178, %r205;
	add.s32 	%r213, %r179, %r205;
	add.s32 	%r214, %r180, %r205;
	add.s32 	%r215, %r181, %r205;
	add.s32 	%r216, %r182, %r205;
	add.s32 	%r217, %r183, %r205;
	add.s32 	%r218, %r184, %r205;
	add.s32 	%r219, %r185, %r205;
	add.s32 	%r220, %r186, %r205;
	add.s32 	%r221, %r187, %r205;
	add.s32 	%r222, %r188, %r205;
	add.s32 	%r223, %r13, %r206;
	add.s32 	%r224, %r174, %r206;
	add.s32 	%r225, %r175, %r206;
	add.s32 	%r226, %r176, %r206;
	add.s32 	%r227, %r177, %r206;
	add.s32 	%r228, %r178, %r206;
	add.s32 	%r229, %r179, %r206;
	add.s32 	%r230, %r180, %r206;
	add.s32 	%r231, %r181, %r206;
	add.s32 	%r232, %r182, %r206;
	add.s32 	%r233, %r183, %r206;
	add.s32 	%r234, %r184, %r206;
	add.s32 	%r235, %r185, %r206;
	add.s32 	%r236, %r186, %r206;
	add.s32 	%r237, %r187, %r206;
	add.s32 	%r238, %r188, %r206;
	cvt.s64.s32 	%rd195, %r207;
	add.s64 	%rd151, %rd163, %rd195;
	cvt.s64.s32 	%rd196, %r208;
	add.s64 	%rd197, %rd163, %rd196;
	cvt.s64.s32 	%rd198, %r209;
	add.s64 	%rd199, %rd163, %rd198;
	cvt.s64.s32 	%rd200, %r210;
	add.s64 	%rd201, %rd163, %rd200;
	cvt.s64.s32 	%rd202, %r211;
	add.s64 	%rd203, %rd163, %rd202;
	cvt.s64.s32 	%rd204, %r212;
	add.s64 	%rd205, %rd163, %rd204;
	cvt.s64.s32 	%rd206, %r213;
	add.s64 	%rd207, %rd163, %rd206;
	cvt.s64.s32 	%rd208, %r214;
	add.s64 	%rd209, %rd163, %rd208;
	cvt.s64.s32 	%rd210, %r215;
	add.s64 	%rd211, %rd163, %rd210;
	cvt.s64.s32 	%rd212, %r216;
	add.s64 	%rd213, %rd163, %rd212;
	cvt.s64.s32 	%rd214, %r217;
	add.s64 	%rd215, %rd163, %rd214;
	cvt.s64.s32 	%rd216, %r218;
	add.s64 	%rd217, %rd163, %rd216;
	cvt.s64.s32 	%rd218, %r219;
	add.s64 	%rd219, %rd163, %rd218;
	cvt.s64.s32 	%rd220, %r220;
	add.s64 	%rd221, %rd163, %rd220;
	cvt.s64.s32 	%rd222, %r221;
	add.s64 	%rd223, %rd163, %rd222;
	cvt.s64.s32 	%rd224, %r222;
	add.s64 	%rd225, %rd163, %rd224;
	cvt.s64.s32 	%rd226, %r223;
	add.s64 	%rd152, %rd163, %rd226;
	cvt.s64.s32 	%rd227, %r224;
	add.s64 	%rd228, %rd163, %rd227;
	cvt.s64.s32 	%rd229, %r225;
	add.s64 	%rd230, %rd163, %rd229;
	cvt.s64.s32 	%rd231, %r226;
	add.s64 	%rd232, %rd163, %rd231;
	cvt.s64.s32 	%rd233, %r227;
	add.s64 	%rd234, %rd163, %rd233;
	cvt.s64.s32 	%rd235, %r228;
	add.s64 	%rd236, %rd163, %rd235;
	cvt.s64.s32 	%rd237, %r229;
	add.s64 	%rd238, %rd163, %rd237;
	cvt.s64.s32 	%rd239, %r230;
	add.s64 	%rd240, %rd163, %rd239;
	cvt.s64.s32 	%rd241, %r231;
	add.s64 	%rd242, %rd163, %rd241;
	cvt.s64.s32 	%rd243, %r232;
	add.s64 	%rd244, %rd163, %rd243;
	cvt.s64.s32 	%rd245, %r233;
	add.s64 	%rd246, %rd163, %rd245;
	cvt.s64.s32 	%rd247, %r234;
	add.s64 	%rd248, %rd163, %rd247;
	cvt.s64.s32 	%rd249, %r235;
	add.s64 	%rd250, %rd163, %rd249;
	cvt.s64.s32 	%rd251, %r236;
	add.s64 	%rd252, %rd163, %rd251;
	cvt.s64.s32 	%rd253, %r237;
	add.s64 	%rd254, %rd163, %rd253;
	cvt.s64.s32 	%rd255, %r238;
	add.s64 	%rd256, %rd163, %rd255;
	add.s32 	%r239, %r132, 31;
	shr.s32 	%r240, %r239, 31;
	shr.u32 	%r241, %r240, 27;
	add.s32 	%r242, %r239, %r241;
	shr.s32 	%r14, %r242, 5;
	setp.gt.s32 	%p13, %r14, 0;
	setp.lt.s32 	%p14, %r13, %r132;
	and.b32  	%r243, %r150, 16;
	xor.b32  	%r244, %r13, %r243;
	shl.b32 	%r245, %r160, 5;
	or.b32  	%r246, %r245, %r244;
	cvt.u64.u32 	%rd1, %r246;
	mov.u64 	%rd257, global_smem;
	add.s64 	%rd258, %rd257, %rd1;
	selp.b32 	%r247, 16, 0, %p14;
	selp.b32 	%r31, %r247, 0, %p13;
	cvt.u32.u64 	%r28, %rd258;
	mov.pred 	%p22, -1;
	// begin inline asm
	@%p22 cp.async.cg.shared.global [ %r28 + 0 ], [ %rd150 + 0 ], 0x10, %r31;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	add.s64 	%rd259, %rd257, 4096;
	add.s64 	%rd260, %rd259, %rd1;
	or.b32  	%r248, %r246, 1024;
	cvt.u64.u32 	%rd2, %r248;
	add.s64 	%rd261, %rd259, %rd2;
	cvt.u32.u64 	%r30, %rd260;
	// begin inline asm
	@%p22 cp.async.cg.shared.global [ %r30 + 0 ], [ %rd151 + 0 ], 0x10, %r31;
	// end inline asm
	cvt.u32.u64 	%r32, %rd261;
	// begin inline asm
	@%p22 cp.async.cg.shared.global [ %r32 + 0 ], [ %rd152 + 0 ], 0x10, %r31;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	setp.gt.s32 	%p15, %r14, 1;
	add.s64 	%rd153, %rd150, 32;
	add.s64 	%rd154, %rd151, 32;
	add.s64 	%rd155, %rd152, 32;
	add.s32 	%r249, %r132, -32;
	setp.lt.s32 	%p16, %r13, %r249;
	bar.sync 	0;
	selp.b32 	%r250, 16, 0, %p16;
	selp.b32 	%r37, %r250, 0, %p15;
	add.s32 	%r34, %r28, 1024;
	// begin inline asm
	@%p22 cp.async.cg.shared.global [ %r34 + 0 ], [ %rd153 + 0 ], 0x10, %r37;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	add.s64 	%rd262, %rd257, 6144;
	add.s64 	%rd263, %rd262, %rd1;
	add.s64 	%rd264, %rd262, %rd2;
	cvt.u32.u64 	%r36, %rd263;
	// begin inline asm
	@%p22 cp.async.cg.shared.global [ %r36 + 0 ], [ %rd154 + 0 ], 0x10, %r37;
	// end inline asm
	cvt.u32.u64 	%r38, %rd264;
	// begin inline asm
	@%p22 cp.async.cg.shared.global [ %r38 + 0 ], [ %rd155 + 0 ], 0x10, %r37;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	setp.gt.s32 	%p17, %r14, 2;
	add.s64 	%rd156, %rd150, 64;
	add.s64 	%rd157, %rd151, 64;
	add.s64 	%rd158, %rd152, 64;
	add.s32 	%r251, %r132, -64;
	setp.lt.s32 	%p18, %r13, %r251;
	bar.sync 	0;
	selp.b32 	%r252, 16, 0, %p18;
	selp.b32 	%r43, %r252, 0, %p17;
	add.s32 	%r40, %r28, 2048;
	// begin inline asm
	@%p22 cp.async.cg.shared.global [ %r40 + 0 ], [ %rd156 + 0 ], 0x10, %r43;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	add.s64 	%rd265, %rd257, 8192;
	add.s64 	%rd266, %rd265, %rd1;
	add.s64 	%rd267, %rd265, %rd2;
	cvt.u32.u64 	%r42, %rd266;
	// begin inline asm
	@%p22 cp.async.cg.shared.global [ %r42 + 0 ], [ %rd157 + 0 ], 0x10, %r43;
	// end inline asm
	cvt.u32.u64 	%r44, %rd267;
	// begin inline asm
	@%p22 cp.async.cg.shared.global [ %r44 + 0 ], [ %rd158 + 0 ], 0x10, %r43;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	setp.gt.s32 	%p19, %r14, 3;
	add.s64 	%rd325, %rd150, 96;
	add.s64 	%rd326, %rd166, 96;
	add.s64 	%rd327, %rd168, 96;
	add.s64 	%rd328, %rd170, 96;
	add.s64 	%rd329, %rd172, 96;
	add.s64 	%rd330, %rd174, 96;
	add.s64 	%rd331, %rd176, 96;
	add.s64 	%rd332, %rd178, 96;
	add.s64 	%rd333, %rd180, 96;
	add.s64 	%rd334, %rd182, 96;
	add.s64 	%rd335, %rd184, 96;
	add.s64 	%rd336, %rd186, 96;
	add.s64 	%rd337, %rd188, 96;
	add.s64 	%rd338, %rd190, 96;
	add.s64 	%rd339, %rd192, 96;
	add.s64 	%rd340, %rd194, 96;
	add.s64 	%rd341, %rd151, 96;
	add.s64 	%rd342, %rd197, 96;
	add.s64 	%rd343, %rd199, 96;
	add.s64 	%rd344, %rd201, 96;
	add.s64 	%rd345, %rd203, 96;
	add.s64 	%rd346, %rd205, 96;
	add.s64 	%rd347, %rd207, 96;
	add.s64 	%rd348, %rd209, 96;
	add.s64 	%rd349, %rd211, 96;
	add.s64 	%rd350, %rd213, 96;
	add.s64 	%rd351, %rd215, 96;
	add.s64 	%rd352, %rd217, 96;
	add.s64 	%rd353, %rd219, 96;
	add.s64 	%rd354, %rd221, 96;
	add.s64 	%rd355, %rd223, 96;
	add.s64 	%rd356, %rd225, 96;
	add.s64 	%rd357, %rd152, 96;
	add.s64 	%rd358, %rd228, 96;
	add.s64 	%rd359, %rd230, 96;
	add.s64 	%rd360, %rd232, 96;
	add.s64 	%rd361, %rd234, 96;
	add.s64 	%rd362, %rd236, 96;
	add.s64 	%rd363, %rd238, 96;
	add.s64 	%rd364, %rd240, 96;
	add.s64 	%rd365, %rd242, 96;
	add.s64 	%rd366, %rd244, 96;
	add.s64 	%rd367, %rd246, 96;
	add.s64 	%rd368, %rd248, 96;
	add.s64 	%rd369, %rd250, 96;
	add.s64 	%rd370, %rd252, 96;
	add.s64 	%rd371, %rd254, 96;
	add.s64 	%rd372, %rd256, 96;
	add.s32 	%r253, %r132, -96;
	setp.lt.s32 	%p20, %r13, %r253;
	bar.sync 	0;
	selp.b32 	%r254, 16, 0, %p20;
	selp.b32 	%r49, %r254, 0, %p19;
	add.s32 	%r46, %r28, 3072;
	// begin inline asm
	@%p22 cp.async.cg.shared.global [ %r46 + 0 ], [ %rd325 + 0 ], 0x10, %r49;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	add.s64 	%rd268, %rd257, 10240;
	add.s64 	%rd269, %rd268, %rd1;
	add.s64 	%rd270, %rd268, %rd2;
	cvt.u32.u64 	%r48, %rd269;
	// begin inline asm
	@%p22 cp.async.cg.shared.global [ %r48 + 0 ], [ %rd341 + 0 ], 0x10, %r49;
	// end inline asm
	cvt.u32.u64 	%r50, %rd270;
	// begin inline asm
	@%p22 cp.async.cg.shared.global [ %r50 + 0 ], [ %rd357 + 0 ], 0x10, %r49;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	// begin inline asm
	cp.async.wait_group 0x6;
	// end inline asm
	bar.sync 	0;
	and.b32  	%r255, %r147, 7;
	bfe.u32 	%r256, %r147, 4, 1;
	bfe.u32 	%r257, %r147, 2, 1;
	xor.b32  	%r258, %r256, %r257;
	shl.b32 	%r259, %r255, 5;
	shl.b32 	%r260, %r155, 8;
	or.b32  	%r261, %r260, %r259;
	shl.b32 	%r262, %r258, 4;
	or.b32  	%r263, %r262, %r261;
	cvt.u64.u32 	%rd51, %r263;
	add.s64 	%rd271, %rd257, %rd51;
	cvt.u32.u64 	%r56, %rd271;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r64, %r67, %r70, %r73 }, [ %r56 + 0 ];
	// end inline asm
	add.s32 	%r61, %r56, 512;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r76, %r79, %r82, %r85 }, [ %r61 + 0 ];
	// end inline asm
	// begin inline asm
	{                           
prmt.b32 %r62, 0, %r64, 0x5140; 
	prmt.b32 %r63, 0, %r64, 0x7362; 
	}
	// end inline asm
	mov.b32 	{%rs321, %rs322}, %r62;
	mov.b32 	{%rs323, %rs324}, %r63;
	// begin inline asm
	{                           
prmt.b32 %r65, 0, %r67, 0x5140; 
	prmt.b32 %r66, 0, %r67, 0x7362; 
	}
	// end inline asm
	mov.b32 	{%rs325, %rs326}, %r65;
	mov.b32 	{%rs327, %rs328}, %r66;
	// begin inline asm
	{                           
prmt.b32 %r68, 0, %r70, 0x5140; 
	prmt.b32 %r69, 0, %r70, 0x7362; 
	}
	// end inline asm
	mov.b32 	{%rs329, %rs330}, %r68;
	mov.b32 	{%rs331, %rs332}, %r69;
	// begin inline asm
	{                           
prmt.b32 %r71, 0, %r73, 0x5140; 
	prmt.b32 %r72, 0, %r73, 0x7362; 
	}
	// end inline asm
	mov.b32 	{%rs333, %rs334}, %r71;
	mov.b32 	{%rs335, %rs336}, %r72;
	// begin inline asm
	{                           
prmt.b32 %r74, 0, %r76, 0x5140; 
	prmt.b32 %r75, 0, %r76, 0x7362; 
	}
	// end inline asm
	mov.b32 	{%rs337, %rs338}, %r74;
	mov.b32 	{%rs339, %rs340}, %r75;
	// begin inline asm
	{                           
prmt.b32 %r77, 0, %r79, 0x5140; 
	prmt.b32 %r78, 0, %r79, 0x7362; 
	}
	// end inline asm
	mov.b32 	{%rs341, %rs342}, %r77;
	mov.b32 	{%rs343, %rs344}, %r78;
	// begin inline asm
	{                           
prmt.b32 %r80, 0, %r82, 0x5140; 
	prmt.b32 %r81, 0, %r82, 0x7362; 
	}
	// end inline asm
	mov.b32 	{%rs345, %rs346}, %r80;
	mov.b32 	{%rs347, %rs348}, %r81;
	// begin inline asm
	{                           
prmt.b32 %r83, 0, %r85, 0x5140; 
	prmt.b32 %r84, 0, %r85, 0x7362; 
	}
	// end inline asm
	mov.b32 	{%rs349, %rs350}, %r83;
	mov.b32 	{%rs351, %rs352}, %r84;
	or.b32  	%r280, %r5, %r156;
	xor.b32  	%r281, %r155, %r257;
	shl.b32 	%r282, %r280, 8;
	or.b32  	%r283, %r282, %r259;
	shl.b32 	%r284, %r281, 4;
	or.b32  	%r285, %r284, %r283;
	cvt.u64.u32 	%rd52, %r285;
	add.s64 	%rd272, %rd259, %rd52;
	cvt.u32.u64 	%r90, %rd272;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r98, %r101, %r104, %r107 }, [ %r90 + 0 ];
	// end inline asm
	add.s32 	%r95, %r90, 1024;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r110, %r113, %r116, %r119 }, [ %r95 + 0 ];
	// end inline asm
	// begin inline asm
	{                           
prmt.b32 %r96, 0, %r98, 0x5140; 
	prmt.b32 %r97, 0, %r98, 0x7362; 
	}
	// end inline asm
	mov.b32 	{%rs353, %rs354}, %r96;
	mov.b32 	{%rs355, %rs356}, %r97;
	// begin inline asm
	{                           
prmt.b32 %r99, 0, %r101, 0x5140; 
	prmt.b32 %r100, 0, %r101, 0x7362; 
	}
	// end inline asm
	mov.b32 	{%rs357, %rs358}, %r99;
	mov.b32 	{%rs359, %rs360}, %r100;
	// begin inline asm
	{                           
prmt.b32 %r102, 0, %r104, 0x5140; 
	prmt.b32 %r103, 0, %r104, 0x7362; 
	}
	// end inline asm
	mov.b32 	{%rs361, %rs362}, %r102;
	mov.b32 	{%rs363, %rs364}, %r103;
	// begin inline asm
	{                           
prmt.b32 %r105, 0, %r107, 0x5140; 
	prmt.b32 %r106, 0, %r107, 0x7362; 
	}
	// end inline asm
	mov.b32 	{%rs365, %rs366}, %r105;
	mov.b32 	{%rs367, %rs368}, %r106;
	// begin inline asm
	{                           
prmt.b32 %r108, 0, %r110, 0x5140; 
	prmt.b32 %r109, 0, %r110, 0x7362; 
	}
	// end inline asm
	mov.b32 	{%rs369, %rs370}, %r108;
	mov.b32 	{%rs371, %rs372}, %r109;
	// begin inline asm
	{                           
prmt.b32 %r111, 0, %r113, 0x5140; 
	prmt.b32 %r112, 0, %r113, 0x7362; 
	}
	// end inline asm
	mov.b32 	{%rs373, %rs374}, %r111;
	mov.b32 	{%rs375, %rs376}, %r112;
	// begin inline asm
	{                           
prmt.b32 %r114, 0, %r116, 0x5140; 
	prmt.b32 %r115, 0, %r116, 0x7362; 
	}
	// end inline asm
	mov.b32 	{%rs377, %rs378}, %r114;
	mov.b32 	{%rs379, %rs380}, %r115;
	// begin inline asm
	{                           
prmt.b32 %r117, 0, %r119, 0x5140; 
	prmt.b32 %r118, 0, %r119, 0x7362; 
	}
	// end inline asm
	mov.b32 	{%rs381, %rs382}, %r117;
	mov.b32 	{%rs383, %rs384}, %r118;
	add.s32 	%r678, %r132, -128;
	mov.f32 	%f225, 0f00000000;
	mov.b32 	%r680, 3;
	mov.b32 	%r679, 0;
	mov.f32 	%f226, %f225;
	mov.f32 	%f227, %f225;
	mov.f32 	%f228, %f225;
	mov.f32 	%f229, %f225;
	mov.f32 	%f230, %f225;
	mov.f32 	%f231, %f225;
	mov.f32 	%f232, %f225;
	mov.f32 	%f233, %f225;
	mov.f32 	%f234, %f225;
	mov.f32 	%f235, %f225;
	mov.f32 	%f236, %f225;
	mov.f32 	%f237, %f225;
	mov.f32 	%f238, %f225;
	mov.f32 	%f239, %f225;
	mov.f32 	%f240, %f225;
	mov.f32 	%f241, %f225;
	mov.f32 	%f242, %f225;
	mov.f32 	%f243, %f225;
	mov.f32 	%f244, %f225;
	mov.f32 	%f245, %f225;
	mov.f32 	%f246, %f225;
	mov.f32 	%f247, %f225;
	mov.f32 	%f248, %f225;
	mov.f32 	%f249, %f225;
	mov.f32 	%f250, %f225;
	mov.f32 	%f251, %f225;
	mov.f32 	%f252, %f225;
	mov.f32 	%f253, %f225;
	mov.f32 	%f254, %f225;
	mov.f32 	%f255, %f225;
	mov.f32 	%f256, %f225;
	mov.u32 	%r681, %r679;
$L__BB0_1:                              // =>This Inner Loop Header: Depth=1
	setp.ge.s32 	%p21, %r679, %r14;
	@%p21 bra 	$L__BB0_3;
// %bb.2:                               //   in Loop: Header=BB0_1 Depth=1
	add.s32 	%r606, %r14, -4;
	setp.lt.s32 	%p56, %r679, %r606;
	add.s64 	%rd325, %rd325, 32;
	add.s64 	%rd326, %rd326, 32;
	add.s64 	%rd327, %rd327, 32;
	add.s64 	%rd328, %rd328, 32;
	add.s64 	%rd329, %rd329, 32;
	add.s64 	%rd330, %rd330, 32;
	add.s64 	%rd331, %rd331, 32;
	add.s64 	%rd332, %rd332, 32;
	add.s64 	%rd333, %rd333, 32;
	add.s64 	%rd334, %rd334, 32;
	add.s64 	%rd335, %rd335, 32;
	add.s64 	%rd336, %rd336, 32;
	add.s64 	%rd337, %rd337, 32;
	add.s64 	%rd338, %rd338, 32;
	add.s64 	%rd339, %rd339, 32;
	add.s64 	%rd340, %rd340, 32;
	add.s64 	%rd341, %rd341, 32;
	add.s64 	%rd342, %rd342, 32;
	add.s64 	%rd343, %rd343, 32;
	add.s64 	%rd344, %rd344, 32;
	add.s64 	%rd345, %rd345, 32;
	add.s64 	%rd346, %rd346, 32;
	add.s64 	%rd347, %rd347, 32;
	add.s64 	%rd348, %rd348, 32;
	add.s64 	%rd349, %rd349, 32;
	add.s64 	%rd350, %rd350, 32;
	add.s64 	%rd351, %rd351, 32;
	add.s64 	%rd352, %rd352, 32;
	add.s64 	%rd353, %rd353, 32;
	add.s64 	%rd354, %rd354, 32;
	add.s64 	%rd355, %rd355, 32;
	add.s64 	%rd356, %rd356, 32;
	add.s64 	%rd357, %rd357, 32;
	add.s64 	%rd358, %rd358, 32;
	add.s64 	%rd359, %rd359, 32;
	add.s64 	%rd360, %rd360, 32;
	add.s64 	%rd361, %rd361, 32;
	add.s64 	%rd362, %rd362, 32;
	add.s64 	%rd363, %rd363, 32;
	add.s64 	%rd364, %rd364, 32;
	add.s64 	%rd365, %rd365, 32;
	add.s64 	%rd366, %rd366, 32;
	add.s64 	%rd367, %rd367, 32;
	add.s64 	%rd368, %rd368, 32;
	add.s64 	%rd369, %rd369, 32;
	add.s64 	%rd370, %rd370, 32;
	add.s64 	%rd371, %rd371, 32;
	add.s64 	%rd372, %rd372, 32;
	add.s32 	%r607, %r680, 1;
	setp.lt.s32 	%p57, %r607, 4;
	selp.b32 	%r680, %r607, 0, %p57;
	setp.lt.s32 	%p58, %r13, %r678;
	shl.b32 	%r608, %r680, 10;
	cvt.u64.u32 	%rd310, %r608;
	add.s64 	%rd312, %rd257, %rd310;
	bar.sync 	0;
	add.s64 	%rd313, %rd312, %rd1;
	selp.b32 	%r609, 16, 0, %p58;
	selp.b32 	%r439, %r609, 0, %p56;
	cvt.u32.u64 	%r436, %rd313;
	// begin inline asm
	@%p22 cp.async.cg.shared.global [ %r436 + 0 ], [ %rd325 + 0 ], 0x10, %r439;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	add.s32 	%r610, %r608, %r608;
	cvt.s64.s32 	%rd314, %r610;
	add.s64 	%rd316, %rd259, %rd314;
	add.s64 	%rd317, %rd316, %rd1;
	add.s64 	%rd318, %rd316, %rd2;
	cvt.u32.u64 	%r438, %rd317;
	// begin inline asm
	@%p22 cp.async.cg.shared.global [ %r438 + 0 ], [ %rd341 + 0 ], 0x10, %r439;
	// end inline asm
	cvt.u32.u64 	%r440, %rd318;
	// begin inline asm
	@%p22 cp.async.cg.shared.global [ %r440 + 0 ], [ %rd357 + 0 ], 0x10, %r439;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	add.s32 	%r611, %r681, 1;
	setp.lt.s32 	%p59, %r611, 4;
	selp.b32 	%r681, %r611, 0, %p59;
	shl.b32 	%r612, %r681, 10;
	cvt.u64.u32 	%rd319, %r612;
	add.s64 	%rd320, %rd257, %rd319;
	// begin inline asm
	cp.async.wait_group 0x6;
	// end inline asm
	bar.sync 	0;
	add.s32 	%r613, %r612, %r612;
	cvt.u64.u32 	%rd321, %r613;
	add.s64 	%rd322, %rd259, %rd321;
	add.s64 	%rd323, %rd320, %rd51;
	cvt.u32.u64 	%r446, %rd323;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r454, %r457, %r460, %r463 }, [ %r446 + 0 ];
	// end inline asm
	add.s32 	%r451, %r446, 512;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r466, %r469, %r472, %r475 }, [ %r451 + 0 ];
	// end inline asm
	// begin inline asm
	{                           
prmt.b32 %r452, 0, %r454, 0x5140; 
	prmt.b32 %r453, 0, %r454, 0x7362; 
	}
	// end inline asm
	mov.b32 	{%rs129, %rs130}, %r452;
	mov.b32 	{%rs131, %rs132}, %r453;
	// begin inline asm
	{                           
prmt.b32 %r455, 0, %r457, 0x5140; 
	prmt.b32 %r456, 0, %r457, 0x7362; 
	}
	// end inline asm
	mov.b32 	{%rs133, %rs134}, %r455;
	mov.b32 	{%rs135, %rs136}, %r456;
	// begin inline asm
	{                           
prmt.b32 %r458, 0, %r460, 0x5140; 
	prmt.b32 %r459, 0, %r460, 0x7362; 
	}
	// end inline asm
	mov.b32 	{%rs137, %rs138}, %r458;
	mov.b32 	{%rs139, %rs140}, %r459;
	// begin inline asm
	{                           
prmt.b32 %r461, 0, %r463, 0x5140; 
	prmt.b32 %r462, 0, %r463, 0x7362; 
	}
	// end inline asm
	mov.b32 	{%rs141, %rs142}, %r461;
	mov.b32 	{%rs143, %rs144}, %r462;
	// begin inline asm
	{                           
prmt.b32 %r464, 0, %r466, 0x5140; 
	prmt.b32 %r465, 0, %r466, 0x7362; 
	}
	// end inline asm
	mov.b32 	{%rs145, %rs146}, %r464;
	mov.b32 	{%rs147, %rs148}, %r465;
	// begin inline asm
	{                           
prmt.b32 %r467, 0, %r469, 0x5140; 
	prmt.b32 %r468, 0, %r469, 0x7362; 
	}
	// end inline asm
	mov.b32 	{%rs149, %rs150}, %r467;
	mov.b32 	{%rs151, %rs152}, %r468;
	// begin inline asm
	{                           
prmt.b32 %r470, 0, %r472, 0x5140; 
	prmt.b32 %r471, 0, %r472, 0x7362; 
	}
	// end inline asm
	mov.b32 	{%rs153, %rs154}, %r470;
	mov.b32 	{%rs155, %rs156}, %r471;
	// begin inline asm
	{                           
prmt.b32 %r473, 0, %r475, 0x5140; 
	prmt.b32 %r474, 0, %r475, 0x7362; 
	}
	// end inline asm
	mov.b32 	{%rs157, %rs158}, %r473;
	mov.b32 	{%rs159, %rs160}, %r474;
	add.s64 	%rd324, %rd322, %rd52;
	cvt.u32.u64 	%r480, %rd324;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r488, %r491, %r494, %r497 }, [ %r480 + 0 ];
	// end inline asm
	add.s32 	%r485, %r480, 1024;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r500, %r503, %r506, %r509 }, [ %r485 + 0 ];
	// end inline asm
	// begin inline asm
	{                           
prmt.b32 %r486, 0, %r488, 0x5140; 
	prmt.b32 %r487, 0, %r488, 0x7362; 
	}
	// end inline asm
	mov.b32 	{%rs161, %rs162}, %r486;
	mov.b32 	{%rs163, %rs164}, %r487;
	// begin inline asm
	{                           
prmt.b32 %r489, 0, %r491, 0x5140; 
	prmt.b32 %r490, 0, %r491, 0x7362; 
	}
	// end inline asm
	mov.b32 	{%rs165, %rs166}, %r489;
	mov.b32 	{%rs167, %rs168}, %r490;
	// begin inline asm
	{                           
prmt.b32 %r492, 0, %r494, 0x5140; 
	prmt.b32 %r493, 0, %r494, 0x7362; 
	}
	// end inline asm
	mov.b32 	{%rs169, %rs170}, %r492;
	mov.b32 	{%rs171, %rs172}, %r493;
	// begin inline asm
	{                           
prmt.b32 %r495, 0, %r497, 0x5140; 
	prmt.b32 %r496, 0, %r497, 0x7362; 
	}
	// end inline asm
	mov.b32 	{%rs173, %rs174}, %r495;
	mov.b32 	{%rs175, %rs176}, %r496;
	// begin inline asm
	{                           
prmt.b32 %r498, 0, %r500, 0x5140; 
	prmt.b32 %r499, 0, %r500, 0x7362; 
	}
	// end inline asm
	mov.b32 	{%rs177, %rs178}, %r498;
	mov.b32 	{%rs179, %rs180}, %r499;
	// begin inline asm
	{                           
prmt.b32 %r501, 0, %r503, 0x5140; 
	prmt.b32 %r502, 0, %r503, 0x7362; 
	}
	// end inline asm
	mov.b32 	{%rs181, %rs182}, %r501;
	mov.b32 	{%rs183, %rs184}, %r502;
	// begin inline asm
	{                           
prmt.b32 %r504, 0, %r506, 0x5140; 
	prmt.b32 %r505, 0, %r506, 0x7362; 
	}
	// end inline asm
	mov.b32 	{%rs185, %rs186}, %r504;
	mov.b32 	{%rs187, %rs188}, %r505;
	// begin inline asm
	{                           
prmt.b32 %r507, 0, %r509, 0x5140; 
	prmt.b32 %r508, 0, %r509, 0x7362; 
	}
	// end inline asm
	mov.b32 	{%rs189, %rs190}, %r507;
	mov.b32 	{%rs191, %rs192}, %r508;
	mov.b32 	%r510, {%rs321, %rs322};
	mov.b32 	%r511, {%rs325, %rs326};
	mov.b32 	%r512, {%rs329, %rs330};
	mov.b32 	%r513, {%rs333, %rs334};
	mov.b32 	%r558, {%rs323, %rs324};
	mov.b32 	%r559, {%rs327, %rs328};
	mov.b32 	%r560, {%rs331, %rs332};
	mov.b32 	%r561, {%rs335, %rs336};
	mov.b32 	%r534, {%rs337, %rs338};
	mov.b32 	%r535, {%rs341, %rs342};
	mov.b32 	%r536, {%rs345, %rs346};
	mov.b32 	%r537, {%rs349, %rs350};
	mov.b32 	%r582, {%rs339, %rs340};
	mov.b32 	%r583, {%rs343, %rs344};
	mov.b32 	%r584, {%rs347, %rs348};
	mov.b32 	%r585, {%rs351, %rs352};
	mov.b32 	%r514, {%rs353, %rs354};
	mov.b32 	%r515, {%rs357, %rs358};
	mov.b32 	%r562, {%rs355, %rs356};
	mov.b32 	%r563, {%rs359, %rs360};
	mov.b32 	%r520, {%rs361, %rs362};
	mov.b32 	%r521, {%rs365, %rs366};
	mov.b32 	%r568, {%rs363, %rs364};
	mov.b32 	%r569, {%rs367, %rs368};
	mov.b32 	%r526, {%rs369, %rs370};
	mov.b32 	%r527, {%rs373, %rs374};
	mov.b32 	%r574, {%rs371, %rs372};
	mov.b32 	%r575, {%rs375, %rs376};
	mov.b32 	%r532, {%rs377, %rs378};
	mov.b32 	%r533, {%rs381, %rs382};
	mov.b32 	%r580, {%rs379, %rs380};
	mov.b32 	%r581, {%rs383, %rs384};
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f225, %f226, %f227, %f228 }, { %r510, %r511, %r512, %r513 }, { %r514, %r515 }, { %f225, %f226, %f227, %f228 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f229, %f230, %f231, %f232 }, { %r510, %r511, %r512, %r513 }, { %r520, %r521 }, { %f229, %f230, %f231, %f232 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f233, %f234, %f235, %f236 }, { %r510, %r511, %r512, %r513 }, { %r526, %r527 }, { %f233, %f234, %f235, %f236 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f237, %f238, %f239, %f240 }, { %r510, %r511, %r512, %r513 }, { %r532, %r533 }, { %f237, %f238, %f239, %f240 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f241, %f242, %f243, %f244 }, { %r534, %r535, %r536, %r537 }, { %r514, %r515 }, { %f241, %f242, %f243, %f244 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f245, %f246, %f247, %f248 }, { %r534, %r535, %r536, %r537 }, { %r520, %r521 }, { %f245, %f246, %f247, %f248 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f249, %f250, %f251, %f252 }, { %r534, %r535, %r536, %r537 }, { %r526, %r527 }, { %f249, %f250, %f251, %f252 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f253, %f254, %f255, %f256 }, { %r534, %r535, %r536, %r537 }, { %r532, %r533 }, { %f253, %f254, %f255, %f256 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f225, %f226, %f227, %f228 }, { %r558, %r559, %r560, %r561 }, { %r562, %r563 }, { %f225, %f226, %f227, %f228 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f229, %f230, %f231, %f232 }, { %r558, %r559, %r560, %r561 }, { %r568, %r569 }, { %f229, %f230, %f231, %f232 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f233, %f234, %f235, %f236 }, { %r558, %r559, %r560, %r561 }, { %r574, %r575 }, { %f233, %f234, %f235, %f236 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f237, %f238, %f239, %f240 }, { %r558, %r559, %r560, %r561 }, { %r580, %r581 }, { %f237, %f238, %f239, %f240 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f241, %f242, %f243, %f244 }, { %r582, %r583, %r584, %r585 }, { %r562, %r563 }, { %f241, %f242, %f243, %f244 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f245, %f246, %f247, %f248 }, { %r582, %r583, %r584, %r585 }, { %r568, %r569 }, { %f245, %f246, %f247, %f248 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f249, %f250, %f251, %f252 }, { %r582, %r583, %r584, %r585 }, { %r574, %r575 }, { %f249, %f250, %f251, %f252 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f253, %f254, %f255, %f256 }, { %r582, %r583, %r584, %r585 }, { %r580, %r581 }, { %f253, %f254, %f255, %f256 };
	// end inline asm
	add.s32 	%r679, %r679, 1;
	add.s32 	%r678, %r678, -32;
	mov.u16 	%rs321, %rs129;
	mov.u16 	%rs322, %rs130;
	mov.u16 	%rs323, %rs131;
	mov.u16 	%rs324, %rs132;
	mov.u16 	%rs325, %rs133;
	mov.u16 	%rs326, %rs134;
	mov.u16 	%rs327, %rs135;
	mov.u16 	%rs328, %rs136;
	mov.u16 	%rs329, %rs137;
	mov.u16 	%rs330, %rs138;
	mov.u16 	%rs331, %rs139;
	mov.u16 	%rs332, %rs140;
	mov.u16 	%rs333, %rs141;
	mov.u16 	%rs334, %rs142;
	mov.u16 	%rs335, %rs143;
	mov.u16 	%rs336, %rs144;
	mov.u16 	%rs337, %rs145;
	mov.u16 	%rs338, %rs146;
	mov.u16 	%rs339, %rs147;
	mov.u16 	%rs340, %rs148;
	mov.u16 	%rs341, %rs149;
	mov.u16 	%rs342, %rs150;
	mov.u16 	%rs343, %rs151;
	mov.u16 	%rs344, %rs152;
	mov.u16 	%rs345, %rs153;
	mov.u16 	%rs346, %rs154;
	mov.u16 	%rs347, %rs155;
	mov.u16 	%rs348, %rs156;
	mov.u16 	%rs349, %rs157;
	mov.u16 	%rs350, %rs158;
	mov.u16 	%rs351, %rs159;
	mov.u16 	%rs352, %rs160;
	mov.u16 	%rs353, %rs161;
	mov.u16 	%rs354, %rs162;
	mov.u16 	%rs355, %rs163;
	mov.u16 	%rs356, %rs164;
	mov.u16 	%rs357, %rs165;
	mov.u16 	%rs358, %rs166;
	mov.u16 	%rs359, %rs167;
	mov.u16 	%rs360, %rs168;
	mov.u16 	%rs361, %rs169;
	mov.u16 	%rs362, %rs170;
	mov.u16 	%rs363, %rs171;
	mov.u16 	%rs364, %rs172;
	mov.u16 	%rs365, %rs173;
	mov.u16 	%rs366, %rs174;
	mov.u16 	%rs367, %rs175;
	mov.u16 	%rs368, %rs176;
	mov.u16 	%rs369, %rs177;
	mov.u16 	%rs370, %rs178;
	mov.u16 	%rs371, %rs179;
	mov.u16 	%rs372, %rs180;
	mov.u16 	%rs373, %rs181;
	mov.u16 	%rs374, %rs182;
	mov.u16 	%rs375, %rs183;
	mov.u16 	%rs376, %rs184;
	mov.u16 	%rs377, %rs185;
	mov.u16 	%rs378, %rs186;
	mov.u16 	%rs379, %rs187;
	mov.u16 	%rs380, %rs188;
	mov.u16 	%rs381, %rs189;
	mov.u16 	%rs382, %rs190;
	mov.u16 	%rs383, %rs191;
	mov.u16 	%rs384, %rs192;
	bra.uni 	$L__BB0_1;
$L__BB0_3:
	setp.eq.s32 	%p42, %r10, 0;
	setp.eq.s32 	%p43, %r5, 0;
	setp.eq.s32 	%p44, %r4, 0;
	setp.eq.s32 	%p45, %r3, 0;
	setp.eq.s32 	%p46, %r2, 0;
	setp.eq.s32 	%p47, %r1, 0;
	// begin inline asm
	cp.async.wait_group 0x0;
	// end inline asm
	bar.sync 	0;
	cvt.rn.f16.f32 	%rs225, %f225;
	cvt.rn.f16.f32 	%rs226, %f226;
	cvt.rn.f16.f32 	%rs227, %f227;
	cvt.rn.f16.f32 	%rs228, %f228;
	cvt.rn.f16.f32 	%rs229, %f229;
	cvt.rn.f16.f32 	%rs230, %f230;
	cvt.rn.f16.f32 	%rs231, %f231;
	cvt.rn.f16.f32 	%rs232, %f232;
	cvt.rn.f16.f32 	%rs233, %f233;
	cvt.rn.f16.f32 	%rs234, %f234;
	cvt.rn.f16.f32 	%rs235, %f235;
	cvt.rn.f16.f32 	%rs236, %f236;
	cvt.rn.f16.f32 	%rs237, %f237;
	cvt.rn.f16.f32 	%rs238, %f238;
	cvt.rn.f16.f32 	%rs239, %f239;
	cvt.rn.f16.f32 	%rs240, %f240;
	cvt.rn.f16.f32 	%rs241, %f241;
	cvt.rn.f16.f32 	%rs242, %f242;
	cvt.rn.f16.f32 	%rs243, %f243;
	cvt.rn.f16.f32 	%rs244, %f244;
	cvt.rn.f16.f32 	%rs245, %f245;
	cvt.rn.f16.f32 	%rs246, %f246;
	cvt.rn.f16.f32 	%rs247, %f247;
	cvt.rn.f16.f32 	%rs248, %f248;
	cvt.rn.f16.f32 	%rs249, %f249;
	cvt.rn.f16.f32 	%rs250, %f250;
	cvt.rn.f16.f32 	%rs251, %f251;
	cvt.rn.f16.f32 	%rs252, %f252;
	cvt.rn.f16.f32 	%rs253, %f253;
	cvt.rn.f16.f32 	%rs254, %f254;
	cvt.rn.f16.f32 	%rs255, %f255;
	cvt.rn.f16.f32 	%rs256, %f256;
	mul.lo.s32 	%r334, %r26, %r6;
	mul.lo.s32 	%r335, %r26, %r7;
	mul.lo.s32 	%r336, %r26, %r8;
	mul.lo.s32 	%r337, %r26, %r9;
	mul.wide.s32 	%rd277, %r334, 2;
	add.s64 	%rd278, %rd149, %rd277;
	mul.wide.s32 	%rd279, %r335, 2;
	add.s64 	%rd280, %rd149, %rd279;
	mul.wide.s32 	%rd281, %r336, 2;
	add.s64 	%rd282, %rd149, %rd281;
	mul.wide.s32 	%rd283, %r337, 2;
	add.s64 	%rd284, %rd149, %rd283;
	mul.wide.s32 	%rd285, %r12, 2;
	add.s64 	%rd273, %rd278, %rd285;
	add.s64 	%rd274, %rd280, %rd285;
	add.s64 	%rd275, %rd282, %rd285;
	add.s64 	%rd276, %rd284, %rd285;
	setp.lt.s32 	%p48, %r6, %r24;
	setp.lt.s32 	%p49, %r7, %r24;
	setp.lt.s32 	%p50, %r8, %r24;
	setp.lt.s32 	%p51, %r9, %r24;
	setp.lt.s32 	%p52, %r12, %r25;
	and.pred  	%p38, %p48, %p52;
	and.pred  	%p39, %p49, %p52;
	and.pred  	%p40, %p50, %p52;
	and.pred  	%p41, %p51, %p52;
	selp.b32 	%r338, 0, 2, %p42;
	selp.b32 	%r339, 0, 4, %p47;
	or.b32  	%r340, %r338, %r339;
	selp.b32 	%r341, 0, 64, %p46;
	or.b32  	%r342, %r340, %r341;
	selp.b32 	%r343, 0, 128, %p45;
	or.b32  	%r344, %r342, %r343;
	selp.b32 	%r345, 0, 256, %p44;
	or.b32  	%r346, %r344, %r345;
	selp.b32 	%r347, 0, 8, %p43;
	or.b32  	%r348, %r346, %r347;
	selp.b32 	%r349, 0, 64, %p45;
	or.b32  	%r350, %r11, %r349;
	selp.b32 	%r351, 0, 128, %p44;
	or.b32  	%r352, %r350, %r351;
	selp.b32 	%r353, 0, 256, %p43;
	or.b32  	%r354, %r352, %r353;
	shr.u32 	%r355, %r346, 3;
	add.s32 	%r356, %r355, %r348;
	shl.b32 	%r357, %r356, 1;
	cvt.u64.u32 	%rd286, %r357;
	add.s64 	%rd288, %rd257, %rd286;
	cvt.u32.u64 	%r302, %rd288;
	// begin inline asm
	@%p22 st.shared.v2.b16 [ %r302 + 0 ], { %rs225, %rs226 };
	// end inline asm
	or.b32  	%r358, %r348, 512;
	shr.u32 	%r359, %r358, 3;
	and.b32  	%r360, %r359, 536870904;
	add.s32 	%r361, %r360, %r358;
	shl.b32 	%r362, %r361, 1;
	cvt.u64.u32 	%rd289, %r362;
	add.s64 	%rd290, %rd257, %rd289;
	cvt.u32.u64 	%r303, %rd290;
	// begin inline asm
	@%p22 st.shared.v2.b16 [ %r303 + 0 ], { %rs227, %rs228 };
	// end inline asm
	add.s32 	%r363, %r357, 32;
	cvt.u64.u32 	%rd291, %r363;
	add.s64 	%rd292, %rd257, %rd291;
	cvt.u32.u64 	%r304, %rd292;
	// begin inline asm
	@%p22 st.shared.v2.b16 [ %r304 + 0 ], { %rs229, %rs230 };
	// end inline asm
	or.b32  	%r364, %r348, 528;
	shr.u32 	%r365, %r364, 3;
	and.b32  	%r366, %r365, 536870904;
	add.s32 	%r367, %r366, %r364;
	shl.b32 	%r368, %r367, 1;
	cvt.u64.u32 	%rd293, %r368;
	add.s64 	%rd294, %rd257, %rd293;
	cvt.u32.u64 	%r305, %rd294;
	// begin inline asm
	@%p22 st.shared.v2.b16 [ %r305 + 0 ], { %rs231, %rs232 };
	// end inline asm
	add.s32 	%r369, %r357, 64;
	cvt.u64.u32 	%rd295, %r369;
	add.s64 	%rd296, %rd257, %rd295;
	cvt.u32.u64 	%r306, %rd296;
	// begin inline asm
	@%p22 st.shared.v2.b16 [ %r306 + 0 ], { %rs233, %rs234 };
	// end inline asm
	or.b32  	%r370, %r348, 544;
	shr.u32 	%r371, %r370, 3;
	and.b32  	%r372, %r371, 536870904;
	add.s32 	%r373, %r372, %r370;
	shl.b32 	%r374, %r373, 1;
	cvt.u64.u32 	%rd297, %r374;
	add.s64 	%rd298, %rd257, %rd297;
	cvt.u32.u64 	%r307, %rd298;
	// begin inline asm
	@%p22 st.shared.v2.b16 [ %r307 + 0 ], { %rs235, %rs236 };
	// end inline asm
	add.s32 	%r375, %r357, 96;
	cvt.u64.u32 	%rd299, %r375;
	add.s64 	%rd300, %rd257, %rd299;
	cvt.u32.u64 	%r308, %rd300;
	// begin inline asm
	@%p22 st.shared.v2.b16 [ %r308 + 0 ], { %rs237, %rs238 };
	// end inline asm
	or.b32  	%r376, %r348, 560;
	shr.u32 	%r377, %r376, 3;
	and.b32  	%r378, %r377, 536870904;
	add.s32 	%r379, %r378, %r376;
	shl.b32 	%r380, %r379, 1;
	cvt.u64.u32 	%rd301, %r380;
	add.s64 	%rd302, %rd257, %rd301;
	cvt.u32.u64 	%r309, %rd302;
	// begin inline asm
	@%p22 st.shared.v2.b16 [ %r309 + 0 ], { %rs239, %rs240 };
	// end inline asm
	bar.sync 	0;
	shr.u32 	%r381, %r354, 3;
	and.b32  	%r382, %r381, 56;
	add.s32 	%r383, %r382, %r354;
	mul.wide.s32 	%rd303, %r383, 2;
	add.s64 	%rd304, %rd257, %rd303;
	ld.shared.v4.u32 	{%r384, %r385, %r386, %r387}, [%rd304];
	mov.b32 	{%rs257, %rs258}, %r384;
	mov.b32 	{%rs259, %rs260}, %r385;
	mov.b32 	{%rs261, %rs262}, %r386;
	mov.b32 	{%rs263, %rs264}, %r387;
	or.b32  	%r392, %r354, 512;
	shr.u32 	%r393, %r392, 3;
	and.b32  	%r394, %r393, 120;
	add.s32 	%r395, %r394, %r392;
	mul.wide.s32 	%rd305, %r395, 2;
	add.s64 	%rd306, %rd257, %rd305;
	ld.shared.v4.u32 	{%r396, %r397, %r398, %r399}, [%rd306];
	mov.b32 	{%rs273, %rs274}, %r396;
	mov.b32 	{%rs275, %rs276}, %r397;
	mov.b32 	{%rs277, %rs278}, %r398;
	mov.b32 	{%rs279, %rs280}, %r399;
	bar.sync 	0;
	// begin inline asm
	@%p22 st.shared.v2.b16 [ %r302 + 0 ], { %rs241, %rs242 };
	// end inline asm
	// begin inline asm
	@%p22 st.shared.v2.b16 [ %r303 + 0 ], { %rs243, %rs244 };
	// end inline asm
	// begin inline asm
	@%p22 st.shared.v2.b16 [ %r304 + 0 ], { %rs245, %rs246 };
	// end inline asm
	// begin inline asm
	@%p22 st.shared.v2.b16 [ %r305 + 0 ], { %rs247, %rs248 };
	// end inline asm
	// begin inline asm
	@%p22 st.shared.v2.b16 [ %r306 + 0 ], { %rs249, %rs250 };
	// end inline asm
	// begin inline asm
	@%p22 st.shared.v2.b16 [ %r307 + 0 ], { %rs251, %rs252 };
	// end inline asm
	// begin inline asm
	@%p22 st.shared.v2.b16 [ %r308 + 0 ], { %rs253, %rs254 };
	// end inline asm
	// begin inline asm
	@%p22 st.shared.v2.b16 [ %r309 + 0 ], { %rs255, %rs256 };
	// end inline asm
	bar.sync 	0;
	ld.shared.v4.u32 	{%r404, %r405, %r406, %r407}, [%rd304];
	mov.b32 	{%rs289, %rs290}, %r404;
	mov.b32 	{%rs291, %rs292}, %r405;
	mov.b32 	{%rs293, %rs294}, %r406;
	mov.b32 	{%rs295, %rs296}, %r407;
	ld.shared.v4.u32 	{%r412, %r413, %r414, %r415}, [%rd306];
	mov.b32 	{%rs305, %rs306}, %r412;
	mov.b32 	{%rs307, %rs308}, %r413;
	mov.b32 	{%rs309, %rs310}, %r414;
	mov.b32 	{%rs311, %rs312}, %r415;
	mov.b32 	%r420, {%rs257, %rs258};
	mov.b32 	%r421, {%rs259, %rs260};
	mov.b32 	%r422, {%rs261, %rs262};
	mov.b32 	%r423, {%rs263, %rs264};
	// begin inline asm
	@%p38 st.global.v4.b32 [ %rd273 + 0 ], { %r420, %r421, %r422, %r423 };
	// end inline asm
	mov.b32 	%r424, {%rs273, %rs274};
	mov.b32 	%r425, {%rs275, %rs276};
	mov.b32 	%r426, {%rs277, %rs278};
	mov.b32 	%r427, {%rs279, %rs280};
	// begin inline asm
	@%p39 st.global.v4.b32 [ %rd274 + 0 ], { %r424, %r425, %r426, %r427 };
	// end inline asm
	mov.b32 	%r428, {%rs289, %rs290};
	mov.b32 	%r429, {%rs291, %rs292};
	mov.b32 	%r430, {%rs293, %rs294};
	mov.b32 	%r431, {%rs295, %rs296};
	// begin inline asm
	@%p40 st.global.v4.b32 [ %rd275 + 0 ], { %r428, %r429, %r430, %r431 };
	// end inline asm
	mov.b32 	%r432, {%rs305, %rs306};
	mov.b32 	%r433, {%rs307, %rs308};
	mov.b32 	%r434, {%rs309, %rs310};
	mov.b32 	%r435, {%rs311, %rs312};
	// begin inline asm
	@%p41 st.global.v4.b32 [ %rd276 + 0 ], { %r432, %r433, %r434, %r435 };
	// end inline asm
	ret;
                                        // -- End function
}
