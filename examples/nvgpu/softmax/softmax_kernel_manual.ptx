//
// Generated by LLVM NVPTX Back-End
//

.version 6.0
.target sm_30
.address_size 64

	// .globl	softmax_kernel          // -- Begin function softmax_kernel
.extern .shared .align 16 .b8 global_smem[];
                                        // @softmax_kernel
.visible .entry softmax_kernel(
	.param .u64 .ptr .global .align 1 softmax_kernel_param_0,
	.param .u64 .ptr .global .align 1 softmax_kernel_param_1,
	.param .u32 softmax_kernel_param_2,
	.param .u32 softmax_kernel_param_3,
	.param .u32 softmax_kernel_param_4,
	.param .u32 softmax_kernel_param_5,
	.param .u64 .ptr .global .align 1 softmax_kernel_param_6
)
.reqntid 256, 1, 1
{
	.reg .pred 	%p<15>;
	.reg .b32 	%r<106>;
	.reg .f32 	%f<41>;
	.reg .b64 	%rd<29>;

// %bb.0:
	ld.param.u32 	%r21, [softmax_kernel_param_5];
	ld.param.u32 	%r20, [softmax_kernel_param_4];
	ld.param.u64 	%rd3, [softmax_kernel_param_1];
	ld.param.u64 	%rd2, [softmax_kernel_param_0];
	// begin inline asm
	mov.u32 %r103, %ctaid.x;
	// end inline asm
	// begin inline asm
	mov.u32 %r23, %nctaid.x;
	// end inline asm
	ld.param.u32 	%r28, [softmax_kernel_param_2];
	mov.u32 	%r3, %tid.x;
	ld.param.u32 	%r29, [softmax_kernel_param_3];
	and.b32  	%r4, %r3, 31;
	shr.u32 	%r5, %r3, 5;
	bfe.u32 	%r30, %r3, 5, 1;
	neg.s32 	%r31, %r30;
	and.b32  	%r32, %r31, 32;
	or.b32  	%r33, %r4, %r32;
	and.b32  	%r34, %r3, 64;
	or.b32  	%r35, %r33, %r34;
	and.b32  	%r36, %r3, 128;
	or.b32  	%r37, %r35, %r36;
	setp.lt.s32 	%p2, %r37, %r21;
	setp.lt.s32 	%p3, %r103, %r20;
	mul.lo.s32 	%r38, %r103, %r28;
	mul.wide.s32 	%rd5, %r38, 4;
	add.s64 	%rd6, %rd3, %rd5;
	cvt.u64.u32 	%rd1, %r37;
	mul.wide.s32 	%rd7, %r37, 4;
	add.s64 	%rd4, %rd6, %rd7;
	mov.u64 	%rd8, global_smem;
	add.s64 	%rd9, %rd8, %rd7;
	selp.b32 	%r39, 4, 0, %p2;
	selp.b32 	%r25, %r39, 0, %p3;
	cvt.u32.u64 	%r24, %rd9;
	mov.pred 	%p1, -1;
	// begin inline asm
	@%p1 cp.async.ca.shared.global [ %r24 + 0 ], [ %rd4 + 0 ], 0x4, %r25;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	add.s32 	%r40, %r103, %r23;
	mul.lo.s32 	%r102, %r28, %r40;
	mul.lo.s32 	%r7, %r23, %r28;
	mul.lo.s32 	%r101, %r103, %r29;
	mul.lo.s32 	%r9, %r23, %r29;
	mov.b32 	%r105, -1;
	mov.b32 	%r104, 0;
	shl.b64 	%rd15, %rd1, 2;
	setp.eq.s32 	%p5, %r4, 0;
	and.b32  	%r74, %r5, 7;
	shl.b32 	%r75, %r74, 2;
	setp.lt.s32 	%p6, %r3, 8;
$L__BB0_1:                              // =>This Inner Loop Header: Depth=1
	setp.ge.s32 	%p4, %r103, %r20;
	@%p4 bra 	$L__BB0_3;
// %bb.2:                               //   in Loop: Header=BB0_1 Depth=1
	cvt.u32.u64 	%r59, %rd1;
	setp.lt.s32 	%p11, %r59, %r21;
	sub.s32 	%r60, %r20, %r23;
	setp.lt.s32 	%p13, %r103, %r60;
	add.s32 	%r61, %r105, 1;
	shr.s32 	%r62, %r61, 31;
	and.b32  	%r105, %r62, %r61;
	// begin inline asm
	cp.async.wait_group 0x0;
	// end inline asm
	bar.sync 	0;
	shl.b32 	%r63, %r105, 8;
	mul.wide.s32 	%rd12, %r63, 4;
	add.s64 	%rd14, %rd8, %rd12;
	add.s64 	%rd16, %rd14, %rd15;
	ld.shared.f32 	%f3, [%rd16];
	selp.f32 	%f4, %f3, 0fFF800000, %p11;
	mov.b32 	%r64, %f4;
	shfl.sync.bfly.b32	%r65, %r64, 16, 31, -1;
	mov.b32 	%f5, %r65;
	max.f32 	%f6, %f4, %f5;
	mov.b32 	%r66, %f6;
	shfl.sync.bfly.b32	%r67, %r66, 8, 31, -1;
	mov.b32 	%f7, %r67;
	max.f32 	%f8, %f6, %f7;
	mov.b32 	%r68, %f8;
	shfl.sync.bfly.b32	%r69, %r68, 4, 31, -1;
	mov.b32 	%f9, %r69;
	max.f32 	%f10, %f8, %f9;
	mov.b32 	%r70, %f10;
	shfl.sync.bfly.b32	%r71, %r70, 2, 31, -1;
	mov.b32 	%f11, %r71;
	max.f32 	%f12, %f10, %f11;
	mov.b32 	%r72, %f12;
	shfl.sync.bfly.b32	%r73, %r72, 1, 31, -1;
	mov.b32 	%f13, %r73;
	max.f32 	%f14, %f12, %f13;
	add.s64 	%rd17, %rd8, 1024;
	cvt.u64.u32 	%rd18, %r75;
	add.s64 	%rd19, %rd17, %rd18;
	mov.b32 	%r42, %f14;
	cvt.u32.u64 	%r41, %rd19;
	// begin inline asm
	@%p5 st.shared.b32 [ %r41 + 0 ], %r42;
	// end inline asm
	bar.sync 	0;
	shl.b32 	%r76, %r3, 2;
	cvt.u64.u32 	%rd20, %r76;
	add.s64 	%rd21, %rd17, %rd20;
	cvt.u32.u64 	%r44, %rd21;
	// begin inline asm
	@%p6 ld.shared.b32 %r43, [ %r44 + 0 ];
	// end inline asm
	mov.b32 	%f15, %r43;
	shfl.sync.bfly.b32	%r77, %r43, 4, 31, -1;
	mov.b32 	%f16, %r77;
	max.f32 	%f17, %f15, %f16;
	mov.b32 	%r78, %f17;
	shfl.sync.bfly.b32	%r79, %r78, 2, 31, -1;
	mov.b32 	%f18, %r79;
	max.f32 	%f19, %f17, %f18;
	mov.b32 	%r80, %f19;
	shfl.sync.bfly.b32	%r81, %r80, 1, 31, -1;
	mov.b32 	%f20, %r81;
	max.f32 	%f21, %f19, %f20;
	and.b32  	%r82, %r4, 7;
	setp.eq.s32 	%p14, %r82, 0;
	and.pred  	%p7, %p6, %p14;
	mov.b32 	%r46, %f21;
	// begin inline asm
	@%p7 st.shared.b32 [ %r44 + 0 ], %r46;
	// end inline asm
	bar.sync 	0;
	ld.shared.f32 	%f22, [global_smem+1024];
	sub.rn.f32 	%f23, %f4, %f22;
	mul.rn.f32 	%f2, %f23, 0f3FB8AA3B;
	// begin inline asm
	ex2.approx.f32 %f1, %f2;
	// end inline asm
	bar.sync 	0;
	mov.b32 	%r54, %f1;
	shfl.sync.bfly.b32	%r83, %r54, 16, 31, -1;
	mov.b32 	%f24, %r83;
	add.rn.f32 	%f25, %f1, %f24;
	mov.b32 	%r84, %f25;
	shfl.sync.bfly.b32	%r85, %r84, 8, 31, -1;
	mov.b32 	%f26, %r85;
	add.rn.f32 	%f27, %f25, %f26;
	mov.b32 	%r86, %f27;
	shfl.sync.bfly.b32	%r87, %r86, 4, 31, -1;
	mov.b32 	%f28, %r87;
	add.rn.f32 	%f29, %f27, %f28;
	mov.b32 	%r88, %f29;
	shfl.sync.bfly.b32	%r89, %r88, 2, 31, -1;
	mov.b32 	%f30, %r89;
	add.rn.f32 	%f31, %f29, %f30;
	mov.b32 	%r90, %f31;
	shfl.sync.bfly.b32	%r91, %r90, 1, 31, -1;
	mov.b32 	%f32, %r91;
	add.rn.f32 	%f33, %f31, %f32;
	mov.b32 	%r48, %f33;
	// begin inline asm
	@%p5 st.shared.b32 [ %r41 + 0 ], %r48;
	// end inline asm
	bar.sync 	0;
	// begin inline asm
	@%p6 ld.shared.b32 %r49, [ %r44 + 0 ];
	// end inline asm
	mov.b32 	%f34, %r49;
	shfl.sync.bfly.b32	%r92, %r49, 4, 31, -1;
	mov.b32 	%f35, %r92;
	add.rn.f32 	%f36, %f34, %f35;
	mov.b32 	%r93, %f36;
	shfl.sync.bfly.b32	%r94, %r93, 2, 31, -1;
	mov.b32 	%f37, %r94;
	add.rn.f32 	%f38, %f36, %f37;
	mov.b32 	%r95, %f38;
	shfl.sync.bfly.b32	%r96, %r95, 1, 31, -1;
	mov.b32 	%f39, %r96;
	add.rn.f32 	%f40, %f38, %f39;
	mov.b32 	%r52, %f40;
	// begin inline asm
	@%p7 st.shared.b32 [ %r44 + 0 ], %r52;
	// end inline asm
	bar.sync 	0;
	ld.shared.u32 	%r55, [global_smem+1024];
	// begin inline asm
	div.full.f32 %r56, %r54, %r55;
	// end inline asm
	mul.wide.s32 	%rd22, %r101, 4;
	add.s64 	%rd23, %rd2, %rd22;
	add.s64 	%rd10, %rd23, %rd15;
	// begin inline asm
	@%p11 st.global.b32 [ %rd10 + 0 ], { %r56 };
	// end inline asm
	add.s32 	%r97, %r104, 1;
	shr.s32 	%r98, %r97, 31;
	and.b32  	%r104, %r98, %r97;
	add.s32 	%r103, %r103, %r23;
	mul.wide.s32 	%rd24, %r102, 4;
	add.s64 	%rd25, %rd3, %rd24;
	add.s64 	%rd11, %rd25, %rd15;
	shl.b32 	%r99, %r104, 10;
	cvt.u64.u32 	%rd26, %r99;
	add.s64 	%rd27, %rd8, %rd26;
	add.s64 	%rd28, %rd27, %rd15;
	selp.b32 	%r100, 4, 0, %p11;
	selp.b32 	%r58, %r100, 0, %p13;
	cvt.u32.u64 	%r57, %rd28;
	// begin inline asm
	@%p1 cp.async.ca.shared.global [ %r57 + 0 ], [ %rd11 + 0 ], 0x4, %r58;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	add.s32 	%r102, %r102, %r7;
	add.s32 	%r101, %r101, %r9;
	bra.uni 	$L__BB0_1;
$L__BB0_3:
	// begin inline asm
	cp.async.wait_group 0x0;
	// end inline asm
	bar.sync 	0;
	ret;
                                        // -- End function
}
