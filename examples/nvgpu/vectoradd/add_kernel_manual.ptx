//
// Generated by LLVM NVPTX Back-End
//

.version 6.0
.target sm_30
.address_size 64

	// .globl	add_kernel              // -- Begin function add_kernel
.extern .shared .align 16 .b8 global_smem[];
                                        // @add_kernel
.visible .entry add_kernel(
	.param .u64 .ptr .global .align 1 add_kernel_param_0,
	.param .u64 .ptr .global .align 1 add_kernel_param_1,
	.param .u64 .ptr .global .align 1 add_kernel_param_2,
	.param .u32 add_kernel_param_3,
	.param .u64 .ptr .global .align 1 add_kernel_param_4
)
.reqntid 128, 1, 1
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<52>;
	.reg .f32 	%f<25>;
	.reg .b64 	%rd<12>;

// %bb.0:
	ld.param.u64 	%rd7, [add_kernel_param_0];
	ld.param.u64 	%rd8, [add_kernel_param_1];
	// begin inline asm
	mov.u32 %r1, %ctaid.x;
	// end inline asm
	shl.b32 	%r26, %r1, 10;
	ld.param.u64 	%rd9, [add_kernel_param_2];
	ld.param.u32 	%r27, [add_kernel_param_3];
	mov.u32 	%r28, %tid.x;
	and.b32  	%r29, %r28, 31;
	and.b32  	%r30, %r28, 1;
	neg.s32 	%r31, %r30;
	and.b32  	%r32, %r31, 4;
	shl.b32 	%r33, %r29, 2;
	and.b32  	%r34, %r33, 8;
	or.b32  	%r35, %r32, %r34;
	and.b32  	%r36, %r33, 16;
	or.b32  	%r37, %r35, %r36;
	and.b32  	%r38, %r33, 32;
	or.b32  	%r39, %r37, %r38;
	and.b32  	%r40, %r33, 64;
	or.b32  	%r41, %r39, %r40;
	bfe.u32 	%r42, %r28, 5, 1;
	neg.s32 	%r43, %r42;
	and.b32  	%r44, %r43, 128;
	or.b32  	%r45, %r41, %r44;
	shl.b32 	%r46, %r28, 2;
	and.b32  	%r47, %r46, 256;
	or.b32  	%r48, %r45, %r47;
	xor.b32  	%r49, %r48, 512;
	add.s32 	%r50, %r26, %r48;
	add.s32 	%r51, %r26, %r49;
	setp.lt.s32 	%p1, %r50, %r27;
	setp.lt.s32 	%p2, %r51, %r27;
	mul.wide.s32 	%rd10, %r50, 4;
	add.s64 	%rd1, %rd7, %rd10;
	mul.wide.s32 	%rd11, %r51, 4;
	add.s64 	%rd2, %rd7, %rd11;
	// begin inline asm
	mov.u32 %r2, 0x0;
	mov.u32 %r3, 0x0;
	mov.u32 %r4, 0x0;
	mov.u32 %r5, 0x0;
	@%p1 ld.global.v4.b32 { %r2, %r3, %r4, %r5 }, [ %rd1 + 0 ];
	// end inline asm
	mov.b32 	%f1, %r2;
	mov.b32 	%f2, %r3;
	mov.b32 	%f3, %r4;
	mov.b32 	%f4, %r5;
	// begin inline asm
	mov.u32 %r6, 0x0;
	mov.u32 %r7, 0x0;
	mov.u32 %r8, 0x0;
	mov.u32 %r9, 0x0;
	@%p2 ld.global.v4.b32 { %r6, %r7, %r8, %r9 }, [ %rd2 + 0 ];
	// end inline asm
	mov.b32 	%f5, %r6;
	mov.b32 	%f6, %r7;
	mov.b32 	%f7, %r8;
	mov.b32 	%f8, %r9;
	add.s64 	%rd3, %rd8, %rd10;
	add.s64 	%rd4, %rd8, %rd11;
	// begin inline asm
	mov.u32 %r10, 0x0;
	mov.u32 %r11, 0x0;
	mov.u32 %r12, 0x0;
	mov.u32 %r13, 0x0;
	@%p1 ld.global.v4.b32 { %r10, %r11, %r12, %r13 }, [ %rd3 + 0 ];
	// end inline asm
	mov.b32 	%f9, %r10;
	mov.b32 	%f10, %r11;
	mov.b32 	%f11, %r12;
	mov.b32 	%f12, %r13;
	// begin inline asm
	mov.u32 %r14, 0x0;
	mov.u32 %r15, 0x0;
	mov.u32 %r16, 0x0;
	mov.u32 %r17, 0x0;
	@%p2 ld.global.v4.b32 { %r14, %r15, %r16, %r17 }, [ %rd4 + 0 ];
	// end inline asm
	mov.b32 	%f13, %r14;
	mov.b32 	%f14, %r15;
	mov.b32 	%f15, %r16;
	mov.b32 	%f16, %r17;
	add.rn.f32 	%f17, %f1, %f9;
	add.rn.f32 	%f18, %f2, %f10;
	add.rn.f32 	%f19, %f3, %f11;
	add.rn.f32 	%f20, %f4, %f12;
	add.rn.f32 	%f21, %f5, %f13;
	add.rn.f32 	%f22, %f6, %f14;
	add.rn.f32 	%f23, %f7, %f15;
	add.rn.f32 	%f24, %f8, %f16;
	add.s64 	%rd5, %rd9, %rd10;
	add.s64 	%rd6, %rd9, %rd11;
	mov.b32 	%r18, %f17;
	mov.b32 	%r19, %f18;
	mov.b32 	%r20, %f19;
	mov.b32 	%r21, %f20;
	// begin inline asm
	@%p1 st.global.v4.b32 [ %rd5 + 0 ], { %r18, %r19, %r20, %r21 };
	// end inline asm
	mov.b32 	%r22, %f21;
	mov.b32 	%r23, %f22;
	mov.b32 	%r24, %f23;
	mov.b32 	%r25, %f24;
	// begin inline asm
	@%p2 st.global.v4.b32 [ %rd6 + 0 ], { %r22, %r23, %r24, %r25 };
	// end inline asm
	ret;
                                        // -- End function
}
